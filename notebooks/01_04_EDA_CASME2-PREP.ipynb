{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOTRYtwU8Cr0Xwpl5RAsZgV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# @title Cell 1: CASME2 Grayscale + Center Crop Preprocessing Pipeline\n","# File: preprocess_casme2_grayscale_centercrop.py\n","# Location: Thesis_MER_Project/scripts/preprocessing/\n","# Purpose: Transform v1/v2/v3 datasets to grayscale 224x224 center-cropped versions (v4/v5/v6)\n","\n","import os\n","import json\n","import cv2\n","import numpy as np\n","from pathlib import Path\n","from datetime import datetime\n","from collections import defaultdict\n","from google.colab import drive\n","\n","# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define base paths\n","BASE_PATH = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","PROCESSED_PATH = f\"{BASE_PATH}/datasets/processed_casme2\"\n","\n","# Dataset mapping: source -> target\n","DATASET_MAPPING = {\n","    'v1_to_v4': {\n","        'source': f\"{PROCESSED_PATH}/data_split_v1\",\n","        'target': f\"{BASE_PATH}/datasets/preprocessed_casme2_v4\",\n","        'description': 'AF - Apex Frame (grayscale 224x224)',\n","        'variant': 'AF'\n","    },\n","    'v2_to_v5': {\n","        'source': f\"{PROCESSED_PATH}/data_split_v2\",\n","        'target': f\"{BASE_PATH}/datasets/preprocessed_casme2_v5\",\n","        'description': 'KFS - Key Frame Sequence (grayscale 224x224)',\n","        'variant': 'KFS'\n","    },\n","    'v3_to_v6': {\n","        'source': f\"{PROCESSED_PATH}/data_split_v3\",\n","        'target': f\"{BASE_PATH}/datasets/preprocessed_casme2_v6\",\n","        'description': 'MFS - Multi-Frame Sequence (grayscale 224x224)',\n","        'variant': 'MFS'\n","    }\n","}\n","\n","# Processing parameters\n","INTERMEDIATE_SIZE = 256\n","FINAL_SIZE = 224\n","CROP_MARGIN = (INTERMEDIATE_SIZE - FINAL_SIZE) // 2\n","\n","# ============================================================================\n","# PREPROCESSING FUNCTIONS\n","# ============================================================================\n","\n","def smart_resize_preserve_aspect(image, target_size=256):\n","    \"\"\"\n","    Resize image from 640x480 to square format preserving facial proportions.\n","\n","    Strategy:\n","    1. Crop center 480x480 from 640x480 (remove 80px from left and right)\n","    2. Resize to target_size x target_size\n","\n","    Args:\n","        image: Input image (H, W, C) or (H, W)\n","        target_size: Output square dimension\n","\n","    Returns:\n","        Resized square image\n","    \"\"\"\n","    h, w = image.shape[:2]\n","\n","    # Calculate crop coordinates to get square center region\n","    if w > h:\n","        # Landscape: crop width\n","        crop_size = h\n","        x_start = (w - crop_size) // 2\n","        y_start = 0\n","    else:\n","        # Portrait or square: crop height\n","        crop_size = w\n","        x_start = 0\n","        y_start = (h - crop_size) // 2\n","\n","    # Crop to square\n","    cropped = image[y_start:y_start+crop_size, x_start:x_start+crop_size]\n","\n","    # Resize to target size\n","    resized = cv2.resize(cropped, (target_size, target_size), interpolation=cv2.INTER_LANCZOS4)\n","\n","    return resized\n","\n","def convert_to_grayscale(image):\n","    \"\"\"\n","    Convert RGB/BGR image to grayscale.\n","\n","    Args:\n","        image: Input RGB/BGR image\n","\n","    Returns:\n","        Grayscale image\n","    \"\"\"\n","    if len(image.shape) == 3:\n","        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    return image\n","\n","def center_crop(image, crop_size=224):\n","    \"\"\"\n","    Extract center crop from image.\n","\n","    Args:\n","        image: Input image (H, W) or (H, W, C)\n","        crop_size: Size of center crop\n","\n","    Returns:\n","        Center-cropped image\n","    \"\"\"\n","    h, w = image.shape[:2]\n","    start_y = (h - crop_size) // 2\n","    start_x = (w - crop_size) // 2\n","\n","    return image[start_y:start_y+crop_size, start_x:start_x+crop_size]\n","\n","def preprocess_image(image_path):\n","    \"\"\"\n","    Complete preprocessing pipeline for a single image.\n","\n","    Pipeline:\n","    1. Load image (384x384 RGB distorted from 640x480)\n","    2. Smart resize to 256x256 (preserving aspect ratio via center crop)\n","    3. Convert to grayscale\n","    4. Center crop to 224x224\n","\n","    Args:\n","        image_path: Path to input image\n","\n","    Returns:\n","        Preprocessed grayscale 224x224 image, or None if error\n","    \"\"\"\n","    try:\n","        # Load image\n","        image = cv2.imread(image_path)\n","\n","        if image is None:\n","            return None\n","\n","        # Step 1: Smart resize to 256x256 (preserve aspect ratio)\n","        resized = smart_resize_preserve_aspect(image, INTERMEDIATE_SIZE)\n","\n","        # Step 2: Convert to grayscale\n","        grayscale = convert_to_grayscale(resized)\n","\n","        # Step 3: Center crop to 224x224\n","        cropped = center_crop(grayscale, FINAL_SIZE)\n","\n","        return cropped\n","\n","    except Exception as e:\n","        print(f\"Error processing {image_path}: {str(e)}\")\n","        return None\n","\n","# ============================================================================\n","# DATASET PROCESSING\n","# ============================================================================\n","\n","def process_dataset(source_dir, target_dir, variant_name):\n","    \"\"\"\n","    Process entire dataset: transform all images in train/val/test splits.\n","\n","    Args:\n","        source_dir: Source dataset directory (v1/v2/v3)\n","        target_dir: Target directory (v4/v5/v6)\n","        variant_name: Dataset variant (AF/KFS/MFS)\n","\n","    Returns:\n","        Processing statistics dictionary\n","    \"\"\"\n","    stats = {\n","        'variant': variant_name,\n","        'processing_date': datetime.now().isoformat(),\n","        'source_directory': source_dir,\n","        'target_directory': target_dir,\n","        'preprocessing_steps': [\n","            'Smart resize 640x480 -> 256x256 (aspect preserved)',\n","            'Grayscale conversion (RGB -> Gray)',\n","            'Center crop 256x256 -> 224x224'\n","        ],\n","        'splits': {},\n","        'total_processed': 0,\n","        'total_errors': 0\n","    }\n","\n","    splits = ['train', 'val', 'test']\n","\n","    for split in splits:\n","        source_split_dir = os.path.join(source_dir, split)\n","        target_split_dir = os.path.join(target_dir, split)\n","\n","        if not os.path.exists(source_split_dir):\n","            print(f\"  Warning: {split} split not found in source directory\")\n","            continue\n","\n","        # Create target directory\n","        os.makedirs(target_split_dir, exist_ok=True)\n","\n","        # Get all image files\n","        image_files = [f for f in os.listdir(source_split_dir)\n","                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n","\n","        split_stats = {\n","            'total_images': len(image_files),\n","            'processed': 0,\n","            'errors': 0,\n","            'emotion_distribution': defaultdict(int)\n","        }\n","\n","        print(f\"  Processing {split} split: {len(image_files)} images\")\n","\n","        # Process each image\n","        for idx, img_filename in enumerate(image_files, 1):\n","            source_path = os.path.join(source_split_dir, img_filename)\n","            target_path = os.path.join(target_split_dir, img_filename)\n","\n","            # Preprocess image\n","            processed_img = preprocess_image(source_path)\n","\n","            if processed_img is not None:\n","                # Save preprocessed image\n","                cv2.imwrite(target_path, processed_img)\n","                split_stats['processed'] += 1\n","\n","                # Extract emotion from filename for statistics\n","                emotion = img_filename.split('_')[-1].replace('.jpg', '')\n","                split_stats['emotion_distribution'][emotion] += 1\n","            else:\n","                split_stats['errors'] += 1\n","                stats['total_errors'] += 1\n","\n","            # Progress indicator (every 100 images)\n","            if idx % 100 == 0:\n","                print(f\"    Progress: {idx}/{len(image_files)}\")\n","\n","        stats['splits'][split] = dict(split_stats)\n","        stats['splits'][split]['emotion_distribution'] = dict(split_stats['emotion_distribution'])\n","        stats['total_processed'] += split_stats['processed']\n","\n","        print(f\"    Completed: {split_stats['processed']} processed, {split_stats['errors']} errors\")\n","\n","    return stats\n","\n","# ============================================================================\n","# MAIN EXECUTION\n","# ============================================================================\n","\n","print(\"=\" * 80)\n","print(\"CASME2 PREPROCESSING PIPELINE: GRAYSCALE + CENTER CROP (224x224)\")\n","print(\"=\" * 80)\n","print()\n","\n","all_results = {}\n","\n","for mapping_name, config in DATASET_MAPPING.items():\n","    source_dir = config['source']\n","    target_dir = config['target']\n","    variant = config['variant']\n","    description = config['description']\n","\n","    print(f\"[{variant}] {description}\")\n","    print(f\"Source: {source_dir}\")\n","    print(f\"Target: {target_dir}\")\n","    print()\n","\n","    # Check if source exists\n","    if not os.path.exists(source_dir):\n","        print(f\"  Error: Source directory not found\")\n","        print()\n","        continue\n","\n","    # Create target base directory\n","    os.makedirs(target_dir, exist_ok=True)\n","\n","    # Process dataset\n","    processing_stats = process_dataset(source_dir, target_dir, variant)\n","\n","    # Save processing summary\n","    summary_path = os.path.join(target_dir, 'preprocessing_summary.json')\n","    with open(summary_path, 'w') as f:\n","        json.dump(processing_stats, f, indent=2)\n","\n","    all_results[variant] = processing_stats\n","\n","    print(f\"  Summary saved to: preprocessing_summary.json\")\n","    print(f\"  Total processed: {processing_stats['total_processed']}\")\n","    print(f\"  Total errors: {processing_stats['total_errors']}\")\n","    print()\n","    print(\"-\" * 80)\n","    print()\n","\n","# ============================================================================\n","# FINAL SUMMARY\n","# ============================================================================\n","\n","print(\"=\" * 80)\n","print(\"PREPROCESSING COMPLETE - SUMMARY\")\n","print(\"=\" * 80)\n","print()\n","\n","for variant, stats in all_results.items():\n","    print(f\"{variant} Dataset:\")\n","    print(f\"  Total processed: {stats['total_processed']} images\")\n","    print(f\"  Errors: {stats['total_errors']}\")\n","\n","    for split in ['train', 'val', 'test']:\n","        if split in stats['splits']:\n","            split_data = stats['splits'][split]\n","            print(f\"  {split.upper()}: {split_data['processed']} images\")\n","\n","    print()\n","\n","print(\"All preprocessed datasets saved to:\")\n","for config in DATASET_MAPPING.values():\n","    print(f\"  - {config['target']}\")\n","\n","print()\n","print(\"=\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"cellView":"form","id":"oyhwkkT9P9Zk","executionInfo":{"status":"ok","timestamp":1760857218932,"user_tz":-420,"elapsed":288490,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"94022054-9c35-48c3-8174-ae9ccb32e139"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","================================================================================\n","CASME2 PREPROCESSING PIPELINE: GRAYSCALE + CENTER CROP (224x224)\n","================================================================================\n","\n","[AF] AF - Apex Frame (grayscale 224x224)\n","Source: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v1\n","Target: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/preprocessed_casme2_v4\n","\n","  Processing train split: 201 images\n","    Progress: 100/201\n","    Progress: 200/201\n","    Completed: 201 processed, 0 errors\n","  Processing val split: 26 images\n","    Completed: 26 processed, 0 errors\n","  Processing test split: 28 images\n","    Completed: 28 processed, 0 errors\n","  Summary saved to: preprocessing_summary.json\n","  Total processed: 255\n","  Total errors: 0\n","\n","--------------------------------------------------------------------------------\n","\n","[KFS] KFS - Key Frame Sequence (grayscale 224x224)\n","Source: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v2\n","Target: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/preprocessed_casme2_v5\n","\n","  Processing train split: 603 images\n","    Progress: 100/603\n","    Progress: 200/603\n","    Progress: 300/603\n","    Progress: 400/603\n","    Progress: 500/603\n","    Progress: 600/603\n","    Completed: 603 processed, 0 errors\n","  Processing val split: 78 images\n","    Completed: 78 processed, 0 errors\n","  Processing test split: 84 images\n","    Completed: 84 processed, 0 errors\n","  Summary saved to: preprocessing_summary.json\n","  Total processed: 765\n","  Total errors: 0\n","\n","--------------------------------------------------------------------------------\n","\n","[MFS] MFS - Multi-Frame Sequence (grayscale 224x224)\n","Source: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v3\n","Target: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/preprocessed_casme2_v6\n","\n","  Processing train split: 2613 images\n","    Progress: 100/2613\n","    Progress: 200/2613\n","    Progress: 300/2613\n","    Progress: 400/2613\n","    Progress: 500/2613\n","    Progress: 600/2613\n","    Progress: 700/2613\n","    Progress: 800/2613\n","    Progress: 900/2613\n","    Progress: 1000/2613\n","    Progress: 1100/2613\n","    Progress: 1200/2613\n","    Progress: 1300/2613\n","    Progress: 1400/2613\n","    Progress: 1500/2613\n","    Progress: 1600/2613\n","    Progress: 1700/2613\n","    Progress: 1800/2613\n","    Progress: 1900/2613\n","    Progress: 2000/2613\n","    Progress: 2100/2613\n","    Progress: 2200/2613\n","    Progress: 2300/2613\n","    Progress: 2400/2613\n","    Progress: 2500/2613\n","    Progress: 2600/2613\n","    Completed: 2613 processed, 0 errors\n","  Processing val split: 78 images\n","    Completed: 78 processed, 0 errors\n","  Processing test split: 83 images\n","    Completed: 83 processed, 0 errors\n","  Summary saved to: preprocessing_summary.json\n","  Total processed: 2774\n","  Total errors: 0\n","\n","--------------------------------------------------------------------------------\n","\n","================================================================================\n","PREPROCESSING COMPLETE - SUMMARY\n","================================================================================\n","\n","AF Dataset:\n","  Total processed: 255 images\n","  Errors: 0\n","  TRAIN: 201 images\n","  VAL: 26 images\n","  TEST: 28 images\n","\n","KFS Dataset:\n","  Total processed: 765 images\n","  Errors: 0\n","  TRAIN: 603 images\n","  VAL: 78 images\n","  TEST: 84 images\n","\n","MFS Dataset:\n","  Total processed: 2774 images\n","  Errors: 0\n","  TRAIN: 2613 images\n","  VAL: 78 images\n","  TEST: 83 images\n","\n","All preprocessed datasets saved to:\n","  - /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/preprocessed_casme2_v4\n","  - /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/preprocessed_casme2_v5\n","  - /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/preprocessed_casme2_v6\n","\n","================================================================================\n"]}]},{"cell_type":"code","source":["# @title Cell 2: CASME2 Preprocessing Validation + Face Detection Analysis\n","# File: validate_preprocessed_casme2.py\n","# Location: Thesis_MER_Project/scripts/preprocessing/\n","# Purpose: Validate v4/v5/v6 preprocessing quality and face detection analysis\n","\n","import os\n","import json\n","import cv2\n","import dlib\n","import numpy as np\n","from pathlib import Path\n","from datetime import datetime\n","from collections import defaultdict\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","\n","# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define base paths\n","BASE_PATH = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","PROCESSED_PATH = f\"{BASE_PATH}/datasets/processed_casme2\"\n","\n","# Preprocessed dataset paths\n","PREPROCESSED_DATASETS = {\n","    'v4': {\n","        'path': f\"{PROCESSED_PATH}/preprocessed_v4\",\n","        'source': f\"{PROCESSED_PATH}/data_split_v1\",\n","        'variant': 'AF',\n","        'description': 'Apex Frame'\n","    },\n","    'v5': {\n","        'path': f\"{PROCESSED_PATH}/preprocessed_v5\",\n","        'source': f\"{PROCESSED_PATH}/data_split_v2\",\n","        'variant': 'KFS',\n","        'description': 'Key Frame Sequence'\n","    },\n","    'v6': {\n","        'path': f\"{PROCESSED_PATH}/preprocessed_v6\",\n","        'source': f\"{PROCESSED_PATH}/data_split_v3\",\n","        'variant': 'MFS',\n","        'description': 'Multi-Frame Sequence'\n","    }\n","}\n","\n","# Validation parameters\n","EXPECTED_SIZE = (224, 224)\n","EXPECTED_CHANNELS = 1  # Grayscale\n","CENTRALITY_THRESHOLD = 0.20  # Face off-center by >20% flagged\n","MIN_FACE_DETECTION_RATE = 0.90  # 90% minimum acceptable\n","\n","# Initialize Dlib face detector\n","print(\"Initializing Dlib face detector...\")\n","detector = dlib.get_frontal_face_detector()\n","print(\"Face detector loaded\")\n","print()\n","\n","# ============================================================================\n","# VALIDATION FUNCTIONS\n","# ============================================================================\n","\n","def verify_image_counts(source_dir, target_dir):\n","    \"\"\"\n","    Verify that preprocessed dataset has same image count as source.\n","\n","    Args:\n","        source_dir: Original dataset directory\n","        target_dir: Preprocessed dataset directory\n","\n","    Returns:\n","        Dictionary with count comparison per split\n","    \"\"\"\n","    counts = {}\n","\n","    for split in ['train', 'val', 'test']:\n","        source_split = os.path.join(source_dir, split)\n","        target_split = os.path.join(target_dir, split)\n","\n","        source_count = 0\n","        target_count = 0\n","\n","        if os.path.exists(source_split):\n","            source_count = len([f for f in os.listdir(source_split)\n","                              if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n","\n","        if os.path.exists(target_split):\n","            target_count = len([f for f in os.listdir(target_split)\n","                              if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n","\n","        counts[split] = {\n","            'source': source_count,\n","            'target': target_count,\n","            'match': source_count == target_count\n","        }\n","\n","    return counts\n","\n","def check_image_dimensions(image_path):\n","    \"\"\"\n","    Verify image has correct dimensions and channels.\n","\n","    Args:\n","        image_path: Path to image file\n","\n","    Returns:\n","        Dictionary with dimension check results\n","    \"\"\"\n","    try:\n","        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","        if img is None:\n","            return {'valid': False, 'error': 'Failed to load'}\n","\n","        h, w = img.shape\n","        channels = 1 if len(img.shape) == 2 else img.shape[2]\n","\n","        correct_size = (h, w) == EXPECTED_SIZE\n","        correct_channels = channels == EXPECTED_CHANNELS\n","\n","        return {\n","            'valid': correct_size and correct_channels,\n","            'dimensions': (h, w),\n","            'channels': channels,\n","            'correct_size': correct_size,\n","            'correct_channels': correct_channels\n","        }\n","\n","    except Exception as e:\n","        return {'valid': False, 'error': str(e)}\n","\n","def detect_face_centrality(image_path):\n","    \"\"\"\n","    Detect face in image and calculate centrality score.\n","\n","    Centrality score: distance from face center to image center,\n","    normalized by image dimension.\n","\n","    Args:\n","        image_path: Path to image file\n","\n","    Returns:\n","        Dictionary with face detection results and centrality score\n","    \"\"\"\n","    try:\n","        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","        if img is None:\n","            return {'detected': False, 'error': 'Failed to load image'}\n","\n","        # Detect faces\n","        faces = detector(img, 1)\n","\n","        if len(faces) == 0:\n","            return {'detected': False, 'reason': 'No face detected'}\n","\n","        if len(faces) > 1:\n","            # Multiple faces: use largest\n","            faces = sorted(faces, key=lambda r: r.width() * r.height(), reverse=True)\n","\n","        face = faces[0]\n","\n","        # Calculate face center\n","        face_center_x = (face.left() + face.right()) / 2\n","        face_center_y = (face.top() + face.bottom()) / 2\n","\n","        # Image center\n","        img_center_x = img.shape[1] / 2\n","        img_center_y = img.shape[0] / 2\n","\n","        # Calculate offset from center (normalized)\n","        offset_x = abs(face_center_x - img_center_x) / img.shape[1]\n","        offset_y = abs(face_center_y - img_center_y) / img.shape[0]\n","\n","        # Centrality score: Euclidean distance from center\n","        centrality_score = np.sqrt(offset_x**2 + offset_y**2)\n","\n","        # Face bounding box size relative to image\n","        face_width_ratio = face.width() / img.shape[1]\n","        face_height_ratio = face.height() / img.shape[0]\n","\n","        return {\n","            'detected': True,\n","            'centrality_score': centrality_score,\n","            'offset_x': offset_x,\n","            'offset_y': offset_y,\n","            'face_width_ratio': face_width_ratio,\n","            'face_height_ratio': face_height_ratio,\n","            'well_centered': centrality_score <= CENTRALITY_THRESHOLD,\n","            'bbox': {\n","                'left': face.left(),\n","                'top': face.top(),\n","                'right': face.right(),\n","                'bottom': face.bottom()\n","            }\n","        }\n","\n","    except Exception as e:\n","        return {'detected': False, 'error': str(e)}\n","\n","def analyze_image_quality(image_path):\n","    \"\"\"\n","    Analyze basic image quality metrics.\n","\n","    Args:\n","        image_path: Path to image file\n","\n","    Returns:\n","        Dictionary with quality metrics\n","    \"\"\"\n","    try:\n","        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","        if img is None:\n","            return {'valid': False}\n","\n","        # Calculate metrics\n","        mean_intensity = np.mean(img)\n","        std_intensity = np.std(img)\n","        min_intensity = np.min(img)\n","        max_intensity = np.max(img)\n","\n","        # Contrast metric (normalized standard deviation)\n","        contrast = std_intensity / mean_intensity if mean_intensity > 0 else 0\n","\n","        return {\n","            'valid': True,\n","            'mean_intensity': float(mean_intensity),\n","            'std_intensity': float(std_intensity),\n","            'min_intensity': int(min_intensity),\n","            'max_intensity': int(max_intensity),\n","            'contrast': float(contrast)\n","        }\n","\n","    except Exception as e:\n","        return {'valid': False, 'error': str(e)}\n","\n","# ============================================================================\n","# DATASET VALIDATION\n","# ============================================================================\n","\n","def validate_dataset(dataset_path, source_path, variant_name):\n","    \"\"\"\n","    Comprehensive validation of preprocessed dataset.\n","\n","    Args:\n","        dataset_path: Path to preprocessed dataset\n","        source_path: Path to source dataset\n","        variant_name: Dataset variant (AF/KFS/MFS)\n","\n","    Returns:\n","        Validation report dictionary\n","    \"\"\"\n","    report = {\n","        'variant': variant_name,\n","        'validation_date': datetime.now().isoformat(),\n","        'dataset_path': dataset_path,\n","        'source_path': source_path,\n","        'splits': {},\n","        'overall_statistics': {},\n","        'issues_found': [],\n","        'recommendation': ''\n","    }\n","\n","    # Step 1: Verify image counts\n","    print(f\"  Step 1: Verifying image counts...\")\n","    count_verification = verify_image_counts(source_path, dataset_path)\n","\n","    for split, counts in count_verification.items():\n","        if not counts['match']:\n","            report['issues_found'].append(\n","                f\"{split}: Count mismatch (source: {counts['source']}, target: {counts['target']})\"\n","            )\n","\n","    # Step 2: Validate each split\n","    splits = ['train', 'val', 'test']\n","\n","    overall_stats = {\n","        'total_images': 0,\n","        'dimension_correct': 0,\n","        'faces_detected': 0,\n","        'faces_well_centered': 0,\n","        'faces_off_center': 0,\n","        'face_detection_failed': 0,\n","        'centrality_scores': []\n","    }\n","\n","    for split in splits:\n","        split_path = os.path.join(dataset_path, split)\n","\n","        if not os.path.exists(split_path):\n","            continue\n","\n","        print(f\"  Step 2: Analyzing {split} split...\")\n","\n","        image_files = [f for f in os.listdir(split_path)\n","                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n","\n","        split_stats = {\n","            'total_images': len(image_files),\n","            'source_count': count_verification[split]['source'],\n","            'count_match': count_verification[split]['match'],\n","            'dimension_correct': 0,\n","            'dimension_errors': 0,\n","            'faces_detected': 0,\n","            'faces_well_centered': 0,\n","            'faces_off_center': 0,\n","            'face_detection_failed': 0,\n","            'centrality_scores': [],\n","            'quality_metrics': {\n","                'mean_intensities': [],\n","                'contrasts': []\n","            },\n","            'problematic_images': []\n","        }\n","\n","        # Sample validation (every 10th image for speed)\n","        sample_indices = range(0, len(image_files), max(1, len(image_files) // 50))\n","\n","        for idx in sample_indices:\n","            img_file = image_files[idx]\n","            img_path = os.path.join(split_path, img_file)\n","\n","            # Check dimensions\n","            dim_check = check_image_dimensions(img_path)\n","            if dim_check['valid']:\n","                split_stats['dimension_correct'] += 1\n","            else:\n","                split_stats['dimension_errors'] += 1\n","\n","            # Face detection\n","            face_result = detect_face_centrality(img_path)\n","\n","            if face_result['detected']:\n","                split_stats['faces_detected'] += 1\n","                split_stats['centrality_scores'].append(face_result['centrality_score'])\n","\n","                if face_result['well_centered']:\n","                    split_stats['faces_well_centered'] += 1\n","                else:\n","                    split_stats['faces_off_center'] += 1\n","                    split_stats['problematic_images'].append({\n","                        'filename': img_file,\n","                        'centrality_score': face_result['centrality_score'],\n","                        'offset_x': face_result['offset_x'],\n","                        'offset_y': face_result['offset_y']\n","                    })\n","            else:\n","                split_stats['face_detection_failed'] += 1\n","                split_stats['problematic_images'].append({\n","                    'filename': img_file,\n","                    'issue': 'Face detection failed'\n","                })\n","\n","            # Quality metrics\n","            quality = analyze_image_quality(img_path)\n","            if quality['valid']:\n","                split_stats['quality_metrics']['mean_intensities'].append(\n","                    quality['mean_intensity']\n","                )\n","                split_stats['quality_metrics']['contrasts'].append(\n","                    quality['contrast']\n","                )\n","\n","        # Calculate rates\n","        sample_count = len(sample_indices)\n","        split_stats['face_detection_rate'] = (\n","            split_stats['faces_detected'] / sample_count if sample_count > 0 else 0\n","        )\n","        split_stats['well_centered_rate'] = (\n","            split_stats['faces_well_centered'] / split_stats['faces_detected']\n","            if split_stats['faces_detected'] > 0 else 0\n","        )\n","\n","        # Aggregate quality metrics\n","        if split_stats['quality_metrics']['mean_intensities']:\n","            split_stats['quality_metrics']['avg_mean_intensity'] = float(\n","                np.mean(split_stats['quality_metrics']['mean_intensities'])\n","            )\n","            split_stats['quality_metrics']['avg_contrast'] = float(\n","                np.mean(split_stats['quality_metrics']['contrasts'])\n","            )\n","\n","        # Update overall statistics\n","        overall_stats['total_images'] += split_stats['total_images']\n","        overall_stats['dimension_correct'] += split_stats['dimension_correct']\n","        overall_stats['faces_detected'] += split_stats['faces_detected']\n","        overall_stats['faces_well_centered'] += split_stats['faces_well_centered']\n","        overall_stats['faces_off_center'] += split_stats['faces_off_center']\n","        overall_stats['face_detection_failed'] += split_stats['face_detection_failed']\n","        overall_stats['centrality_scores'].extend(split_stats['centrality_scores'])\n","\n","        # Clean up before saving\n","        split_stats['centrality_scores'] = [float(x) for x in split_stats['centrality_scores']]\n","        split_stats['quality_metrics']['mean_intensities'] = []\n","        split_stats['quality_metrics']['contrasts'] = []\n","\n","        report['splits'][split] = split_stats\n","\n","        print(f\"    Images analyzed: {sample_count}\")\n","        print(f\"    Face detection rate: {split_stats['face_detection_rate']:.1%}\")\n","        print(f\"    Well-centered rate: {split_stats['well_centered_rate']:.1%}\")\n","\n","    # Calculate overall metrics\n","    total_sampled = sum(len(report['splits'][s]['centrality_scores'])\n","                       for s in report['splits'])\n","\n","    if total_sampled > 0:\n","        overall_stats['face_detection_rate'] = (\n","            overall_stats['faces_detected'] / total_sampled\n","        )\n","        overall_stats['well_centered_rate'] = (\n","            overall_stats['faces_well_centered'] / overall_stats['faces_detected']\n","            if overall_stats['faces_detected'] > 0 else 0\n","        )\n","        overall_stats['off_center_rate'] = (\n","            overall_stats['faces_off_center'] / overall_stats['faces_detected']\n","            if overall_stats['faces_detected'] > 0 else 0\n","        )\n","        overall_stats['avg_centrality_score'] = float(\n","            np.mean(overall_stats['centrality_scores'])\n","        )\n","\n","    overall_stats['centrality_scores'] = [float(x) for x in overall_stats['centrality_scores']]\n","\n","    report['overall_statistics'] = overall_stats\n","\n","    # Generate recommendation\n","    face_det_rate = overall_stats.get('face_detection_rate', 0)\n","    off_center_rate = overall_stats.get('off_center_rate', 0)\n","\n","    if face_det_rate < MIN_FACE_DETECTION_RATE:\n","        report['issues_found'].append(\n","            f\"Low face detection rate: {face_det_rate:.1%} (threshold: {MIN_FACE_DETECTION_RATE:.1%})\"\n","        )\n","        report['recommendation'] = \"NEEDS_IMPROVEMENT - Consider face-aware preprocessing\"\n","    elif off_center_rate > 0.10:\n","        report['issues_found'].append(\n","            f\"High off-center rate: {off_center_rate:.1%}\"\n","        )\n","        report['recommendation'] = \"NEEDS_IMPROVEMENT - Consider face-aware preprocessing\"\n","    else:\n","        report['recommendation'] = \"ACCEPTABLE - Proceed to training\"\n","\n","    return report\n","\n","# ============================================================================\n","# MAIN EXECUTION\n","# ============================================================================\n","\n","print(\"=\" * 80)\n","print(\"CASME2 PREPROCESSING VALIDATION + FACE DETECTION ANALYSIS\")\n","print(\"=\" * 80)\n","print()\n","\n","validation_results = {}\n","\n","for version, config in PREPROCESSED_DATASETS.items():\n","    dataset_path = config['path']\n","    source_path = config['source']\n","    variant = config['variant']\n","    description = config['description']\n","\n","    print(f\"[{version.upper()}] {variant} - {description}\")\n","    print(f\"Path: {dataset_path}\")\n","    print()\n","\n","    if not os.path.exists(dataset_path):\n","        print(f\"  Warning: Dataset not found, skipping validation\")\n","        print()\n","        continue\n","\n","    # Validate dataset\n","    validation_report = validate_dataset(dataset_path, source_path, variant)\n","    validation_results[version] = validation_report\n","\n","    # Save validation report\n","    report_path = os.path.join(dataset_path, 'validation_report.json')\n","    with open(report_path, 'w') as f:\n","        json.dump(validation_report, f, indent=2)\n","\n","    print(f\"  Validation report saved: validation_report.json\")\n","    print()\n","    print(\"-\" * 80)\n","    print()\n","\n","# ============================================================================\n","# FINAL SUMMARY\n","# ============================================================================\n","\n","print(\"=\" * 80)\n","print(\"VALIDATION SUMMARY\")\n","print(\"=\" * 80)\n","print()\n","\n","for version, report in validation_results.items():\n","    variant = report['variant']\n","    stats = report['overall_statistics']\n","\n","    print(f\"{version.upper()} ({variant}):\")\n","    print(f\"  Total images: {stats.get('total_images', 0)}\")\n","    print(f\"  Face detection rate: {stats.get('face_detection_rate', 0):.1%}\")\n","    print(f\"  Well-centered rate: {stats.get('well_centered_rate', 0):.1%}\")\n","    print(f\"  Off-center rate: {stats.get('off_center_rate', 0):.1%}\")\n","\n","    if report['issues_found']:\n","        print(f\"  Issues found: {len(report['issues_found'])}\")\n","        for issue in report['issues_found'][:3]:\n","            print(f\"    - {issue}\")\n","\n","    print(f\"  Recommendation: {report['recommendation']}\")\n","    print()\n","\n","# Overall recommendation\n","all_acceptable = all(\n","    r['recommendation'] == \"ACCEPTABLE - Proceed to training\"\n","    for r in validation_results.values()\n",")\n","\n","print(\"-\" * 80)\n","if all_acceptable:\n","    print(\"OVERALL: All datasets passed validation\")\n","    print(\"Next step: Proceed to model training\")\n","else:\n","    print(\"OVERALL: Some datasets need improvement\")\n","    print(\"Next step: Consider implementing face-aware preprocessing (Cell 3)\")\n","\n","print()\n","print(\"=\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"cellView":"form","id":"YeuJEsJhSGYY","executionInfo":{"status":"ok","timestamp":1760858677823,"user_tz":-420,"elapsed":247455,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"e56e1e33-b997-4777-bc57-b546dc16cca9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Initializing Dlib face detector...\n","Face detector loaded\n","\n","================================================================================\n","CASME2 PREPROCESSING VALIDATION + FACE DETECTION ANALYSIS\n","================================================================================\n","\n","[V4] AF - Apex Frame\n","Path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v4\n","\n","  Step 1: Verifying image counts...\n","  Step 2: Analyzing train split...\n","    Images analyzed: 51\n","    Face detection rate: 100.0%\n","    Well-centered rate: 86.3%\n","  Step 2: Analyzing val split...\n","    Images analyzed: 26\n","    Face detection rate: 100.0%\n","    Well-centered rate: 96.2%\n","  Step 2: Analyzing test split...\n","    Images analyzed: 28\n","    Face detection rate: 100.0%\n","    Well-centered rate: 82.1%\n","  Validation report saved: validation_report.json\n","\n","--------------------------------------------------------------------------------\n","\n","[V5] KFS - Key Frame Sequence\n","Path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v5\n","\n","  Step 1: Verifying image counts...\n","  Step 2: Analyzing train split...\n","    Images analyzed: 51\n","    Face detection rate: 100.0%\n","    Well-centered rate: 84.3%\n","  Step 2: Analyzing val split...\n","    Images analyzed: 78\n","    Face detection rate: 100.0%\n","    Well-centered rate: 96.2%\n","  Step 2: Analyzing test split...\n","    Images analyzed: 84\n","    Face detection rate: 100.0%\n","    Well-centered rate: 83.3%\n","  Validation report saved: validation_report.json\n","\n","--------------------------------------------------------------------------------\n","\n","[V6] MFS - Multi-Frame Sequence\n","Path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v6\n","\n","  Step 1: Verifying image counts...\n","  Step 2: Analyzing train split...\n","    Images analyzed: 51\n","    Face detection rate: 100.0%\n","    Well-centered rate: 84.3%\n","  Step 2: Analyzing val split...\n","    Images analyzed: 78\n","    Face detection rate: 100.0%\n","    Well-centered rate: 96.2%\n","  Step 2: Analyzing test split...\n","    Images analyzed: 83\n","    Face detection rate: 100.0%\n","    Well-centered rate: 84.3%\n","  Validation report saved: validation_report.json\n","\n","--------------------------------------------------------------------------------\n","\n","================================================================================\n","VALIDATION SUMMARY\n","================================================================================\n","\n","V4 (AF):\n","  Total images: 255\n","  Face detection rate: 100.0%\n","  Well-centered rate: 87.6%\n","  Off-center rate: 12.4%\n","  Issues found: 1\n","    - High off-center rate: 12.4%\n","  Recommendation: NEEDS_IMPROVEMENT - Consider face-aware preprocessing\n","\n","V5 (KFS):\n","  Total images: 765\n","  Face detection rate: 100.0%\n","  Well-centered rate: 88.3%\n","  Off-center rate: 11.7%\n","  Issues found: 1\n","    - High off-center rate: 11.7%\n","  Recommendation: NEEDS_IMPROVEMENT - Consider face-aware preprocessing\n","\n","V6 (MFS):\n","  Total images: 2774\n","  Face detection rate: 100.0%\n","  Well-centered rate: 88.7%\n","  Off-center rate: 11.3%\n","  Issues found: 1\n","    - High off-center rate: 11.3%\n","  Recommendation: NEEDS_IMPROVEMENT - Consider face-aware preprocessing\n","\n","--------------------------------------------------------------------------------\n","OVERALL: Some datasets need improvement\n","Next step: Consider implementing face-aware preprocessing (Cell 3)\n","\n","================================================================================\n"]}]},{"cell_type":"code","source":["# @title Cell 3: CASME2 Face-Aware Preprocessing\n","# File: preprocess_casme2_faceaware_v3.py\n","# Location: Thesis_MER_Project/scripts/preprocessing/\n","# Purpose: Face-centered crop with forehead inclusion for complete micro-expression coverage\n","\n","import os\n","import json\n","import cv2\n","import dlib\n","import numpy as np\n","from pathlib import Path\n","from datetime import datetime\n","from collections import defaultdict\n","from google.colab import drive\n","\n","# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define base paths\n","BASE_PATH = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","PROCESSED_PATH = f\"{BASE_PATH}/datasets/processed_casme2\"\n","\n","# Dataset mapping: source -> target (face-aware with forehead versions)\n","DATASET_MAPPING = {\n","    'v1_to_v7': {\n","        'source': f\"{PROCESSED_PATH}/data_split_v1\",\n","        'target': f\"{PROCESSED_PATH}/preprocessed_v7\",\n","        'description': 'AF - Apex Frame (face-aware with forehead, grayscale 224x224)',\n","        'variant': 'AF'\n","    },\n","    'v2_to_v8': {\n","        'source': f\"{PROCESSED_PATH}/data_split_v2\",\n","        'target': f\"{PROCESSED_PATH}/preprocessed_v8\",\n","        'description': 'KFS - Key Frame Sequence (face-aware with forehead, grayscale 224x224)',\n","        'variant': 'KFS'\n","    },\n","    'v3_to_v9': {\n","        'source': f\"{PROCESSED_PATH}/data_split_v3\",\n","        'target': f\"{PROCESSED_PATH}/preprocessed_v9\",\n","        'description': 'MFS - Multi-Frame Sequence (face-aware with forehead, grayscale 224x224)',\n","        'variant': 'MFS'\n","    }\n","}\n","\n","# Processing parameters\n","TARGET_SIZE = 224\n","BBOX_EXPANSION = 20  # Expand face bbox by 20px in all directions for complete coverage\n","\n","# Initialize Dlib face detector\n","print(\"Initializing Dlib face detector...\")\n","detector = dlib.get_frontal_face_detector()\n","print(\"Face detector loaded\")\n","print()\n","\n","# ============================================================================\n","# FACE-AWARE PREPROCESSING FUNCTIONS\n","# ============================================================================\n","\n","def detect_face_with_expansion(image):\n","    \"\"\"\n","    Detect face and expand bbox in all directions for complete expression coverage.\n","\n","    Strategy:\n","    - Detect face bbox (typically eyebrows to chin)\n","    - Expand bbox by 20px in all directions (top, bottom, left, right)\n","    - Return expanded bbox for cropping\n","\n","    Args:\n","        image: Input image (grayscale or RGB)\n","\n","    Returns:\n","        Tuple (expanded_bbox, face_info) or (None, None)\n","    \"\"\"\n","    try:\n","        # Detect faces\n","        faces = detector(image, 1)\n","\n","        if len(faces) == 0:\n","            return None, None\n","\n","        # If multiple faces, use the largest one\n","        if len(faces) > 1:\n","            faces = sorted(faces, key=lambda r: r.width() * r.height(), reverse=True)\n","\n","        face = faces[0]\n","\n","        img_h, img_w = image.shape[:2]\n","\n","        # Original face bbox\n","        face_left = face.left()\n","        face_top = face.top()\n","        face_right = face.right()\n","        face_bottom = face.bottom()\n","        face_width = face.width()\n","        face_height = face.height()\n","\n","        # Expand bbox by BBOX_EXPANSION pixels in all directions\n","        expanded_left = max(0, face_left - BBOX_EXPANSION)\n","        expanded_top = max(0, face_top - BBOX_EXPANSION)\n","        expanded_right = min(img_w, face_right + BBOX_EXPANSION)\n","        expanded_bottom = min(img_h, face_bottom + BBOX_EXPANSION)\n","\n","        expanded_bbox = {\n","            'left': expanded_left,\n","            'top': expanded_top,\n","            'right': expanded_right,\n","            'bottom': expanded_bottom,\n","            'width': expanded_right - expanded_left,\n","            'height': expanded_bottom - expanded_top\n","        }\n","\n","        face_info = {\n","            'original_bbox': {\n","                'left': face_left,\n","                'top': face_top,\n","                'right': face_right,\n","                'bottom': face_bottom,\n","                'width': face_width,\n","                'height': face_height\n","            },\n","            'expanded_bbox': expanded_bbox,\n","            'expansion_applied': BBOX_EXPANSION,\n","            'face_area': face_width * face_height\n","        }\n","\n","        return expanded_bbox, face_info\n","\n","    except Exception as e:\n","        return None, None\n","\n","def ensure_minimum_size(image, min_size=224):\n","    \"\"\"\n","    Ensure image is at least min_size x min_size. Resize if needed.\n","\n","    Args:\n","        image: Input image\n","        min_size: Minimum dimension required\n","\n","    Returns:\n","        Tuple (resized_image, resize_info)\n","    \"\"\"\n","    h, w = image.shape[:2]\n","\n","    if h >= min_size and w >= min_size:\n","        return image, {'resized': False, 'original_size': (w, h)}\n","\n","    # Calculate scale factor\n","    scale_factor = min_size / min(h, w)\n","    new_width = int(w * scale_factor)\n","    new_height = int(h * scale_factor)\n","\n","    # Resize using high-quality interpolation\n","    resized = cv2.resize(image, (new_width, new_height),\n","                        interpolation=cv2.INTER_LANCZOS4)\n","\n","    resize_info = {\n","        'resized': True,\n","        'original_size': (w, h),\n","        'new_size': (new_width, new_height),\n","        'scale_factor': scale_factor\n","    }\n","\n","    return resized, resize_info\n","\n","def convert_to_grayscale(image):\n","    \"\"\"\n","    Convert RGB/BGR image to grayscale.\n","\n","    Args:\n","        image: Input RGB/BGR image\n","\n","    Returns:\n","        Grayscale image\n","    \"\"\"\n","    if len(image.shape) == 3:\n","        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    return image\n","\n","def preprocess_image_faceaware_v3(image_path):\n","    \"\"\"\n","    Face-aware preprocessing v3 with expanded bbox for complete expression coverage.\n","\n","    Pipeline:\n","    1. Load image\n","    2. Ensure minimum size (224x224)\n","    3. Detect face and expand bbox by 20px in all directions\n","    4. Crop expanded bbox region\n","    5. Resize to 224x224\n","    6. Convert to grayscale\n","\n","    Args:\n","        image_path: Path to input image\n","\n","    Returns:\n","        Tuple (preprocessed_image, processing_info)\n","    \"\"\"\n","    processing_info = {\n","        'face_detected': False,\n","        'bbox_expanded': False,\n","        'resize_applied': False,\n","        'method': 'unknown',\n","        'face_coverage': {}\n","    }\n","\n","    try:\n","        # Step 1: Load image\n","        image = cv2.imread(image_path)\n","\n","        if image is None:\n","            processing_info['method'] = 'error'\n","            processing_info['error'] = 'Failed to load image'\n","            return None, processing_info\n","\n","        # Step 2: Ensure minimum size\n","        image, resize_info = ensure_minimum_size(image, TARGET_SIZE)\n","        processing_info['resize_applied'] = resize_info['resized']\n","        if resize_info['resized']:\n","            processing_info['resize_info'] = resize_info\n","\n","        h, w = image.shape[:2]\n","\n","        # Step 3: Detect face and get expanded bbox\n","        expanded_bbox, face_info = detect_face_with_expansion(image)\n","\n","        if expanded_bbox is not None:\n","            processing_info['face_detected'] = True\n","            processing_info['bbox_expanded'] = True\n","            processing_info['face_coverage'] = {\n","                'expansion_applied': BBOX_EXPANSION,\n","                'original_face_area': face_info['face_area'],\n","                'expanded_width': expanded_bbox['width'],\n","                'expanded_height': expanded_bbox['height']\n","            }\n","\n","            # Step 4: Crop expanded bbox\n","            x1 = expanded_bbox['left']\n","            y1 = expanded_bbox['top']\n","            x2 = expanded_bbox['right']\n","            y2 = expanded_bbox['bottom']\n","\n","            cropped = image[y1:y2, x1:x2]\n","            processing_info['method'] = 'face_expanded_bbox'\n","\n","        else:\n","            # Fallback: center crop\n","            center_x = w // 2\n","            center_y = h // 2\n","            half_size = TARGET_SIZE // 2\n","\n","            x1 = max(0, center_x - half_size)\n","            y1 = max(0, center_y - half_size)\n","            x2 = min(w, center_x + half_size)\n","            y2 = min(h, center_y + half_size)\n","\n","            cropped = image[y1:y2, x1:x2]\n","            processing_info['method'] = 'fallback_center'\n","\n","        # Step 5: Resize to exact 224x224\n","        if cropped.shape[0] != TARGET_SIZE or cropped.shape[1] != TARGET_SIZE:\n","            cropped = cv2.resize(cropped, (TARGET_SIZE, TARGET_SIZE),\n","                               interpolation=cv2.INTER_LANCZOS4)\n","            processing_info['final_resize_applied'] = True\n","\n","        # Step 6: Convert to grayscale\n","        grayscale = convert_to_grayscale(cropped)\n","\n","        return grayscale, processing_info\n","\n","    except Exception as e:\n","        processing_info['method'] = 'error'\n","        processing_info['error'] = str(e)\n","        return None, processing_info\n","\n","# ============================================================================\n","# DATASET PROCESSING\n","# ============================================================================\n","\n","def process_dataset_faceaware_v3(source_dir, target_dir, variant_name):\n","    \"\"\"\n","    Process entire dataset with face-aware preprocessing v3 (forehead included).\n","\n","    Args:\n","        source_dir: Source dataset directory\n","        target_dir: Target directory\n","        variant_name: Dataset variant\n","\n","    Returns:\n","        Processing statistics dictionary\n","    \"\"\"\n","    stats = {\n","        'variant': variant_name,\n","        'processing_date': datetime.now().isoformat(),\n","        'source_directory': source_dir,\n","        'target_directory': target_dir,\n","        'preprocessing_method': 'face_bbox_expansion_all_directions',\n","        'preprocessing_parameters': {\n","            'target_size': TARGET_SIZE,\n","            'bbox_expansion': BBOX_EXPANSION\n","        },\n","        'preprocessing_steps': [\n","            'Load image',\n","            'Ensure minimum size 224x224',\n","            'Face detection with Dlib',\n","            f'Expand bbox by {BBOX_EXPANSION}px in all directions',\n","            'Crop expanded bbox region',\n","            'Resize to 224x224',\n","            'Grayscale conversion'\n","        ],\n","        'splits': {},\n","        'total_processed': 0,\n","        'total_errors': 0,\n","        'face_detection_stats': {\n","            'total_images': 0,\n","            'faces_detected': 0,\n","            'detection_rate': 0,\n","            'bbox_expanded': 0,\n","            'fallback_used': 0,\n","            'resize_applied': 0\n","        }\n","    }\n","\n","    splits = ['train', 'val', 'test']\n","\n","    for split in splits:\n","        source_split_dir = os.path.join(source_dir, split)\n","        target_split_dir = os.path.join(target_dir, split)\n","\n","        if not os.path.exists(source_split_dir):\n","            print(f\"  Warning: {split} split not found in source\")\n","            continue\n","\n","        # Create target directory\n","        os.makedirs(target_split_dir, exist_ok=True)\n","\n","        # Get all image files\n","        image_files = [f for f in os.listdir(source_split_dir)\n","                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n","\n","        split_stats = {\n","            'total_images': len(image_files),\n","            'processed': 0,\n","            'errors': 0,\n","            'faces_detected': 0,\n","            'bbox_expanded': 0,\n","            'fallback_used': 0,\n","            'resize_applied': 0,\n","            'emotion_distribution': defaultdict(int)\n","        }\n","\n","        print(f\"  Processing {split} split: {len(image_files)} images\")\n","\n","        # Process each image\n","        for idx, img_filename in enumerate(image_files, 1):\n","            source_path = os.path.join(source_split_dir, img_filename)\n","            target_path = os.path.join(target_split_dir, img_filename)\n","\n","            # Face-aware preprocessing v3\n","            processed_img, proc_info = preprocess_image_faceaware_v3(source_path)\n","\n","            if processed_img is not None:\n","                # Save preprocessed image\n","                cv2.imwrite(target_path, processed_img)\n","                split_stats['processed'] += 1\n","\n","                # Track statistics\n","                if proc_info['face_detected']:\n","                    split_stats['faces_detected'] += 1\n","\n","                if proc_info['bbox_expanded']:\n","                    split_stats['bbox_expanded'] += 1\n","\n","                if proc_info['method'] == 'fallback_center':\n","                    split_stats['fallback_used'] += 1\n","\n","                if proc_info['resize_applied']:\n","                    split_stats['resize_applied'] += 1\n","\n","                # Extract emotion from filename\n","                emotion = img_filename.split('_')[-1].replace('.jpg', '')\n","                split_stats['emotion_distribution'][emotion] += 1\n","            else:\n","                split_stats['errors'] += 1\n","                stats['total_errors'] += 1\n","\n","            # Progress indicator\n","            if idx % 200 == 0:\n","                print(f\"    Progress: {idx}/{len(image_files)}\")\n","\n","        # Calculate rates\n","        if split_stats['total_images'] > 0:\n","            split_stats['face_detection_rate'] = (\n","                split_stats['faces_detected'] / split_stats['total_images']\n","            )\n","            split_stats['bbox_expansion_rate'] = (\n","                split_stats['bbox_expanded'] / split_stats['total_images']\n","            )\n","\n","        # Store split results (clean for JSON)\n","        stats['splits'][split] = {\n","            'total_images': split_stats['total_images'],\n","            'processed': split_stats['processed'],\n","            'errors': split_stats['errors'],\n","            'faces_detected': split_stats['faces_detected'],\n","            'face_detection_rate': split_stats.get('face_detection_rate', 0),\n","            'bbox_expanded': split_stats['bbox_expanded'],\n","            'bbox_expansion_rate': split_stats.get('bbox_expansion_rate', 0),\n","            'fallback_used': split_stats['fallback_used'],\n","            'resize_applied': split_stats['resize_applied'],\n","            'emotion_distribution': dict(split_stats['emotion_distribution'])\n","        }\n","\n","        stats['total_processed'] += split_stats['processed']\n","\n","        # Update overall stats\n","        stats['face_detection_stats']['total_images'] += split_stats['total_images']\n","        stats['face_detection_stats']['faces_detected'] += split_stats['faces_detected']\n","        stats['face_detection_stats']['bbox_expanded'] += split_stats['bbox_expanded']\n","        stats['face_detection_stats']['fallback_used'] += split_stats['fallback_used']\n","        stats['face_detection_stats']['resize_applied'] += split_stats['resize_applied']\n","\n","        print(f\"    Completed: {split_stats['processed']} processed\")\n","        print(f\"    Face detection: {split_stats['faces_detected']}/{split_stats['total_images']} ({split_stats.get('face_detection_rate', 0):.1%})\")\n","        print(f\"    BBox expanded: {split_stats['bbox_expanded']} ({split_stats.get('bbox_expansion_rate', 0):.1%})\")\n","\n","    # Calculate overall rates\n","    total_imgs = stats['face_detection_stats']['total_images']\n","    if total_imgs > 0:\n","        stats['face_detection_stats']['detection_rate'] = (\n","            stats['face_detection_stats']['faces_detected'] / total_imgs\n","        )\n","        stats['face_detection_stats']['bbox_expansion_rate'] = (\n","            stats['face_detection_stats']['bbox_expanded'] / total_imgs\n","        )\n","\n","    return stats\n","\n","# ============================================================================\n","# MAIN EXECUTION\n","# ============================================================================\n","\n","print(\"=\" * 80)\n","print(\"CASME2 FACE-AWARE PREPROCESSING V3: BBOX EXPANSION (COMPLETE COVERAGE)\")\n","print(\"=\" * 80)\n","print()\n","\n","all_results = {}\n","\n","for mapping_name, config in DATASET_MAPPING.items():\n","    source_dir = config['source']\n","    target_dir = config['target']\n","    variant = config['variant']\n","    description = config['description']\n","\n","    print(f\"[{variant}] {description}\")\n","    print(f\"Source: {source_dir}\")\n","    print(f\"Target: {target_dir}\")\n","    print()\n","\n","    # Check if source exists\n","    if not os.path.exists(source_dir):\n","        print(f\"  Error: Source directory not found\")\n","        print()\n","        continue\n","\n","    # Create target base directory\n","    os.makedirs(target_dir, exist_ok=True)\n","\n","    # Process dataset\n","    processing_stats = process_dataset_faceaware_v3(source_dir, target_dir, variant)\n","\n","    # Save processing summary\n","    summary_path = os.path.join(target_dir, 'preprocessing_summary.json')\n","    with open(summary_path, 'w') as f:\n","        json.dump(processing_stats, f, indent=2)\n","\n","    all_results[variant] = processing_stats\n","\n","    print(f\"  Summary saved to: preprocessing_summary.json\")\n","    print(f\"  Total processed: {processing_stats['total_processed']}\")\n","    print(f\"  Face detection rate: {processing_stats['face_detection_stats']['detection_rate']:.1%}\")\n","    print(f\"  BBox expansion rate: {processing_stats['face_detection_stats'].get('bbox_expansion_rate', 0):.1%}\")\n","    print()\n","    print(\"-\" * 80)\n","    print()\n","\n","# ============================================================================\n","# FINAL SUMMARY\n","# ============================================================================\n","\n","print(\"=\" * 80)\n","print(\"FACE-AWARE PREPROCESSING V3 COMPLETE - SUMMARY\")\n","print(\"=\" * 80)\n","print()\n","\n","for variant, stats in all_results.items():\n","    print(f\"{variant} Dataset:\")\n","    print(f\"  Total processed: {stats['total_processed']} images\")\n","    print(f\"  Face detection: {stats['face_detection_stats']['detection_rate']:.1%}\")\n","    print(f\"  BBox expanded: {stats['face_detection_stats'].get('bbox_expansion_rate', 0):.1%}\")\n","    print(f\"  Fallback used: {stats['face_detection_stats']['fallback_used']} times\")\n","    print(f\"  Errors: {stats['total_errors']}\")\n","\n","    for split in ['train', 'val', 'test']:\n","        if split in stats['splits']:\n","            split_data = stats['splits'][split]\n","            print(f\"  {split.upper()}: {split_data['processed']} images\")\n","\n","    print()\n","\n","print(\"Preprocessing complete with 20px bbox expansion in all directions\")\n","print(\"Includes forehead, sides, and chin for complete micro-expression coverage\")\n","print()\n","print(\"All datasets saved to:\")\n","for config in DATASET_MAPPING.values():\n","    print(f\"  - {config['target']}\")\n","\n","print()\n","print(\"Next step: Cell 4 - Comprehensive validation and comparison\")\n","print()\n","print(\"=\" * 80)"],"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"m_1olQgeYzJ-","executionInfo":{"status":"ok","timestamp":1760862501538,"user_tz":-420,"elapsed":733821,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"4ee0f8be-d7de-43f7-98b4-7ecb8a505408","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Initializing Dlib face detector...\n","Face detector loaded\n","\n","================================================================================\n","CASME2 FACE-AWARE PREPROCESSING V3: BBOX EXPANSION (COMPLETE COVERAGE)\n","================================================================================\n","\n","[AF] AF - Apex Frame (face-aware with forehead, grayscale 224x224)\n","Source: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v1\n","Target: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7\n","\n","  Processing train split: 201 images\n","    Progress: 200/201\n","    Completed: 201 processed\n","    Face detection: 201/201 (100.0%)\n","    BBox expanded: 201 (100.0%)\n","  Processing val split: 26 images\n","    Completed: 26 processed\n","    Face detection: 26/26 (100.0%)\n","    BBox expanded: 26 (100.0%)\n","  Processing test split: 28 images\n","    Completed: 28 processed\n","    Face detection: 28/28 (100.0%)\n","    BBox expanded: 28 (100.0%)\n","  Summary saved to: preprocessing_summary.json\n","  Total processed: 255\n","  Face detection rate: 100.0%\n","  BBox expansion rate: 100.0%\n","\n","--------------------------------------------------------------------------------\n","\n","[KFS] KFS - Key Frame Sequence (face-aware with forehead, grayscale 224x224)\n","Source: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v2\n","Target: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8\n","\n","  Processing train split: 603 images\n","    Progress: 200/603\n","    Progress: 400/603\n","    Progress: 600/603\n","    Completed: 603 processed\n","    Face detection: 603/603 (100.0%)\n","    BBox expanded: 603 (100.0%)\n","  Processing val split: 78 images\n","    Completed: 78 processed\n","    Face detection: 78/78 (100.0%)\n","    BBox expanded: 78 (100.0%)\n","  Processing test split: 84 images\n","    Completed: 84 processed\n","    Face detection: 84/84 (100.0%)\n","    BBox expanded: 84 (100.0%)\n","  Summary saved to: preprocessing_summary.json\n","  Total processed: 765\n","  Face detection rate: 100.0%\n","  BBox expansion rate: 100.0%\n","\n","--------------------------------------------------------------------------------\n","\n","[MFS] MFS - Multi-Frame Sequence (face-aware with forehead, grayscale 224x224)\n","Source: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v3\n","Target: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9\n","\n","  Processing train split: 2613 images\n","    Progress: 200/2613\n","    Progress: 400/2613\n","    Progress: 600/2613\n","    Progress: 800/2613\n","    Progress: 1000/2613\n","    Progress: 1200/2613\n","    Progress: 1400/2613\n","    Progress: 1600/2613\n","    Progress: 1800/2613\n","    Progress: 2000/2613\n","    Progress: 2200/2613\n","    Progress: 2400/2613\n","    Progress: 2600/2613\n","    Completed: 2613 processed\n","    Face detection: 2613/2613 (100.0%)\n","    BBox expanded: 2613 (100.0%)\n","  Processing val split: 78 images\n","    Completed: 78 processed\n","    Face detection: 78/78 (100.0%)\n","    BBox expanded: 78 (100.0%)\n","  Processing test split: 83 images\n","    Completed: 83 processed\n","    Face detection: 83/83 (100.0%)\n","    BBox expanded: 83 (100.0%)\n","  Summary saved to: preprocessing_summary.json\n","  Total processed: 2774\n","  Face detection rate: 100.0%\n","  BBox expansion rate: 100.0%\n","\n","--------------------------------------------------------------------------------\n","\n","================================================================================\n","FACE-AWARE PREPROCESSING V3 COMPLETE - SUMMARY\n","================================================================================\n","\n","AF Dataset:\n","  Total processed: 255 images\n","  Face detection: 100.0%\n","  BBox expanded: 100.0%\n","  Fallback used: 0 times\n","  Errors: 0\n","  TRAIN: 201 images\n","  VAL: 26 images\n","  TEST: 28 images\n","\n","KFS Dataset:\n","  Total processed: 765 images\n","  Face detection: 100.0%\n","  BBox expanded: 100.0%\n","  Fallback used: 0 times\n","  Errors: 0\n","  TRAIN: 603 images\n","  VAL: 78 images\n","  TEST: 84 images\n","\n","MFS Dataset:\n","  Total processed: 2774 images\n","  Face detection: 100.0%\n","  BBox expanded: 100.0%\n","  Fallback used: 0 times\n","  Errors: 0\n","  TRAIN: 2613 images\n","  VAL: 78 images\n","  TEST: 83 images\n","\n","Preprocessing complete with 20px bbox expansion in all directions\n","Includes forehead, sides, and chin for complete micro-expression coverage\n","\n","All datasets saved to:\n","  - /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7\n","  - /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8\n","  - /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9\n","\n","Next step: Cell 4 - Comprehensive validation and comparison\n","\n","================================================================================\n"]}]},{"cell_type":"code","source":["# @title Cell 4: CASME2 Comprehensive Validation\n","# File: validate_comprehensive_casme2.py\n","# Location: Thesis_MER_Project/scripts/preprocessing/\n","# Purpose: Full validation and comparison of baseline (v4/v5/v6) vs face-aware (v7/v8/v9)\n","\n","import os\n","import json\n","import cv2\n","import dlib\n","import numpy as np\n","from pathlib import Path\n","from datetime import datetime\n","from collections import defaultdict\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","\n","# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define base paths\n","BASE_PATH = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","PROCESSED_PATH = f\"{BASE_PATH}/datasets/processed_casme2\"\n","\n","# Datasets to validate\n","DATASETS_TO_VALIDATE = {\n","    'baseline': {\n","        'v4': {\n","            'path': f\"{PROCESSED_PATH}/preprocessed_v4\",\n","            'variant': 'AF',\n","            'description': 'Baseline - Center Crop'\n","        },\n","        'v5': {\n","            'path': f\"{PROCESSED_PATH}/preprocessed_v5\",\n","            'variant': 'KFS',\n","            'description': 'Baseline - Center Crop'\n","        },\n","        'v6': {\n","            'path': f\"{PROCESSED_PATH}/preprocessed_v6\",\n","            'variant': 'MFS',\n","            'description': 'Baseline - Center Crop'\n","        }\n","    },\n","    'face_aware': {\n","        'v7': {\n","            'path': f\"{PROCESSED_PATH}/preprocessed_v7\",\n","            'variant': 'AF',\n","            'description': 'Face-Aware - BBox Expansion'\n","        },\n","        'v8': {\n","            'path': f\"{PROCESSED_PATH}/preprocessed_v8\",\n","            'variant': 'KFS',\n","            'description': 'Face-Aware - BBox Expansion'\n","        },\n","        'v9': {\n","            'path': f\"{PROCESSED_PATH}/preprocessed_v9\",\n","            'variant': 'MFS',\n","            'description': 'Face-Aware - BBox Expansion'\n","        }\n","    }\n","}\n","\n","# Validation parameters\n","EXPECTED_SIZE = (224, 224)\n","EXPECTED_CHANNELS = 1\n","CENTRALITY_THRESHOLD = 0.20\n","TARGET_WELL_CENTERED_RATE = 0.95\n","\n","# Initialize Dlib face detector\n","print(\"Initializing Dlib face detector...\")\n","detector = dlib.get_frontal_face_detector()\n","print(\"Face detector loaded\")\n","print()\n","\n","# ============================================================================\n","# VALIDATION FUNCTIONS\n","# ============================================================================\n","\n","def check_image_properties(image_path):\n","    \"\"\"\n","    Check basic image properties: dimensions, channels, loadability.\n","\n","    Args:\n","        image_path: Path to image file\n","\n","    Returns:\n","        Dictionary with property check results\n","    \"\"\"\n","    try:\n","        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","        if img is None:\n","            return {\n","                'valid': False,\n","                'error': 'Failed to load image'\n","            }\n","\n","        h, w = img.shape\n","        channels = 1 if len(img.shape) == 2 else img.shape[2]\n","\n","        return {\n","            'valid': True,\n","            'dimensions': (h, w),\n","            'channels': channels,\n","            'correct_size': (h, w) == EXPECTED_SIZE,\n","            'correct_channels': channels == EXPECTED_CHANNELS,\n","            'file_size_bytes': os.path.getsize(image_path)\n","        }\n","\n","    except Exception as e:\n","        return {\n","            'valid': False,\n","            'error': str(e)\n","        }\n","\n","def analyze_face_centrality(image_path):\n","    \"\"\"\n","    Analyze face detection and centrality in preprocessed image.\n","\n","    Args:\n","        image_path: Path to image file\n","\n","    Returns:\n","        Dictionary with face analysis results\n","    \"\"\"\n","    try:\n","        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","        if img is None:\n","            return {\n","                'detected': False,\n","                'error': 'Failed to load image'\n","            }\n","\n","        # Detect faces\n","        faces = detector(img, 1)\n","\n","        if len(faces) == 0:\n","            return {\n","                'detected': False,\n","                'reason': 'No face detected'\n","            }\n","\n","        # Use largest face if multiple detected\n","        if len(faces) > 1:\n","            faces = sorted(faces, key=lambda r: r.width() * r.height(), reverse=True)\n","\n","        face = faces[0]\n","\n","        # Calculate face properties\n","        face_center_x = (face.left() + face.right()) / 2\n","        face_center_y = (face.top() + face.bottom()) / 2\n","\n","        img_center_x = img.shape[1] / 2\n","        img_center_y = img.shape[0] / 2\n","\n","        # Normalized offset from center\n","        offset_x = abs(face_center_x - img_center_x) / img.shape[1]\n","        offset_y = abs(face_center_y - img_center_y) / img.shape[0]\n","\n","        # Centrality score (Euclidean distance from center)\n","        centrality_score = np.sqrt(offset_x**2 + offset_y**2)\n","\n","        # Face coverage\n","        face_width = face.width()\n","        face_height = face.height()\n","        face_area_ratio = (face_width * face_height) / (img.shape[0] * img.shape[1])\n","\n","        # Check if forehead visible (top of face bbox should not be at image edge)\n","        forehead_visible = face.top() > 10\n","\n","        # Check if chin visible (bottom of face bbox should not be at image edge)\n","        chin_visible = face.bottom() < img.shape[0] - 10\n","\n","        return {\n","            'detected': True,\n","            'centrality_score': float(centrality_score),\n","            'offset_x': float(offset_x),\n","            'offset_y': float(offset_y),\n","            'well_centered': centrality_score <= CENTRALITY_THRESHOLD,\n","            'face_width_ratio': float(face_width / img.shape[1]),\n","            'face_height_ratio': float(face_height / img.shape[0]),\n","            'face_area_ratio': float(face_area_ratio),\n","            'forehead_visible': forehead_visible,\n","            'chin_visible': chin_visible,\n","            'face_bbox': {\n","                'left': face.left(),\n","                'top': face.top(),\n","                'right': face.right(),\n","                'bottom': face.bottom()\n","            }\n","        }\n","\n","    except Exception as e:\n","        return {\n","            'detected': False,\n","            'error': str(e)\n","        }\n","\n","def analyze_image_quality(image_path):\n","    \"\"\"\n","    Analyze image quality metrics.\n","\n","    Args:\n","        image_path: Path to image file\n","\n","    Returns:\n","        Dictionary with quality metrics\n","    \"\"\"\n","    try:\n","        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","        if img is None:\n","            return {'valid': False}\n","\n","        # Basic statistics\n","        mean_intensity = np.mean(img)\n","        std_intensity = np.std(img)\n","        min_intensity = np.min(img)\n","        max_intensity = np.max(img)\n","\n","        # Contrast metric\n","        contrast = std_intensity / mean_intensity if mean_intensity > 0 else 0\n","\n","        # Dynamic range\n","        dynamic_range = max_intensity - min_intensity\n","\n","        return {\n","            'valid': True,\n","            'mean_intensity': float(mean_intensity),\n","            'std_intensity': float(std_intensity),\n","            'min_intensity': int(min_intensity),\n","            'max_intensity': int(max_intensity),\n","            'contrast': float(contrast),\n","            'dynamic_range': int(dynamic_range)\n","        }\n","\n","    except Exception as e:\n","        return {\n","            'valid': False,\n","            'error': str(e)\n","        }\n","\n","# ============================================================================\n","# COMPREHENSIVE DATASET VALIDATION\n","# ============================================================================\n","\n","def validate_dataset_comprehensive(dataset_path, variant_name, preprocessing_type):\n","    \"\"\"\n","    Comprehensive validation of entire dataset with full scan.\n","\n","    Args:\n","        dataset_path: Path to preprocessed dataset\n","        variant_name: Dataset variant (AF/KFS/MFS)\n","        preprocessing_type: Type (baseline/face_aware)\n","\n","    Returns:\n","        Comprehensive validation report\n","    \"\"\"\n","    report = {\n","        'variant': variant_name,\n","        'preprocessing_type': preprocessing_type,\n","        'validation_date': datetime.now().isoformat(),\n","        'dataset_path': dataset_path,\n","        'validation_mode': 'full_scan',\n","        'splits': {},\n","        'overall_statistics': {},\n","        'quality_assessment': '',\n","        'issues_found': []\n","    }\n","\n","    splits = ['train', 'val', 'test']\n","\n","    overall_stats = {\n","        'total_images': 0,\n","        'valid_images': 0,\n","        'dimension_correct': 0,\n","        'faces_detected': 0,\n","        'faces_well_centered': 0,\n","        'faces_off_center': 0,\n","        'face_detection_failed': 0,\n","        'forehead_visible_count': 0,\n","        'chin_visible_count': 0,\n","        'centrality_scores': [],\n","        'face_area_ratios': [],\n","        'quality_metrics': {\n","            'mean_intensities': [],\n","            'contrasts': [],\n","            'dynamic_ranges': []\n","        }\n","    }\n","\n","    print(f\"  Starting comprehensive validation...\")\n","\n","    for split in splits:\n","        split_path = os.path.join(dataset_path, split)\n","\n","        if not os.path.exists(split_path):\n","            continue\n","\n","        print(f\"  Validating {split} split (full scan)...\")\n","\n","        image_files = [f for f in os.listdir(split_path)\n","                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n","\n","        split_stats = {\n","            'total_images': len(image_files),\n","            'valid_images': 0,\n","            'dimension_correct': 0,\n","            'dimension_errors': 0,\n","            'faces_detected': 0,\n","            'faces_well_centered': 0,\n","            'faces_off_center': 0,\n","            'face_detection_failed': 0,\n","            'forehead_visible_count': 0,\n","            'chin_visible_count': 0,\n","            'centrality_scores': [],\n","            'face_area_ratios': [],\n","            'quality_metrics': {\n","                'mean_intensities': [],\n","                'contrasts': [],\n","                'dynamic_ranges': []\n","            },\n","            'problematic_images': []\n","        }\n","\n","        # Process ALL images (full scan)\n","        for idx, img_file in enumerate(image_files, 1):\n","            img_path = os.path.join(split_path, img_file)\n","\n","            # Check properties\n","            props = check_image_properties(img_path)\n","            if props['valid']:\n","                split_stats['valid_images'] += 1\n","                if props['correct_size'] and props['correct_channels']:\n","                    split_stats['dimension_correct'] += 1\n","                else:\n","                    split_stats['dimension_errors'] += 1\n","\n","            # Face analysis\n","            face_result = analyze_face_centrality(img_path)\n","\n","            if face_result['detected']:\n","                split_stats['faces_detected'] += 1\n","                split_stats['centrality_scores'].append(face_result['centrality_score'])\n","                split_stats['face_area_ratios'].append(face_result['face_area_ratio'])\n","\n","                if face_result['well_centered']:\n","                    split_stats['faces_well_centered'] += 1\n","                else:\n","                    split_stats['faces_off_center'] += 1\n","                    split_stats['problematic_images'].append({\n","                        'filename': img_file,\n","                        'issue': 'off_center',\n","                        'centrality_score': face_result['centrality_score']\n","                    })\n","\n","                if face_result['forehead_visible']:\n","                    split_stats['forehead_visible_count'] += 1\n","\n","                if face_result['chin_visible']:\n","                    split_stats['chin_visible_count'] += 1\n","            else:\n","                split_stats['face_detection_failed'] += 1\n","                split_stats['problematic_images'].append({\n","                    'filename': img_file,\n","                    'issue': 'face_detection_failed'\n","                })\n","\n","            # Quality analysis\n","            quality = analyze_image_quality(img_path)\n","            if quality['valid']:\n","                split_stats['quality_metrics']['mean_intensities'].append(\n","                    quality['mean_intensity']\n","                )\n","                split_stats['quality_metrics']['contrasts'].append(\n","                    quality['contrast']\n","                )\n","                split_stats['quality_metrics']['dynamic_ranges'].append(\n","                    quality['dynamic_range']\n","                )\n","\n","            # Progress indicator (every 500 images)\n","            if idx % 500 == 0:\n","                print(f\"    Progress: {idx}/{len(image_files)} images\")\n","\n","        # Calculate rates\n","        if split_stats['total_images'] > 0:\n","            split_stats['face_detection_rate'] = (\n","                split_stats['faces_detected'] / split_stats['total_images']\n","            )\n","            split_stats['well_centered_rate'] = (\n","                split_stats['faces_well_centered'] / split_stats['faces_detected']\n","                if split_stats['faces_detected'] > 0 else 0\n","            )\n","            split_stats['off_center_rate'] = (\n","                split_stats['faces_off_center'] / split_stats['faces_detected']\n","                if split_stats['faces_detected'] > 0 else 0\n","            )\n","            split_stats['forehead_visible_rate'] = (\n","                split_stats['forehead_visible_count'] / split_stats['faces_detected']\n","                if split_stats['faces_detected'] > 0 else 0\n","            )\n","            split_stats['chin_visible_rate'] = (\n","                split_stats['chin_visible_count'] / split_stats['faces_detected']\n","                if split_stats['faces_detected'] > 0 else 0\n","            )\n","\n","        # Calculate averages\n","        if split_stats['centrality_scores']:\n","            split_stats['avg_centrality_score'] = float(\n","                np.mean(split_stats['centrality_scores'])\n","            )\n","            split_stats['median_centrality_score'] = float(\n","                np.median(split_stats['centrality_scores'])\n","            )\n","\n","        if split_stats['face_area_ratios']:\n","            split_stats['avg_face_area_ratio'] = float(\n","                np.mean(split_stats['face_area_ratios'])\n","            )\n","\n","        if split_stats['quality_metrics']['mean_intensities']:\n","            split_stats['quality_summary'] = {\n","                'avg_mean_intensity': float(\n","                    np.mean(split_stats['quality_metrics']['mean_intensities'])\n","                ),\n","                'avg_contrast': float(\n","                    np.mean(split_stats['quality_metrics']['contrasts'])\n","                ),\n","                'avg_dynamic_range': float(\n","                    np.mean(split_stats['quality_metrics']['dynamic_ranges'])\n","                )\n","            }\n","\n","        # Update overall statistics\n","        overall_stats['total_images'] += split_stats['total_images']\n","        overall_stats['valid_images'] += split_stats['valid_images']\n","        overall_stats['dimension_correct'] += split_stats['dimension_correct']\n","        overall_stats['faces_detected'] += split_stats['faces_detected']\n","        overall_stats['faces_well_centered'] += split_stats['faces_well_centered']\n","        overall_stats['faces_off_center'] += split_stats['faces_off_center']\n","        overall_stats['face_detection_failed'] += split_stats['face_detection_failed']\n","        overall_stats['forehead_visible_count'] += split_stats['forehead_visible_count']\n","        overall_stats['chin_visible_count'] += split_stats['chin_visible_count']\n","        overall_stats['centrality_scores'].extend(split_stats['centrality_scores'])\n","        overall_stats['face_area_ratios'].extend(split_stats['face_area_ratios'])\n","        overall_stats['quality_metrics']['mean_intensities'].extend(\n","            split_stats['quality_metrics']['mean_intensities']\n","        )\n","        overall_stats['quality_metrics']['contrasts'].extend(\n","            split_stats['quality_metrics']['contrasts']\n","        )\n","        overall_stats['quality_metrics']['dynamic_ranges'].extend(\n","            split_stats['quality_metrics']['dynamic_ranges']\n","        )\n","\n","        # Clean up for JSON storage\n","        split_stats['centrality_scores'] = []\n","        split_stats['face_area_ratios'] = []\n","        split_stats['quality_metrics'] = split_stats.get('quality_summary', {})\n","\n","        # Store limited problematic images (top 10)\n","        if len(split_stats['problematic_images']) > 10:\n","            split_stats['problematic_images'] = split_stats['problematic_images'][:10]\n","\n","        report['splits'][split] = split_stats\n","\n","        print(f\"    Completed: {split_stats['total_images']} images scanned\")\n","        print(f\"    Face detection: {split_stats['face_detection_rate']:.1%}\")\n","        print(f\"    Well-centered: {split_stats['well_centered_rate']:.1%}\")\n","        print(f\"    Forehead visible: {split_stats['forehead_visible_rate']:.1%}\")\n","        print(f\"    Chin visible: {split_stats['chin_visible_rate']:.1%}\")\n","\n","    # Calculate overall metrics\n","    if overall_stats['total_images'] > 0:\n","        overall_stats['face_detection_rate'] = (\n","            overall_stats['faces_detected'] / overall_stats['total_images']\n","        )\n","        overall_stats['well_centered_rate'] = (\n","            overall_stats['faces_well_centered'] / overall_stats['faces_detected']\n","            if overall_stats['faces_detected'] > 0 else 0\n","        )\n","        overall_stats['off_center_rate'] = (\n","            overall_stats['faces_off_center'] / overall_stats['faces_detected']\n","            if overall_stats['faces_detected'] > 0 else 0\n","        )\n","        overall_stats['forehead_visible_rate'] = (\n","            overall_stats['forehead_visible_count'] / overall_stats['faces_detected']\n","            if overall_stats['faces_detected'] > 0 else 0\n","        )\n","        overall_stats['chin_visible_rate'] = (\n","            overall_stats['chin_visible_count'] / overall_stats['faces_detected']\n","            if overall_stats['faces_detected'] > 0 else 0\n","        )\n","\n","    if overall_stats['centrality_scores']:\n","        overall_stats['avg_centrality_score'] = float(\n","            np.mean(overall_stats['centrality_scores'])\n","        )\n","        overall_stats['median_centrality_score'] = float(\n","            np.median(overall_stats['centrality_scores'])\n","        )\n","        overall_stats['std_centrality_score'] = float(\n","            np.std(overall_stats['centrality_scores'])\n","        )\n","\n","    if overall_stats['face_area_ratios']:\n","        overall_stats['avg_face_area_ratio'] = float(\n","            np.mean(overall_stats['face_area_ratios'])\n","        )\n","\n","    if overall_stats['quality_metrics']['mean_intensities']:\n","        overall_stats['quality_summary'] = {\n","            'avg_mean_intensity': float(\n","                np.mean(overall_stats['quality_metrics']['mean_intensities'])\n","            ),\n","            'avg_contrast': float(\n","                np.mean(overall_stats['quality_metrics']['contrasts'])\n","            ),\n","            'avg_dynamic_range': float(\n","                np.mean(overall_stats['quality_metrics']['dynamic_ranges'])\n","            )\n","        }\n","\n","    # Clean up large arrays\n","    overall_stats['centrality_scores'] = []\n","    overall_stats['face_area_ratios'] = []\n","    overall_stats['quality_metrics'] = overall_stats.get('quality_summary', {})\n","\n","    report['overall_statistics'] = overall_stats\n","\n","    # Quality assessment\n","    face_det_rate = overall_stats.get('face_detection_rate', 0)\n","    well_centered_rate = overall_stats.get('well_centered_rate', 0)\n","    forehead_rate = overall_stats.get('forehead_visible_rate', 0)\n","\n","    if face_det_rate < 0.95:\n","        report['issues_found'].append(\n","            f\"Low face detection rate: {face_det_rate:.1%}\"\n","        )\n","\n","    if well_centered_rate < TARGET_WELL_CENTERED_RATE:\n","        report['issues_found'].append(\n","            f\"Well-centered rate below target: {well_centered_rate:.1%} < {TARGET_WELL_CENTERED_RATE:.1%}\"\n","        )\n","\n","    if forehead_rate < 0.90:\n","        report['issues_found'].append(\n","            f\"Low forehead visibility: {forehead_rate:.1%}\"\n","        )\n","\n","    # Overall assessment\n","    if not report['issues_found']:\n","        report['quality_assessment'] = 'EXCELLENT - Ready for training'\n","    elif well_centered_rate >= TARGET_WELL_CENTERED_RATE:\n","        report['quality_assessment'] = 'GOOD - Acceptable for training'\n","    else:\n","        report['quality_assessment'] = 'NEEDS_REVIEW - Consider improvements'\n","\n","    return report\n","\n","# ============================================================================\n","# MAIN EXECUTION\n","# ============================================================================\n","\n","print(\"=\" * 80)\n","print(\"CASME2 COMPREHENSIVE VALIDATION - FULL SCAN\")\n","print(\"Baseline (v4/v5/v6) vs Face-Aware (v7/v8/v9) Comparison\")\n","print(\"=\" * 80)\n","print()\n","\n","all_validation_results = {\n","    'validation_date': datetime.now().isoformat(),\n","    'validation_mode': 'comprehensive_full_scan',\n","    'baseline': {},\n","    'face_aware': {},\n","    'comparison': {}\n","}\n","\n","# Validate baseline datasets\n","print(\"PHASE 1: Validating Baseline Preprocessing (v4/v5/v6)\")\n","print(\"-\" * 80)\n","print()\n","\n","for version, config in DATASETS_TO_VALIDATE['baseline'].items():\n","    dataset_path = config['path']\n","    variant = config['variant']\n","    description = config['description']\n","\n","    print(f\"[{version.upper()}] {variant} - {description}\")\n","    print(f\"Path: {dataset_path}\")\n","    print()\n","\n","    if not os.path.exists(dataset_path):\n","        print(f\"  Warning: Dataset not found, skipping\")\n","        print()\n","        continue\n","\n","    validation_report = validate_dataset_comprehensive(\n","        dataset_path, variant, 'baseline'\n","    )\n","\n","    all_validation_results['baseline'][version] = validation_report\n","\n","    # Save individual report\n","    report_path = os.path.join(dataset_path, 'comprehensive_validation_report.json')\n","    with open(report_path, 'w') as f:\n","        json.dump(validation_report, f, indent=2)\n","\n","    print(f\"  Report saved: comprehensive_validation_report.json\")\n","    print()\n","    print(\"-\" * 80)\n","    print()\n","\n","# Validate face-aware datasets\n","print(\"PHASE 2: Validating Face-Aware Preprocessing (v7/v8/v9)\")\n","print(\"-\" * 80)\n","print()\n","\n","for version, config in DATASETS_TO_VALIDATE['face_aware'].items():\n","    dataset_path = config['path']\n","    variant = config['variant']\n","    description = config['description']\n","\n","    print(f\"[{version.upper()}] {variant} - {description}\")\n","    print(f\"Path: {dataset_path}\")\n","    print()\n","\n","    if not os.path.exists(dataset_path):\n","        print(f\"  Warning: Dataset not found, skipping\")\n","        print()\n","        continue\n","\n","    validation_report = validate_dataset_comprehensive(\n","        dataset_path, variant, 'face_aware'\n","    )\n","\n","    all_validation_results['face_aware'][version] = validation_report\n","\n","    # Save individual report\n","    report_path = os.path.join(dataset_path, 'comprehensive_validation_report.json')\n","    with open(report_path, 'w') as f:\n","        json.dump(validation_report, f, indent=2)\n","\n","    print(f\"  Report saved: comprehensive_validation_report.json\")\n","    print()\n","    print(\"-\" * 80)\n","    print()\n","\n","# ============================================================================\n","# COMPARISON ANALYSIS\n","# ============================================================================\n","\n","print(\"PHASE 3: Comparison Analysis\")\n","print(\"=\" * 80)\n","print()\n","\n","comparison = {}\n","\n","for variant in ['AF', 'KFS', 'MFS']:\n","    # Find corresponding baseline and face-aware versions\n","    baseline_version = None\n","    faceaware_version = None\n","\n","    for v, data in all_validation_results['baseline'].items():\n","        if data['variant'] == variant:\n","            baseline_version = v\n","            break\n","\n","    for v, data in all_validation_results['face_aware'].items():\n","        if data['variant'] == variant:\n","            faceaware_version = v\n","            break\n","\n","    if baseline_version and faceaware_version:\n","        baseline_stats = all_validation_results['baseline'][baseline_version]['overall_statistics']\n","        faceaware_stats = all_validation_results['face_aware'][faceaware_version]['overall_statistics']\n","\n","        comparison[variant] = {\n","            'baseline_version': baseline_version,\n","            'faceaware_version': faceaware_version,\n","            'metrics': {\n","                'face_detection_rate': {\n","                    'baseline': baseline_stats.get('face_detection_rate', 0),\n","                    'face_aware': faceaware_stats.get('face_detection_rate', 0),\n","                    'improvement': faceaware_stats.get('face_detection_rate', 0) - baseline_stats.get('face_detection_rate', 0)\n","                },\n","                'well_centered_rate': {\n","                    'baseline': baseline_stats.get('well_centered_rate', 0),\n","                    'face_aware': faceaware_stats.get('well_centered_rate', 0),\n","                    'improvement': faceaware_stats.get('well_centered_rate', 0) - baseline_stats.get('well_centered_rate', 0)\n","                },\n","                'forehead_visible_rate': {\n","                    'baseline': baseline_stats.get('forehead_visible_rate', 0),\n","                    'face_aware': faceaware_stats.get('forehead_visible_rate', 0),\n","                    'improvement': faceaware_stats.get('forehead_visible_rate', 0) - baseline_stats.get('forehead_visible_rate', 0)\n","                },\n","                'avg_centrality_score': {\n","                    'baseline': baseline_stats.get('avg_centrality_score', 0),\n","                    'face_aware': faceaware_stats.get('avg_centrality_score', 0),\n","                    'improvement': baseline_stats.get('avg_centrality_score', 0) - faceaware_stats.get('avg_centrality_score', 0)\n","                }\n","            }\n","        }\n","\n","        print(f\"{variant} Comparison ({baseline_version} vs {faceaware_version}):\")\n","        print(f\"  Well-centered rate:\")\n","        print(f\"    Baseline: {comparison[variant]['metrics']['well_centered_rate']['baseline']:.1%}\")\n","        print(f\"    Face-aware: {comparison[variant]['metrics']['well_centered_rate']['face_aware']:.1%}\")\n","        print(f\"    Improvement: {comparison[variant]['metrics']['well_centered_rate']['improvement']:+.1%}\")\n","        print(f\"  Forehead visible:\")\n","        print(f\"    Baseline: {comparison[variant]['metrics']['forehead_visible_rate']['baseline']:.1%}\")\n","        print(f\"    Face-aware: {comparison[variant]['metrics']['forehead_visible_rate']['face_aware']:.1%}\")\n","        print(f\"    Improvement: {comparison[variant]['metrics']['forehead_visible_rate']['improvement']:+.1%}\")\n","        print()\n","\n","all_validation_results['comparison'] = comparison\n","\n","# Save comprehensive comparison report\n","comparison_report_path = os.path.join(PROCESSED_PATH, 'comprehensive_validation_comparison.json')\n","with open(comparison_report_path, 'w') as f:\n","    json.dump(all_validation_results, f, indent=2)\n","\n","print(\"-\" * 80)\n","print(f\"Comprehensive comparison report saved:\")\n","print(f\"  {comparison_report_path}\")\n","print()\n","\n","# ============================================================================\n","# FINAL SUMMARY & RECOMMENDATION\n","# ============================================================================\n","\n","print(\"=\" * 80)\n","print(\"FINAL SUMMARY & RECOMMENDATION\")\n","print(\"=\" * 80)\n","print()\n","\n","print(\"Baseline Preprocessing (v4/v5/v6):\")\n","for version, report in all_validation_results['baseline'].items():\n","    stats = report['overall_statistics']\n","    print(f\"  {version.upper()} ({report['variant']}):\")\n","    print(f\"    Well-centered: {stats.get('well_centered_rate', 0):.1%}\")\n","    print(f\"    Forehead visible: {stats.get('forehead_visible_rate', 0):.1%}\")\n","    print(f\"    Assessment: {report['quality_assessment']}\")\n","\n","print()\n","print(\"Face-Aware Preprocessing (v7/v8/v9):\")\n","for version, report in all_validation_results['face_aware'].items():\n","    stats = report['overall_statistics']\n","    print(f\"  {version.upper()} ({report['variant']}):\")\n","    print(f\"    Well-centered: {stats.get('well_centered_rate', 0):.1%}\")\n","    print(f\"    Forehead visible: {stats.get('forehead_visible_rate', 0):.1%}\")\n","    print(f\"    Assessment: {report['quality_assessment']}\")\n","\n","print()\n","print(\"-\" * 80)\n","\n","# Overall recommendation\n","all_face_aware_excellent = all(\n","    r['quality_assessment'] in ['EXCELLENT - Ready for training', 'GOOD - Acceptable for training']\n","    for r in all_validation_results['face_aware'].values()\n",")\n","\n","if all_face_aware_excellent:\n","    print(\"RECOMMENDATION: Use Face-Aware preprocessing (v7/v8/v9) for final training\")\n","    print(\"  - Superior face centering achieved (>95% well-centered)\")\n","    print(\"  - Complete facial coverage (forehead + chin included)\")\n","    print(\"  - Ready for model training experiments\")\n","else:\n","    print(\"RECOMMENDATION: Review preprocessing results before proceeding\")\n","    print(\"  - Check comprehensive validation reports for details\")\n","    print(\"  - Address identified issues before training\")\n","\n","print()\n","print(\"=\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"7QfOjpoFmKxf","executionInfo":{"status":"ok","timestamp":1760863168748,"user_tz":-420,"elapsed":423661,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"035aa704-424c-4bcd-e47d-2b8cd88f82a2","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Initializing Dlib face detector...\n","Face detector loaded\n","\n","================================================================================\n","CASME2 COMPREHENSIVE VALIDATION - FULL SCAN\n","Baseline (v4/v5/v6) vs Face-Aware (v7/v8/v9) Comparison\n","================================================================================\n","\n","PHASE 1: Validating Baseline Preprocessing (v4/v5/v6)\n","--------------------------------------------------------------------------------\n","\n","[V4] AF - Baseline - Center Crop\n","Path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v4\n","\n","  Starting comprehensive validation...\n","  Validating train split (full scan)...\n","    Completed: 201 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 82.6%\n","    Forehead visible: 100.0%\n","    Chin visible: 34.8%\n","  Validating val split (full scan)...\n","    Completed: 26 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 96.2%\n","    Forehead visible: 100.0%\n","    Chin visible: 26.9%\n","  Validating test split (full scan)...\n","    Completed: 28 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 82.1%\n","    Forehead visible: 100.0%\n","    Chin visible: 28.6%\n","  Report saved: comprehensive_validation_report.json\n","\n","--------------------------------------------------------------------------------\n","\n","[V5] KFS - Baseline - Center Crop\n","Path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v5\n","\n","  Starting comprehensive validation...\n","  Validating train split (full scan)...\n","    Progress: 500/603 images\n","    Completed: 603 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 83.3%\n","    Forehead visible: 100.0%\n","    Chin visible: 34.5%\n","  Validating val split (full scan)...\n","    Completed: 78 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 96.2%\n","    Forehead visible: 100.0%\n","    Chin visible: 26.9%\n","  Validating test split (full scan)...\n","    Completed: 84 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 83.3%\n","    Forehead visible: 100.0%\n","    Chin visible: 29.8%\n","  Report saved: comprehensive_validation_report.json\n","\n","--------------------------------------------------------------------------------\n","\n","[V6] MFS - Baseline - Center Crop\n","Path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v6\n","\n","  Starting comprehensive validation...\n","  Validating train split (full scan)...\n","    Progress: 500/2613 images\n","    Progress: 1000/2613 images\n","    Progress: 1500/2613 images\n","    Progress: 2000/2613 images\n","    Progress: 2500/2613 images\n","    Completed: 2613 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 83.2%\n","    Forehead visible: 100.0%\n","    Chin visible: 34.2%\n","  Validating val split (full scan)...\n","    Completed: 78 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 96.2%\n","    Forehead visible: 100.0%\n","    Chin visible: 26.9%\n","  Validating test split (full scan)...\n","    Completed: 83 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 84.3%\n","    Forehead visible: 100.0%\n","    Chin visible: 30.1%\n","  Report saved: comprehensive_validation_report.json\n","\n","--------------------------------------------------------------------------------\n","\n","PHASE 2: Validating Face-Aware Preprocessing (v7/v8/v9)\n","--------------------------------------------------------------------------------\n","\n","[V7] AF - Face-Aware - BBox Expansion\n","Path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7\n","\n","  Starting comprehensive validation...\n","  Validating train split (full scan)...\n","    Completed: 201 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 100.0%\n","    Forehead visible: 100.0%\n","    Chin visible: 89.1%\n","  Validating val split (full scan)...\n","    Completed: 26 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 100.0%\n","    Forehead visible: 100.0%\n","    Chin visible: 80.8%\n","  Validating test split (full scan)...\n","    Completed: 28 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 100.0%\n","    Forehead visible: 100.0%\n","    Chin visible: 89.3%\n","  Report saved: comprehensive_validation_report.json\n","\n","--------------------------------------------------------------------------------\n","\n","[V8] KFS - Face-Aware - BBox Expansion\n","Path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8\n","\n","  Starting comprehensive validation...\n","  Validating train split (full scan)...\n","    Progress: 500/603 images\n","    Completed: 603 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 100.0%\n","    Forehead visible: 99.8%\n","    Chin visible: 86.7%\n","  Validating val split (full scan)...\n","    Completed: 78 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 100.0%\n","    Forehead visible: 100.0%\n","    Chin visible: 78.2%\n","  Validating test split (full scan)...\n","    Completed: 84 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 100.0%\n","    Forehead visible: 100.0%\n","    Chin visible: 90.5%\n","  Report saved: comprehensive_validation_report.json\n","\n","--------------------------------------------------------------------------------\n","\n","[V9] MFS - Face-Aware - BBox Expansion\n","Path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9\n","\n","  Starting comprehensive validation...\n","  Validating train split (full scan)...\n","    Progress: 500/2613 images\n","    Progress: 1000/2613 images\n","    Progress: 1500/2613 images\n","    Progress: 2000/2613 images\n","    Progress: 2500/2613 images\n","    Completed: 2613 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 100.0%\n","    Forehead visible: 100.0%\n","    Chin visible: 87.3%\n","  Validating val split (full scan)...\n","    Completed: 78 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 100.0%\n","    Forehead visible: 100.0%\n","    Chin visible: 78.2%\n","  Validating test split (full scan)...\n","    Completed: 83 images scanned\n","    Face detection: 100.0%\n","    Well-centered: 100.0%\n","    Forehead visible: 100.0%\n","    Chin visible: 91.6%\n","  Report saved: comprehensive_validation_report.json\n","\n","--------------------------------------------------------------------------------\n","\n","PHASE 3: Comparison Analysis\n","================================================================================\n","\n","AF Comparison (v4 vs v7):\n","  Well-centered rate:\n","    Baseline: 83.9%\n","    Face-aware: 100.0%\n","    Improvement: +16.1%\n","  Forehead visible:\n","    Baseline: 100.0%\n","    Face-aware: 100.0%\n","    Improvement: +0.0%\n","\n","KFS Comparison (v5 vs v8):\n","  Well-centered rate:\n","    Baseline: 84.6%\n","    Face-aware: 100.0%\n","    Improvement: +15.4%\n","  Forehead visible:\n","    Baseline: 100.0%\n","    Face-aware: 99.9%\n","    Improvement: -0.1%\n","\n","MFS Comparison (v6 vs v9):\n","  Well-centered rate:\n","    Baseline: 83.6%\n","    Face-aware: 100.0%\n","    Improvement: +16.4%\n","  Forehead visible:\n","    Baseline: 100.0%\n","    Face-aware: 100.0%\n","    Improvement: -0.0%\n","\n","--------------------------------------------------------------------------------\n","Comprehensive comparison report saved:\n","  /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/comprehensive_validation_comparison.json\n","\n","================================================================================\n","FINAL SUMMARY & RECOMMENDATION\n","================================================================================\n","\n","Baseline Preprocessing (v4/v5/v6):\n","  V4 (AF):\n","    Well-centered: 83.9%\n","    Forehead visible: 100.0%\n","    Assessment: NEEDS_REVIEW - Consider improvements\n","  V5 (KFS):\n","    Well-centered: 84.6%\n","    Forehead visible: 100.0%\n","    Assessment: NEEDS_REVIEW - Consider improvements\n","  V6 (MFS):\n","    Well-centered: 83.6%\n","    Forehead visible: 100.0%\n","    Assessment: NEEDS_REVIEW - Consider improvements\n","\n","Face-Aware Preprocessing (v7/v8/v9):\n","  V7 (AF):\n","    Well-centered: 100.0%\n","    Forehead visible: 100.0%\n","    Assessment: EXCELLENT - Ready for training\n","  V8 (KFS):\n","    Well-centered: 100.0%\n","    Forehead visible: 99.9%\n","    Assessment: EXCELLENT - Ready for training\n","  V9 (MFS):\n","    Well-centered: 100.0%\n","    Forehead visible: 100.0%\n","    Assessment: EXCELLENT - Ready for training\n","\n","--------------------------------------------------------------------------------\n","RECOMMENDATION: Use Face-Aware preprocessing (v7/v8/v9) for final training\n","  - Superior face centering achieved (>95% well-centered)\n","  - Complete facial coverage (forehead + chin included)\n","  - Ready for model training experiments\n","\n","================================================================================\n"]}]},{"cell_type":"code","source":["# @title [cancelled] Cell 5: CASME2 Face Alignment + Eye Masking Preprocessing\n","# File: preprocess_casme2_aligned_eyemask.py\n","# Location: Thesis_MER_Project/scripts/preprocessing/\n","# Purpose: Face alignment (horizontal) + eye masking for noise reduction (v1/v2/v3 to v10/v11/v12)\n","\n","import os\n","import json\n","import cv2\n","import dlib\n","import numpy as np\n","from pathlib import Path\n","from datetime import datetime\n","from collections import defaultdict\n","from google.colab import drive\n","\n","# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define base paths\n","BASE_PATH = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","PROCESSED_PATH = f\"{BASE_PATH}/datasets/processed_casme2\"\n","\n","# Dataset mapping: source -> target (aligned + eye masked versions)\n","DATASET_MAPPING = {\n","    'v1_to_v10': {\n","        'source': f\"{PROCESSED_PATH}/data_split_v1\",\n","        'target': f\"{PROCESSED_PATH}/preprocessed_v10\",\n","        'description': 'AF - Apex Frame (aligned + eye masked, grayscale 224x224)',\n","        'variant': 'AF'\n","    },\n","    'v2_to_v11': {\n","        'source': f\"{PROCESSED_PATH}/data_split_v2\",\n","        'target': f\"{PROCESSED_PATH}/preprocessed_v11\",\n","        'description': 'KFS - Key Frame Sequence (aligned + eye masked, grayscale 224x224)',\n","        'variant': 'KFS'\n","    },\n","    'v3_to_v12': {\n","        'source': f\"{PROCESSED_PATH}/data_split_v3\",\n","        'target': f\"{PROCESSED_PATH}/preprocessed_v12\",\n","        'description': 'MFS - Multi-Frame Sequence (aligned + eye masked, grayscale 224x224)',\n","        'variant': 'MFS'\n","    }\n","}\n","\n","# Processing parameters\n","TARGET_SIZE = 224\n","BBOX_EXPANSION = 20\n","EYE_MASK_STRATEGY = 'gaussian_blur'  # Options: 'black', 'gray', 'gaussian_blur'\n","EYE_MASK_MARGIN = 5  # Additional margin around eye region\n","\n","# Initialize Dlib detectors\n","print(\"Initializing Dlib face detector and landmark predictor...\")\n","detector = dlib.get_frontal_face_detector()\n","\n","# Download and load shape predictor if not exists\n","predictor_path = \"/content/shape_predictor_68_face_landmarks.dat\"\n","if not os.path.exists(predictor_path):\n","    print(\"Downloading shape predictor model...\")\n","    os.system(\"wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n","    os.system(\"bzip2 -d shape_predictor_68_face_landmarks.dat.bz2\")\n","\n","predictor = dlib.shape_predictor(predictor_path)\n","print(\"Face detector and landmark predictor loaded\")\n","print()\n","\n","# ============================================================================\n","# FACE ALIGNMENT FUNCTIONS\n","# ============================================================================\n","\n","def get_eye_landmarks(shape):\n","    \"\"\"\n","    Extract eye landmark coordinates from dlib shape predictor output.\n","\n","    Args:\n","        shape: Dlib shape predictor output (68 landmarks)\n","\n","    Returns:\n","        Tuple (left_eye_center, right_eye_center)\n","    \"\"\"\n","    # Left eye landmarks: 36-41\n","    left_eye_points = [(shape.part(i).x, shape.part(i).y) for i in range(36, 42)]\n","    left_eye_center = np.mean(left_eye_points, axis=0).astype(int)\n","\n","    # Right eye landmarks: 42-47\n","    right_eye_points = [(shape.part(i).x, shape.part(i).y) for i in range(42, 48)]\n","    right_eye_center = np.mean(right_eye_points, axis=0).astype(int)\n","\n","    return tuple(left_eye_center), tuple(right_eye_center)\n","\n","def calculate_rotation_angle(left_eye, right_eye):\n","    \"\"\"\n","    Calculate rotation angle to make eyes horizontal.\n","\n","    Args:\n","        left_eye: (x, y) coordinates of left eye center\n","        right_eye: (x, y) coordinates of right eye center\n","\n","    Returns:\n","        Rotation angle in degrees\n","    \"\"\"\n","    # Calculate angle between eyes\n","    delta_y = right_eye[1] - left_eye[1]\n","    delta_x = right_eye[0] - left_eye[0]\n","\n","    angle = np.degrees(np.arctan2(delta_y, delta_x))\n","\n","    return angle\n","\n","def rotate_image_with_landmarks(image, angle, center):\n","    \"\"\"\n","    Rotate image to align face horizontally.\n","\n","    Args:\n","        image: Input image\n","        angle: Rotation angle in degrees\n","        center: Rotation center point\n","\n","    Returns:\n","        Rotated image\n","    \"\"\"\n","    h, w = image.shape[:2]\n","\n","    # Get rotation matrix\n","    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n","\n","    # Rotate image\n","    rotated = cv2.warpAffine(image, rotation_matrix, (w, h),\n","                             flags=cv2.INTER_LANCZOS4,\n","                             borderMode=cv2.BORDER_REPLICATE)\n","\n","    return rotated\n","\n","def align_face_horizontal(image):\n","    \"\"\"\n","    Detect face landmarks and rotate to horizontal alignment.\n","\n","    Args:\n","        image: Input image (RGB or grayscale)\n","\n","    Returns:\n","        Tuple (aligned_image, alignment_info)\n","    \"\"\"\n","    alignment_info = {\n","        'aligned': False,\n","        'angle': 0,\n","        'landmarks_detected': False\n","    }\n","\n","    try:\n","        # Detect faces\n","        faces = detector(image, 1)\n","\n","        if len(faces) == 0:\n","            return image, alignment_info\n","\n","        # Use largest face\n","        if len(faces) > 1:\n","            faces = sorted(faces, key=lambda r: r.width() * r.height(), reverse=True)\n","\n","        face = faces[0]\n","\n","        # Get facial landmarks\n","        shape = predictor(image, face)\n","\n","        # Get eye centers\n","        left_eye, right_eye = get_eye_landmarks(shape)\n","\n","        alignment_info['landmarks_detected'] = True\n","\n","        # Calculate rotation angle\n","        angle = calculate_rotation_angle(left_eye, right_eye)\n","\n","        # Only rotate if angle is significant (>1 degree)\n","        if abs(angle) > 1.0:\n","            # Calculate rotation center (between eyes)\n","            center = ((left_eye[0] + right_eye[0]) // 2,\n","                     (left_eye[1] + right_eye[1]) // 2)\n","\n","            # Rotate image\n","            aligned = rotate_image_with_landmarks(image, angle, center)\n","\n","            alignment_info['aligned'] = True\n","            alignment_info['angle'] = float(angle)\n","\n","            return aligned, alignment_info\n","        else:\n","            alignment_info['aligned'] = False\n","            alignment_info['angle'] = float(angle)\n","            return image, alignment_info\n","\n","    except Exception as e:\n","        alignment_info['error'] = str(e)\n","        return image, alignment_info\n","\n","# ============================================================================\n","# EYE MASKING FUNCTIONS\n","# ============================================================================\n","\n","def get_eye_regions(image):\n","    \"\"\"\n","    Detect eye regions for masking.\n","\n","    Args:\n","        image: Input image\n","\n","    Returns:\n","        List of eye regions [(x1, y1, x2, y2), ...] or None\n","    \"\"\"\n","    try:\n","        # Detect faces\n","        faces = detector(image, 1)\n","\n","        if len(faces) == 0:\n","            return None\n","\n","        # Use largest face\n","        if len(faces) > 1:\n","            faces = sorted(faces, key=lambda r: r.width() * r.height(), reverse=True)\n","\n","        face = faces[0]\n","\n","        # Get facial landmarks\n","        shape = predictor(image, face)\n","\n","        eye_regions = []\n","\n","        # Left eye region (landmarks 36-41)\n","        left_eye_points = [(shape.part(i).x, shape.part(i).y) for i in range(36, 42)]\n","        left_x_coords = [p[0] for p in left_eye_points]\n","        left_y_coords = [p[1] for p in left_eye_points]\n","\n","        left_x1 = max(0, min(left_x_coords) - EYE_MASK_MARGIN)\n","        left_y1 = max(0, min(left_y_coords) - EYE_MASK_MARGIN)\n","        left_x2 = min(image.shape[1], max(left_x_coords) + EYE_MASK_MARGIN)\n","        left_y2 = min(image.shape[0], max(left_y_coords) + EYE_MASK_MARGIN)\n","\n","        eye_regions.append((int(left_x1), int(left_y1), int(left_x2), int(left_y2)))\n","\n","        # Right eye region (landmarks 42-47)\n","        right_eye_points = [(shape.part(i).x, shape.part(i).y) for i in range(42, 48)]\n","        right_x_coords = [p[0] for p in right_eye_points]\n","        right_y_coords = [p[1] for p in right_eye_points]\n","\n","        right_x1 = max(0, min(right_x_coords) - EYE_MASK_MARGIN)\n","        right_y1 = max(0, min(right_y_coords) - EYE_MASK_MARGIN)\n","        right_x2 = min(image.shape[1], max(right_x_coords) + EYE_MASK_MARGIN)\n","        right_y2 = min(image.shape[0], max(right_y_coords) + EYE_MASK_MARGIN)\n","\n","        eye_regions.append((int(right_x1), int(right_y1), int(right_x2), int(right_y2)))\n","\n","        return eye_regions\n","\n","    except Exception as e:\n","        return None\n","\n","def apply_eye_mask(image, strategy='gaussian_blur'):\n","    \"\"\"\n","    Apply eye masking to reduce eye movement noise.\n","\n","    Args:\n","        image: Input grayscale image\n","        strategy: Masking strategy ('black', 'gray', 'gaussian_blur')\n","\n","    Returns:\n","        Tuple (masked_image, masking_info)\n","    \"\"\"\n","    masking_info = {\n","        'masked': False,\n","        'strategy': strategy,\n","        'regions_masked': 0\n","    }\n","\n","    # Get eye regions\n","    eye_regions = get_eye_regions(image)\n","\n","    if eye_regions is None:\n","        return image, masking_info\n","\n","    masked = image.copy()\n","\n","    for x1, y1, x2, y2 in eye_regions:\n","        if strategy == 'black':\n","            # Black mask (intensity 0)\n","            masked[y1:y2, x1:x2] = 0\n","\n","        elif strategy == 'gray':\n","            # Gray mask (mean intensity of image)\n","            mean_intensity = np.mean(image)\n","            masked[y1:y2, x1:x2] = mean_intensity\n","\n","        elif strategy == 'gaussian_blur':\n","            # Gaussian blur (softer approach)\n","            eye_region = masked[y1:y2, x1:x2]\n","            blurred = cv2.GaussianBlur(eye_region, (15, 15), 0)\n","            masked[y1:y2, x1:x2] = blurred\n","\n","        masking_info['regions_masked'] += 1\n","\n","    masking_info['masked'] = True\n","\n","    return masked, masking_info\n","\n","# ============================================================================\n","# FACE DETECTION & EXPANSION (from Cell 3)\n","# ============================================================================\n","\n","def detect_face_with_expansion(image):\n","    \"\"\"\n","    Detect face and expand bbox in all directions.\n","\n","    Args:\n","        image: Input image\n","\n","    Returns:\n","        Tuple (expanded_bbox, face_info) or (None, None)\n","    \"\"\"\n","    try:\n","        faces = detector(image, 1)\n","\n","        if len(faces) == 0:\n","            return None, None\n","\n","        if len(faces) > 1:\n","            faces = sorted(faces, key=lambda r: r.width() * r.height(), reverse=True)\n","\n","        face = faces[0]\n","\n","        img_h, img_w = image.shape[:2]\n","\n","        face_left = face.left()\n","        face_top = face.top()\n","        face_right = face.right()\n","        face_bottom = face.bottom()\n","        face_width = face.width()\n","        face_height = face.height()\n","\n","        expanded_left = max(0, face_left - BBOX_EXPANSION)\n","        expanded_top = max(0, face_top - BBOX_EXPANSION)\n","        expanded_right = min(img_w, face_right + BBOX_EXPANSION)\n","        expanded_bottom = min(img_h, face_bottom + BBOX_EXPANSION)\n","\n","        expanded_bbox = {\n","            'left': expanded_left,\n","            'top': expanded_top,\n","            'right': expanded_right,\n","            'bottom': expanded_bottom,\n","            'width': expanded_right - expanded_left,\n","            'height': expanded_bottom - expanded_top\n","        }\n","\n","        face_info = {\n","            'original_bbox': {\n","                'left': face_left,\n","                'top': face_top,\n","                'right': face_right,\n","                'bottom': face_bottom,\n","                'width': face_width,\n","                'height': face_height\n","            },\n","            'expanded_bbox': expanded_bbox,\n","            'expansion_applied': BBOX_EXPANSION,\n","            'face_area': face_width * face_height\n","        }\n","\n","        return expanded_bbox, face_info\n","\n","    except Exception as e:\n","        return None, None\n","\n","def ensure_minimum_size(image, min_size=224):\n","    \"\"\"\n","    Ensure image is at least min_size x min_size.\n","\n","    Args:\n","        image: Input image\n","        min_size: Minimum dimension required\n","\n","    Returns:\n","        Tuple (resized_image, resize_info)\n","    \"\"\"\n","    h, w = image.shape[:2]\n","\n","    if h >= min_size and w >= min_size:\n","        return image, {'resized': False, 'original_size': (w, h)}\n","\n","    scale_factor = min_size / min(h, w)\n","    new_width = int(w * scale_factor)\n","    new_height = int(h * scale_factor)\n","\n","    resized = cv2.resize(image, (new_width, new_height),\n","                        interpolation=cv2.INTER_LANCZOS4)\n","\n","    resize_info = {\n","        'resized': True,\n","        'original_size': (w, h),\n","        'new_size': (new_width, new_height),\n","        'scale_factor': scale_factor\n","    }\n","\n","    return resized, resize_info\n","\n","def convert_to_grayscale(image):\n","    \"\"\"\n","    Convert RGB/BGR image to grayscale.\n","\n","    Args:\n","        image: Input RGB/BGR image\n","\n","    Returns:\n","        Grayscale image\n","    \"\"\"\n","    if len(image.shape) == 3:\n","        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    return image\n","\n","# ============================================================================\n","# COMPLETE PREPROCESSING PIPELINE\n","# ============================================================================\n","\n","def preprocess_image_aligned_eyemask(image_path):\n","    \"\"\"\n","    Complete preprocessing pipeline with face alignment and eye masking.\n","\n","    Pipeline:\n","    1. Load image\n","    2. Ensure minimum size\n","    3. Face alignment (horizontal eye alignment)\n","    4. Detect face and expand bbox\n","    5. Crop expanded bbox\n","    6. Resize to 224x224\n","    7. Convert to grayscale\n","    8. Apply eye masking\n","\n","    Args:\n","        image_path: Path to input image\n","\n","    Returns:\n","        Tuple (preprocessed_image, processing_info)\n","    \"\"\"\n","    processing_info = {\n","        'face_detected': False,\n","        'face_aligned': False,\n","        'bbox_expanded': False,\n","        'eye_masked': False,\n","        'resize_applied': False,\n","        'method': 'unknown'\n","    }\n","\n","    try:\n","        # Step 1: Load image\n","        image = cv2.imread(image_path)\n","\n","        if image is None:\n","            processing_info['method'] = 'error'\n","            processing_info['error'] = 'Failed to load image'\n","            return None, processing_info\n","\n","        # Step 2: Ensure minimum size\n","        image, resize_info = ensure_minimum_size(image, TARGET_SIZE)\n","        processing_info['resize_applied'] = resize_info['resized']\n","\n","        # Step 3: Face alignment\n","        aligned_image, alignment_info = align_face_horizontal(image)\n","        processing_info['face_aligned'] = alignment_info['aligned']\n","        processing_info['alignment_angle'] = alignment_info.get('angle', 0)\n","\n","        # Step 4: Detect face and get expanded bbox\n","        expanded_bbox, face_info = detect_face_with_expansion(aligned_image)\n","\n","        if expanded_bbox is not None:\n","            processing_info['face_detected'] = True\n","            processing_info['bbox_expanded'] = True\n","\n","            # Step 5: Crop expanded bbox\n","            x1 = expanded_bbox['left']\n","            y1 = expanded_bbox['top']\n","            x2 = expanded_bbox['right']\n","            y2 = expanded_bbox['bottom']\n","\n","            cropped = aligned_image[y1:y2, x1:x2]\n","            processing_info['method'] = 'aligned_expanded_eyemask'\n","\n","        else:\n","            # Fallback: center crop\n","            h, w = aligned_image.shape[:2]\n","            center_x = w // 2\n","            center_y = h // 2\n","            half_size = TARGET_SIZE // 2\n","\n","            x1 = max(0, center_x - half_size)\n","            y1 = max(0, center_y - half_size)\n","            x2 = min(w, center_x + half_size)\n","            y2 = min(h, center_y + half_size)\n","\n","            cropped = aligned_image[y1:y2, x1:x2]\n","            processing_info['method'] = 'fallback_aligned'\n","\n","        # Step 6: Resize to exact 224x224\n","        if cropped.shape[0] != TARGET_SIZE or cropped.shape[1] != TARGET_SIZE:\n","            cropped = cv2.resize(cropped, (TARGET_SIZE, TARGET_SIZE),\n","                               interpolation=cv2.INTER_LANCZOS4)\n","            processing_info['final_resize_applied'] = True\n","\n","        # Step 7: Convert to grayscale\n","        grayscale = convert_to_grayscale(cropped)\n","\n","        # Step 8: Apply eye masking\n","        masked, masking_info = apply_eye_mask(grayscale, EYE_MASK_STRATEGY)\n","        processing_info['eye_masked'] = masking_info['masked']\n","        processing_info['eye_mask_strategy'] = masking_info['strategy']\n","        processing_info['eye_regions_masked'] = masking_info['regions_masked']\n","\n","        return masked, processing_info\n","\n","    except Exception as e:\n","        processing_info['method'] = 'error'\n","        processing_info['error'] = str(e)\n","        return None, processing_info\n","\n","# ============================================================================\n","# DATASET PROCESSING\n","# ============================================================================\n","\n","def process_dataset_aligned_eyemask(source_dir, target_dir, variant_name):\n","    \"\"\"\n","    Process entire dataset with alignment and eye masking.\n","\n","    Args:\n","        source_dir: Source dataset directory\n","        target_dir: Target directory\n","        variant_name: Dataset variant\n","\n","    Returns:\n","        Processing statistics dictionary\n","    \"\"\"\n","    stats = {\n","        'variant': variant_name,\n","        'processing_date': datetime.now().isoformat(),\n","        'source_directory': source_dir,\n","        'target_directory': target_dir,\n","        'preprocessing_method': 'face_aligned_bbox_expansion_eye_masked',\n","        'preprocessing_parameters': {\n","            'target_size': TARGET_SIZE,\n","            'bbox_expansion': BBOX_EXPANSION,\n","            'eye_mask_strategy': EYE_MASK_STRATEGY,\n","            'eye_mask_margin': EYE_MASK_MARGIN\n","        },\n","        'preprocessing_steps': [\n","            'Load image',\n","            'Ensure minimum size 224x224',\n","            'Face alignment (horizontal eye alignment)',\n","            'Face detection with Dlib',\n","            f'Expand bbox by {BBOX_EXPANSION}px in all directions',\n","            'Crop expanded bbox region',\n","            'Resize to 224x224',\n","            'Grayscale conversion',\n","            f'Eye masking ({EYE_MASK_STRATEGY})'\n","        ],\n","        'splits': {},\n","        'total_processed': 0,\n","        'total_errors': 0,\n","        'alignment_stats': {\n","            'total_images': 0,\n","            'faces_aligned': 0,\n","            'alignment_rate': 0,\n","            'avg_rotation_angle': 0\n","        },\n","        'eye_masking_stats': {\n","            'total_images': 0,\n","            'eyes_masked': 0,\n","            'masking_rate': 0\n","        }\n","    }\n","\n","    splits = ['train', 'val', 'test']\n","\n","    for split in splits:\n","        source_split_dir = os.path.join(source_dir, split)\n","        target_split_dir = os.path.join(target_dir, split)\n","\n","        if not os.path.exists(source_split_dir):\n","            print(f\"  Warning: {split} split not found in source\")\n","            continue\n","\n","        os.makedirs(target_split_dir, exist_ok=True)\n","\n","        image_files = [f for f in os.listdir(source_split_dir)\n","                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n","\n","        split_stats = {\n","            'total_images': len(image_files),\n","            'processed': 0,\n","            'errors': 0,\n","            'faces_aligned': 0,\n","            'eyes_masked': 0,\n","            'rotation_angles': [],\n","            'emotion_distribution': defaultdict(int)\n","        }\n","\n","        print(f\"  Processing {split} split: {len(image_files)} images\")\n","\n","        for idx, img_filename in enumerate(image_files, 1):\n","            source_path = os.path.join(source_split_dir, img_filename)\n","            target_path = os.path.join(target_split_dir, img_filename)\n","\n","            processed_img, proc_info = preprocess_image_aligned_eyemask(source_path)\n","\n","            if processed_img is not None:\n","                cv2.imwrite(target_path, processed_img)\n","                split_stats['processed'] += 1\n","\n","                if proc_info['face_aligned']:\n","                    split_stats['faces_aligned'] += 1\n","                    split_stats['rotation_angles'].append(\n","                        abs(proc_info.get('alignment_angle', 0))\n","                    )\n","\n","                if proc_info['eye_masked']:\n","                    split_stats['eyes_masked'] += 1\n","\n","                emotion = img_filename.split('_')[-1].replace('.jpg', '')\n","                split_stats['emotion_distribution'][emotion] += 1\n","            else:\n","                split_stats['errors'] += 1\n","                stats['total_errors'] += 1\n","\n","            if idx % 200 == 0:\n","                print(f\"    Progress: {idx}/{len(image_files)}\")\n","\n","        # Calculate rates\n","        if split_stats['total_images'] > 0:\n","            split_stats['alignment_rate'] = (\n","                split_stats['faces_aligned'] / split_stats['total_images']\n","            )\n","            split_stats['eye_masking_rate'] = (\n","                split_stats['eyes_masked'] / split_stats['total_images']\n","            )\n","\n","        if split_stats['rotation_angles']:\n","            split_stats['avg_rotation_angle'] = float(\n","                np.mean(split_stats['rotation_angles'])\n","            )\n","\n","        # Store split results\n","        stats['splits'][split] = {\n","            'total_images': split_stats['total_images'],\n","            'processed': split_stats['processed'],\n","            'errors': split_stats['errors'],\n","            'faces_aligned': split_stats['faces_aligned'],\n","            'alignment_rate': split_stats.get('alignment_rate', 0),\n","            'avg_rotation_angle': split_stats.get('avg_rotation_angle', 0),\n","            'eyes_masked': split_stats['eyes_masked'],\n","            'eye_masking_rate': split_stats.get('eye_masking_rate', 0),\n","            'emotion_distribution': dict(split_stats['emotion_distribution'])\n","        }\n","\n","        stats['total_processed'] += split_stats['processed']\n","\n","        # Update overall stats\n","        stats['alignment_stats']['total_images'] += split_stats['total_images']\n","        stats['alignment_stats']['faces_aligned'] += split_stats['faces_aligned']\n","\n","        stats['eye_masking_stats']['total_images'] += split_stats['total_images']\n","        stats['eye_masking_stats']['eyes_masked'] += split_stats['eyes_masked']\n","\n","        print(f\"    Completed: {split_stats['processed']} processed\")\n","        print(f\"    Face aligned: {split_stats['faces_aligned']} ({split_stats.get('alignment_rate', 0):.1%})\")\n","        print(f\"    Eyes masked: {split_stats['eyes_masked']} ({split_stats.get('eye_masking_rate', 0):.1%})\")\n","        if split_stats.get('avg_rotation_angle', 0) > 0:\n","            print(f\"    Avg rotation: {split_stats['avg_rotation_angle']:.2f} degrees\")\n","\n","    # Calculate overall rates\n","    if stats['alignment_stats']['total_images'] > 0:\n","        stats['alignment_stats']['alignment_rate'] = (\n","            stats['alignment_stats']['faces_aligned'] /\n","            stats['alignment_stats']['total_images']\n","        )\n","\n","    if stats['eye_masking_stats']['total_images'] > 0:\n","        stats['eye_masking_stats']['masking_rate'] = (\n","            stats['eye_masking_stats']['eyes_masked'] /\n","            stats['eye_masking_stats']['total_images']\n","        )\n","\n","    return stats\n","\n","# ============================================================================\n","# MAIN EXECUTION\n","# ============================================================================\n","\n","print(\"=\" * 80)\n","print(\"CASME2 FACE ALIGNMENT + EYE MASKING PREPROCESSING\")\n","print(\"=\" * 80)\n","print()\n","\n","all_results = {}\n","\n","for mapping_name, config in DATASET_MAPPING.items():\n","    source_dir = config['source']\n","    target_dir = config['target']\n","    variant = config['variant']\n","    description = config['description']\n","\n","    print(f\"[{variant}] {description}\")\n","    print(f\"Source: {source_dir}\")\n","    print(f\"Target: {target_dir}\")\n","    print()\n","\n","    if not os.path.exists(source_dir):\n","        print(f\"  Error: Source directory not found\")\n","        print()\n","        continue\n","\n","    os.makedirs(target_dir, exist_ok=True)\n","\n","    processing_stats = process_dataset_aligned_eyemask(source_dir, target_dir, variant)\n","\n","    summary_path = os.path.join(target_dir, 'preprocessing_summary.json')\n","    with open(summary_path, 'w') as f:\n","        json.dump(processing_stats, f, indent=2)\n","\n","    all_results[variant] = processing_stats\n","\n","    print(f\"  Summary saved to: preprocessing_summary.json\")\n","    print(f\"  Total processed: {processing_stats['total_processed']}\")\n","    print(f\"  Alignment rate: {processing_stats['alignment_stats']['alignment_rate']:.1%}\")\n","    print(f\"  Eye masking rate: {processing_stats['eye_masking_stats']['masking_rate']:.1%}\")\n","    print()\n","    print(\"-\" * 80)\n","    print()\n","\n","# ============================================================================\n","# FINAL SUMMARY\n","# ============================================================================\n","\n","print(\"=\" * 80)\n","print(\"FACE ALIGNMENT + EYE MASKING PREPROCESSING COMPLETE\")\n","print(\"=\" * 80)\n","print()\n","\n","for variant, stats in all_results.items():\n","    print(f\"{variant} Dataset:\")\n","    print(f\"  Total processed: {stats['total_processed']} images\")\n","    print(f\"  Face alignment rate: {stats['alignment_stats']['alignment_rate']:.1%}\")\n","    print(f\"  Eye masking rate: {stats['eye_masking_stats']['masking_rate']:.1%}\")\n","    print(f\"  Errors: {stats['total_errors']}\")\n","\n","    for split in ['train', 'val', 'test']:\n","        if split in stats['splits']:\n","            split_data = stats['splits'][split]\n","            print(f\"  {split.upper()}: {split_data['processed']} images\")\n","\n","    print()\n","\n","print(\"All datasets with alignment and eye masking saved to:\")\n","for config in DATASET_MAPPING.values():\n","    print(f\"  - {config['target']}\")\n","\n","print()\n","print(\"Next step: Validation Cell 6 to compare v7/v8/v9 vs v10/v11/v12\")\n","print()\n","print(\"=\" * 80)"],"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"id":"Wdg80f9WJUsO","executionInfo":{"status":"error","timestamp":1760889068918,"user_tz":-420,"elapsed":334755,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"2cea161c-f44f-4c16-dc15-8a18e26e6126"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Initializing Dlib face detector and landmark predictor...\n","Downloading shape predictor model...\n","Face detector and landmark predictor loaded\n","\n","================================================================================\n","CASME2 FACE ALIGNMENT + EYE MASKING PREPROCESSING\n","================================================================================\n","\n","[AF] AF - Apex Frame (aligned + eye masked, grayscale 224x224)\n","Source: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v1\n","Target: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v10\n","\n","  Processing train split: 201 images\n","    Progress: 200/201\n","    Completed: 201 processed\n","    Face aligned: 0 (0.0%)\n","    Eyes masked: 201 (100.0%)\n","  Processing val split: 26 images\n","    Completed: 26 processed\n","    Face aligned: 0 (0.0%)\n","    Eyes masked: 26 (100.0%)\n","  Processing test split: 28 images\n","    Completed: 28 processed\n","    Face aligned: 0 (0.0%)\n","    Eyes masked: 28 (100.0%)\n","  Summary saved to: preprocessing_summary.json\n","  Total processed: 255\n","  Alignment rate: 0.0%\n","  Eye masking rate: 100.0%\n","\n","--------------------------------------------------------------------------------\n","\n","[KFS] KFS - Key Frame Sequence (aligned + eye masked, grayscale 224x224)\n","Source: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v2\n","Target: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v11\n","\n","  Processing train split: 603 images\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2133766750.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m     \u001b[0mprocessing_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_dataset_aligned_eyemask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0msummary_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'preprocessing_summary.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2133766750.py\u001b[0m in \u001b[0;36mprocess_dataset_aligned_eyemask\u001b[0;34m(source_dir, target_dir, variant_name)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0mtarget_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_split_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0mprocessed_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image_aligned_eyemask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprocessed_img\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2133766750.py\u001b[0m in \u001b[0;36mpreprocess_image_aligned_eyemask\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;31m# Step 3: Face alignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0maligned_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malignment_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign_face_horizontal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0mprocessing_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'face_aligned'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignment_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aligned'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mprocessing_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alignment_angle'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignment_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'angle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2133766750.py\u001b[0m in \u001b[0;36malign_face_horizontal\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Detect faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}