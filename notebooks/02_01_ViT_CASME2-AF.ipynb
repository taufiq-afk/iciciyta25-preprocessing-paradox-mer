{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyPnNSYrrNQnoLVlWkUTQdHJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d391aece1f034468bdf8ecfe23652dd3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_44f77eae167641ce84e6e0a99070a091","IPY_MODEL_92b57e4acb6a4454aa98c3cfaaf2c9fc","IPY_MODEL_e90802defc0e44cb9e4a81cfe367d4cd"],"layout":"IPY_MODEL_d7d3939569f8477b8bdce8c08adb8a58"}},"44f77eae167641ce84e6e0a99070a091":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_549db3ecae184e49aa5aa02303c93bbc","placeholder":"​","style":"IPY_MODEL_f3173f75bed54fbb80737fffe9654c35","value":"Fetching 1 files: 100%"}},"92b57e4acb6a4454aa98c3cfaaf2c9fc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95e100013ee44f4881ffff8ec610e143","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a949489f327b40ccabe8f08616dbb0b4","value":1}},"e90802defc0e44cb9e4a81cfe367d4cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c414cc81edc46b49ff32122e98ebabc","placeholder":"​","style":"IPY_MODEL_bf63a2db3db84736a22ce3246af8b074","value":" 1/1 [00:01&lt;00:00,  1.10s/it]"}},"d7d3939569f8477b8bdce8c08adb8a58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"549db3ecae184e49aa5aa02303c93bbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3173f75bed54fbb80737fffe9654c35":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95e100013ee44f4881ffff8ec610e143":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a949489f327b40ccabe8f08616dbb0b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4c414cc81edc46b49ff32122e98ebabc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf63a2db3db84736a22ce3246af8b074":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"370316a5df45494db886001e96568018":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d92034c0bfdf40c485ccb22d4d8c57f2","IPY_MODEL_1d9f23185f0541eb855a07408b44dce8","IPY_MODEL_94013efa01294c44b25dacbc6d572a32"],"layout":"IPY_MODEL_833bc2198ba04de2a584a563eccffcd3"}},"d92034c0bfdf40c485ccb22d4d8c57f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a01eb8d58a66440c956226c80f7740ad","placeholder":"​","style":"IPY_MODEL_a659300e0f4c4862be62e06e23a8580d","value":"preprocessor_config.json: 100%"}},"1d9f23185f0541eb855a07408b44dce8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cf2a5ed83604a8e9073b31b7865c1a0","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_261331618215489ab81ad80b2618acaa","value":160}},"94013efa01294c44b25dacbc6d572a32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_239af11342b247cf8e362251d231bfc3","placeholder":"​","style":"IPY_MODEL_2c2cb7c0a80d47769b69a275d953b5fb","value":" 160/160 [00:00&lt;00:00, 17.1kB/s]"}},"833bc2198ba04de2a584a563eccffcd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a01eb8d58a66440c956226c80f7740ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a659300e0f4c4862be62e06e23a8580d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0cf2a5ed83604a8e9073b31b7865c1a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"261331618215489ab81ad80b2618acaa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"239af11342b247cf8e362251d231bfc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c2cb7c0a80d47769b69a275d953b5fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f864d2efbc6a4bbea9985a3b965dae2e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae2632568dab4771af8881cbee4f8e1d","IPY_MODEL_a3c5478045da4485afa0436f13467800","IPY_MODEL_fb8973166745412883000b1777cbc881"],"layout":"IPY_MODEL_c61cc5b772204fbb8e5c736c751cb9ef"}},"ae2632568dab4771af8881cbee4f8e1d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3e142130260414491a91369d11633f6","placeholder":"​","style":"IPY_MODEL_0f3dd9e118274daa8af5622069bb32ba","value":"config.json: 100%"}},"a3c5478045da4485afa0436f13467800":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f86b1b73a5ce4acfa895c9bea6adbc49","max":502,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ce317716178430992a3d553dd061faf","value":502}},"fb8973166745412883000b1777cbc881":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac49d1d7ac4048d9848e5d9ffd8983dd","placeholder":"​","style":"IPY_MODEL_4818d66d81854c72a9b75a84decfed1c","value":" 502/502 [00:00&lt;00:00, 69.3kB/s]"}},"c61cc5b772204fbb8e5c736c751cb9ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3e142130260414491a91369d11633f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f3dd9e118274daa8af5622069bb32ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f86b1b73a5ce4acfa895c9bea6adbc49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ce317716178430992a3d553dd061faf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac49d1d7ac4048d9848e5d9ffd8983dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4818d66d81854c72a9b75a84decfed1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69fdd6709b3a4f37baacfe2fe788f255":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_71565fcc1be540eea1b0475c7a83eb69","IPY_MODEL_c4e3b101f6d242ef83a219a1a324d250","IPY_MODEL_6de43efcf9814f94af38dbaeb1125b4e"],"layout":"IPY_MODEL_8200cb10112447c4a3c0a16412d3c1da"}},"71565fcc1be540eea1b0475c7a83eb69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65473cf813aa4332974c4cd74d3d218f","placeholder":"​","style":"IPY_MODEL_c8e26efb6d094578a2bab0dce43331ef","value":"model.safetensors: 100%"}},"c4e3b101f6d242ef83a219a1a324d250":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f200469964bc44dabcacc1a59f49d49a","max":345579424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e35a09c4f254b7c9f1a893fe0cbbbef","value":345579424}},"6de43efcf9814f94af38dbaeb1125b4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae7c01691cf64d54b509f2bea6ea4be3","placeholder":"​","style":"IPY_MODEL_667f7b2711c342488cfedeaef6d2160a","value":" 346M/346M [00:02&lt;00:00, 274MB/s]"}},"8200cb10112447c4a3c0a16412d3c1da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65473cf813aa4332974c4cd74d3d218f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8e26efb6d094578a2bab0dce43331ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f200469964bc44dabcacc1a59f49d49a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e35a09c4f254b7c9f1a893fe0cbbbef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae7c01691cf64d54b509f2bea6ea4be3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"667f7b2711c342488cfedeaef6d2160a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Intha79l_NL8","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d391aece1f034468bdf8ecfe23652dd3","44f77eae167641ce84e6e0a99070a091","92b57e4acb6a4454aa98c3cfaaf2c9fc","e90802defc0e44cb9e4a81cfe367d4cd","d7d3939569f8477b8bdce8c08adb8a58","549db3ecae184e49aa5aa02303c93bbc","f3173f75bed54fbb80737fffe9654c35","95e100013ee44f4881ffff8ec610e143","a949489f327b40ccabe8f08616dbb0b4","4c414cc81edc46b49ff32122e98ebabc","bf63a2db3db84736a22ce3246af8b074","370316a5df45494db886001e96568018","d92034c0bfdf40c485ccb22d4d8c57f2","1d9f23185f0541eb855a07408b44dce8","94013efa01294c44b25dacbc6d572a32","833bc2198ba04de2a584a563eccffcd3","a01eb8d58a66440c956226c80f7740ad","a659300e0f4c4862be62e06e23a8580d","0cf2a5ed83604a8e9073b31b7865c1a0","261331618215489ab81ad80b2618acaa","239af11342b247cf8e362251d231bfc3","2c2cb7c0a80d47769b69a275d953b5fb","f864d2efbc6a4bbea9985a3b965dae2e","ae2632568dab4771af8881cbee4f8e1d","a3c5478045da4485afa0436f13467800","fb8973166745412883000b1777cbc881","c61cc5b772204fbb8e5c736c751cb9ef","f3e142130260414491a91369d11633f6","0f3dd9e118274daa8af5622069bb32ba","f86b1b73a5ce4acfa895c9bea6adbc49","7ce317716178430992a3d553dd061faf","ac49d1d7ac4048d9848e5d9ffd8983dd","4818d66d81854c72a9b75a84decfed1c","69fdd6709b3a4f37baacfe2fe788f255","71565fcc1be540eea1b0475c7a83eb69","c4e3b101f6d242ef83a219a1a324d250","6de43efcf9814f94af38dbaeb1125b4e","8200cb10112447c4a3c0a16412d3c1da","65473cf813aa4332974c4cd74d3d218f","c8e26efb6d094578a2bab0dce43331ef","f200469964bc44dabcacc1a59f49d49a","7e35a09c4f254b7c9f1a893fe0cbbbef","ae7c01691cf64d54b509f2bea6ea4be3","667f7b2711c342488cfedeaef6d2160a"]},"executionInfo":{"status":"ok","timestamp":1759063346174,"user_tz":-420,"elapsed":63243,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"0f9b41e8-8a07-432c-99fc-3a11d58a0264","collapsed":true,"cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","CASME II ViT OPTIMIZED BASELINE INFRASTRUCTURE\n","============================================================\n","\n","[1] Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully\n","\n","[2] Importing required libraries...\n","CASME II ViT Optimized Baseline - Infrastructure Configuration\n","=========================================================\n","\n","Validating infrastructure and checkpoint requirements...\n","Drive access: OK\n","Metadata files: OK\n","Disk space: OK\n","Directory permissions: OK\n","Infrastructure validation: ALL CHECKS PASSED\n","\n","Loading CASME II dataset metadata...\n","Dataset: CASME2\n","Total samples: 255\n","Split strategy: stratified_80_10_10\n","Using ViT-Base Patch16 for fine-grained micro-expression analysis\n","\n","==================================================\n","OPTIMIZED EXPERIMENT CONFIGURATION\n","==================================================\n","Loss Function: Focal Loss\n","  Gamma: 2.0\n","  Alpha Weights (per-class): [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","  Alpha Sum Validation: 0.999\n","ViT Model: google/vit-base-patch16-224-in21k\n","Patch Size: 16px\n","==================================================\n","\n","Device: cuda\n","GPU: NVIDIA L4 (23.8 GB)\n","L4: Balanced performance configuration\n","\n","Train distribution: {'others': 79, 'disgust': 50, 'happiness': 25, 'repression': 21, 'surprise': 20, 'sadness': 5, 'fear': 1}\n","Validation distribution: {'others': 10, 'disgust': 6, 'happiness': 3, 'repression': 3, 'surprise': 2, 'sadness': 1, 'fear': 1}\n","Test distribution: {'others': 10, 'disgust': 7, 'happiness': 4, 'repression': 3, 'surprise': 3, 'sadness': 1}\n","Applied Focal Loss alpha weights: [0.053 0.067 0.094 0.102 0.106 0.201 0.376]\n","Alpha weights sum: 0.999\n","\n","ViT Configuration Summary:\n","  Model: google/vit-base-patch16-224-in21k\n","  Input size: 384px\n","  Learning rate: 1e-05\n","  Batch size: 16\n","\n","Setting up ViT Image Processor for 384px input...\n"]},{"output_type":"display_data","data":{"text/plain":["Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d391aece1f034468bdf8ecfe23652dd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"370316a5df45494db886001e96568018"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ViT Image Processor configured for 384px with interpolation support\n","\n","Creating checkpoint directories with enhanced validation...\n","All directories created successfully:\n","  ✓ /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/02_01_vit_casme2-af\n","  ✓ /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/02_01_vit_casme2-af/training_logs\n","  ✓ /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/02_01_vit_casme2-af/evaluation_results\n","\n","Dataset paths:\n","Train: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split/train\n","Validation: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split/val\n","Test: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split/test\n","\n","ViT CASME II architecture validation...\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f864d2efbc6a4bbea9985a3b965dae2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69fdd6709b3a4f37baacfe2fe788f255"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["ViT feature dimension: 768\n","ViT CASME II: 768 -> 512 -> 128 -> 7\n","Validation successful: Output shape torch.Size([1, 7])\n","Expected tokens for 384px with patch16: 576 tokens\n","\n","============================================================\n","CASME II ViT OPTIMIZED BASELINE CONFIGURATION COMPLETE\n","============================================================\n","Loss Configuration:\n","  Function: Optimized Focal Loss\n","  Gamma: 2.0\n","  Per-class Alpha: [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","  Alpha Sum: 0.999\n","\n","Model Configuration:\n","  Architecture: google/vit-base-patch16-224-in21k\n","  Patch Size: 16px\n","  Input Resolution: 384px\n","  Expected Tokens: 576\n","\n","Dataset Configuration:\n","  Classes: 7\n","  Weight Optimization: Per-class Alpha\n","\n","Checkpoint Infrastructure Status:\n","  Directory Creation: SUCCESS\n","  Disk Space: OK\n","  Permissions: OK\n","\n","Next: Cell 2 - Dataset Loading and Optimized Training Pipeline\n"]}],"source":["# @title Cell 1: CASME II ViT Infrastructure Configuration\n","\n","# File: 02_01_ViT_Direct_Optimized_Baseline_Cell1.py\n","# Location: experiments/02_01_ViT_Direct_Baseline.ipynb\n","# Purpose: Optimized ViT-Base for CASME II micro-expression recognition with advanced class weight optimization and focal loss\n","\n","# Mount Google Drive\n","from google.colab import drive\n","print(\"=\" * 60)\n","print(\"CASME II ViT OPTIMIZED BASELINE INFRASTRUCTURE\")\n","print(\"=\" * 60)\n","print(\"\\n[1] Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Google Drive mounted successfully\")\n","\n","print(\"\\n[2] Importing required libraries...\")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import timm\n","import json\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import time\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Project paths configuration - preserved existing structure\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","DATASET_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/data_split_v1\"\n","CHECKPOINT_ROOT = f\"{PROJECT_ROOT}/models/02_01_vit_casme2-af\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/02_01_vit_casme2-af\"\n","\n","# Load CASME II dataset metadata - preserved existing paths\n","METADATA_TRAIN = f\"{DATASET_ROOT}/split_metadata.json\"\n","PROCESSING_SUMMARY = f\"{DATASET_ROOT}/processing_summary.json\"\n","\n","print(\"CASME II ViT Optimized Baseline - Infrastructure Configuration\")\n","print(\"=========================================================\")\n","\n","# Load dataset metadata\n","print(\"Loading CASME II dataset metadata...\")\n","with open(METADATA_TRAIN, 'r') as f:\n","    casme2_metadata = json.load(f)\n","\n","with open(PROCESSING_SUMMARY, 'r') as f:\n","    processing_info = json.load(f)\n","\n","print(f\"Dataset: {processing_info['dataset']}\")\n","print(f\"Total samples: {processing_info['total_samples']}\")\n","print(f\"Split strategy: {processing_info['split_strategy']}\")\n","\n","# =====================================================\n","# ADVANCED EXPERIMENT CONFIGURATION - Optimized Parameters\n","# =====================================================\n","\n","# FOCAL LOSS CONFIGURATION - Toggle and Advanced Parameters\n","USE_FOCAL_LOSS = True  # Set True to enable Focal Loss, False for CrossEntropy\n","FOCAL_LOSS_GAMMA = 2.0  # Focal loss focusing parameter (typically 1.0 - 3.0)\n","\n","# OPTIMIZED CLASS WEIGHTS CONFIGURATION - Inverse Square Root Frequency Approach\n","# CASME II classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","# Train distribution: [99, 63, 32, 27, 25, 7, 2] - inverse sqrt frequency approach\n","\n","# CrossEntropy Loss - Optimized inverse square root frequency weights\n","CROSSENTROPY_CLASS_WEIGHTS = [1.00, 1.25, 1.76, 1.91, 1.99, 3.76, 7.04]\n","\n","# Focal Loss - Normalized per-class alpha values (sum = 1.0)\n","FOCAL_LOSS_ALPHA_WEIGHTS = [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","\n","# ViT MODEL CONFIGURATION - Support patch16 and patch32\n","VIT_MODEL_VARIANT = 'patch16'  # Options: 'patch16' or 'patch32'\n","\n","# Dynamic ViT model selection based on patch size\n","if VIT_MODEL_VARIANT == 'patch16':\n","    VIT_MODEL_NAME = 'google/vit-base-patch16-224-in21k'\n","    PATCH_SIZE = 16\n","    print(\"Using ViT-Base Patch16 for fine-grained micro-expression analysis\")\n","elif VIT_MODEL_VARIANT == 'patch32':\n","    VIT_MODEL_NAME = 'google/vit-base-patch32-224-in21k'\n","    PATCH_SIZE = 32\n","    print(\"Using ViT-Base Patch32 for efficient micro-expression recognition\")\n","else:\n","    raise ValueError(f\"Unsupported VIT_MODEL_VARIANT: {VIT_MODEL_VARIANT}\")\n","\n","# Display experiment configuration\n","print(\"\\n\" + \"=\" * 50)\n","print(\"OPTIMIZED EXPERIMENT CONFIGURATION\")\n","print(\"=\" * 50)\n","print(f\"Loss Function: {'Focal Loss' if USE_FOCAL_LOSS else 'CrossEntropy Loss'}\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Alpha Weights (per-class): {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum Validation: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Class Weights (inverse sqrt freq): {CROSSENTROPY_CLASS_WEIGHTS}\")\n","print(f\"ViT Model: {VIT_MODEL_NAME}\")\n","print(f\"Patch Size: {PATCH_SIZE}px\")\n","print(\"=\" * 50)\n","\n","# Enhanced GPU configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n","gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n","\n","print(f\"\\nDevice: {device}\")\n","print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n","\n","# Hardware-optimized batch size for 384px input\n","if 'A100' in gpu_name:\n","    BATCH_SIZE = 24\n","    NUM_WORKERS = 8\n","    torch.backends.cudnn.benchmark = True\n","    print(\"A100: Optimized batch size for ViT-Base 384px\")\n","elif 'L4' in gpu_name:\n","    BATCH_SIZE = 16\n","    NUM_WORKERS = 6\n","    torch.backends.cudnn.benchmark = True\n","    print(\"L4: Balanced performance configuration\")\n","else:\n","    BATCH_SIZE = 8\n","    NUM_WORKERS = 4\n","    print(\"Default GPU: Conservative settings\")\n","\n","# CASME II class mapping and analysis - preserved existing structure\n","CASME2_CLASSES = ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CASME2_CLASSES)}\n","\n","# Analyze class distribution from metadata\n","train_dist = casme2_metadata['train']['class_distribution']\n","val_dist = casme2_metadata['val']['class_distribution']\n","test_dist = casme2_metadata['test']['class_distribution']\n","\n","print(f\"\\nTrain distribution: {train_dist}\")\n","print(f\"Validation distribution: {val_dist}\")\n","print(f\"Test distribution: {test_dist}\")\n","\n","# Apply optimized class weights based on loss function selection\n","if USE_FOCAL_LOSS:\n","    # For Focal Loss - use normalized alpha weights (per-class importance)\n","    class_weights = torch.tensor(FOCAL_LOSS_ALPHA_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied Focal Loss alpha weights: {class_weights.cpu().numpy()}\")\n","    print(f\"Alpha weights sum: {class_weights.sum().item():.3f}\")\n","else:\n","    # For CrossEntropy - use inverse sqrt frequency weights\n","    class_weights = torch.tensor(CROSSENTROPY_CLASS_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied CrossEntropy class weights: {class_weights.cpu().numpy()}\")\n","\n","# CASME II ViT Optimized Configuration\n","CASME2_VIT_CONFIG = {\n","    # Architecture configuration - enhanced flexibility\n","    'vit_model': VIT_MODEL_NAME,\n","    'patch_size': PATCH_SIZE,\n","    'input_size': 384,\n","    'num_classes': 7,\n","    'dropout_rate': 0.2,\n","    'expected_feature_dim': 768,\n","    'interpolate_pos_encoding': True,\n","\n","    # Training configuration (proven effective from medical imaging)\n","    'learning_rate': 1e-5,\n","    'weight_decay': 1e-5,\n","    'gradient_clip': 1.0,\n","    'num_epochs': 50,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'device': device,\n","\n","    # Scheduler configuration\n","    'scheduler_type': 'plateau',\n","    'scheduler_mode': 'max',\n","    'scheduler_factor': 0.5,\n","    'scheduler_patience': 3,\n","    'scheduler_min_lr': 1e-6,\n","    'scheduler_monitor': 'val_f1_macro',\n","\n","    # Optimized loss configuration\n","    'use_focal_loss': USE_FOCAL_LOSS,\n","    'focal_loss_gamma': FOCAL_LOSS_GAMMA,\n","    'focal_loss_alpha_weights': FOCAL_LOSS_ALPHA_WEIGHTS,\n","    'crossentropy_class_weights': CROSSENTROPY_CLASS_WEIGHTS,\n","    'class_weights': class_weights,\n","\n","    # Evaluation configuration\n","    'use_macro_avg': True,\n","    'early_stopping': False,\n","    'save_best_f1': True,\n","    'save_strategy': 'best_only'\n","}\n","\n","print(f\"\\nViT Configuration Summary:\")\n","print(f\"  Model: {CASME2_VIT_CONFIG['vit_model']}\")\n","print(f\"  Input size: {CASME2_VIT_CONFIG['input_size']}px\")\n","print(f\"  Learning rate: {CASME2_VIT_CONFIG['learning_rate']}\")\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","\n","# =====================================================\n","# ADVANCED FOCAL LOSS IMPLEMENTATION - Per-Class Alpha Support\n","# =====================================================\n","\n","class OptimizedFocalLoss(nn.Module):\n","    \"\"\"\n","    Advanced Focal Loss implementation with per-class alpha support\n","    Paper: \"Focal Loss for Dense Object Detection\" (Lin et al., 2017)\n","\n","    Enhanced Formula: FL(p_t) = -α_t(1-p_t)^γ log(p_t)\n","\n","    Args:\n","        alpha (list/tensor): Per-class alpha weights (must sum to 1.0)\n","        gamma (float): Focusing parameter for hard examples (default: 2.0)\n","        reduction (str): Reduction method ('mean', 'sum', 'none')\n","    \"\"\"\n","\n","    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n","        super(OptimizedFocalLoss, self).__init__()\n","\n","        if alpha is not None:\n","            if isinstance(alpha, list):\n","                self.alpha = torch.tensor(alpha, dtype=torch.float32)\n","            else:\n","                self.alpha = alpha\n","\n","            # Validation: alpha should sum to 1.0 for proper normalization\n","            alpha_sum = self.alpha.sum().item()\n","            if abs(alpha_sum - 1.0) > 0.01:\n","                print(f\"Warning: Alpha weights sum to {alpha_sum:.3f}, expected 1.0\")\n","\n","        else:\n","            self.alpha = None\n","\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        # Calculate cross entropy loss\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","\n","        # Calculate p_t (probability of true class)\n","        pt = torch.exp(-ce_loss)\n","\n","        # Apply per-class alpha if provided\n","        if self.alpha is not None:\n","            if self.alpha.device != targets.device:\n","                self.alpha = self.alpha.to(targets.device)\n","            alpha_t = self.alpha.gather(0, targets)\n","        else:\n","            alpha_t = 1.0\n","\n","        # Apply focal loss formula: α_t(1-p_t)^γ * CE_loss\n","        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n","\n","        # Apply reduction\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","\n","# ViT Architecture for CASME II - preserved existing structure\n","class ViTCASME2Baseline(nn.Module):\n","    \"\"\"ViT baseline for CASME II micro-expression recognition with Hugging Face transformers\"\"\"\n","\n","    def __init__(self, num_classes, dropout_rate=0.2):\n","        super(ViTCASME2Baseline, self).__init__()\n","\n","        # Hugging Face ViT model with interpolation support\n","        from transformers import ViTModel\n","\n","        self.vit = ViTModel.from_pretrained(\n","            CASME2_VIT_CONFIG['vit_model'],\n","            add_pooling_layer=False\n","        )\n","\n","        # Enable fine-tuning for micro-expression domain\n","        for param in self.vit.parameters():\n","            param.requires_grad = True\n","\n","        # Get ViT feature dimensions\n","        self.vit_feature_dim = self.vit.config.hidden_size\n","\n","        print(f\"ViT feature dimension: {self.vit_feature_dim}\")\n","\n","        # Classification head with LayerNorm for stability\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.vit_feature_dim, 512),\n","            nn.LayerNorm(512),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","\n","            nn.Linear(512, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","        )\n","\n","        # Final classification layer for CASME II classes\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","        print(f\"ViT CASME II: {self.vit_feature_dim} -> 512 -> 128 -> {num_classes}\")\n","\n","    def forward(self, pixel_values):\n","        # ViT forward pass with position embedding interpolation\n","        vit_outputs = self.vit(\n","            pixel_values=pixel_values,\n","            interpolate_pos_encoding=True\n","        )\n","\n","        # Extract CLS token features\n","        vit_features = vit_outputs.last_hidden_state[:, 0]\n","\n","        # Classification pipeline\n","        processed_features = self.classifier_layers(vit_features)\n","        output = self.classifier(processed_features)\n","\n","        return output\n","\n","# Enhanced optimizer and scheduler factory\n","def create_optimizer_scheduler_casme2(model, config):\n","    \"\"\"Create optimizer and scheduler for CASME II ViT training\"\"\"\n","\n","    # AdamW optimizer with proven configuration\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=config['learning_rate'],\n","        weight_decay=config['weight_decay'],\n","        betas=(0.9, 0.999)\n","    )\n","\n","    # ReduceLROnPlateau scheduler monitoring validation F1\n","    if config['scheduler_type'] == 'plateau':\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer,\n","            mode=config['scheduler_mode'],\n","            factor=config['scheduler_factor'],\n","            patience=config['scheduler_patience'],\n","            min_lr=config['scheduler_min_lr']\n","        )\n","        print(f\"Scheduler: ReduceLROnPlateau monitoring {config['scheduler_monitor']}\")\n","    else:\n","        scheduler = None\n","\n","    return optimizer, scheduler\n","\n","# ViT Image Processor setup for 384px input\n","from transformers import ViTImageProcessor\n","\n","print(\"\\nSetting up ViT Image Processor for 384px input...\")\n","\n","vit_processor = ViTImageProcessor.from_pretrained(\n","    CASME2_VIT_CONFIG['vit_model'],\n","    do_resize=True,\n","    size={'height': 384, 'width': 384},\n","    do_normalize=True,\n","    do_rescale=True,\n","    do_center_crop=False\n",")\n","\n","# Transform functions for ViT\n","def vit_transform_train(image):\n","    \"\"\"Training transform with ViT Image Processor\"\"\"\n","    inputs = vit_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","def vit_transform_val(image):\n","    \"\"\"Validation transform with ViT Image Processor\"\"\"\n","    inputs = vit_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","print(\"ViT Image Processor configured for 384px with interpolation support\")\n","\n","# Custom Dataset class for CASME II - preserved existing structure\n","class CASME2Dataset(Dataset):\n","    \"\"\"Custom dataset class for CASME II with JSON metadata support\"\"\"\n","\n","    def __init__(self, split_metadata, dataset_root, transform=None, split='train'):\n","        self.metadata = split_metadata[split]['samples']\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","        self.split = split\n","\n","        print(f\"Loaded {len(self.metadata)} samples for {split} split\")\n","\n","    def __len__(self):\n","        return len(self.metadata)\n","\n","    def __getitem__(self, idx):\n","        sample = self.metadata[idx]\n","\n","        # Load image\n","        image_path = os.path.join(self.dataset_root, self.split, sample['image_filename'])\n","        image = Image.open(image_path).convert('RGB')\n","\n","        # Apply transform\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Get class label\n","        emotion = sample['emotion']\n","        label = CLASS_TO_IDX[emotion]\n","\n","        return image, label, sample['sample_id']\n","\n","# Create directories - preserved existing paths\n","os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/training_logs\", exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/evaluation_results\", exist_ok=True)\n","\n","# Dataset paths - preserved existing structure\n","TRAIN_PATH = f\"{DATASET_ROOT}/train\"\n","VAL_PATH = f\"{DATASET_ROOT}/val\"\n","TEST_PATH = f\"{DATASET_ROOT}/test\"\n","\n","print(f\"\\nDataset paths:\")\n","print(f\"Train: {TRAIN_PATH}\")\n","print(f\"Validation: {VAL_PATH}\")\n","print(f\"Test: {TEST_PATH}\")\n","\n","# Enhanced architecture validation with dynamic patch calculation\n","print(\"\\nViT CASME II architecture validation...\")\n","\n","try:\n","    test_model = ViTCASME2Baseline(num_classes=7, dropout_rate=0.2).to(device)\n","    test_input = torch.randn(1, 3, 384, 384).to(device)\n","    test_output = test_model(test_input)\n","\n","    # Dynamic token calculation based on configured patch size\n","    expected_tokens = (384 // CASME2_VIT_CONFIG['patch_size']) ** 2\n","\n","    print(f\"Validation successful: Output shape {test_output.shape}\")\n","    print(f\"Expected tokens for 384px with patch{CASME2_VIT_CONFIG['patch_size']}: {expected_tokens} tokens\")\n","\n","    del test_model, test_input, test_output\n","    torch.cuda.empty_cache()\n","\n","except Exception as e:\n","    print(f\"Validation failed: {e}\")\n","\n","# Optimized loss function factory with advanced configuration\n","def create_criterion_casme2(weights, use_focal_loss=False, alpha_weights=None, gamma=2.0):\n","    \"\"\"\n","    Optimized factory function to create loss criterion based on advanced configuration\n","\n","    Args:\n","        weights (Tensor): Class weights for CrossEntropy (ignored if focal loss used)\n","        use_focal_loss (bool): Whether to use Focal Loss or CrossEntropy\n","        alpha_weights (list): Per-class alpha weights for Focal Loss (must sum to 1.0)\n","        gamma (float): Focal loss gamma parameter\n","\n","    Returns:\n","        Loss function (nn.Module)\n","    \"\"\"\n","    if use_focal_loss:\n","        print(f\"Using Optimized Focal Loss with gamma={gamma}\")\n","        if alpha_weights:\n","            print(f\"Per-class alpha weights: {alpha_weights}\")\n","            print(f\"Alpha sum: {sum(alpha_weights):.3f}\")\n","        return OptimizedFocalLoss(alpha=alpha_weights, gamma=gamma)\n","    else:\n","        print(f\"Using CrossEntropy Loss with optimized class weights\")\n","        print(f\"Class weights: {weights.cpu().numpy()}\")\n","        return nn.CrossEntropyLoss(weight=weights)\n","\n","# Global configuration for training pipeline - enhanced with optimized features\n","GLOBAL_CONFIG_CASME2 = {\n","    'device': device,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'num_classes': 7,\n","    'class_weights': class_weights,\n","    'class_names': CASME2_CLASSES,\n","    'class_to_idx': CLASS_TO_IDX,\n","    'transform_train': vit_transform_train,\n","    'transform_val': vit_transform_val,\n","    'vit_config': CASME2_VIT_CONFIG,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'results_root': RESULTS_ROOT,\n","    'train_path': TRAIN_PATH,\n","    'val_path': VAL_PATH,\n","    'test_path': TEST_PATH,\n","    'metadata': casme2_metadata,\n","    'optimizer_scheduler_factory': create_optimizer_scheduler_casme2,\n","    'criterion_factory': create_criterion_casme2\n","}\n","\n","# Configuration validation and summary\n","print(\"\\n\" + \"=\" * 60)\n","print(\"CASME II ViT OPTIMIZED BASELINE CONFIGURATION COMPLETE\")\n","print(\"=\" * 60)\n","\n","print(f\"Loss Configuration:\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Function: Optimized Focal Loss\")\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Per-class Alpha: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Function: CrossEntropy with Optimized Weights\")\n","    print(f\"  Class Weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","\n","print(f\"\\nModel Configuration:\")\n","print(f\"  Architecture: {VIT_MODEL_NAME}\")\n","print(f\"  Patch Size: {PATCH_SIZE}px\")\n","print(f\"  Input Resolution: 384px\")\n","print(f\"  Expected Tokens: {(384 // PATCH_SIZE) ** 2}\")\n","\n","print(f\"\\nDataset Configuration:\")\n","print(f\"  Classes: {len(CASME2_CLASSES)}\")\n","print(f\"  Weight Optimization: {'Per-class Alpha' if USE_FOCAL_LOSS else 'Inverse Sqrt Frequency'}\")\n","\n","print(\"\\nNext: Cell 2 - Dataset Loading and Optimized Training Pipeline\")"]},{"cell_type":"code","source":["# @title Cell 2: CASME II ViT Training Pipeline\n","\n","# File: 02_01_ViT_Direct_Enhanced_Baseline_Cell2.py\n","# Location: experiments/02_01_ViT_Direct_Baseline.ipynb\n","# Purpose: Enhanced training pipeline for CASME II ViT micro-expression recognition with robust checkpoint saving and error handling\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing as mp\n","\n","print(\"CASME II ViT Enhanced Training Pipeline with Configurable Loss\")\n","print(\"=\" * 65)\n","print(f\"Loss Function: {'Optimized Focal Loss' if CASME2_VIT_CONFIG['use_focal_loss'] else 'CrossEntropy Loss'}\")\n","if CASME2_VIT_CONFIG['use_focal_loss']:\n","    print(f\"Focal Loss Parameters:\")\n","    print(f\"  Gamma: {CASME2_VIT_CONFIG['focal_loss_gamma']}\")\n","    print(f\"  Per-class Alpha: {CASME2_VIT_CONFIG['focal_loss_alpha_weights']}\")\n","    print(f\"  Alpha Sum: {sum(CASME2_VIT_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","else:\n","    print(f\"CrossEntropy Parameters:\")\n","    print(f\"  Optimized Class Weights: {CASME2_VIT_CONFIG['crossentropy_class_weights']}\")\n","print(f\"Training epochs: {CASME2_VIT_CONFIG['num_epochs']}\")\n","print(f\"Scheduler patience: {CASME2_VIT_CONFIG['scheduler_patience']}\")\n","print(f\"Early stopping: {CASME2_VIT_CONFIG['early_stopping']}\")\n","\n","# Enhanced CASME II Dataset with optimized RAM caching - preserved existing structure\n","class CASME2DatasetTraining(Dataset):\n","    \"\"\"Enhanced CASME II dataset for training with RAM caching optimization\"\"\"\n","\n","    def __init__(self, split_metadata, dataset_root, transform=None, split='train', use_ram_cache=True):\n","        self.metadata = split_metadata[split]['samples']\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","        self.split = split\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.sample_ids = []\n","        self.cached_images = []\n","\n","        print(f\"Loading CASME II {split} dataset for training...\")\n","\n","        # Process metadata\n","        for sample in self.metadata:\n","            image_path = os.path.join(dataset_root, split, sample['image_filename'])\n","            self.images.append(image_path)\n","            self.labels.append(CLASS_TO_IDX[sample['emotion']])\n","            self.sample_ids.append(sample['sample_id'])\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples\")\n","        self._print_distribution()\n","\n","        # RAM caching for training efficiency\n","        if self.use_ram_cache:\n","            self._preload_to_ram()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def _preload_to_ram(self):\n","        \"\"\"RAM preloading for training efficiency\"\"\"\n","        print(f\"Preloading {len(self.images)} {self.split} images to RAM...\")\n","\n","        max_workers = min(mp.cpu_count(), 8)\n","        valid_images = 0\n","        self.cached_images = [None] * len(self.images)\n","\n","        for i, img_path in enumerate(tqdm(self.images, desc=f\"Loading {self.split}\")):\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (384, 384):\n","                    image = image.resize((384, 384), Image.Resampling.LANCZOS)\n","                self.cached_images[i] = image\n","                valid_images += 1\n","            except Exception as e:\n","                print(f\"Error loading {img_path}: {e}\")\n","                self.cached_images[i] = Image.new('RGB', (384, 384), (128, 128, 128))\n","\n","        ram_usage_gb = len(self.cached_images) * 384 * 384 * 3 * 4 / 1e9\n","        print(f\"{self.split.upper()} RAM caching completed: {valid_images} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (384, 384):\n","                    image = image.resize((384, 384), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (384, 384), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.sample_ids[idx]\n","\n","# Enhanced metrics calculation with comprehensive error handling\n","def calculate_metrics_safe_robust(outputs, labels, class_names, average='macro'):\n","    \"\"\"Calculate metrics with enhanced error handling and validation\"\"\"\n","    try:\n","        # Validate input tensors\n","        if outputs.size(0) != labels.size(0):\n","            raise ValueError(f\"Batch size mismatch: outputs {outputs.size(0)} vs labels {labels.size(0)}\")\n","\n","        if isinstance(outputs, torch.Tensor):\n","            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n","        else:\n","            predictions = np.array(outputs)\n","\n","        if isinstance(labels, torch.Tensor):\n","            labels = labels.cpu().numpy()\n","        else:\n","            labels = np.array(labels)\n","\n","        # Validate predictions are in valid range\n","        unique_preds = np.unique(predictions)\n","        unique_labels = np.unique(labels)\n","\n","        accuracy = accuracy_score(labels, predictions)\n","        precision, recall, f1, _ = precision_recall_fscore_support(\n","            labels, predictions,\n","            average=average,\n","            zero_division=0,\n","            labels=list(range(len(class_names)))\n","        )\n","\n","        return {\n","            'accuracy': float(accuracy),\n","            'precision': float(precision),\n","            'recall': float(recall),\n","            'f1_score': float(f1)\n","        }\n","    except Exception as e:\n","        print(f\"Warning: Enhanced metrics calculation error: {e}\")\n","        return {\n","            'accuracy': 0.0,\n","            'precision': 0.0,\n","            'recall': 0.0,\n","            'f1_score': 0.0\n","        }\n","\n","# Enhanced training epoch function with robust model output validation\n","def train_epoch(model, dataloader, criterion, optimizer, device, epoch, total_epochs):\n","    \"\"\"Enhanced training epoch with robust error handling and output validation\"\"\"\n","    model.train()\n","    running_loss = 0.0\n","    all_outputs = []\n","    all_labels = []\n","\n","    progress_bar = tqdm(dataloader, desc=f\"CASME II Training Epoch {epoch+1}/{total_epochs}\")\n","\n","    for batch_idx, (images, labels, sample_ids) in enumerate(progress_bar):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Enhanced model output validation - handle multiple output formats\n","        model_output = model(images)\n","\n","        # Robust output structure validation\n","        if isinstance(model_output, (tuple, list)):\n","            outputs = model_output[0]\n","        elif isinstance(model_output, dict):\n","            outputs = model_output.get('logits', model_output.get('prediction', model_output))\n","        else:\n","            outputs = model_output\n","\n","        # Validate output shape for 7 CASME II classes\n","        if outputs.dim() != 2 or outputs.size(1) != 7:\n","            raise ValueError(f\"Invalid CASME II output shape: {outputs.shape}, expected [batch_size, 7]\")\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), CASME2_VIT_CONFIG['gradient_clip'])\n","\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        # Memory optimized: Move to CPU before accumulating\n","        all_outputs.append(outputs.detach().cpu())\n","        all_labels.append(labels.detach().cpu())\n","\n","        # Update progress\n","        if batch_idx % 5 == 0:\n","            avg_loss = running_loss / (batch_idx + 1)\n","            current_lr = optimizer.param_groups[0]['lr']\n","            progress_bar.set_postfix({\n","                'Loss': f'{avg_loss:.4f}',\n","                'LR': f'{current_lr:.2e}'\n","            })\n","\n","    # Enhanced metrics calculation with error recovery\n","    try:\n","        epoch_outputs = torch.cat(all_outputs, dim=0)\n","        epoch_labels = torch.cat(all_labels, dim=0)\n","        metrics = calculate_metrics_safe_robust(epoch_outputs, epoch_labels, CASME2_CLASSES, average='macro')\n","    except Exception as e:\n","        print(f\"Warning: Training metrics calculation failed: {e}\")\n","        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n","\n","    avg_loss = running_loss / len(dataloader)\n","    return avg_loss, metrics\n","\n","# Enhanced validation epoch function with robust model output validation\n","def validate_epoch(model, dataloader, criterion, device, epoch, total_epochs):\n","    \"\"\"Enhanced validation epoch with robust error handling and output validation\"\"\"\n","    model.eval()\n","    running_loss = 0.0\n","    all_outputs = []\n","    all_labels = []\n","    all_sample_ids = []\n","\n","    with torch.no_grad():\n","        progress_bar = tqdm(dataloader, desc=f\"CASME II Validation Epoch {epoch+1}/{total_epochs}\")\n","\n","        for batch_idx, (images, labels, sample_ids) in enumerate(progress_bar):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            # Enhanced model output validation - handle multiple output formats\n","            model_output = model(images)\n","\n","            # Robust output structure validation\n","            if isinstance(model_output, (tuple, list)):\n","                outputs = model_output[0]\n","            elif isinstance(model_output, dict):\n","                outputs = model_output.get('logits', model_output.get('prediction', model_output))\n","            else:\n","                outputs = model_output\n","\n","            loss = criterion(outputs, labels)\n","            running_loss += loss.item()\n","\n","            # Memory optimized: Move to CPU before accumulating\n","            all_outputs.append(outputs.detach().cpu())\n","            all_labels.append(labels.detach().cpu())\n","            all_sample_ids.extend(sample_ids)\n","\n","            if batch_idx % 3 == 0:\n","                avg_loss = running_loss / (batch_idx + 1)\n","                progress_bar.set_postfix({'Val Loss': f'{avg_loss:.4f}'})\n","\n","    # Enhanced metrics calculation with error recovery\n","    try:\n","        epoch_outputs = torch.cat(all_outputs, dim=0)\n","        epoch_labels = torch.cat(all_labels, dim=0)\n","        metrics = calculate_metrics_safe_robust(epoch_outputs, epoch_labels, CASME2_CLASSES, average='macro')\n","    except Exception as e:\n","        print(f\"Warning: Validation metrics calculation failed: {e}\")\n","        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n","\n","    avg_loss = running_loss / len(dataloader)\n","    return avg_loss, metrics, all_sample_ids\n","\n","# Enhanced checkpoint saving function with error recovery\n","def save_checkpoint_robust(model, optimizer, scheduler, epoch, train_metrics, val_metrics,\n","                         checkpoint_dir, best_metrics, config, max_retries=3):\n","    \"\"\"Enhanced checkpoint saving with error recovery and retry logic\"\"\"\n","\n","    # Convert all tensors to serializable format\n","    def make_serializable(obj):\n","        if isinstance(obj, torch.Tensor):\n","            return obj.cpu().item() if obj.numel() == 1 else obj.cpu().tolist()\n","        elif isinstance(obj, np.ndarray):\n","            return obj.tolist()\n","        elif isinstance(obj, np.integer):\n","            return int(obj)\n","        elif isinstance(obj, np.floating):\n","            return float(obj)\n","        elif isinstance(obj, dict):\n","            return {k: make_serializable(v) for k, v in obj.items()}\n","        elif isinstance(obj, (list, tuple)):\n","            return [make_serializable(item) for item in obj]\n","        else:\n","            return obj\n","\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","        'train_metrics': make_serializable(train_metrics),\n","        'val_metrics': make_serializable(val_metrics),\n","        'casme2_config': make_serializable(config),\n","        'best_f1': float(best_metrics['f1']),\n","        'best_loss': float(best_metrics['loss']),\n","        'best_acc': float(best_metrics['accuracy']),\n","        'class_names': CASME2_CLASSES,\n","        'num_classes': 7\n","    }\n","\n","    best_path = f\"{checkpoint_dir}/casme2_vit_direct_best_f1.pth\"\n","\n","    # Enhanced save with retry logic\n","    for attempt in range(max_retries):\n","        try:\n","            torch.save(checkpoint, best_path)\n","            print(f\"Checkpoint saved successfully: {best_path}\")\n","            return best_path\n","        except Exception as e:\n","            print(f\"Checkpoint save attempt {attempt + 1} failed: {e}\")\n","            if attempt < max_retries - 1:\n","                time.sleep(1)  # Brief pause before retry\n","                continue\n","            else:\n","                print(f\"All {max_retries} checkpoint save attempts failed\")\n","                return None\n","\n","    return None\n","\n","# Safe JSON serialization function - preserved existing function\n","def safe_json_serialize(obj):\n","    \"\"\"Convert objects to JSON-serializable format\"\"\"\n","    if isinstance(obj, torch.Tensor):\n","        return obj.cpu().item() if obj.numel() == 1 else obj.cpu().numpy().tolist()\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    elif isinstance(obj, (np.integer, np.int32, np.int64)):\n","        return int(obj)\n","    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n","        return float(obj)\n","    elif isinstance(obj, dict):\n","        return {k: safe_json_serialize(v) for k, v in obj.items()}\n","    elif isinstance(obj, (list, tuple)):\n","        return [safe_json_serialize(item) for item in obj]\n","    elif hasattr(obj, '__dict__'):\n","        return safe_json_serialize(obj.__dict__)\n","    else:\n","        try:\n","            # Try to convert to basic Python types\n","            return float(obj) if isinstance(obj, (int, float)) else str(obj)\n","        except:\n","            return str(obj)\n","\n","# Create enhanced datasets - preserved existing structure\n","print(\"\\nCreating CASME II training datasets...\")\n","\n","train_dataset = CASME2DatasetTraining(\n","    split_metadata=GLOBAL_CONFIG_CASME2['metadata'],\n","    dataset_root=GLOBAL_CONFIG_CASME2['train_path'].replace('/train', ''),\n","    transform=GLOBAL_CONFIG_CASME2['transform_train'],\n","    split='train',\n","    use_ram_cache=True\n",")\n","\n","val_dataset = CASME2DatasetTraining(\n","    split_metadata=GLOBAL_CONFIG_CASME2['metadata'],\n","    dataset_root=GLOBAL_CONFIG_CASME2['val_path'].replace('/val', ''),\n","    transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","    split='val',\n","    use_ram_cache=True\n",")\n","\n","# Create data loaders - preserved existing structure\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=CASME2_VIT_CONFIG['batch_size'],\n","    shuffle=True,\n","    num_workers=CASME2_VIT_CONFIG['num_workers'],\n","    pin_memory=True,\n","    prefetch_factor=2\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=CASME2_VIT_CONFIG['batch_size'],\n","    shuffle=False,\n","    num_workers=CASME2_VIT_CONFIG['num_workers'],\n","    pin_memory=True,\n","    prefetch_factor=2\n",")\n","\n","print(f\"Training batches: {len(train_loader)} (samples: {len(train_dataset)})\")\n","print(f\"Validation batches: {len(val_loader)} (samples: {len(val_dataset)})\")\n","\n","# Initialize model, criterion, optimizer, scheduler\n","print(\"\\nInitializing CASME II ViT enhanced model...\")\n","model = ViTCASME2Baseline(\n","    num_classes=GLOBAL_CONFIG_CASME2['num_classes'],\n","    dropout_rate=CASME2_VIT_CONFIG['dropout_rate']\n",").to(GLOBAL_CONFIG_CASME2['device'])\n","\n","# Enhanced criterion creation using configurable factory function\n","if CASME2_VIT_CONFIG['use_focal_loss']:\n","    criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","        weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","        use_focal_loss=True,\n","        alpha_weights=CASME2_VIT_CONFIG['focal_loss_alpha_weights'],\n","        gamma=CASME2_VIT_CONFIG['focal_loss_gamma']\n","    )\n","else:\n","    criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","        weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","        use_focal_loss=False,\n","        alpha_weights=None,\n","        gamma=2.0\n","    )\n","\n","optimizer, scheduler = GLOBAL_CONFIG_CASME2['optimizer_scheduler_factory'](\n","    model, CASME2_VIT_CONFIG\n",")\n","\n","print(f\"Optimizer: AdamW (LR={CASME2_VIT_CONFIG['learning_rate']})\")\n","print(f\"Scheduler: ReduceLROnPlateau (patience={CASME2_VIT_CONFIG['scheduler_patience']})\")\n","print(f\"Criterion: {'Optimized Focal Loss' if CASME2_VIT_CONFIG['use_focal_loss'] else 'CrossEntropy'}\")\n","\n","# Training history tracking - preserved existing structure\n","training_history = {\n","    'train_loss': [],\n","    'val_loss': [],\n","    'train_f1': [],\n","    'val_f1': [],\n","    'train_acc': [],\n","    'val_acc': [],\n","    'learning_rate': [],\n","    'epoch_time': []\n","}\n","\n","# Enhanced best metrics tracking for multi-criteria checkpoint saving\n","best_metrics = {\n","    'f1': 0.0,\n","    'loss': float('inf'),\n","    'accuracy': 0.0,\n","    'epoch': 0\n","}\n","\n","print(\"\\nStarting CASME II ViT enhanced training...\")\n","print(f\"Training configuration: {CASME2_VIT_CONFIG['num_epochs']} epochs\")\n","print(\"=\" * 65)\n","\n","# Main training loop with enhanced checkpoint reliability\n","start_time = time.time()\n","\n","for epoch in range(CASME2_VIT_CONFIG['num_epochs']):\n","    epoch_start_time = time.time()\n","    print(f\"\\nEpoch {epoch+1}/{CASME2_VIT_CONFIG['num_epochs']}\")\n","\n","    # Training phase\n","    train_loss, train_metrics = train_epoch(\n","        model, train_loader, criterion, optimizer,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_VIT_CONFIG['num_epochs']\n","    )\n","\n","    # Validation phase\n","    val_loss, val_metrics, val_sample_ids = validate_epoch(\n","        model, val_loader, criterion,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_VIT_CONFIG['num_epochs']\n","    )\n","\n","    # Update scheduler\n","    if scheduler:\n","        scheduler.step(val_metrics['f1_score'])\n","\n","    # Record training history\n","    epoch_time = time.time() - epoch_start_time\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    training_history['train_loss'].append(float(train_loss))\n","    training_history['val_loss'].append(float(val_loss))\n","    training_history['train_f1'].append(float(train_metrics['f1_score']))\n","    training_history['val_f1'].append(float(val_metrics['f1_score']))\n","    training_history['train_acc'].append(float(train_metrics['accuracy']))\n","    training_history['val_acc'].append(float(val_metrics['accuracy']))\n","    training_history['learning_rate'].append(float(current_lr))\n","    training_history['epoch_time'].append(float(epoch_time))\n","\n","    # Print epoch summary\n","    print(f\"Train - Loss: {train_loss:.4f}, F1: {train_metrics['f1_score']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n","    print(f\"Val   - Loss: {val_loss:.4f}, F1: {val_metrics['f1_score']:.4f}, Acc: {val_metrics['accuracy']:.4f}\")\n","    print(f\"Time  - Epoch: {epoch_time:.1f}s, LR: {current_lr:.2e}\")\n","\n","    # Enhanced multi-criteria checkpoint saving logic\n","    save_model = False\n","    improvement_reason = \"\"\n","\n","    # Multi-criteria evaluation hierarchy\n","    if val_metrics['f1_score'] > best_metrics['f1']:\n","        save_model = True\n","        improvement_reason = \"Higher F1\"\n","    elif val_metrics['f1_score'] == best_metrics['f1']:\n","        if val_loss < best_metrics['loss']:\n","            save_model = True\n","            improvement_reason = \"Same F1, Lower Loss\"\n","        elif val_loss == best_metrics['loss'] and val_metrics['accuracy'] > best_metrics['accuracy']:\n","            save_model = True\n","            improvement_reason = \"Same F1&Loss, Higher Accuracy\"\n","\n","    if save_model:\n","        best_metrics['f1'] = val_metrics['f1_score']\n","        best_metrics['loss'] = val_loss\n","        best_metrics['accuracy'] = val_metrics['accuracy']\n","        best_metrics['epoch'] = epoch + 1\n","\n","        best_model_path = save_checkpoint_robust(\n","            model, optimizer, scheduler, epoch,\n","            train_metrics, val_metrics, GLOBAL_CONFIG_CASME2['checkpoint_root'],\n","            best_metrics, CASME2_VIT_CONFIG\n","        )\n","\n","        if best_model_path:\n","            print(f\"New best model: {improvement_reason} - F1: {best_metrics['f1']:.4f}\")\n","        else:\n","            print(f\"Warning: Failed to save checkpoint despite improvement\")\n","\n","    # Progress tracking\n","    elapsed_time = time.time() - start_time\n","    estimated_total = (elapsed_time / (epoch + 1)) * CASME2_VIT_CONFIG['num_epochs']\n","    remaining_time = estimated_total - elapsed_time\n","    progress_pct = ((epoch + 1) / CASME2_VIT_CONFIG['num_epochs']) * 100\n","\n","    print(f\"Progress: {progress_pct:.1f}% | Best F1: {best_metrics['f1']:.4f} | ETA: {remaining_time/60:.1f}min\")\n","\n","# Training completion\n","total_time = time.time() - start_time\n","actual_epochs = CASME2_VIT_CONFIG['num_epochs']\n","\n","print(\"\\n\" + \"=\" * 65)\n","print(\"CASME II ViT ENHANCED BASELINE TRAINING COMPLETED\")\n","print(\"=\" * 65)\n","print(f\"Training time: {total_time/60:.1f} minutes\")\n","print(f\"Epochs completed: {actual_epochs}\")\n","print(f\"Best validation F1: {best_metrics['f1']:.4f} (epoch {best_metrics['epoch']})\")\n","print(f\"Final train F1: {training_history['train_f1'][-1]:.4f}\")\n","print(f\"Final validation F1: {training_history['val_f1'][-1]:.4f}\")\n","\n","# Enhanced training documentation export - corrected filename\n","results_dir = GLOBAL_CONFIG_CASME2['results_root']\n","os.makedirs(f\"{results_dir}/training_logs\", exist_ok=True)\n","\n","# Corrected filename as requested\n","training_history_path = f\"{results_dir}/training_logs/casme2_vit_direct_training_history.json\"\n","\n","print(\"\\nExporting enhanced training documentation...\")\n","\n","try:\n","    # Create comprehensive training summary with enhanced experiment configuration\n","    training_summary = {\n","        'experiment_type': 'CASME2_ViT_Enhanced_Baseline',\n","        'experiment_configuration': {\n","            'loss_function': 'Optimized Focal Loss' if CASME2_VIT_CONFIG['use_focal_loss'] else 'CrossEntropy',\n","            'weight_approach': 'Per-class Alpha (sum=1.0)' if CASME2_VIT_CONFIG['use_focal_loss'] else 'Inverse Sqrt Frequency',\n","            'focal_loss_gamma': CASME2_VIT_CONFIG['focal_loss_gamma'],\n","            'focal_loss_alpha_weights': CASME2_VIT_CONFIG['focal_loss_alpha_weights'],\n","            'crossentropy_class_weights': CASME2_VIT_CONFIG['crossentropy_class_weights'],\n","            'vit_model': CASME2_VIT_CONFIG['vit_model'],\n","            'patch_size': CASME2_VIT_CONFIG['patch_size']\n","        },\n","        'training_history': safe_json_serialize(training_history),\n","        'best_val_f1': float(best_metrics['f1']),\n","        'best_val_loss': float(best_metrics['loss']),\n","        'best_val_accuracy': float(best_metrics['accuracy']),\n","        'best_epoch': int(best_metrics['epoch']),\n","        'total_epochs': int(actual_epochs),\n","        'total_time_minutes': float(total_time / 60),\n","        'average_epoch_time_seconds': float(np.mean(training_history['epoch_time'])),\n","        'config': safe_json_serialize(CASME2_VIT_CONFIG),\n","        'final_train_f1': float(training_history['train_f1'][-1]),\n","        'final_val_f1': float(training_history['val_f1'][-1]),\n","        'model_checkpoint': 'casme2_vit_direct_best_f1.pth',\n","        'dataset_info': {\n","            'name': 'CASME_II',\n","            'total_samples': 255,\n","            'train_samples': len(train_dataset),\n","            'val_samples': len(val_dataset),\n","            'num_classes': 7,\n","            'class_names': CASME2_CLASSES\n","        },\n","        'architecture_info': {\n","            'model_type': 'ViTCASME2Baseline',\n","            'backbone': CASME2_VIT_CONFIG['vit_model'],\n","            'input_size': f\"{CASME2_VIT_CONFIG['input_size']}x{CASME2_VIT_CONFIG['input_size']}\",\n","            'patch_size': CASME2_VIT_CONFIG['patch_size'],\n","            'expected_tokens': (CASME2_VIT_CONFIG['input_size'] // CASME2_VIT_CONFIG['patch_size']) ** 2,\n","            'classification_head': '768->512->128->7'\n","        },\n","        'enhanced_features': {\n","            'robust_checkpoint_saving': True,\n","            'model_output_validation': True,\n","            'enhanced_error_handling': True,\n","            'multi_criteria_checkpoint_logic': True,\n","            'memory_optimized_training': True,\n","            'retry_checkpoint_logic': True\n","        }\n","    }\n","\n","    # Save with proper JSON serialization\n","    with open(training_history_path, 'w') as f:\n","        json.dump(training_summary, f, indent=2)\n","\n","    print(f\"Enhanced training documentation saved successfully: {training_history_path}\")\n","    print(f\"Experiment details: {training_summary['experiment_configuration']['loss_function']} loss\")\n","    if CASME2_VIT_CONFIG['use_focal_loss']:\n","        print(f\"  Gamma: {CASME2_VIT_CONFIG['focal_loss_gamma']}, Alpha Sum: {sum(CASME2_VIT_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","    print(f\"Model variant: {CASME2_VIT_CONFIG['vit_model']}\")\n","\n","except Exception as e:\n","    print(f\"Warning: Could not save training documentation: {e}\")\n","    print(\"Training completed successfully, but documentation export failed\")\n","\n","# Enhanced memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\nNext: Cell 3 - CASME II ViT Enhanced Evaluation\")\n","print(\"Enhanced training pipeline completed successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"cellView":"form","id":"n6M0bum3MVdd","executionInfo":{"status":"error","timestamp":1759063618320,"user_tz":-420,"elapsed":261140,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"b780366e-2ab9-4249-d122-d9a9a1f5a9e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II ViT Enhanced Training Pipeline with Atomic Checkpoint Saving\n","======================================================================\n","Loss Function: Optimized Focal Loss\n","Focal Loss Parameters:\n","  Gamma: 2.0\n","  Per-class Alpha: [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","  Alpha Sum: 0.999\n","Training epochs: 50\n","Scheduler patience: 3\n","Early stopping: False\n","Checkpoint Infrastructure: VALIDATED\n","\n","Creating CASME II training datasets...\n","Loading CASME II train dataset for training...\n","Loaded 201 CASME II train samples\n","  others: 79 samples (39.3%)\n","  disgust: 50 samples (24.9%)\n","  happiness: 25 samples (12.4%)\n","  repression: 21 samples (10.4%)\n","  surprise: 20 samples (10.0%)\n","  sadness: 5 samples (2.5%)\n","  fear: 1 samples (0.5%)\n","Preloading 201 train images to RAM...\n"]},{"output_type":"stream","name":"stderr","text":["Loading train: 100%|██████████| 201/201 [02:49<00:00,  1.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TRAIN RAM caching completed: 201 images, ~0.36GB\n","Loading CASME II val dataset for training...\n","Loaded 26 CASME II val samples\n","  others: 10 samples (38.5%)\n","  disgust: 6 samples (23.1%)\n","  happiness: 3 samples (11.5%)\n","  repression: 3 samples (11.5%)\n","  surprise: 2 samples (7.7%)\n","  sadness: 1 samples (3.8%)\n","  fear: 1 samples (3.8%)\n","Preloading 26 val images to RAM...\n"]},{"output_type":"stream","name":"stderr","text":["Loading val: 100%|██████████| 26/26 [00:21<00:00,  1.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["VAL RAM caching completed: 26 images, ~0.05GB\n","Training batches: 13 (samples: 201)\n","Validation batches: 2 (samples: 26)\n","\n","Initializing CASME II ViT enhanced model...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["ViT feature dimension: 768\n","ViT CASME II: 768 -> 512 -> 128 -> 7\n","Using Optimized Focal Loss with gamma=2.0\n","Per-class alpha weights: [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","Alpha sum: 0.999\n","Scheduler: ReduceLROnPlateau monitoring val_f1_macro\n","Optimizer: AdamW (LR=1e-05)\n","Scheduler: ReduceLROnPlateau (patience=3)\n","Criterion: Optimized Focal Loss\n","Checkpoint System: Atomic Saving with Integrity Verification\n","\n","Starting CASME II ViT enhanced training with atomic checkpoint saving...\n","Training configuration: 50 epochs\n","======================================================================\n","\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 1/50: 100%|██████████| 13/13 [00:10<00:00,  1.21it/s, Loss=0.1098, LR=1.00e-05]\n","CASME II Validation Epoch 1/50: 100%|██████████| 2/2 [00:00<00:00,  2.59it/s, Val Loss=0.0523]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.1039, F1: 0.1555, Acc: 0.2438\n","Val   - Loss: 0.1435, F1: 0.2588, Acc: 0.5385\n","Time  - Epoch: 11.5s, LR: 1.00e-05\n","Saving new best model: Higher F1\n","Initiating atomic checkpoint save (estimated size: 1.5GB)...\n","Pre-save validation passed: 69.0GB available\n","Checkpoint save attempt 1/3\n","Temporary checkpoint saved: 987.4MB in 2.0s\n","Atomic checkpoint save successful: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/02_01_vit_casme2-af/casme2_vit_direct_best_f1.pth\n","File integrity verified: 363d3b4415f30269...\n","Atomic checkpoint save successful - F1: 0.2588\n","Progress: 2.0% | Best F1: 0.2588 | ETA: 16.6min\n","\n","Epoch 2/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 2/50: 100%|██████████| 13/13 [00:09<00:00,  1.38it/s, Loss=0.0804, LR=1.00e-05]\n","CASME II Validation Epoch 2/50: 100%|██████████| 2/2 [00:00<00:00,  2.72it/s, Val Loss=0.0458]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0886, F1: 0.2451, Acc: 0.4478\n","Val   - Loss: 0.1507, F1: 0.2034, Acc: 0.4615\n","Time  - Epoch: 10.2s, LR: 1.00e-05\n","Progress: 4.0% | Best F1: 0.2588 | ETA: 12.2min\n","\n","Epoch 3/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 3/50: 100%|██████████| 13/13 [00:09<00:00,  1.37it/s, Loss=0.0813, LR=1.00e-05]\n","CASME II Validation Epoch 3/50: 100%|██████████| 2/2 [00:00<00:00,  2.71it/s, Val Loss=0.0464]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0787, F1: 0.3213, Acc: 0.4975\n","Val   - Loss: 0.1526, F1: 0.3143, Acc: 0.5769\n","Time  - Epoch: 10.2s, LR: 1.00e-05\n","Saving new best model: Higher F1\n","Initiating atomic checkpoint save (estimated size: 1.5GB)...\n","Pre-save validation passed: 68.1GB available\n","Created checkpoint backup: casme2_vit_direct_best_f1.pth.backup_1759063589\n","Checkpoint save attempt 1/3\n","Temporary checkpoint saved: 987.4MB in 2.6s\n","Atomic checkpoint save successful: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/02_01_vit_casme2-af/casme2_vit_direct_best_f1.pth\n","File integrity verified: d3c763a18d85910e...\n","Previous checkpoint backup cleaned up\n","Atomic checkpoint save successful - F1: 0.3143\n","Progress: 6.0% | Best F1: 0.3143 | ETA: 13.9min\n","\n","Epoch 4/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 4/50: 100%|██████████| 13/13 [00:09<00:00,  1.36it/s, Loss=0.0709, LR=1.00e-05]\n","CASME II Validation Epoch 4/50: 100%|██████████| 2/2 [00:00<00:00,  2.72it/s, Val Loss=0.0463]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0722, F1: 0.3442, Acc: 0.5075\n","Val   - Loss: 0.1509, F1: 0.3470, Acc: 0.5385\n","Time  - Epoch: 10.3s, LR: 1.00e-05\n","Saving new best model: Higher F1\n","Initiating atomic checkpoint save (estimated size: 1.5GB)...\n","Pre-save validation passed: 66.3GB available\n","Created checkpoint backup: casme2_vit_direct_best_f1.pth.backup_1759063612\n","Checkpoint save attempt 1/3\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2959983029.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saving new best model: {improvement_reason}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         best_model_path = save_checkpoint_robust(\n\u001b[0m\u001b[1;32m    716\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGLOBAL_CONFIG_CASME2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkpoint_root'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2959983029.py\u001b[0m in \u001b[0;36msave_checkpoint_robust\u001b[0;34m(model, optimizer, scheduler, epoch, train_metrics, val_metrics, checkpoint_dir, best_metrics, config, max_retries)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;31m# Save to temporary file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0msave_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m                 \u001b[0msave_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msave_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    968\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1264\u001b[0m                     \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m                     \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;34m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# @title Cell 3: CASME II ViT Direct Baseline Evaluation with Comprehensive Analysis\n","\n","# File: 02_01_ViT_Direct_Baseline_Cell3.py\n","# Location: experiments/02_01_ViT_Direct_Baseline.ipynb\n","# Purpose: Comprehensive evaluation framework for trained CASME II ViT micro-expression recognition model\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from datetime import datetime\n","\n","# Evaluation specific imports\n","from sklearn.metrics import (\n","    accuracy_score, precision_recall_fscore_support,\n","    classification_report, confusion_matrix,\n","    roc_curve, auc\n",")\n","from sklearn.preprocessing import label_binarize\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing as mp\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enhanced test dataset for CASME II evaluation\n","class CASME2DatasetEvaluation(Dataset):\n","    \"\"\"Enhanced CASME II test dataset with comprehensive evaluation support\"\"\"\n","\n","    def __init__(self, split_metadata, dataset_root, transform=None, split='test', use_ram_cache=True):\n","        self.metadata = split_metadata[split]['samples']\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","        self.split = split\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.sample_ids = []\n","        self.emotions = []\n","        self.subjects = []\n","        self.cached_images = []\n","\n","        print(f\"Loading CASME II {split} dataset for evaluation...\")\n","\n","        # Process metadata for evaluation\n","        for sample in self.metadata:\n","            image_path = os.path.join(dataset_root, split, sample['image_filename'])\n","            self.images.append(image_path)\n","            self.labels.append(CLASS_TO_IDX[sample['emotion']])\n","            self.sample_ids.append(sample['sample_id'])\n","            self.emotions.append(sample['emotion'])\n","            self.subjects.append(sample['subject'])\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples for evaluation\")\n","        self._print_evaluation_distribution()\n","\n","        # RAM caching for fast evaluation\n","        if self.use_ram_cache:\n","            self._preload_to_ram_evaluation()\n","\n","    def _print_evaluation_distribution(self):\n","        \"\"\"Print comprehensive distribution for evaluation analysis\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"No test samples found!\")\n","            return\n","\n","        label_counts = {}\n","        subject_counts = {}\n","\n","        for label, subject in zip(self.labels, self.subjects):\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","            subject_counts[subject] = subject_counts.get(subject, 0) + 1\n","\n","        print(\"Test set class distribution:\")\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","        print(f\"Test set covers {len(subject_counts)} subjects\")\n","\n","        # Check for missing classes\n","        missing_classes = []\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i not in label_counts:\n","                missing_classes.append(class_name)\n","\n","        if missing_classes:\n","            print(f\"Missing classes in test set: {missing_classes}\")\n","\n","    def _preload_to_ram_evaluation(self):\n","        \"\"\"RAM preloading optimized for evaluation\"\"\"\n","        print(f\"Preloading {len(self.images)} test images to RAM...\")\n","\n","        max_workers = min(mp.cpu_count(), 6)\n","        valid_images = 0\n","        self.cached_images = [None] * len(self.images)\n","\n","        # Simple loading for evaluation stability\n","        for i, img_path in enumerate(tqdm(self.images, desc=\"Loading test images\")):\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (384, 384):\n","                    image = image.resize((384, 384), Image.Resampling.LANCZOS)\n","                self.cached_images[i] = image\n","                valid_images += 1\n","            except Exception as e:\n","                print(f\"Error loading {img_path}: {e}\")\n","                # Create neutral placeholder\n","                self.cached_images[i] = Image.new('RGB', (384, 384), (128, 128, 128))\n","\n","        ram_usage_gb = len(self.cached_images) * 384 * 384 * 3 * 4 / 1e9\n","        print(f\"Test RAM caching completed: {valid_images} valid images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (384, 384):\n","                    image = image.resize((384, 384), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (384, 384), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return (image, self.labels[idx], self.sample_ids[idx],\n","                self.emotions[idx], self.subjects[idx], os.path.basename(self.images[idx]))\n","\n","# CASME II evaluation configuration\n","EVALUATION_CONFIG_CASME2 = {\n","    'model_type': 'ViT_CASME2_Direct_Baseline',\n","    'task_type': 'micro_expression_recognition',\n","    'num_classes': 7,\n","    'class_names': CASME2_CLASSES,\n","    'checkpoint_file': 'casme2_vit_direct_best_f1.pth',\n","    'dataset_name': 'CASME_II',\n","    'input_size': '384x384',\n","    'evaluation_protocol': 'stratified_split'\n","}\n","\n","print(\"CASME II ViT Direct Baseline Evaluation Framework\")\n","print(\"=\" * 60)\n","print(f\"Model: {EVALUATION_CONFIG_CASME2['model_type']}\")\n","print(f\"Task: {EVALUATION_CONFIG_CASME2['task_type']}\")\n","print(f\"Classes: {EVALUATION_CONFIG_CASME2['class_names']}\")\n","print(f\"Input size: {EVALUATION_CONFIG_CASME2['input_size']}\")\n","\n","def extract_logits_safe_casme2(outputs_all):\n","    \"\"\"Robust logits extraction for CASME II ViT model\"\"\"\n","    if isinstance(outputs_all, torch.Tensor):\n","        return outputs_all\n","    if isinstance(outputs_all, (tuple, list)):\n","        for item in outputs_all:\n","            if isinstance(item, torch.Tensor):\n","                return item\n","    if isinstance(outputs_all, dict):\n","        for key in ('logits', 'logit', 'predictions', 'outputs', 'scores'):\n","            value = outputs_all.get(key)\n","            if isinstance(value, torch.Tensor):\n","                return value\n","        # Fallback to first tensor value\n","        for value in outputs_all.values():\n","            if isinstance(value, torch.Tensor):\n","                return value\n","    raise RuntimeError(\"Unable to extract tensor logits from CASME II ViT model output\")\n","\n","def load_trained_model_casme2(checkpoint_path, device):\n","    \"\"\"Load trained CASME II ViT model with comprehensive compatibility\"\"\"\n","    print(f\"Loading trained CASME II ViT model from: {checkpoint_path}\")\n","\n","    if not os.path.exists(checkpoint_path):\n","        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n","\n","    # Multiple loading approaches for maximum compatibility\n","    checkpoint = None\n","    loading_method = \"unknown\"\n","\n","    try:\n","        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","        loading_method = \"standard\"\n","    except Exception as e1:\n","        try:\n","            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n","            loading_method = \"weights_only_false\"\n","        except Exception as e2:\n","            try:\n","                import pickle\n","                with open(checkpoint_path, 'rb') as f:\n","                    checkpoint = pickle.load(f)\n","                loading_method = \"pickle\"\n","            except Exception as e3:\n","                raise RuntimeError(f\"All loading methods failed: {e1}, {e2}, {e3}\")\n","\n","    print(f\"Checkpoint loaded using: {loading_method}\")\n","\n","    # Initialize CASME II ViT model\n","    model = ViTCASME2Baseline(\n","        num_classes=EVALUATION_CONFIG_CASME2['num_classes'],\n","        dropout_rate=CASME2_VIT_CONFIG['dropout_rate']\n","    ).to(device)\n","\n","    # Load state dict with fallback approaches\n","    state_dict = checkpoint.get('model_state_dict', checkpoint)\n","\n","    try:\n","        model.load_state_dict(state_dict, strict=True)\n","        print(\"Model state loaded with strict=True\")\n","    except Exception as e:\n","        print(f\"Strict loading failed, trying non-strict: {str(e)[:100]}...\")\n","        try:\n","            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n","            if missing_keys or unexpected_keys:\n","                print(f\"Non-strict loading: Missing {len(missing_keys)}, Unexpected {len(unexpected_keys)}\")\n","            else:\n","                print(\"Model state loaded with strict=False (no key mismatches)\")\n","        except Exception as e2:\n","            raise RuntimeError(f\"Both loading approaches failed: {e2}\")\n","\n","    model.eval()\n","\n","    # Extract training information\n","    training_info = {\n","        'best_val_f1': float(checkpoint.get('best_f1', 0.0)),\n","        'best_val_loss': float(checkpoint.get('best_loss', float('inf'))),\n","        'best_val_accuracy': float(checkpoint.get('best_acc', 0.0)),\n","        'best_epoch': int(checkpoint.get('epoch', 0)) + 1,\n","        'model_checkpoint': EVALUATION_CONFIG_CASME2['checkpoint_file'],\n","        'num_classes': EVALUATION_CONFIG_CASME2['num_classes'],\n","        'config': checkpoint.get('casme2_config', {})\n","    }\n","\n","    print(f\"Model loaded successfully:\")\n","    print(f\"  Best validation F1: {training_info['best_val_f1']:.4f}\")\n","    print(f\"  Best validation accuracy: {training_info['best_val_accuracy']:.4f}\")\n","    print(f\"  Best epoch: {training_info['best_epoch']}\")\n","    print(f\"  Model classes: {EVALUATION_CONFIG_CASME2['num_classes']}\")\n","\n","    return model, training_info\n","\n","def run_model_inference_casme2(model, test_loader, device):\n","    \"\"\"Run CASME II ViT model inference with comprehensive tracking\"\"\"\n","    print(\"Running CASME II ViT model inference on test set...\")\n","\n","    model.eval()\n","    all_predictions = []\n","    all_probabilities = []\n","    all_labels = []\n","    all_sample_ids = []\n","    all_emotions = []\n","    all_subjects = []\n","    all_filenames = []\n","\n","    inference_start = time.time()\n","\n","    with torch.no_grad():\n","        for batch_idx, (images, labels, sample_ids, emotions, subjects, filenames) in enumerate(\n","            tqdm(test_loader, desc=\"CASME II Inference\")):\n","\n","            images = images.to(device)\n","\n","            # Forward pass with robust output extraction\n","            try:\n","                outputs_raw = model(images)\n","                outputs = extract_logits_safe_casme2(outputs_raw)\n","            except Exception as e:\n","                print(f\"Error in model forward pass: {e}\")\n","                outputs = model(images)\n","                if not isinstance(outputs, torch.Tensor):\n","                    outputs = outputs[0] if isinstance(outputs, (list, tuple)) else outputs\n","\n","            # Validate output shape for 7 CASME II classes\n","            if outputs.shape[1] != 7:\n","                print(f\"Warning: Expected 7 classes output, got {outputs.shape[1]}\")\n","\n","            # Get probabilities and predictions\n","            probabilities = torch.softmax(outputs, dim=1)\n","            predictions = torch.argmax(probabilities, dim=1)\n","\n","            # Store results (CPU for memory efficiency)\n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_probabilities.extend(probabilities.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","            all_sample_ids.extend(sample_ids)\n","            all_emotions.extend(emotions)\n","            all_subjects.extend(subjects)\n","            all_filenames.extend(filenames)\n","\n","    inference_time = time.time() - inference_start\n","\n","    # Convert to arrays\n","    predictions_array = np.array(all_predictions)\n","    probabilities_array = np.array(all_probabilities)\n","    labels_array = np.array(all_labels)\n","\n","    print(f\"CASME II inference completed: {len(predictions_array)} samples in {inference_time:.2f}s\")\n","\n","    # Analyze prediction distribution\n","    unique_predictions, pred_counts = np.unique(predictions_array, return_counts=True)\n","    print(f\"Predicted classes: {[CASME2_CLASSES[i] for i in unique_predictions]}\")\n","\n","    unique_labels, label_counts = np.unique(labels_array, return_counts=True)\n","    print(f\"True classes in test: {[CASME2_CLASSES[i] for i in unique_labels]}\")\n","\n","    return {\n","        'predictions': predictions_array,\n","        'probabilities': probabilities_array,\n","        'labels': labels_array,\n","        'sample_ids': all_sample_ids,\n","        'emotions': all_emotions,\n","        'subjects': all_subjects,\n","        'filenames': all_filenames,\n","        'inference_time': inference_time,\n","        'samples_count': len(predictions_array)\n","    }\n","\n","def analyze_wrong_predictions_casme2(inference_results):\n","    \"\"\"Comprehensive wrong predictions analysis for CASME II\"\"\"\n","    print(\"Analyzing wrong predictions for CASME II micro-expression recognition...\")\n","\n","    predictions = inference_results['predictions']\n","    labels = inference_results['labels']\n","    sample_ids = inference_results['sample_ids']\n","    emotions = inference_results['emotions']\n","    subjects = inference_results['subjects']\n","    filenames = inference_results['filenames']\n","\n","    # Find wrong predictions\n","    wrong_mask = predictions != labels\n","    wrong_indices = np.where(wrong_mask)[0]\n","\n","    # Organize by true emotion class\n","    wrong_predictions_by_class = {}\n","    subject_error_analysis = {}\n","\n","    for class_name in CASME2_CLASSES:\n","        wrong_predictions_by_class[class_name] = []\n","\n","    # Analyze wrong predictions\n","    for idx in wrong_indices:\n","        true_label = labels[idx]\n","        pred_label = predictions[idx]\n","        sample_id = sample_ids[idx]\n","        emotion = emotions[idx]\n","        subject = subjects[idx]\n","        filename = filenames[idx]\n","\n","        true_class = CASME2_CLASSES[true_label]\n","        pred_class = CASME2_CLASSES[pred_label]\n","\n","        wrong_info = {\n","            'sample_id': sample_id,\n","            'filename': filename,\n","            'subject': subject,\n","            'true_label': int(true_label),\n","            'true_class': true_class,\n","            'predicted_label': int(pred_label),\n","            'predicted_class': pred_class,\n","            'emotion': emotion\n","        }\n","\n","        wrong_predictions_by_class[true_class].append(wrong_info)\n","\n","        # Subject error tracking\n","        if subject not in subject_error_analysis:\n","            subject_error_analysis[subject] = {'total': 0, 'wrong': 0, 'errors': []}\n","        subject_error_analysis[subject]['wrong'] += 1\n","        subject_error_analysis[subject]['errors'].append(wrong_info)\n","\n","    # Count total samples per subject\n","    for subject in subjects:\n","        if subject in subject_error_analysis:\n","            subject_error_analysis[subject]['total'] += 1\n","        else:\n","            subject_error_analysis[subject] = {'total': 1, 'wrong': 0, 'errors': []}\n","\n","    # Calculate error rates per subject\n","    for subject in subject_error_analysis:\n","        total = subject_error_analysis[subject]['total']\n","        wrong = subject_error_analysis[subject]['wrong']\n","        subject_error_analysis[subject]['error_rate'] = wrong / total if total > 0 else 0.0\n","\n","    # Summary statistics\n","    total_wrong = len(wrong_indices)\n","    total_samples = len(predictions)\n","    error_rate = (total_wrong / total_samples) * 100\n","\n","    # Confusion patterns analysis\n","    confusion_patterns = {}\n","    for idx in wrong_indices:\n","        true_label = labels[idx]\n","        pred_label = predictions[idx]\n","        pattern = f\"{CASME2_CLASSES[true_label]}_to_{CASME2_CLASSES[pred_label]}\"\n","        confusion_patterns[pattern] = confusion_patterns.get(pattern, 0) + 1\n","\n","    analysis_results = {\n","        'analysis_metadata': {\n","            'evaluation_timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n","            'model_type': EVALUATION_CONFIG_CASME2['model_type'],\n","            'dataset': EVALUATION_CONFIG_CASME2['dataset_name'],\n","            'total_samples': int(total_samples),\n","            'total_wrong_predictions': int(total_wrong),\n","            'overall_error_rate': float(error_rate)\n","        },\n","        'wrong_predictions_by_class': wrong_predictions_by_class,\n","        'subject_error_analysis': subject_error_analysis,\n","        'confusion_patterns': confusion_patterns,\n","        'error_summary': {\n","            class_name: len(wrong_predictions_by_class[class_name])\n","            for class_name in CASME2_CLASSES\n","        }\n","    }\n","\n","    return analysis_results\n","\n","def calculate_comprehensive_metrics_casme2(inference_results):\n","    \"\"\"Calculate comprehensive evaluation metrics for CASME II micro-expression recognition\"\"\"\n","    print(\"Calculating comprehensive metrics for CASME II micro-expression recognition...\")\n","\n","    predictions = inference_results['predictions']\n","    probabilities = inference_results['probabilities']\n","    labels = inference_results['labels']\n","\n","    if len(predictions) == 0:\n","        raise ValueError(\"No predictions to evaluate!\")\n","\n","    # Identify available classes in test set\n","    unique_test_labels = sorted(np.unique(labels))\n","    unique_predictions = sorted(np.unique(predictions))\n","\n","    print(f\"Test set contains labels: {[CASME2_CLASSES[i] for i in unique_test_labels]}\")\n","    print(f\"Model predicted classes: {[CASME2_CLASSES[i] for i in unique_predictions]}\")\n","\n","    # Basic metrics\n","    accuracy = accuracy_score(labels, predictions)\n","\n","    # Macro metrics (only for available classes)\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions, labels=unique_test_labels, average='macro', zero_division=0\n","    )\n","\n","    print(f\"Macro F1 (available classes): {f1:.4f}\")\n","\n","    # Per-class metrics (all 7 classes)\n","    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n","        labels, predictions, labels=range(7), average=None, zero_division=0\n","    )\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(labels, predictions, labels=range(7))\n","\n","    # Multi-class AUC (only for classes with test samples)\n","    auc_scores = {}\n","    fpr_dict = {}\n","    tpr_dict = {}\n","\n","    try:\n","        labels_binarized = label_binarize(labels, classes=range(7))\n","\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i in unique_test_labels and len(np.unique(labels_binarized[:, i])) > 1:\n","                fpr, tpr, _ = roc_curve(labels_binarized[:, i], probabilities[:, i])\n","                auc_score = auc(fpr, tpr)\n","                auc_scores[class_name] = float(auc_score)\n","                fpr_dict[class_name] = fpr.tolist()\n","                tpr_dict[class_name] = tpr.tolist()\n","            else:\n","                auc_scores[class_name] = 0.0\n","                fpr_dict[class_name] = [0.0, 1.0]\n","                tpr_dict[class_name] = [0.0, 0.0]\n","\n","        # Macro AUC for available classes\n","        available_auc_scores = [auc_scores[CASME2_CLASSES[i]] for i in unique_test_labels]\n","        macro_auc = float(np.mean(available_auc_scores)) if available_auc_scores else 0.0\n","\n","    except Exception as e:\n","        print(f\"Warning: AUC calculation failed: {e}\")\n","        auc_scores = {class_name: 0.0 for class_name in CASME2_CLASSES}\n","        macro_auc = 0.0\n","\n","    # Subject-level analysis\n","    subjects = inference_results['subjects']\n","    subject_performance = {}\n","\n","    for subject in set(subjects):\n","        subject_mask = [s == subject for s in subjects]\n","        subject_predictions = predictions[subject_mask]\n","        subject_labels = labels[subject_mask]\n","\n","        if len(subject_predictions) > 0:\n","            subject_acc = accuracy_score(subject_labels, subject_predictions)\n","            subject_performance[subject] = {\n","                'accuracy': float(subject_acc),\n","                'samples': int(len(subject_predictions)),\n","                'correct': int(np.sum(subject_predictions == subject_labels))\n","            }\n","\n","    # Comprehensive results\n","    comprehensive_results = {\n","        'evaluation_metadata': {\n","            'model_type': EVALUATION_CONFIG_CASME2['model_type'],\n","            'dataset': EVALUATION_CONFIG_CASME2['dataset_name'],\n","            'evaluation_timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n","            'num_classes': EVALUATION_CONFIG_CASME2['num_classes'],\n","            'class_names': EVALUATION_CONFIG_CASME2['class_names'],\n","            'test_samples': int(len(labels)),\n","            'available_classes': [CASME2_CLASSES[i] for i in unique_test_labels],\n","            'missing_classes': [CASME2_CLASSES[i] for i in range(7) if i not in unique_test_labels]\n","        },\n","\n","        'overall_performance': {\n","            'accuracy': float(accuracy),\n","            'macro_precision': float(precision),\n","            'macro_recall': float(recall),\n","            'macro_f1': float(f1),\n","            'macro_auc': macro_auc\n","        },\n","\n","        'per_class_performance': {},\n","\n","        'confusion_matrix': cm.tolist(),\n","\n","        'subject_level_performance': subject_performance,\n","\n","        'roc_analysis': {\n","            'auc_scores': auc_scores,\n","            'fpr_curves': fpr_dict,\n","            'tpr_curves': tpr_dict\n","        },\n","\n","        'inference_performance': {\n","            'total_time_seconds': float(inference_results['inference_time']),\n","            'average_time_ms_per_sample': float(inference_results['inference_time'] * 1000 / len(labels))\n","        }\n","    }\n","\n","    # Per-class performance details\n","    for i, class_name in enumerate(CASME2_CLASSES):\n","        comprehensive_results['per_class_performance'][class_name] = {\n","            'precision': float(precision_per_class[i]),\n","            'recall': float(recall_per_class[i]),\n","            'f1_score': float(f1_per_class[i]),\n","            'support': int(support_per_class[i]),\n","            'auc': auc_scores[class_name],\n","            'in_test_set': i in unique_test_labels\n","        }\n","\n","    return comprehensive_results\n","\n","def save_evaluation_results_casme2(evaluation_results, wrong_predictions_results, results_dir):\n","    \"\"\"Save comprehensive evaluation results for CASME II\"\"\"\n","    os.makedirs(results_dir, exist_ok=True)\n","\n","    # Save main evaluation results\n","    results_file = f\"{results_dir}/casme2_vit_direct_evaluation_results.json\"\n","    with open(results_file, 'w') as f:\n","        json.dump(evaluation_results, f, indent=2, default=str)\n","\n","    # Save wrong predictions analysis\n","    wrong_predictions_file = f\"{results_dir}/casme2_vit_direct_wrong_predictions.json\"\n","    with open(wrong_predictions_file, 'w') as f:\n","        json.dump(wrong_predictions_results, f, indent=2, default=str)\n","\n","    print(f\"Evaluation results saved:\")\n","    print(f\"  Main results: {results_file}\")\n","    print(f\"  Wrong predictions: {wrong_predictions_file}\")\n","\n","    return results_file, wrong_predictions_file\n","\n","# Main evaluation execution\n","try:\n","    print(\"Starting CASME II ViT Direct Baseline comprehensive evaluation...\")\n","\n","    # Create test dataset\n","    print(\"Creating CASME II test dataset...\")\n","    casme2_test_dataset = CASME2DatasetEvaluation(\n","        split_metadata=GLOBAL_CONFIG_CASME2['metadata'],\n","        dataset_root=GLOBAL_CONFIG_CASME2['test_path'].replace('/test', ''),\n","        transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","        split='test',\n","        use_ram_cache=True\n","    )\n","\n","    if len(casme2_test_dataset) == 0:\n","        raise ValueError(\"No test samples found! Check test data path.\")\n","\n","    casme2_test_loader = DataLoader(\n","        casme2_test_dataset,\n","        batch_size=CASME2_VIT_CONFIG['batch_size'],\n","        shuffle=False,\n","        num_workers=CASME2_VIT_CONFIG['num_workers'],\n","        pin_memory=True\n","    )\n","\n","    # Load trained model\n","    checkpoint_path = f\"{GLOBAL_CONFIG_CASME2['checkpoint_root']}/{EVALUATION_CONFIG_CASME2['checkpoint_file']}\"\n","    casme2_model, training_info = load_trained_model_casme2(checkpoint_path, GLOBAL_CONFIG_CASME2['device'])\n","\n","    # Run inference\n","    inference_results = run_model_inference_casme2(casme2_model, casme2_test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","    # Calculate comprehensive metrics\n","    evaluation_results = calculate_comprehensive_metrics_casme2(inference_results)\n","\n","    # Analyze wrong predictions\n","    wrong_predictions_results = analyze_wrong_predictions_casme2(inference_results)\n","\n","    # Add training information\n","    evaluation_results['training_information'] = training_info\n","\n","    # Save results\n","    results_dir = f\"{GLOBAL_CONFIG_CASME2['results_root']}/evaluation_results\"\n","    results_file, wrong_file = save_evaluation_results_casme2(\n","        evaluation_results, wrong_predictions_results, results_dir\n","    )\n","\n","    # Display comprehensive results\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"CASME II ViT DIRECT BASELINE EVALUATION RESULTS\")\n","    print(\"=\" * 60)\n","\n","    # Overall performance\n","    overall = evaluation_results['overall_performance']\n","    print(f\"Overall Performance (Macro - Available Classes):\")\n","    print(f\"  Accuracy:  {overall['accuracy']:.4f}\")\n","    print(f\"  Precision: {overall['macro_precision']:.4f}\")\n","    print(f\"  Recall:    {overall['macro_recall']:.4f}\")\n","    print(f\"  F1 Score:  {overall['macro_f1']:.4f}\")\n","    print(f\"  AUC:       {overall['macro_auc']:.4f}\")\n","\n","    # Per-class performance\n","    print(f\"\\nPer-Class Performance:\")\n","    for class_name, metrics in evaluation_results['per_class_performance'].items():\n","        in_test = \"Present\" if metrics['in_test_set'] else \"Missing\"\n","        print(f\"  {class_name} [{in_test}]: F1={metrics['f1_score']:.4f}, \"\n","              f\"AUC={metrics['auc']:.4f}, Support={metrics['support']}\")\n","\n","    # Training vs test comparison\n","    print(f\"\\nTraining vs Test Performance:\")\n","    training_f1 = training_info['best_val_f1']\n","    training_acc = training_info['best_val_accuracy']\n","    test_f1 = overall['macro_f1']\n","    test_acc = overall['accuracy']\n","\n","    print(f\"  Training Val F1:  {training_f1:.4f}\")\n","    print(f\"  Test F1:          {test_f1:.4f}\")\n","    print(f\"  F1 Difference:    {training_f1 - test_f1:+.4f}\")\n","    print(f\"  Training Val Acc: {training_acc:.4f}\")\n","    print(f\"  Test Accuracy:    {test_acc:.4f}\")\n","    print(f\"  Acc Difference:   {training_acc - test_acc:+.4f}\")\n","    print(f\"  Best Epoch:       {training_info['best_epoch']}\")\n","\n","    # Wrong predictions summary\n","    print(f\"\\n\" + \"=\" * 40)\n","    print(\"WRONG PREDICTIONS ANALYSIS\")\n","    print(\"=\" * 40)\n","\n","    wrong_meta = wrong_predictions_results['analysis_metadata']\n","    print(f\"Total wrong predictions: {wrong_meta['total_wrong_predictions']} / {wrong_meta['total_samples']}\")\n","    print(f\"Overall error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","    print(f\"\\nErrors by True Class:\")\n","    for class_name, error_count in wrong_predictions_results['error_summary'].items():\n","        if error_count > 0:\n","            wrong_samples = wrong_predictions_results['wrong_predictions_by_class'][class_name]\n","            print(f\"  {class_name}: {error_count} errors\")\n","            for sample in wrong_samples[:3]:  # Show first 3\n","                print(f\"    - {sample['filename']} -> predicted as {sample['predicted_class']}\")\n","            if len(wrong_samples) > 3:\n","                print(f\"    ... and {len(wrong_samples) - 3} more\")\n","\n","    # Subject-level analysis\n","    print(f\"\\nSubject-Level Performance:\")\n","    subject_perfs = list(evaluation_results['subject_level_performance'].items())\n","    subject_perfs.sort(key=lambda x: x[1]['accuracy'], reverse=True)\n","    for subject, perf in subject_perfs[:5]:  # Show top 5\n","        print(f\"  {subject}: {perf['accuracy']:.3f} ({perf['correct']}/{perf['samples']})\")\n","\n","    # Most common confusion patterns\n","    print(f\"\\nMost Common Confusion Patterns:\")\n","    patterns = sorted(wrong_predictions_results['confusion_patterns'].items(),\n","                     key=lambda x: x[1], reverse=True)\n","    for pattern, count in patterns[:3]:\n","        print(f\"  {pattern}: {count} cases\")\n","\n","    print(f\"\\nInference Performance:\")\n","    print(f\"  Total time: {inference_results['inference_time']:.2f}s\")\n","    print(f\"  Speed: {evaluation_results['inference_performance']['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","    print(f\"\\nMissing Classes: {evaluation_results['evaluation_metadata']['missing_classes']}\")\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"CASME II ViT DIRECT BASELINE EVALUATION COMPLETED\")\n","    print(\"=\" * 60)\n","\n","except Exception as e:\n","    print(f\"Evaluation failed: {e}\")\n","    import traceback\n","    traceback.print_exc()\n","\n","finally:\n","    # Memory cleanup\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","        torch.cuda.empty_cache()\n","\n","print(\"Next: Analysis and comparison with literature baselines\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"cellView":"form","id":"-qyDe-ChN28j","executionInfo":{"status":"ok","timestamp":1758802375333,"user_tz":-420,"elapsed":38405,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"a3ac436e-b766-4bd9-91db-488542217586"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II ViT Direct Baseline Evaluation Framework\n","============================================================\n","Model: ViT_CASME2_Direct_Baseline\n","Task: micro_expression_recognition\n","Classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","Input size: 384x384\n","Starting CASME II ViT Direct Baseline comprehensive evaluation...\n","Creating CASME II test dataset...\n","Loading CASME II test dataset for evaluation...\n","Loaded 28 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 10 samples (35.7%)\n","  disgust: 7 samples (25.0%)\n","  happiness: 4 samples (14.3%)\n","  repression: 3 samples (10.7%)\n","  surprise: 3 samples (10.7%)\n","  sadness: 1 samples (3.6%)\n","Test set covers 16 subjects\n","Missing classes in test set: ['fear']\n","Preloading 28 test images to RAM...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test images: 100%|██████████| 28/28 [00:35<00:00,  1.26s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Test RAM caching completed: 28 valid images, ~0.05GB\n","Loading trained CASME II ViT model from: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/02_01_vit_direct/casme2_vit_direct_best_f1.pth\n","Checkpoint loaded using: standard\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["ViT feature dimension: 768\n","ViT CASME II: 768 -> 512 -> 128 -> 7\n","Model state loaded with strict=True\n","Model loaded successfully:\n","  Best validation F1: 0.2816\n","  Best validation accuracy: 0.4231\n","  Best epoch: 5\n","  Model classes: 7\n","Running CASME II ViT model inference on test set...\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Inference: 100%|██████████| 2/2 [00:00<00:00,  2.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["CASME II inference completed: 28 samples in 0.88s\n","Predicted classes: ['others', 'disgust', 'happiness', 'repression', 'sadness']\n","True classes in test: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness']\n","Calculating comprehensive metrics for CASME II micro-expression recognition...\n","Test set contains labels: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness']\n","Model predicted classes: ['others', 'disgust', 'happiness', 'repression', 'sadness']\n","Macro F1 (available classes): 0.2514\n","Analyzing wrong predictions for CASME II micro-expression recognition...\n","Evaluation results saved:\n","  Main results: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/02_01_vit_direct/evaluation_results/casme2_vit_direct_evaluation_results.json\n","  Wrong predictions: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/02_01_vit_direct/evaluation_results/casme2_vit_direct_wrong_predictions.json\n","\n","============================================================\n","CASME II ViT DIRECT BASELINE EVALUATION RESULTS\n","============================================================\n","Overall Performance (Macro - Available Classes):\n","  Accuracy:  0.4286\n","  Precision: 0.2189\n","  Recall:    0.2992\n","  F1 Score:  0.2514\n","  AUC:       0.5930\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.6087, AUC=0.8222, Support=10\n","  disgust [Present]: F1=0.4000, AUC=0.6871, Support=7\n","  happiness [Present]: F1=0.0000, AUC=0.6667, Support=4\n","  repression [Present]: F1=0.5000, AUC=0.6000, Support=3\n","  surprise [Present]: F1=0.0000, AUC=0.5600, Support=3\n","  sadness [Present]: F1=0.0000, AUC=0.2222, Support=1\n","  fear [Missing]: F1=0.0000, AUC=0.0000, Support=0\n","\n","Training vs Test Performance:\n","  Training Val F1:  0.2816\n","  Test F1:          0.2514\n","  F1 Difference:    +0.0302\n","  Training Val Acc: 0.4231\n","  Test Accuracy:    0.4286\n","  Acc Difference:   -0.0055\n","  Best Epoch:       5\n","\n","========================================\n","WRONG PREDICTIONS ANALYSIS\n","========================================\n","Total wrong predictions: 16 / 28\n","Overall error rate: 57.14%\n","\n","Errors by True Class:\n","  others: 3 errors\n","    - sub04_EP13_02f_others.jpg -> predicted as disgust\n","    - sub03_EP07_03_others.jpg -> predicted as disgust\n","    - sub24_EP08_02_others.jpg -> predicted as sadness\n","  disgust: 4 errors\n","    - sub05_EP09_05f_disgust.jpg -> predicted as others\n","    - sub15_EP08_02_disgust.jpg -> predicted as others\n","    - sub17_EP13_04_disgust.jpg -> predicted as repression\n","    ... and 1 more\n","  happiness: 4 errors\n","    - sub23_EP02_01_happiness.jpg -> predicted as others\n","    - sub14_EP09_06_happiness.jpg -> predicted as disgust\n","    - sub12_EP03_04_happiness.jpg -> predicted as disgust\n","    ... and 1 more\n","  repression: 1 errors\n","    - sub16_EP01_08_repression.jpg -> predicted as others\n","  surprise: 3 errors\n","    - sub17_EP01_13_surprise.jpg -> predicted as happiness\n","    - sub12_EP02_05_surprise.jpg -> predicted as disgust\n","    - sub02_EP11_01_surprise.jpg -> predicted as others\n","  sadness: 1 errors\n","    - sub17_EP15_03_sadness.jpg -> predicted as repression\n","\n","Subject-Level Performance:\n","  sub09: 1.000 (1/1)\n","  sub26: 1.000 (3/3)\n","  sub10: 1.000 (1/1)\n","  sub20: 1.000 (1/1)\n","  sub05: 0.667 (2/3)\n","\n","Most Common Confusion Patterns:\n","  disgust_to_others: 3 cases\n","  others_to_disgust: 2 cases\n","  happiness_to_disgust: 2 cases\n","\n","Inference Performance:\n","  Total time: 0.88s\n","  Speed: 31.4 ms/sample\n","\n","Missing Classes: ['fear']\n","\n","============================================================\n","CASME II ViT DIRECT BASELINE EVALUATION COMPLETED\n","============================================================\n","Next: Analysis and comparison with literature baselines\n"]}]},{"cell_type":"code","source":["# @title Cell 4: CASME II ViT Direct Baseline Confusion Matrix Generation\n","\n","# File: 02_01_ViT_Direct_Baseline_Cell4.py\n","# Location: experiments/02_01_ViT_Direct_Baseline.ipynb\n","# Purpose: Generate professional confusion matrix and comprehensive analysis for CASME II micro-expression recognition\n","# Dependencies: Trained model evaluation results from Cell 3\n","\n","import json\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import seaborn as sns\n","from datetime import datetime\n","\n","print(\"CASME II ViT Direct Baseline Confusion Matrix Generation\")\n","print(\"=\" * 60)\n","\n","# Project paths configuration\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/02_01_vit_casme2-af\"\n","\n","def find_evaluation_json_files_casme2(results_path):\n","    \"\"\"Find CASME II evaluation JSON files\"\"\"\n","    json_files = {}\n","\n","    eval_dir = f\"{results_path}/evaluation_results\"\n","\n","    if os.path.exists(eval_dir):\n","        # Look for main evaluation results\n","        eval_files = glob.glob(f\"{eval_dir}/casme2_vit_direct_evaluation_results.json\")\n","        if eval_files:\n","            json_files['main'] = eval_files[0]\n","            print(f\"Found CASME II evaluation file: {os.path.basename(eval_files[0])}\")\n","\n","        # Look for wrong predictions analysis\n","        wrong_files = glob.glob(f\"{eval_dir}/casme2_vit_direct_wrong_predictions.json\")\n","        if wrong_files:\n","            json_files['wrong_predictions'] = wrong_files[0]\n","            print(f\"Found wrong predictions file: {os.path.basename(wrong_files[0])}\")\n","\n","        if not json_files:\n","            print(f\"WARNING: No evaluation results found in {eval_dir}\")\n","            print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","    else:\n","        print(f\"ERROR: Evaluation directory not found: {eval_dir}\")\n","\n","    return json_files\n","\n","def load_evaluation_results_casme2(json_path):\n","    \"\"\"Load and parse CASME II evaluation results JSON\"\"\"\n","    try:\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"Successfully loaded evaluation results from: {os.path.basename(json_path)}\")\n","        return data\n","    except Exception as e:\n","        print(f\"ERROR loading {json_path}: {str(e)}\")\n","        return None\n","\n","def calculate_weighted_f1_casme2(per_class_performance):\n","    \"\"\"Calculate weighted F1 score for CASME II micro-expression classes\"\"\"\n","    # Only count classes that have test samples (support > 0)\n","    total_support = sum([class_data['support'] for class_data in per_class_performance.values()\n","                        if class_data['support'] > 0])\n","\n","    if total_support == 0:\n","        return 0.0\n","\n","    weighted_f1 = 0.0\n","\n","    for class_name, class_data in per_class_performance.items():\n","        if class_data['support'] > 0:  # Only include classes with test samples\n","            weight = class_data['support'] / total_support\n","            weighted_f1 += class_data['f1_score'] * weight\n","\n","    return weighted_f1\n","\n","def calculate_balanced_accuracy_casme2(confusion_matrix):\n","    \"\"\"\n","    Calculate balanced accuracy for CASME II 7-class micro-expression recognition\n","    Handles classes with zero support (missing in test set)\n","    \"\"\"\n","    cm = np.array(confusion_matrix)\n","    n_classes = cm.shape[0]\n","\n","    per_class_balanced_acc = []\n","\n","    # Find classes with actual test samples\n","    classes_with_samples = []\n","    for i in range(n_classes):\n","        if cm[i, :].sum() > 0:  # Class has test samples\n","            classes_with_samples.append(i)\n","\n","    for i in classes_with_samples:\n","        # True positives, false negatives, false positives, true negatives for class i\n","        tp = cm[i, i]\n","        fn = cm[i, :].sum() - tp  # Sum of row i minus diagonal\n","        fp = cm[:, i].sum() - tp  # Sum of column i minus diagonal\n","        tn = cm.sum() - tp - fn - fp  # Total minus TP, FN, FP\n","\n","        # Calculate sensitivity and specificity for class i\n","        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n","\n","        # Per-class balanced accuracy\n","        class_balanced_acc = (sensitivity + specificity) / 2\n","        per_class_balanced_acc.append(class_balanced_acc)\n","\n","    # Overall balanced accuracy (mean of available classes)\n","    balanced_acc = np.mean(per_class_balanced_acc) if per_class_balanced_acc else 0.0\n","\n","    return balanced_acc\n","\n","def determine_text_color_casme2(color_value, threshold=0.5):\n","    \"\"\"Determine optimal text color based on background intensity\"\"\"\n","    return 'white' if color_value > threshold else 'black'\n","\n","def analyze_missing_classes_casme2(data):\n","    \"\"\"Analyze missing classes in CASME II test set\"\"\"\n","    meta = data['evaluation_metadata']\n","    available_classes = meta.get('available_classes', [])\n","    missing_classes = meta.get('missing_classes', [])\n","\n","    print(f\"Class Analysis:\")\n","    print(f\"  Available in test: {available_classes}\")\n","    print(f\"  Missing from test: {missing_classes}\")\n","\n","    return {\n","        'available': available_classes,\n","        'missing': missing_classes,\n","        'total_classes': len(meta['class_names'])\n","    }\n","\n","def create_confusion_matrix_plot_casme2(data, output_path):\n","    \"\"\"Create professional confusion matrix visualization for CASME II micro-expression recognition\"\"\"\n","\n","    # Extract data\n","    meta = data['evaluation_metadata']\n","    class_names = meta['class_names']\n","    cm = np.array(data['confusion_matrix'], dtype=int)\n","    overall = data['overall_performance']\n","    per_class = data['per_class_performance']\n","\n","    print(f\"Processing confusion matrix for CASME II classes: {class_names}\")\n","    print(f\"Confusion matrix shape: {cm.shape}\")\n","\n","    # Calculate comprehensive metrics\n","    macro_f1 = overall.get('macro_f1', 0.0)\n","    accuracy = overall.get('accuracy', 0.0)\n","    weighted_f1 = calculate_weighted_f1_casme2(per_class)\n","    balanced_acc = calculate_balanced_accuracy_casme2(cm)\n","\n","    print(f\"Calculated metrics - Macro F1: {macro_f1:.4f}, Weighted F1: {weighted_f1:.4f}, \"\n","          f\"Balanced Acc: {balanced_acc:.4f}, Accuracy: {accuracy:.4f}\")\n","\n","    # Row-wise normalization for percentage display\n","    row_sums = cm.sum(axis=1, keepdims=True)\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        cm_pct = np.divide(cm, row_sums, where=(row_sums!=0))\n","        cm_pct = np.nan_to_num(cm_pct)\n","\n","    # Create visualization with appropriate size for 7 classes\n","    fig, ax = plt.subplots(figsize=(12, 10))\n","\n","    # Color scheme optimized for micro-expression research\n","    cmap = 'Blues'\n","\n","    # Create heatmap with improved color scaling\n","    im = ax.imshow(cm_pct, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=0.8)\n","\n","    # Add colorbar\n","    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","    cbar.set_label('True Class Percentage', rotation=270, labelpad=15, fontsize=11)\n","\n","    # Annotate cells with count and percentage\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            count = cm[i, j]\n","\n","            # Handle percentage calculation for classes with 0 samples\n","            if row_sums[i, 0] > 0:\n","                percentage = cm_pct[i, j] * 100\n","                text = f\"{count}\\n{percentage:.1f}%\"\n","            else:\n","                text = f\"{count}\\n(N/A)\"  # Class has no test samples\n","\n","            # Determine text color based on cell intensity\n","            cell_value = cm_pct[i, j]\n","            text_color = determine_text_color_casme2(cell_value, threshold=0.4)\n","\n","            ax.text(j, i, text, ha=\"center\", va=\"center\",\n","                   color=text_color, fontsize=9, fontweight='bold')\n","\n","    # Configure axes with micro-expression class names\n","    ax.set_xticks(np.arange(len(class_names)))\n","    ax.set_yticks(np.arange(len(class_names)))\n","    ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=10)\n","    ax.set_yticklabels(class_names, fontsize=10)\n","    ax.set_xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n","    ax.set_ylabel(\"True Label\", fontsize=12, fontweight='bold')\n","\n","    # Add note about missing classes if any\n","    missing_classes = meta.get('missing_classes', [])\n","    if missing_classes:\n","        note_text = f\"Note: Missing classes in test set: {', '.join(missing_classes)}\"\n","        ax.text(0.02, 0.98, note_text, transform=ax.transAxes, fontsize=9,\n","                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n","\n","    # Create comprehensive title for micro-expression research\n","    title = f\"CASME II Micro-Expression Recognition\\n\"\n","    title += f\"Acc: {accuracy:.4f}  |  Macro F1: {macro_f1:.4f}  |  Weighted F1: {weighted_f1:.4f}  |  Balanced Acc: {balanced_acc:.4f}\"\n","    ax.set_title(title, fontsize=12, pad=25, fontweight='bold')\n","\n","    # Adjust layout and save\n","    plt.tight_layout()\n","    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n","    plt.close(fig)\n","\n","    print(f\"Confusion matrix saved to: {os.path.basename(output_path)}\")\n","\n","    return {\n","        'accuracy': accuracy,\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        'balanced_accuracy': balanced_acc,\n","        'missing_classes': missing_classes\n","    }\n","\n","def create_per_class_performance_chart_casme2(data, output_path):\n","    \"\"\"Create per-class performance visualization for CASME II\"\"\"\n","    per_class = data['per_class_performance']\n","    class_names = data['evaluation_metadata']['class_names']\n","\n","    # Extract metrics for each class\n","    classes = []\n","    f1_scores = []\n","    precisions = []\n","    recalls = []\n","    supports = []\n","    in_test_flags = []\n","\n","    for class_name in class_names:\n","        class_data = per_class[class_name]\n","        classes.append(class_name)\n","        f1_scores.append(class_data['f1_score'])\n","        precisions.append(class_data['precision'])\n","        recalls.append(class_data['recall'])\n","        supports.append(class_data['support'])\n","        in_test_flags.append(class_data['in_test_set'])\n","\n","    # Create grouped bar chart\n","    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n","\n","    x = np.arange(len(classes))\n","    width = 0.25\n","\n","    # Top plot: F1, Precision, Recall\n","    bars1 = ax1.bar(x - width, f1_scores, width, label='F1 Score', alpha=0.8, color='steelblue')\n","    bars2 = ax1.bar(x, precisions, width, label='Precision', alpha=0.8, color='orange')\n","    bars3 = ax1.bar(x + width, recalls, width, label='Recall', alpha=0.8, color='green')\n","\n","    ax1.set_xlabel('Emotion Classes', fontweight='bold')\n","    ax1.set_ylabel('Score', fontweight='bold')\n","    ax1.set_title('CASME II Per-Class Performance Metrics', fontweight='bold', pad=20)\n","    ax1.set_xticks(x)\n","    ax1.set_xticklabels(classes, rotation=45, ha='right')\n","    ax1.legend()\n","    ax1.grid(axis='y', alpha=0.3)\n","    ax1.set_ylim(0, 1.0)\n","\n","    # Add value labels on bars\n","    for bars in [bars1, bars2, bars3]:\n","        for bar, in_test in zip(bars, in_test_flags):\n","            height = bar.get_height()\n","            if in_test:\n","                ax1.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n","                           xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n","            else:\n","                # Mark missing classes\n","                ax1.annotate('N/A', xy=(bar.get_x() + bar.get_width() / 2, height),\n","                           xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom',\n","                           fontsize=8, color='red', fontweight='bold')\n","\n","    # Bottom plot: Support (sample count)\n","    bars4 = ax2.bar(x, supports, color='purple', alpha=0.7)\n","    ax2.set_xlabel('Emotion Classes', fontweight='bold')\n","    ax2.set_ylabel('Number of Test Samples', fontweight='bold')\n","    ax2.set_title('CASME II Test Set Class Distribution', fontweight='bold')\n","    ax2.set_xticks(x)\n","    ax2.set_xticklabels(classes, rotation=45, ha='right')\n","    ax2.grid(axis='y', alpha=0.3)\n","\n","    # Add value labels on support bars\n","    for bar, support in zip(bars4, supports):\n","        height = bar.get_height()\n","        ax2.annotate(f'{int(support)}', xy=(bar.get_x() + bar.get_width() / 2, height),\n","                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=10)\n","\n","    plt.tight_layout()\n","    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n","    plt.close(fig)\n","\n","    print(f\"Per-class performance chart saved to: {os.path.basename(output_path)}\")\n","\n","def generate_performance_summary_casme2(evaluation_data, wrong_predictions_data=None):\n","    \"\"\"Generate comprehensive performance summary for CASME II\"\"\"\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\")\n","    print(\"=\" * 60)\n","\n","    # Overall performance\n","    overall = evaluation_data['overall_performance']\n","    meta = evaluation_data['evaluation_metadata']\n","\n","    print(f\"Dataset: {meta['dataset']}\")\n","    print(f\"Test samples: {meta['test_samples']}\")\n","    print(f\"Model: {meta['model_type']}\")\n","    print(f\"Evaluation date: {meta['evaluation_timestamp']}\")\n","\n","    print(f\"\\nOverall Performance:\")\n","    print(f\"  Accuracy:         {overall['accuracy']:.4f}\")\n","    print(f\"  Macro Precision:  {overall['macro_precision']:.4f}\")\n","    print(f\"  Macro Recall:     {overall['macro_recall']:.4f}\")\n","    print(f\"  Macro F1:         {overall['macro_f1']:.4f}\")\n","    print(f\"  Macro AUC:        {overall['macro_auc']:.4f}\")\n","\n","    # Per-class performance\n","    print(f\"\\nPer-Class Performance:\")\n","    per_class = evaluation_data['per_class_performance']\n","\n","    print(f\"{'Class':<12} {'F1':<8} {'Precision':<10} {'Recall':<8} {'AUC':<8} {'Support':<8} {'In Test'}\")\n","    print(\"-\" * 65)\n","\n","    for class_name, metrics in per_class.items():\n","        in_test = \"Yes\" if metrics['in_test_set'] else \"No\"\n","        print(f\"{class_name:<12} {metrics['f1_score']:<8.4f} {metrics['precision']:<10.4f} \"\n","              f\"{metrics['recall']:<8.4f} {metrics['auc']:<8.4f} {metrics['support']:<8} {in_test}\")\n","\n","    # Training vs test performance\n","    if 'training_information' in evaluation_data:\n","        training = evaluation_data['training_information']\n","        print(f\"\\nTraining vs Test Comparison:\")\n","        print(f\"  Training Val F1:  {training['best_val_f1']:.4f}\")\n","        print(f\"  Test F1:          {overall['macro_f1']:.4f}\")\n","        print(f\"  Performance Gap:  {training['best_val_f1'] - overall['macro_f1']:+.4f}\")\n","        print(f\"  Best Epoch:       {training['best_epoch']}\")\n","\n","    # Class imbalance analysis\n","    missing_classes = meta.get('missing_classes', [])\n","    available_classes = meta.get('available_classes', [])\n","\n","    print(f\"\\nClass Availability Analysis:\")\n","    print(f\"  Available classes: {len(available_classes)}/7\")\n","    print(f\"  Missing classes: {missing_classes if missing_classes else 'None'}\")\n","\n","    # Wrong predictions summary if available\n","    if wrong_predictions_data:\n","        wrong_meta = wrong_predictions_data['analysis_metadata']\n","        print(f\"\\nError Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']}/{wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        # Top confusion patterns\n","        patterns = wrong_predictions_data.get('confusion_patterns', {})\n","        if patterns:\n","            print(f\"\\nTop Confusion Patterns:\")\n","            sorted_patterns = sorted(patterns.items(), key=lambda x: x[1], reverse=True)[:3]\n","            for pattern, count in sorted_patterns:\n","                print(f\"  {pattern}: {count} cases\")\n","\n","    print(f\"\\nInference Performance:\")\n","    inference = evaluation_data['inference_performance']\n","    print(f\"  Total time: {inference['total_time_seconds']:.2f}s\")\n","    print(f\"  Speed: {inference['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","# Find evaluation JSON files\n","json_files = find_evaluation_json_files_casme2(RESULTS_ROOT)\n","\n","if not json_files:\n","    print(f\"ERROR: No evaluation JSON files found in {RESULTS_ROOT}\")\n","    print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","else:\n","    print(f\"Found {len(json_files)} evaluation file(s)\")\n","\n","# Create output directory\n","output_dir = f\"{RESULTS_ROOT}/confusion_matrix_analysis\"\n","Path(output_dir).mkdir(parents=True, exist_ok=True)\n","\n","# Process evaluation results\n","results_summary = {}\n","generated_files = []\n","\n","if 'main' in json_files:\n","    # Load main evaluation data\n","    eval_data = load_evaluation_results_casme2(json_files['main'])\n","\n","    # Load wrong predictions data if available\n","    wrong_data = None\n","    if 'wrong_predictions' in json_files:\n","        wrong_data = load_evaluation_results_casme2(json_files['wrong_predictions'])\n","\n","    if eval_data is not None:\n","        try:\n","            # Analyze missing classes\n","            class_analysis = analyze_missing_classes_casme2(eval_data)\n","\n","            # Generate confusion matrix\n","            cm_output_path = os.path.join(output_dir, \"confusion_matrix_CASME2_ViT_Direct.png\")\n","            metrics = create_confusion_matrix_plot_casme2(eval_data, cm_output_path)\n","            generated_files.append(cm_output_path)\n","\n","            # Generate per-class performance chart\n","            perf_output_path = os.path.join(output_dir, \"per_class_performance_CASME2_ViT_Direct.png\")\n","            create_per_class_performance_chart_casme2(eval_data, perf_output_path)\n","            generated_files.append(perf_output_path)\n","\n","            results_summary['casme2'] = metrics\n","            results_summary['casme2']['class_analysis'] = class_analysis\n","\n","            print(f\"SUCCESS: Visualization files generated successfully\")\n","\n","        except Exception as e:\n","            print(f\"ERROR: Failed to generate visualizations: {str(e)}\")\n","            import traceback\n","            traceback.print_exc()\n","\n","        # Generate comprehensive summary\n","        generate_performance_summary_casme2(eval_data, wrong_data)\n","\n","    else:\n","        print(\"ERROR: Could not load evaluation data\")\n","else:\n","    print(\"ERROR: No main evaluation results found\")\n","\n","# Final summary\n","if generated_files:\n","    print(f\"\\n\" + \"=\" * 60)\n","    print(\"CASME II CONFUSION MATRIX GENERATION COMPLETED\")\n","    print(\"=\" * 60)\n","\n","    print(f\"Generated visualization files:\")\n","    for file_path in generated_files:\n","        filename = os.path.basename(file_path)\n","        print(f\"  {filename}\")\n","\n","    if 'casme2' in results_summary:\n","        casme2 = results_summary['casme2']\n","        print(f\"\\nFinal Performance Summary:\")\n","        print(f\"  Accuracy:       {casme2['accuracy']:.4f}\")\n","        print(f\"  Macro F1:       {casme2['macro_f1']:.4f}\")\n","        print(f\"  Weighted F1:    {casme2['weighted_f1']:.4f}\")\n","        print(f\"  Balanced Acc:   {casme2['balanced_accuracy']:.4f}\")\n","\n","        if 'class_analysis' in casme2:\n","            analysis = casme2['class_analysis']\n","            print(f\"  Available classes: {len(analysis['available'])}/{analysis['total_classes']}\")\n","            print(f\"  Missing classes: {len(analysis['missing'])}\")\n","\n","    print(f\"\\nFiles saved in: {output_dir}\")\n","    print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","else:\n","    print(f\"\\nERROR: No visualizations were generated\")\n","    print(\"Please check:\")\n","    print(\"1. Cell 3 evaluation results exist\")\n","    print(\"2. JSON file structure is correct\")\n","    print(\"3. No file permission issues\")\n","\n","print(\"\\nCell 4 completed - CASME II confusion matrix analysis generated\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"cellView":"form","id":"kCCFh-VxP02Z","executionInfo":{"status":"ok","timestamp":1758802379012,"user_tz":-420,"elapsed":3301,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"426f9e91-7193-494f-86df-4e3699dec413"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II ViT Direct Baseline Confusion Matrix Generation\n","============================================================\n","Found CASME II evaluation file: casme2_vit_direct_evaluation_results.json\n","Found wrong predictions file: casme2_vit_direct_wrong_predictions.json\n","Found 2 evaluation file(s)\n","Successfully loaded evaluation results from: casme2_vit_direct_evaluation_results.json\n","Successfully loaded evaluation results from: casme2_vit_direct_wrong_predictions.json\n","Class Analysis:\n","  Available in test: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness']\n","  Missing from test: ['fear']\n","Processing confusion matrix for CASME II classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","Confusion matrix shape: (7, 7)\n","Calculated metrics - Macro F1: 0.2514, Weighted F1: 0.3710, Balanced Acc: 0.5854, Accuracy: 0.4286\n","Confusion matrix saved to: confusion_matrix_CASME2_ViT_Direct.png\n","Per-class performance chart saved to: per_class_performance_CASME2_ViT_Direct.png\n","SUCCESS: Visualization files generated successfully\n","\n","============================================================\n","CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\n","============================================================\n","Dataset: CASME_II\n","Test samples: 28\n","Model: ViT_CASME2_Direct_Baseline\n","Evaluation date: 20250925_121255\n","\n","Overall Performance:\n","  Accuracy:         0.4286\n","  Macro Precision:  0.2189\n","  Macro Recall:     0.2992\n","  Macro F1:         0.2514\n","  Macro AUC:        0.5930\n","\n","Per-Class Performance:\n","Class        F1       Precision  Recall   AUC      Support  In Test\n","-----------------------------------------------------------------\n","others       0.6087   0.5385     0.7000   0.8222   10       Yes\n","disgust      0.4000   0.3750     0.4286   0.6871   7        Yes\n","happiness    0.0000   0.0000     0.0000   0.6667   4        Yes\n","repression   0.5000   0.4000     0.6667   0.6000   3        Yes\n","surprise     0.0000   0.0000     0.0000   0.5600   3        Yes\n","sadness      0.0000   0.0000     0.0000   0.2222   1        Yes\n","fear         0.0000   0.0000     0.0000   0.0000   0        No\n","\n","Training vs Test Comparison:\n","  Training Val F1:  0.2816\n","  Test F1:          0.2514\n","  Performance Gap:  +0.0302\n","  Best Epoch:       5\n","\n","Class Availability Analysis:\n","  Available classes: 6/7\n","  Missing classes: ['fear']\n","\n","Error Analysis:\n","  Total errors: 16/28\n","  Error rate: 57.14%\n","\n","Top Confusion Patterns:\n","  disgust_to_others: 3 cases\n","  others_to_disgust: 2 cases\n","  happiness_to_disgust: 2 cases\n","\n","Inference Performance:\n","  Total time: 0.88s\n","  Speed: 31.4 ms/sample\n","\n","============================================================\n","CASME II CONFUSION MATRIX GENERATION COMPLETED\n","============================================================\n","Generated visualization files:\n","  confusion_matrix_CASME2_ViT_Direct.png\n","  per_class_performance_CASME2_ViT_Direct.png\n","\n","Final Performance Summary:\n","  Accuracy:       0.4286\n","  Macro F1:       0.2514\n","  Weighted F1:    0.3710\n","  Balanced Acc:   0.5854\n","  Available classes: 6/7\n","  Missing classes: 1\n","\n","Files saved in: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/02_01_vit_direct/confusion_matrix_analysis\n","Analysis completed at: 2025-09-25 12:12:59\n","\n","Cell 4 completed - CASME II confusion matrix analysis generated\n"]}]}]}