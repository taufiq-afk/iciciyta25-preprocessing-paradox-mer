{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM4OLdZnSgGkbIOHYDVaglB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"432b35a1f7374eb7af018d8ce6caba23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c985c8e699f3429b9470e5d864e86613","IPY_MODEL_25cb31abc30f4ce896512da16887884f","IPY_MODEL_808c4997251a4112a4d762fd8e433522"],"layout":"IPY_MODEL_27fdfb8d226848a68cd4a4c56bb29268"}},"c985c8e699f3429b9470e5d864e86613":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57c28082d36746d3aa9415962950756a","placeholder":"​","style":"IPY_MODEL_e22c592ea213444b80cced171aa15c5f","value":"preprocessor_config.json: 100%"}},"25cb31abc30f4ce896512da16887884f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a237ad4389a4208837443ef77669d3a","max":283,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23b6ea05cdb3498dafec4d86f1afea67","value":283}},"808c4997251a4112a4d762fd8e433522":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87775f880db6435db2d1353b45f5feb6","placeholder":"​","style":"IPY_MODEL_5a025c662155442a8a926134a7907525","value":" 283/283 [00:00&lt;00:00, 28.0kB/s]"}},"27fdfb8d226848a68cd4a4c56bb29268":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57c28082d36746d3aa9415962950756a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e22c592ea213444b80cced171aa15c5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a237ad4389a4208837443ef77669d3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23b6ea05cdb3498dafec4d86f1afea67":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87775f880db6435db2d1353b45f5feb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a025c662155442a8a926134a7907525":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27e98d72311444bca4059604234f9375":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c912374a59d048ffa310e4ff13758041","IPY_MODEL_3770be4cbd1c4a60abec294de5b037b9","IPY_MODEL_35fd47d8a7014427897a94048c2c8a03"],"layout":"IPY_MODEL_c69dff309dba490bb2911279822d618c"}},"c912374a59d048ffa310e4ff13758041":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fb3dd5b4796479bb60f6fee72dbc2fc","placeholder":"​","style":"IPY_MODEL_9d6934f967624069bd4ba03db5cff957","value":"config.json: "}},"3770be4cbd1c4a60abec294de5b037b9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8abec1a206374a0ba323ac5f6065496f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_05b991d1b5ff47d18830da0ca9c1bcfa","value":1}},"35fd47d8a7014427897a94048c2c8a03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e90ade2b4e334546adab8c187efc468a","placeholder":"​","style":"IPY_MODEL_b90407b9301645e2b6aea25715c09fcb","value":" 69.9k/? [00:00&lt;00:00, 4.35MB/s]"}},"c69dff309dba490bb2911279822d618c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fb3dd5b4796479bb60f6fee72dbc2fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d6934f967624069bd4ba03db5cff957":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8abec1a206374a0ba323ac5f6065496f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"05b991d1b5ff47d18830da0ca9c1bcfa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e90ade2b4e334546adab8c187efc468a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b90407b9301645e2b6aea25715c09fcb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e653de87442c4051aacc0b46136f4ef6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e53bb769eacb44349de40daada4d2fdc","IPY_MODEL_d62f620e77cb4b86b31c6548d6bebb6e","IPY_MODEL_197c210a0f404000a1bb5dcf3c05d514"],"layout":"IPY_MODEL_aec8cf8bc3e54f11ab25f33bec55b98a"}},"e53bb769eacb44349de40daada4d2fdc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db77d92b3075451abac99c8e63cbea64","placeholder":"​","style":"IPY_MODEL_01e5c37533da4eac9c4feef7845e03b6","value":"model.safetensors: 100%"}},"d62f620e77cb4b86b31c6548d6bebb6e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de40bed0cf0746608b12ca17f5d45a56","max":293950574,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e1af581ad0f746958e4a74bee467b692","value":293950574}},"197c210a0f404000a1bb5dcf3c05d514":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3dbc04ef26024294b1a98652b1acd62f","placeholder":"​","style":"IPY_MODEL_487b868b1fca493082e3f025489427c9","value":" 294M/294M [00:03&lt;00:00, 113MB/s]"}},"aec8cf8bc3e54f11ab25f33bec55b98a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db77d92b3075451abac99c8e63cbea64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01e5c37533da4eac9c4feef7845e03b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de40bed0cf0746608b12ca17f5d45a56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1af581ad0f746958e4a74bee467b692":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3dbc04ef26024294b1a98652b1acd62f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"487b868b1fca493082e3f025489427c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["432b35a1f7374eb7af018d8ce6caba23","c985c8e699f3429b9470e5d864e86613","25cb31abc30f4ce896512da16887884f","808c4997251a4112a4d762fd8e433522","27fdfb8d226848a68cd4a4c56bb29268","57c28082d36746d3aa9415962950756a","e22c592ea213444b80cced171aa15c5f","5a237ad4389a4208837443ef77669d3a","23b6ea05cdb3498dafec4d86f1afea67","87775f880db6435db2d1353b45f5feb6","5a025c662155442a8a926134a7907525","27e98d72311444bca4059604234f9375","c912374a59d048ffa310e4ff13758041","3770be4cbd1c4a60abec294de5b037b9","35fd47d8a7014427897a94048c2c8a03","c69dff309dba490bb2911279822d618c","4fb3dd5b4796479bb60f6fee72dbc2fc","9d6934f967624069bd4ba03db5cff957","8abec1a206374a0ba323ac5f6065496f","05b991d1b5ff47d18830da0ca9c1bcfa","e90ade2b4e334546adab8c187efc468a","b90407b9301645e2b6aea25715c09fcb","e653de87442c4051aacc0b46136f4ef6","e53bb769eacb44349de40daada4d2fdc","d62f620e77cb4b86b31c6548d6bebb6e","197c210a0f404000a1bb5dcf3c05d514","aec8cf8bc3e54f11ab25f33bec55b98a","db77d92b3075451abac99c8e63cbea64","01e5c37533da4eac9c4feef7845e03b6","de40bed0cf0746608b12ca17f5d45a56","e1af581ad0f746958e4a74bee467b692","3dbc04ef26024294b1a98652b1acd62f","487b868b1fca493082e3f025489427c9"]},"collapsed":true,"cellView":"form","id":"UADLlV-BGDLV","executionInfo":{"status":"ok","timestamp":1761799533479,"user_tz":-420,"elapsed":70338,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"194e58f4-b0df-4d31-b8e7-bda127c9c462"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","CASME II MULTI-FRAME POOLFORMER INFRASTRUCTURE\n","============================================================\n","\n","[1] Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully\n","\n","[2] Importing required libraries...\n","CASME II Multi-Frame PoolFormer - Infrastructure Configuration\n","============================================================\n","Loading CASME II Phase 3 dataset metadata...\n","Dataset: CASME2_MultiFrameSampling\n","Phase: Phase 3\n","Total images: 2774\n","Source samples: 255\n","Extraction strategy: {'train': 'multi_frame_windows_with_fallback', 'val': 'key_frames_only', 'test': 'key_frames_only', 'fallback_method': 'nearest_frame_duplication'}\n","Using PoolFormer-M48 for enhanced micro-expression recognition (73M parameters)\n","\n","==================================================\n","OPTIMIZED EXPERIMENT CONFIGURATION\n","==================================================\n","Loss Function: Focal Loss\n","  Gamma: 2.0\n","  Alpha Weights (per-class): [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","  Alpha Sum Validation: 0.999\n","PoolFormer Model: sail/poolformer_m48\n","Model Parameters: 73M\n","==================================================\n","\n","Device: cpu\n","GPU: CPU (0.0 GB)\n","Default GPU: Conservative settings\n","RAM preload workers: 32 (parallel image loading)\n","\n","Loading class distribution...\n","Using class distribution from split_metadata (v3 format)\n","\n","Train distribution: {'others': 1027, 'disgust': 650, 'happiness': 325, 'repression': 273, 'surprise': 260, 'sadness': 65, 'fear': 13}\n","Validation distribution: {'others': 30, 'disgust': 18, 'happiness': 9, 'repression': 9, 'surprise': 6, 'sadness': 3, 'fear': 3}\n","Test distribution: {'others': 30, 'disgust': 21, 'happiness': 12, 'repression': 8, 'surprise': 9, 'sadness': 3}\n","Applied Focal Loss alpha weights: [0.053 0.067 0.094 0.102 0.106 0.201 0.376]\n","Alpha weights sum: 0.999\n","\n","PoolFormer Configuration Summary:\n","  Model: sail/poolformer_m48\n","  Variant: M48\n","  Parameters: 73M\n","  Input size: 384px\n","  Learning rate: 1e-05\n","  Batch size: 12\n","  Dataset phase: v3\n","  Frame strategy: multi_frame_sampling\n","  Train augmentation: temporal_windows\n","  Frame types: ['onset', 'apex', 'offset']\n","  Train images: 2613\n","\n","Setting up PoolFormer Image Processor for 384px input...\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/283 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"432b35a1f7374eb7af018d8ce6caba23"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["PoolFormer Image Processor configured for 384px with token mixing\n","\n","Dataset paths:\n","Train: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v3/train\n","Validation: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v3/val\n","Test: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v3/test\n","\n","PoolFormer CASME II architecture validation...\n"]},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e98d72311444bca4059604234f9375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/294M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e653de87442c4051aacc0b46136f4ef6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["PoolFormer feature dimension: 768\n","PoolFormer CASME II: 768 -> 512 -> 128 -> 7\n","Validation successful: Output shape torch.Size([1, 7])\n","PoolFormer M48 with 73M parameters\n","Token mixing with pooling operations\n","\n","============================================================\n","CASME II MULTI-FRAME POOLFORMER CONFIGURATION COMPLETE\n","============================================================\n","Loss Configuration:\n","  Function: Optimized Focal Loss\n","  Gamma: 2.0\n","  Per-class Alpha: [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","  Alpha Sum: 0.999\n","\n","Model Configuration:\n","  Architecture: sail/poolformer_m48\n","  Variant: M48\n","  Parameters: 73M\n","  Input Resolution: 384px\n","  Feature Dimension: 768\n","\n","Dataset Configuration:\n","  Phase: v3\n","  Frame strategy: multi_frame_sampling\n","  Train augmentation: temporal_windows\n","  Classes: 7\n","  Frame types: ['onset', 'apex', 'offset']\n","  Train samples: 2613\n","  Weight Optimization: Per-class Alpha\n","\n","Next: Cell 2 - Dataset Loading and Multi-Frame PoolFormer Training Pipeline\n"]}],"source":["# @title Cell 1: CASME II Multi-Frame PoolFormer Infrastructure Configuration\n","\n","# File: 04_03_PoolFormer_CASME2_MFS_Cell1.py\n","# Location: experiments/04_03_PoolFormer_CASME2-MFS.ipynb\n","# Purpose: PoolFormer for CASME II micro-expression recognition with multi-frame sampling strategy\n","\n","# Mount Google Drive\n","from google.colab import drive\n","print(\"=\" * 60)\n","print(\"CASME II MULTI-FRAME POOLFORMER INFRASTRUCTURE\")\n","print(\"=\" * 60)\n","print(\"\\n[1] Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Google Drive mounted successfully\")\n","\n","print(\"\\n[2] Importing required libraries...\")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import json\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import time\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Project paths configuration - Phase 3 structure\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","DATASET_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/data_split_v3\"\n","CHECKPOINT_ROOT = f\"{PROJECT_ROOT}/models/04_03_poolformer_casme2_mfs\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/04_03_poolformer_casme2_mfs\"\n","\n","# Load CASME II Phase 3 dataset metadata\n","METADATA_TRAIN = f\"{DATASET_ROOT}/split_metadata_v3.json\"\n","PROCESSING_SUMMARY = f\"{DATASET_ROOT}/processing_summary_v3.json\"\n","\n","print(\"CASME II Multi-Frame PoolFormer - Infrastructure Configuration\")\n","print(\"=\" * 60)\n","\n","# Validate Phase 3 metadata files exist\n","if not os.path.exists(METADATA_TRAIN):\n","    raise FileNotFoundError(f\"Phase 3 metadata not found: {METADATA_TRAIN}\")\n","if not os.path.exists(PROCESSING_SUMMARY):\n","    raise FileNotFoundError(f\"Phase 3 processing summary not found: {PROCESSING_SUMMARY}\")\n","\n","# Load Phase 3 dataset metadata\n","print(\"Loading CASME II Phase 3 dataset metadata...\")\n","with open(METADATA_TRAIN, 'r') as f:\n","    casme2_metadata = json.load(f)\n","\n","with open(PROCESSING_SUMMARY, 'r') as f:\n","    processing_info = json.load(f)\n","\n","print(f\"Dataset: {processing_info['dataset']}\")\n","print(f\"Phase: {processing_info['phase']}\")\n","print(f\"Total images: {processing_info['total_images_copied']}\")\n","print(f\"Source samples: {processing_info.get('source_phase1_samples', 255)}\")\n","print(f\"Extraction strategy: {processing_info.get('extraction_strategy', {})}\")\n","\n","# =====================================================\n","# ADVANCED EXPERIMENT CONFIGURATION - Optimized Parameters\n","# =====================================================\n","\n","# FOCAL LOSS CONFIGURATION - Toggle and Advanced Parameters\n","USE_FOCAL_LOSS = True  # Set True to enable Focal Loss, False for CrossEntropy\n","FOCAL_LOSS_GAMMA = 2.0  # Focal loss focusing parameter (typically 1.0 - 3.0)\n","\n","# OPTIMIZED CLASS WEIGHTS CONFIGURATION - Inverse Square Root Frequency Approach\n","# CASME II classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","# Train distribution: [99, 63, 32, 27, 25, 7, 2] - inverse sqrt frequency approach\n","\n","# CrossEntropy Loss - Optimized inverse square root frequency weights\n","CROSSENTROPY_CLASS_WEIGHTS = [1.00, 1.25, 1.76, 1.91, 1.99, 3.76, 7.04]\n","\n","# Focal Loss - Normalized per-class alpha values (sum = 1.0)\n","FOCAL_LOSS_ALPHA_WEIGHTS = [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","\n","# POOLFORMER MODEL CONFIGURATION - Support m36 and m48 variants\n","POOLFORMER_MODEL_VARIANT = 'm48'  # Options: 'm36' or 'm48'\n","\n","# Dynamic PoolFormer model selection based on variant\n","if POOLFORMER_MODEL_VARIANT == 'm36':\n","    POOLFORMER_MODEL_NAME = 'sail/poolformer_m36'\n","    MODEL_PARAMS = '56M'\n","    print(\"Using PoolFormer-M36 for micro-expression analysis (56M parameters)\")\n","elif POOLFORMER_MODEL_VARIANT == 'm48':\n","    POOLFORMER_MODEL_NAME = 'sail/poolformer_m48'\n","    MODEL_PARAMS = '73M'\n","    print(\"Using PoolFormer-M48 for enhanced micro-expression recognition (73M parameters)\")\n","else:\n","    raise ValueError(f\"Unsupported POOLFORMER_MODEL_VARIANT: {POOLFORMER_MODEL_VARIANT}\")\n","\n","# Display experiment configuration\n","print(\"\\n\" + \"=\" * 50)\n","print(\"OPTIMIZED EXPERIMENT CONFIGURATION\")\n","print(\"=\" * 50)\n","print(f\"Loss Function: {'Focal Loss' if USE_FOCAL_LOSS else 'CrossEntropy Loss'}\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Alpha Weights (per-class): {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum Validation: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Class Weights (inverse sqrt freq): {CROSSENTROPY_CLASS_WEIGHTS}\")\n","print(f\"PoolFormer Model: {POOLFORMER_MODEL_NAME}\")\n","print(f\"Model Parameters: {MODEL_PARAMS}\")\n","print(\"=\" * 50)\n","\n","# Enhanced GPU configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n","gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n","\n","print(f\"\\nDevice: {device}\")\n","print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n","\n","# Hardware-optimized batch size for 384px input\n","if 'A100' in gpu_name:\n","    BATCH_SIZE = 20\n","    NUM_WORKERS = 8\n","    torch.backends.cudnn.benchmark = True\n","    print(\"A100: Optimized batch size for PoolFormer 384px\")\n","elif 'L4' in gpu_name:\n","    BATCH_SIZE = 16\n","    NUM_WORKERS = 8\n","    torch.backends.cudnn.benchmark = True\n","    print(\"L4: Balanced performance configuration\")\n","else:\n","    BATCH_SIZE = 12\n","    NUM_WORKERS = 4\n","    print(\"Default GPU: Conservative settings\")\n","\n","# RAM preloading workers (separate from DataLoader workers)\n","# Optimized for I/O-bound parallel image loading from Google Drive\n","RAM_PRELOAD_WORKERS = 32\n","print(f\"RAM preload workers: {RAM_PRELOAD_WORKERS} (parallel image loading)\")\n","\n","# CASME II class mapping and analysis\n","CASME2_CLASSES = ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CASME2_CLASSES)}\n","\n","# Smart class distribution loading - handles v1, v2, and v3 JSON formats\n","print(\"\\nLoading class distribution...\")\n","try:\n","    # Try v3 format first (from split_metadata with 'splits' key)\n","    if 'splits' in casme2_metadata:\n","        train_dist = casme2_metadata['splits']['train']['class_distribution']\n","        val_dist = casme2_metadata['splits']['val']['class_distribution']\n","        test_dist = casme2_metadata['splits']['test']['class_distribution']\n","        print(\"Using class distribution from split_metadata (v3 format)\")\n","    # Try v1 format (from split_metadata with direct keys)\n","    elif 'train' in casme2_metadata and 'class_distribution' in casme2_metadata['train']:\n","        train_dist = casme2_metadata['train']['class_distribution']\n","        val_dist = casme2_metadata['val']['class_distribution']\n","        test_dist = casme2_metadata['test']['class_distribution']\n","        print(\"Using class distribution from split_metadata (v1 format)\")\n","    else:\n","        # Fallback to v2 format (from processing_summary)\n","        train_dist = processing_info['class_preservation']['train']\n","        val_dist = processing_info['class_preservation']['val']\n","        test_dist = processing_info['class_preservation']['test']\n","        print(\"Using class distribution from processing_summary (v2 format)\")\n","except KeyError as e:\n","    raise KeyError(f\"Could not load class distribution from metadata. Missing key: {e}\")\n","\n","print(f\"\\nTrain distribution: {train_dist}\")\n","print(f\"Validation distribution: {val_dist}\")\n","print(f\"Test distribution: {test_dist}\")\n","\n","# Apply optimized class weights based on loss function selection\n","if USE_FOCAL_LOSS:\n","    # For Focal Loss - use normalized alpha weights (per-class importance)\n","    class_weights = torch.tensor(FOCAL_LOSS_ALPHA_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied Focal Loss alpha weights: {class_weights.cpu().numpy()}\")\n","    print(f\"Alpha weights sum: {class_weights.sum().item():.3f}\")\n","else:\n","    # For CrossEntropy - use inverse sqrt frequency weights\n","    class_weights = torch.tensor(CROSSENTROPY_CLASS_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied CrossEntropy class weights: {class_weights.cpu().numpy()}\")\n","\n","# CASME II PoolFormer Configuration\n","CASME2_POOLFORMER_CONFIG = {\n","    # Architecture configuration - PoolFormer specific\n","    'poolformer_model': POOLFORMER_MODEL_NAME,\n","    'model_variant': POOLFORMER_MODEL_VARIANT,\n","    'model_params': MODEL_PARAMS,\n","    'input_size': 384,\n","    'num_classes': 7,\n","    'dropout_rate': 0.2,\n","    'expected_feature_dim': 768,\n","\n","    # Training configuration (proven effective from medical imaging)\n","    'learning_rate': 1e-5,\n","    'weight_decay': 1e-5,\n","    'gradient_clip': 1.0,\n","    'num_epochs': 50,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'device': device,\n","\n","    # Scheduler configuration\n","    'scheduler_type': 'plateau',\n","    'scheduler_mode': 'max',\n","    'scheduler_factor': 0.5,\n","    'scheduler_patience': 3,\n","    'scheduler_min_lr': 1e-6,\n","    'scheduler_monitor': 'val_f1_macro',\n","\n","    # Optimized loss configuration\n","    'use_focal_loss': USE_FOCAL_LOSS,\n","    'focal_loss_gamma': FOCAL_LOSS_GAMMA,\n","    'focal_loss_alpha_weights': FOCAL_LOSS_ALPHA_WEIGHTS,\n","    'crossentropy_class_weights': CROSSENTROPY_CLASS_WEIGHTS,\n","    'class_weights': class_weights,\n","\n","    # Evaluation configuration\n","    'use_macro_avg': True,\n","    'early_stopping': False,\n","    'save_best_f1': True,\n","    'save_strategy': 'best_only',\n","\n","    # Phase 3 specific configuration\n","    'dataset_phase': 'v3',\n","    'frame_strategy': 'multi_frame_sampling',\n","    'train_augmentation': 'temporal_windows',\n","    'frame_types': ['onset', 'apex', 'offset'],\n","    'extraction_strategy': processing_info.get('extraction_strategy', {}),\n","    'copy_statistics': processing_info.get('copy_statistics', {})\n","}\n","\n","print(f\"\\nPoolFormer Configuration Summary:\")\n","print(f\"  Model: {CASME2_POOLFORMER_CONFIG['poolformer_model']}\")\n","print(f\"  Variant: {CASME2_POOLFORMER_CONFIG['model_variant'].upper()}\")\n","print(f\"  Parameters: {CASME2_POOLFORMER_CONFIG['model_params']}\")\n","print(f\"  Input size: {CASME2_POOLFORMER_CONFIG['input_size']}px\")\n","print(f\"  Learning rate: {CASME2_POOLFORMER_CONFIG['learning_rate']}\")\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","print(f\"  Dataset phase: {CASME2_POOLFORMER_CONFIG['dataset_phase']}\")\n","print(f\"  Frame strategy: {CASME2_POOLFORMER_CONFIG['frame_strategy']}\")\n","print(f\"  Train augmentation: {CASME2_POOLFORMER_CONFIG['train_augmentation']}\")\n","print(f\"  Frame types: {CASME2_POOLFORMER_CONFIG['frame_types']}\")\n","print(f\"  Train images: {processing_info.get('copy_statistics', {}).get('train', {}).get('total_images', 2613)}\")\n","\n","# =====================================================\n","# ADVANCED FOCAL LOSS IMPLEMENTATION - Per-Class Alpha Support\n","# =====================================================\n","\n","class OptimizedFocalLoss(nn.Module):\n","    \"\"\"\n","    Advanced Focal Loss implementation with per-class alpha support\n","    Paper: \"Focal Loss for Dense Object Detection\" (Lin et al., 2017)\n","\n","    Enhanced Formula: FL(p_t) = -α_t(1-p_t)^γ log(p_t)\n","\n","    Args:\n","        alpha (list/tensor): Per-class alpha weights (must sum to 1.0)\n","        gamma (float): Focusing parameter for hard examples (default: 2.0)\n","        reduction (str): Reduction method ('mean', 'sum', 'none')\n","    \"\"\"\n","\n","    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n","        super(OptimizedFocalLoss, self).__init__()\n","\n","        if alpha is not None:\n","            if isinstance(alpha, list):\n","                self.alpha = torch.tensor(alpha, dtype=torch.float32)\n","            else:\n","                self.alpha = alpha\n","\n","            # Validation: alpha should sum to 1.0 for proper normalization\n","            alpha_sum = self.alpha.sum().item()\n","            if abs(alpha_sum - 1.0) > 0.01:\n","                print(f\"Warning: Alpha weights sum to {alpha_sum:.3f}, expected 1.0\")\n","\n","        else:\n","            self.alpha = None\n","\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        # Calculate cross entropy loss\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","\n","        # Calculate p_t (probability of true class)\n","        pt = torch.exp(-ce_loss)\n","\n","        # Apply per-class alpha if provided\n","        if self.alpha is not None:\n","            if self.alpha.device != targets.device:\n","                self.alpha = self.alpha.to(targets.device)\n","            alpha_t = self.alpha.gather(0, targets)\n","        else:\n","            alpha_t = 1.0\n","\n","        # Apply focal loss formula: α_t(1-p_t)^γ * CE_loss\n","        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n","\n","        # Apply reduction\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","\n","# PoolFormer Architecture for CASME II\n","class PoolFormerCASME2Baseline(nn.Module):\n","    \"\"\"PoolFormer baseline for CASME II micro-expression recognition with token mixing\"\"\"\n","\n","    def __init__(self, num_classes, dropout_rate=0.2):\n","        super(PoolFormerCASME2Baseline, self).__init__()\n","\n","        # Hugging Face PoolFormer model\n","        from transformers import PoolFormerModel\n","\n","        self.poolformer = PoolFormerModel.from_pretrained(\n","            CASME2_POOLFORMER_CONFIG['poolformer_model']\n","        )\n","\n","        # Enable fine-tuning for micro-expression domain\n","        for param in self.poolformer.parameters():\n","            param.requires_grad = True\n","\n","        # Get PoolFormer feature dimensions\n","        self.poolformer_feature_dim = self.poolformer.config.hidden_sizes[-1]\n","\n","        print(f\"PoolFormer feature dimension: {self.poolformer_feature_dim}\")\n","\n","        # Classification head with LayerNorm for stability\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.poolformer_feature_dim, 512),\n","            nn.LayerNorm(512),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","\n","            nn.Linear(512, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","        )\n","\n","        # Final classification layer for CASME II classes\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","        print(f\"PoolFormer CASME II: {self.poolformer_feature_dim} -> 512 -> 128 -> {num_classes}\")\n","\n","    def forward(self, pixel_values):\n","        # PoolFormer forward pass\n","        poolformer_outputs = self.poolformer(pixel_values=pixel_values)\n","\n","        # Extract pooled features from PoolFormer last hidden state\n","        # PoolFormer output: [batch_size, channels, height, width]\n","        poolformer_features = poolformer_outputs.last_hidden_state\n","\n","        # Global average pooling across spatial dimensions for classification\n","        # [batch_size, channels, height, width] -> [batch_size, channels]\n","        poolformer_features = poolformer_features.mean(dim=[2, 3])\n","\n","        # Classification pipeline\n","        processed_features = self.classifier_layers(poolformer_features)\n","        output = self.classifier(processed_features)\n","\n","        return output\n","\n","# Enhanced optimizer and scheduler factory\n","def create_optimizer_scheduler_casme2(model, config):\n","    \"\"\"Create optimizer and scheduler for CASME II PoolFormer training\"\"\"\n","\n","    # AdamW optimizer with proven configuration\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=config['learning_rate'],\n","        weight_decay=config['weight_decay'],\n","        betas=(0.9, 0.999)\n","    )\n","\n","    # ReduceLROnPlateau scheduler monitoring validation F1\n","    if config['scheduler_type'] == 'plateau':\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer,\n","            mode=config['scheduler_mode'],\n","            factor=config['scheduler_factor'],\n","            patience=config['scheduler_patience'],\n","            min_lr=config['scheduler_min_lr']\n","        )\n","        print(f\"Scheduler: ReduceLROnPlateau monitoring {config['scheduler_monitor']}\")\n","    else:\n","        scheduler = None\n","\n","    return optimizer, scheduler\n","\n","# PoolFormer Image Processor setup for 384px input\n","from transformers import PoolFormerImageProcessor\n","\n","print(\"\\nSetting up PoolFormer Image Processor for 384px input...\")\n","\n","poolformer_processor = PoolFormerImageProcessor.from_pretrained(\n","    CASME2_POOLFORMER_CONFIG['poolformer_model'],\n","    do_resize=True,\n","    size={'height': 384, 'width': 384},\n","    do_normalize=True,\n","    do_rescale=True,\n","    do_center_crop=False\n",")\n","\n","# Transform functions for PoolFormer\n","def poolformer_transform_train(image):\n","    \"\"\"Training transform with PoolFormer Image Processor\"\"\"\n","    inputs = poolformer_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","def poolformer_transform_val(image):\n","    \"\"\"Validation transform with PoolFormer Image Processor\"\"\"\n","    inputs = poolformer_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","print(\"PoolFormer Image Processor configured for 384px with token mixing\")\n","\n","# Custom Dataset class for CASME II\n","class CASME2Dataset(Dataset):\n","    \"\"\"Custom dataset class for CASME II with JSON metadata support\"\"\"\n","\n","    def __init__(self, split_metadata, dataset_root, transform=None, split='train'):\n","        self.metadata = split_metadata[split]['samples']\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","        self.split = split\n","\n","        print(f\"Loaded {len(self.metadata)} samples for {split} split\")\n","\n","    def __len__(self):\n","        return len(self.metadata)\n","\n","    def __getitem__(self, idx):\n","        sample = self.metadata[idx]\n","\n","        # Load image\n","        image_path = os.path.join(self.dataset_root, self.split, sample['image_filename'])\n","        image = Image.open(image_path).convert('RGB')\n","\n","        # Apply transform\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Get class label\n","        emotion = sample['emotion']\n","        label = CLASS_TO_IDX[emotion]\n","\n","        return image, label, sample['sample_id']\n","\n","# Create directories\n","os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/training_logs\", exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/evaluation_results\", exist_ok=True)\n","\n","# Dataset paths - Phase 3 structure\n","TRAIN_PATH = f\"{DATASET_ROOT}/train\"\n","VAL_PATH = f\"{DATASET_ROOT}/val\"\n","TEST_PATH = f\"{DATASET_ROOT}/test\"\n","\n","print(f\"\\nDataset paths:\")\n","print(f\"Train: {TRAIN_PATH}\")\n","print(f\"Validation: {VAL_PATH}\")\n","print(f\"Test: {TEST_PATH}\")\n","\n","# Enhanced architecture validation with PoolFormer feature calculation\n","print(\"\\nPoolFormer CASME II architecture validation...\")\n","\n","try:\n","    test_model = PoolFormerCASME2Baseline(num_classes=7, dropout_rate=0.2).to(device)\n","    test_input = torch.randn(1, 3, 384, 384).to(device)\n","    test_output = test_model(test_input)\n","\n","    print(f\"Validation successful: Output shape {test_output.shape}\")\n","    print(f\"PoolFormer {POOLFORMER_MODEL_VARIANT.upper()} with {MODEL_PARAMS} parameters\")\n","    print(f\"Token mixing with pooling operations\")\n","\n","    del test_model, test_input, test_output\n","    torch.cuda.empty_cache()\n","\n","except Exception as e:\n","    print(f\"Validation failed: {e}\")\n","\n","# Optimized loss function factory with advanced configuration\n","def create_criterion_casme2(weights, use_focal_loss=False, alpha_weights=None, gamma=2.0):\n","    \"\"\"\n","    Optimized factory function to create loss criterion based on advanced configuration\n","\n","    Args:\n","        weights (Tensor): Class weights for CrossEntropy (ignored if focal loss used)\n","        use_focal_loss (bool): Whether to use Focal Loss or CrossEntropy\n","        alpha_weights (list): Per-class alpha weights for Focal Loss (must sum to 1.0)\n","        gamma (float): Focal loss gamma parameter\n","\n","    Returns:\n","        Loss function (nn.Module)\n","    \"\"\"\n","    if use_focal_loss:\n","        print(f\"Using Optimized Focal Loss with gamma={gamma}\")\n","        if alpha_weights:\n","            print(f\"Per-class alpha weights: {alpha_weights}\")\n","            print(f\"Alpha sum: {sum(alpha_weights):.3f}\")\n","        return OptimizedFocalLoss(alpha=alpha_weights, gamma=gamma)\n","    else:\n","        print(f\"Using CrossEntropy Loss with optimized class weights\")\n","        print(f\"Class weights: {weights.cpu().numpy()}\")\n","        return nn.CrossEntropyLoss(weight=weights)\n","\n","# Global configuration for training pipeline - enhanced with optimized features\n","GLOBAL_CONFIG_CASME2 = {\n","    'device': device,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'num_classes': 7,\n","    'class_weights': class_weights,\n","    'class_names': CASME2_CLASSES,\n","    'class_to_idx': CLASS_TO_IDX,\n","    'transform_train': poolformer_transform_train,\n","    'transform_val': poolformer_transform_val,\n","    'poolformer_config': CASME2_POOLFORMER_CONFIG,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'results_root': RESULTS_ROOT,\n","    'train_path': TRAIN_PATH,\n","    'val_path': VAL_PATH,\n","    'test_path': TEST_PATH,\n","    'metadata': casme2_metadata,\n","    'optimizer_scheduler_factory': create_optimizer_scheduler_casme2,\n","    'criterion_factory': create_criterion_casme2\n","}\n","\n","# Configuration validation and summary\n","print(\"\\n\" + \"=\" * 60)\n","print(\"CASME II MULTI-FRAME POOLFORMER CONFIGURATION COMPLETE\")\n","print(\"=\" * 60)\n","\n","print(f\"Loss Configuration:\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Function: Optimized Focal Loss\")\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Per-class Alpha: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Function: CrossEntropy with Optimized Weights\")\n","    print(f\"  Class Weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","\n","print(f\"\\nModel Configuration:\")\n","print(f\"  Architecture: {POOLFORMER_MODEL_NAME}\")\n","print(f\"  Variant: {POOLFORMER_MODEL_VARIANT.upper()}\")\n","print(f\"  Parameters: {MODEL_PARAMS}\")\n","print(f\"  Input Resolution: 384px\")\n","print(f\"  Feature Dimension: {CASME2_POOLFORMER_CONFIG['expected_feature_dim']}\")\n","\n","print(f\"\\nDataset Configuration:\")\n","print(f\"  Phase: {CASME2_POOLFORMER_CONFIG['dataset_phase']}\")\n","print(f\"  Frame strategy: {CASME2_POOLFORMER_CONFIG['frame_strategy']}\")\n","print(f\"  Train augmentation: {CASME2_POOLFORMER_CONFIG['train_augmentation']}\")\n","print(f\"  Classes: {len(CASME2_CLASSES)}\")\n","print(f\"  Frame types: {CASME2_POOLFORMER_CONFIG['frame_types']}\")\n","print(f\"  Train samples: {processing_info.get('copy_statistics', {}).get('train', {}).get('total_images', 2613)}\")\n","print(f\"  Weight Optimization: {'Per-class Alpha' if USE_FOCAL_LOSS else 'Inverse Sqrt Frequency'}\")\n","\n","print(\"\\nNext: Cell 2 - Dataset Loading and Multi-Frame PoolFormer Training Pipeline\")"]},{"cell_type":"code","source":["# @title Cell 2: CASME II Multi-Frame PoolFormer Training Pipeline\n","\n","# File: 04_03_PoolFormer_CASME2_MFS_Cell2.py\n","# Location: experiments/04_03_PoolFormer_CASME2-MFS.ipynb\n","# Purpose: Enhanced training pipeline for CASME II Multi-Frame PoolFormer with temporal window augmentation\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing as mp\n","import shutil\n","import tempfile\n","\n","print(\"CASME II Multi-Frame PoolFormer Training Pipeline\")\n","print(\"=\" * 70)\n","print(f\"Loss Function: {'Optimized Focal Loss' if CASME2_POOLFORMER_CONFIG['use_focal_loss'] else 'CrossEntropy Loss'}\")\n","if CASME2_POOLFORMER_CONFIG['use_focal_loss']:\n","    print(f\"Focal Loss Parameters:\")\n","    print(f\"  Gamma: {CASME2_POOLFORMER_CONFIG['focal_loss_gamma']}\")\n","    print(f\"  Per-class Alpha: {CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights']}\")\n","    print(f\"  Alpha Sum: {sum(CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","else:\n","    print(f\"CrossEntropy Parameters:\")\n","    print(f\"  Optimized Class Weights: {CASME2_POOLFORMER_CONFIG['crossentropy_class_weights']}\")\n","print(f\"Dataset Phase: {CASME2_POOLFORMER_CONFIG['dataset_phase']}\")\n","print(f\"Frame Strategy: {CASME2_POOLFORMER_CONFIG['frame_strategy']}\")\n","print(f\"Train Augmentation: {CASME2_POOLFORMER_CONFIG['train_augmentation']}\")\n","print(f\"Training epochs: {CASME2_POOLFORMER_CONFIG['num_epochs']}\")\n","print(f\"Scheduler patience: {CASME2_POOLFORMER_CONFIG['scheduler_patience']}\")\n","\n","# Smart metadata normalizer for v1, v2, and v3 compatibility\n","def normalize_metadata_structure(metadata):\n","    \"\"\"\n","    Normalize metadata structure to handle v1, v2, and v3 formats\n","\n","    v1 format: metadata['train']['samples']\n","    v2 format: metadata['splits']['train']['samples']\n","    v3 format: metadata['splits']['train']['samples'] (with multi-frame augmentation)\n","\n","    Returns: Normalized metadata with consistent structure\n","    \"\"\"\n","    # Check if this is v2/v3 format (has 'splits' key)\n","    if 'splits' in metadata:\n","        print(\"Detected v2/v3 metadata format (with 'splits' key)\")\n","        return metadata['splits']\n","    # Check if this is v1 format (direct split keys)\n","    elif 'train' in metadata:\n","        print(\"Detected v1 metadata format (direct split keys)\")\n","        return metadata\n","    else:\n","        raise ValueError(\"Unknown metadata format: missing both 'splits' and 'train' keys\")\n","\n","# Enhanced CASME II Dataset with optimized RAM caching\n","class CASME2DatasetTraining(Dataset):\n","    \"\"\"Enhanced CASME II dataset for training with RAM caching optimization\"\"\"\n","\n","    def __init__(self, split_metadata, dataset_root, transform=None, split='train', use_ram_cache=True):\n","        self.metadata = split_metadata[split]['samples']\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","        self.split = split\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.sample_ids = []\n","        self.cached_images = []\n","\n","        print(f\"Loading CASME II {split} dataset for training...\")\n","\n","        # Process metadata\n","        for sample in self.metadata:\n","            image_path = os.path.join(dataset_root, split, sample['image_filename'])\n","            self.images.append(image_path)\n","            self.labels.append(CLASS_TO_IDX[sample['emotion']])\n","            self.sample_ids.append(sample['sample_id'])\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples\")\n","        self._print_distribution()\n","\n","        # RAM caching for training efficiency\n","        if self.use_ram_cache:\n","            self._preload_to_ram()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def _preload_to_ram(self):\n","        \"\"\"RAM preloading with parallel loading for training efficiency\"\"\"\n","        print(f\"Preloading {len(self.images)} {self.split} images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            \"\"\"Load single image with error handling\"\"\"\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (384, 384):\n","                    image = image.resize((384, 384), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (384, 384), (128, 128, 128)), False\n","\n","        # Parallel loading with ThreadPoolExecutor\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            # Submit all loading tasks\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            # Collect results with progress bar\n","            for future in tqdm(futures, desc=f\"Loading {self.split} to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * 384 * 384 * 3 * 4 / 1e9\n","        print(f\"{self.split.upper()} RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (384, 384):\n","                    image = image.resize((384, 384), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (384, 384), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.sample_ids[idx]\n","\n","# Enhanced metrics calculation with comprehensive error handling\n","def calculate_metrics_safe_robust(outputs, labels, class_names, average='macro'):\n","    \"\"\"Calculate metrics with enhanced error handling and validation\"\"\"\n","    try:\n","        # Validate input tensors\n","        if outputs.size(0) != labels.size(0):\n","            raise ValueError(f\"Batch size mismatch: outputs {outputs.size(0)} vs labels {labels.size(0)}\")\n","\n","        if isinstance(outputs, torch.Tensor):\n","            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n","        else:\n","            predictions = np.array(outputs)\n","\n","        if isinstance(labels, torch.Tensor):\n","            labels = labels.cpu().numpy()\n","        else:\n","            labels = np.array(labels)\n","\n","        # Validate predictions are in valid range\n","        unique_preds = np.unique(predictions)\n","        unique_labels = np.unique(labels)\n","\n","        accuracy = accuracy_score(labels, predictions)\n","        precision, recall, f1, _ = precision_recall_fscore_support(\n","            labels, predictions,\n","            average=average,\n","            zero_division=0,\n","            labels=list(range(len(class_names)))\n","        )\n","\n","        return {\n","            'accuracy': float(accuracy),\n","            'precision': float(precision),\n","            'recall': float(recall),\n","            'f1_score': float(f1)\n","        }\n","    except Exception as e:\n","        print(f\"Warning: Enhanced metrics calculation error: {e}\")\n","        return {\n","            'accuracy': 0.0,\n","            'precision': 0.0,\n","            'recall': 0.0,\n","            'f1_score': 0.0\n","        }\n","\n","# Enhanced training epoch function with robust model output validation\n","def train_epoch(model, dataloader, criterion, optimizer, device, epoch, total_epochs):\n","    \"\"\"Enhanced training epoch with robust error handling and output validation\"\"\"\n","    model.train()\n","    running_loss = 0.0\n","    all_outputs = []\n","    all_labels = []\n","\n","    progress_bar = tqdm(dataloader, desc=f\"CASME II Training Epoch {epoch+1}/{total_epochs}\")\n","\n","    for batch_idx, (images, labels, sample_ids) in enumerate(progress_bar):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Enhanced model output validation - handle multiple output formats\n","        model_output = model(images)\n","\n","        # Robust output structure validation\n","        if isinstance(model_output, (tuple, list)):\n","            outputs = model_output[0]\n","        elif isinstance(model_output, dict):\n","            outputs = model_output.get('logits', model_output.get('prediction', model_output))\n","        else:\n","            outputs = model_output\n","\n","        # Validate output shape for 7 CASME II classes\n","        if outputs.dim() != 2 or outputs.size(1) != 7:\n","            raise ValueError(f\"Invalid CASME II output shape: {outputs.shape}, expected [batch_size, 7]\")\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), CASME2_POOLFORMER_CONFIG['gradient_clip'])\n","\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        # Memory optimized: Move to CPU before accumulating\n","        all_outputs.append(outputs.detach().cpu())\n","        all_labels.append(labels.detach().cpu())\n","\n","        # Update progress\n","        if batch_idx % 5 == 0:\n","            avg_loss = running_loss / (batch_idx + 1)\n","            current_lr = optimizer.param_groups[0]['lr']\n","            progress_bar.set_postfix({\n","                'Loss': f'{avg_loss:.4f}',\n","                'LR': f'{current_lr:.2e}'\n","            })\n","\n","    # Enhanced metrics calculation with error recovery\n","    try:\n","        epoch_outputs = torch.cat(all_outputs, dim=0)\n","        epoch_labels = torch.cat(all_labels, dim=0)\n","        metrics = calculate_metrics_safe_robust(epoch_outputs, epoch_labels, CASME2_CLASSES, average='macro')\n","    except Exception as e:\n","        print(f\"Warning: Training metrics calculation failed: {e}\")\n","        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n","\n","    avg_loss = running_loss / len(dataloader)\n","    return avg_loss, metrics\n","\n","# Enhanced validation epoch function with robust model output validation\n","def validate_epoch(model, dataloader, criterion, device, epoch, total_epochs):\n","    \"\"\"Enhanced validation epoch with robust error handling and output validation\"\"\"\n","    model.eval()\n","    running_loss = 0.0\n","    all_outputs = []\n","    all_labels = []\n","    all_sample_ids = []\n","\n","    with torch.no_grad():\n","        progress_bar = tqdm(dataloader, desc=f\"CASME II Validation Epoch {epoch+1}/{total_epochs}\")\n","\n","        for batch_idx, (images, labels, sample_ids) in enumerate(progress_bar):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            # Enhanced model output validation - handle multiple output formats\n","            model_output = model(images)\n","\n","            # Robust output structure validation\n","            if isinstance(model_output, (tuple, list)):\n","                outputs = model_output[0]\n","            elif isinstance(model_output, dict):\n","                outputs = model_output.get('logits', model_output.get('prediction', model_output))\n","            else:\n","                outputs = model_output\n","\n","            loss = criterion(outputs, labels)\n","            running_loss += loss.item()\n","\n","            # Memory optimized: Move to CPU before accumulating\n","            all_outputs.append(outputs.detach().cpu())\n","            all_labels.append(labels.detach().cpu())\n","            all_sample_ids.extend(sample_ids)\n","\n","            if batch_idx % 3 == 0:\n","                avg_loss = running_loss / (batch_idx + 1)\n","                progress_bar.set_postfix({'Val Loss': f'{avg_loss:.4f}'})\n","\n","    # Enhanced metrics calculation with error recovery\n","    try:\n","        epoch_outputs = torch.cat(all_outputs, dim=0)\n","        epoch_labels = torch.cat(all_labels, dim=0)\n","        metrics = calculate_metrics_safe_robust(epoch_outputs, epoch_labels, CASME2_CLASSES, average='macro')\n","    except Exception as e:\n","        print(f\"Warning: Validation metrics calculation failed: {e}\")\n","        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n","\n","    avg_loss = running_loss / len(dataloader)\n","    return avg_loss, metrics, all_sample_ids\n","\n","# Hardened checkpoint saving function with atomic write and validation\n","def save_checkpoint_robust(model, optimizer, scheduler, epoch, train_metrics, val_metrics,\n","                         checkpoint_dir, best_metrics, config, max_retries=3):\n","    \"\"\"\n","    Hardened checkpoint saving with atomic write, validation, and retry logic\n","\n","    Improvements:\n","    1. Force all tensors to CPU before serialization\n","    2. Atomic save using temporary file\n","    3. Validate checkpoint after save by loading it back\n","    4. Detailed error logging\n","    5. Retry mechanism with exponential backoff\n","    \"\"\"\n","\n","    # Convert all tensors to CPU and serializable format\n","    def make_serializable_cpu(obj):\n","        if isinstance(obj, torch.Tensor):\n","            # Force CPU conversion for all tensors\n","            cpu_obj = obj.detach().cpu()\n","            return cpu_obj.item() if cpu_obj.numel() == 1 else cpu_obj.tolist()\n","        elif isinstance(obj, np.ndarray):\n","            return obj.tolist()\n","        elif isinstance(obj, np.integer):\n","            return int(obj)\n","        elif isinstance(obj, np.floating):\n","            return float(obj)\n","        elif isinstance(obj, dict):\n","            return {k: make_serializable_cpu(v) for k, v in obj.items()}\n","        elif isinstance(obj, (list, tuple)):\n","            return [make_serializable_cpu(item) for item in obj]\n","        else:\n","            return obj\n","\n","    # Prepare checkpoint with CPU-converted tensors\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","        'train_metrics': make_serializable_cpu(train_metrics),\n","        'val_metrics': make_serializable_cpu(val_metrics),\n","        'casme2_config': make_serializable_cpu(config),\n","        'best_f1': float(best_metrics['f1']),\n","        'best_loss': float(best_metrics['loss']),\n","        'best_acc': float(best_metrics['accuracy']),\n","        'class_names': CASME2_CLASSES,\n","        'num_classes': 7\n","    }\n","\n","    final_path = f\"{checkpoint_dir}/casme2_poolformer_multiframe_best_f1.pth\"\n","\n","    # Atomic save with retry logic\n","    for attempt in range(max_retries):\n","        try:\n","            # Step 1: Save to temporary file\n","            temp_fd, temp_path = tempfile.mkstemp(dir=checkpoint_dir, suffix='.pth.tmp')\n","            os.close(temp_fd)\n","\n","            print(f\"Attempt {attempt + 1}: Saving checkpoint to temporary file...\")\n","            torch.save(checkpoint, temp_path)\n","\n","            # Step 2: Validate checkpoint by loading it back\n","            print(\"Validating checkpoint integrity...\")\n","            validation_checkpoint = torch.load(temp_path, map_location='cpu')\n","\n","            # Verify critical keys exist\n","            required_keys = ['model_state_dict', 'epoch', 'best_f1', 'num_classes']\n","            for key in required_keys:\n","                if key not in validation_checkpoint:\n","                    raise ValueError(f\"Checkpoint validation failed: missing key '{key}'\")\n","\n","            # Verify epoch matches\n","            if validation_checkpoint['epoch'] != epoch:\n","                raise ValueError(f\"Checkpoint epoch mismatch: saved {epoch}, loaded {validation_checkpoint['epoch']}\")\n","\n","            print(\"Checkpoint validation passed\")\n","\n","            # Step 3: Atomic rename (overwrite existing checkpoint)\n","            print(f\"Moving validated checkpoint to final location...\")\n","            shutil.move(temp_path, final_path)\n","\n","            print(f\"Checkpoint saved and validated successfully: {os.path.basename(final_path)}\")\n","            print(f\"  Epoch: {epoch + 1}\")\n","            print(f\"  Val F1: {best_metrics['f1']:.4f}\")\n","            print(f\"  Val Loss: {best_metrics['loss']:.4f}\")\n","            print(f\"  Val Acc: {best_metrics['accuracy']:.4f}\")\n","\n","            return final_path\n","\n","        except Exception as e:\n","            print(f\"Checkpoint save attempt {attempt + 1}/{max_retries} failed: {e}\")\n","\n","            # Clean up temporary file if it exists\n","            if os.path.exists(temp_path):\n","                try:\n","                    os.remove(temp_path)\n","                except:\n","                    pass\n","\n","            if attempt < max_retries - 1:\n","                wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s\n","                print(f\"Retrying in {wait_time} seconds...\")\n","                time.sleep(wait_time)\n","            else:\n","                print(f\"All {max_retries} checkpoint save attempts failed\")\n","                return None\n","\n","    return None\n","\n","# Safe JSON serialization function\n","def safe_json_serialize(obj):\n","    \"\"\"Convert objects to JSON-serializable format\"\"\"\n","    if isinstance(obj, torch.Tensor):\n","        return obj.cpu().item() if obj.numel() == 1 else obj.cpu().numpy().tolist()\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    elif isinstance(obj, (np.integer, np.int32, np.int64)):\n","        return int(obj)\n","    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n","        return float(obj)\n","    elif isinstance(obj, dict):\n","        return {k: safe_json_serialize(v) for k, v in obj.items()}\n","    elif isinstance(obj, (list, tuple)):\n","        return [safe_json_serialize(item) for item in obj]\n","    elif hasattr(obj, '__dict__'):\n","        return safe_json_serialize(obj.__dict__)\n","    else:\n","        try:\n","            # Try to convert to basic Python types\n","            return float(obj) if isinstance(obj, (int, float)) else str(obj)\n","        except:\n","            return str(obj)\n","\n","# Create enhanced datasets with normalized metadata\n","print(\"\\nCreating CASME II Multi-Frame training datasets...\")\n","\n","# Normalize metadata structure for v1/v2/v3 compatibility\n","normalized_metadata = normalize_metadata_structure(GLOBAL_CONFIG_CASME2['metadata'])\n","\n","train_dataset = CASME2DatasetTraining(\n","    split_metadata=normalized_metadata,\n","    dataset_root=GLOBAL_CONFIG_CASME2['train_path'].replace('/train', ''),\n","    transform=GLOBAL_CONFIG_CASME2['transform_train'],\n","    split='train',\n","    use_ram_cache=True\n",")\n","\n","val_dataset = CASME2DatasetTraining(\n","    split_metadata=normalized_metadata,\n","    dataset_root=GLOBAL_CONFIG_CASME2['val_path'].replace('/val', ''),\n","    transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","    split='val',\n","    use_ram_cache=True\n",")\n","\n","# Create data loaders\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=CASME2_POOLFORMER_CONFIG['batch_size'],\n","    shuffle=True,\n","    num_workers=CASME2_POOLFORMER_CONFIG['num_workers'],\n","    pin_memory=True,\n","    prefetch_factor=2\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=CASME2_POOLFORMER_CONFIG['batch_size'],\n","    shuffle=False,\n","    num_workers=CASME2_POOLFORMER_CONFIG['num_workers'],\n","    pin_memory=True,\n","    prefetch_factor=2\n",")\n","\n","print(f\"Training batches: {len(train_loader)} (samples: {len(train_dataset)})\")\n","print(f\"Validation batches: {len(val_loader)} (samples: {len(val_dataset)})\")\n","\n","# Initialize model, criterion, optimizer, scheduler\n","print(\"\\nInitializing CASME II Multi-Frame PoolFormer model...\")\n","model = PoolFormerCASME2Baseline(\n","    num_classes=GLOBAL_CONFIG_CASME2['num_classes'],\n","    dropout_rate=CASME2_POOLFORMER_CONFIG['dropout_rate']\n",").to(GLOBAL_CONFIG_CASME2['device'])\n","\n","# Enhanced criterion creation using configurable factory function\n","if CASME2_POOLFORMER_CONFIG['use_focal_loss']:\n","    criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","        weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","        use_focal_loss=True,\n","        alpha_weights=CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights'],\n","        gamma=CASME2_POOLFORMER_CONFIG['focal_loss_gamma']\n","    )\n","else:\n","    criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","        weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","        use_focal_loss=False,\n","        alpha_weights=None,\n","        gamma=2.0\n","    )\n","\n","optimizer, scheduler = GLOBAL_CONFIG_CASME2['optimizer_scheduler_factory'](\n","    model, CASME2_POOLFORMER_CONFIG\n",")\n","\n","print(f\"Optimizer: AdamW (LR={CASME2_POOLFORMER_CONFIG['learning_rate']})\")\n","print(f\"Scheduler: ReduceLROnPlateau (patience={CASME2_POOLFORMER_CONFIG['scheduler_patience']})\")\n","print(f\"Criterion: {'Optimized Focal Loss' if CASME2_POOLFORMER_CONFIG['use_focal_loss'] else 'CrossEntropy'}\")\n","\n","# Training history tracking\n","training_history = {\n","    'train_loss': [],\n","    'val_loss': [],\n","    'train_f1': [],\n","    'val_f1': [],\n","    'train_acc': [],\n","    'val_acc': [],\n","    'learning_rate': [],\n","    'epoch_time': []\n","}\n","\n","# Enhanced best metrics tracking for multi-criteria checkpoint saving\n","best_metrics = {\n","    'f1': 0.0,\n","    'loss': float('inf'),\n","    'accuracy': 0.0,\n","    'epoch': 0\n","}\n","\n","print(\"\\nStarting CASME II Multi-Frame PoolFormer training...\")\n","print(f\"Training configuration: {CASME2_POOLFORMER_CONFIG['num_epochs']} epochs\")\n","print(f\"Expected training time: 3-4x longer per epoch due to 10x more samples\")\n","print(\"=\" * 70)\n","\n","# Main training loop with hardened checkpoint system\n","start_time = time.time()\n","\n","for epoch in range(CASME2_POOLFORMER_CONFIG['num_epochs']):\n","    epoch_start_time = time.time()\n","    print(f\"\\nEpoch {epoch+1}/{CASME2_POOLFORMER_CONFIG['num_epochs']}\")\n","\n","    # Training phase\n","    train_loss, train_metrics = train_epoch(\n","        model, train_loader, criterion, optimizer,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_POOLFORMER_CONFIG['num_epochs']\n","    )\n","\n","    # Validation phase\n","    val_loss, val_metrics, val_sample_ids = validate_epoch(\n","        model, val_loader, criterion,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_POOLFORMER_CONFIG['num_epochs']\n","    )\n","\n","    # Update scheduler\n","    if scheduler:\n","        scheduler.step(val_metrics['f1_score'])\n","\n","    # Record training history\n","    epoch_time = time.time() - epoch_start_time\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    training_history['train_loss'].append(float(train_loss))\n","    training_history['val_loss'].append(float(val_loss))\n","    training_history['train_f1'].append(float(train_metrics['f1_score']))\n","    training_history['val_f1'].append(float(val_metrics['f1_score']))\n","    training_history['train_acc'].append(float(train_metrics['accuracy']))\n","    training_history['val_acc'].append(float(val_metrics['accuracy']))\n","    training_history['learning_rate'].append(float(current_lr))\n","    training_history['epoch_time'].append(float(epoch_time))\n","\n","    # Print epoch summary\n","    print(f\"Train - Loss: {train_loss:.4f}, F1: {train_metrics['f1_score']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n","    print(f\"Val   - Loss: {val_loss:.4f}, F1: {val_metrics['f1_score']:.4f}, Acc: {val_metrics['accuracy']:.4f}\")\n","    print(f\"Time  - Epoch: {epoch_time:.1f}s, LR: {current_lr:.2e}\")\n","\n","    # Enhanced multi-criteria checkpoint saving logic\n","    save_model = False\n","    improvement_reason = \"\"\n","\n","    # Multi-criteria evaluation hierarchy\n","    if val_metrics['f1_score'] > best_metrics['f1']:\n","        save_model = True\n","        improvement_reason = \"Higher F1\"\n","    elif val_metrics['f1_score'] == best_metrics['f1']:\n","        if val_loss < best_metrics['loss']:\n","            save_model = True\n","            improvement_reason = \"Same F1, Lower Loss\"\n","        elif val_loss == best_metrics['loss'] and val_metrics['accuracy'] > best_metrics['accuracy']:\n","            save_model = True\n","            improvement_reason = \"Same F1&Loss, Higher Accuracy\"\n","\n","    if save_model:\n","        best_metrics['f1'] = val_metrics['f1_score']\n","        best_metrics['loss'] = val_loss\n","        best_metrics['accuracy'] = val_metrics['accuracy']\n","        best_metrics['epoch'] = epoch + 1\n","\n","        best_model_path = save_checkpoint_robust(\n","            model, optimizer, scheduler, epoch,\n","            train_metrics, val_metrics, GLOBAL_CONFIG_CASME2['checkpoint_root'],\n","            best_metrics, CASME2_POOLFORMER_CONFIG\n","        )\n","\n","        if best_model_path:\n","            print(f\"New best model: {improvement_reason} - F1: {best_metrics['f1']:.4f}\")\n","        else:\n","            print(f\"Warning: Failed to save checkpoint despite improvement\")\n","\n","    # Progress tracking\n","    elapsed_time = time.time() - start_time\n","    estimated_total = (elapsed_time / (epoch + 1)) * CASME2_POOLFORMER_CONFIG['num_epochs']\n","    remaining_time = estimated_total - elapsed_time\n","    progress_pct = ((epoch + 1) / CASME2_POOLFORMER_CONFIG['num_epochs']) * 100\n","\n","    print(f\"Progress: {progress_pct:.1f}% | Best F1: {best_metrics['f1']:.4f} | ETA: {remaining_time/60:.1f}min\")\n","\n","# Training completion\n","total_time = time.time() - start_time\n","actual_epochs = CASME2_POOLFORMER_CONFIG['num_epochs']\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II MULTI-FRAME POOLFORMER TRAINING COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Training time: {total_time/60:.1f} minutes\")\n","print(f\"Epochs completed: {actual_epochs}\")\n","print(f\"Best validation F1: {best_metrics['f1']:.4f} (epoch {best_metrics['epoch']})\")\n","print(f\"Final train F1: {training_history['train_f1'][-1]:.4f}\")\n","print(f\"Final validation F1: {training_history['val_f1'][-1]:.4f}\")\n","\n","# Enhanced training documentation export\n","results_dir = GLOBAL_CONFIG_CASME2['results_root']\n","os.makedirs(f\"{results_dir}/training_logs\", exist_ok=True)\n","\n","training_history_path = f\"{results_dir}/training_logs/casme2_poolformer_multiframe_training_history.json\"\n","\n","print(\"\\nExporting enhanced training documentation...\")\n","\n","try:\n","    # Create comprehensive training summary with enhanced experiment configuration\n","    training_summary = {\n","        'experiment_type': 'CASME2_PoolFormer_MultiFrame_Baseline',\n","        'experiment_configuration': {\n","            'dataset_phase': CASME2_POOLFORMER_CONFIG['dataset_phase'],\n","            'frame_strategy': CASME2_POOLFORMER_CONFIG['frame_strategy'],\n","            'train_augmentation': CASME2_POOLFORMER_CONFIG['train_augmentation'],\n","            'frame_types': CASME2_POOLFORMER_CONFIG['frame_types'],\n","            'extraction_strategy': CASME2_POOLFORMER_CONFIG['extraction_strategy'],\n","            'copy_statistics': CASME2_POOLFORMER_CONFIG['copy_statistics'],\n","            'loss_function': 'Optimized Focal Loss' if CASME2_POOLFORMER_CONFIG['use_focal_loss'] else 'CrossEntropy',\n","            'weight_approach': 'Per-class Alpha (sum=1.0)' if CASME2_POOLFORMER_CONFIG['use_focal_loss'] else 'Inverse Sqrt Frequency',\n","            'focal_loss_gamma': CASME2_POOLFORMER_CONFIG['focal_loss_gamma'],\n","            'focal_loss_alpha_weights': CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights'],\n","            'crossentropy_class_weights': CASME2_POOLFORMER_CONFIG['crossentropy_class_weights'],\n","            'poolformer_model': CASME2_POOLFORMER_CONFIG['poolformer_model'],\n","            'model_variant': CASME2_POOLFORMER_CONFIG['model_variant'],\n","            'model_params': CASME2_POOLFORMER_CONFIG['model_params']\n","        },\n","        'training_history': safe_json_serialize(training_history),\n","        'best_val_f1': float(best_metrics['f1']),\n","        'best_val_loss': float(best_metrics['loss']),\n","        'best_val_accuracy': float(best_metrics['accuracy']),\n","        'best_epoch': int(best_metrics['epoch']),\n","        'total_epochs': int(actual_epochs),\n","        'total_time_minutes': float(total_time / 60),\n","        'average_epoch_time_seconds': float(np.mean(training_history['epoch_time'])),\n","        'config': safe_json_serialize(CASME2_POOLFORMER_CONFIG),\n","        'final_train_f1': float(training_history['train_f1'][-1]),\n","        'final_val_f1': float(training_history['val_f1'][-1]),\n","        'model_checkpoint': 'casme2_poolformer_multiframe_best_f1.pth',\n","        'dataset_info': {\n","            'name': 'CASME_II',\n","            'phase': CASME2_POOLFORMER_CONFIG['dataset_phase'],\n","            'frame_strategy': CASME2_POOLFORMER_CONFIG['frame_strategy'],\n","            'train_augmentation': CASME2_POOLFORMER_CONFIG['train_augmentation'],\n","            'frame_types': CASME2_POOLFORMER_CONFIG['frame_types'],\n","            'train_samples': len(train_dataset),\n","            'val_samples': len(val_dataset),\n","            'num_classes': 7,\n","            'class_names': CASME2_CLASSES\n","        },\n","        'architecture_info': {\n","            'model_type': 'PoolFormerCASME2Baseline',\n","            'backbone': CASME2_POOLFORMER_CONFIG['poolformer_model'],\n","            'variant': CASME2_POOLFORMER_CONFIG['model_variant'],\n","            'params': CASME2_POOLFORMER_CONFIG['model_params'],\n","            'input_size': f\"{CASME2_POOLFORMER_CONFIG['input_size']}x{CASME2_POOLFORMER_CONFIG['input_size']}\",\n","            'expected_feature_dim': CASME2_POOLFORMER_CONFIG['expected_feature_dim'],\n","            'classification_head': f\"{CASME2_POOLFORMER_CONFIG['expected_feature_dim']}->512->128->7\"\n","        },\n","        'enhanced_features': {\n","            'hardened_checkpoint_system': True,\n","            'atomic_checkpoint_save': True,\n","            'checkpoint_validation': True,\n","            'model_output_validation': True,\n","            'enhanced_error_handling': True,\n","            'multi_criteria_checkpoint_logic': True,\n","            'memory_optimized_training': True,\n","            'retry_with_backoff': True,\n","            'multi_frame_temporal_windows': True,\n","            'token_mixing_pooling': True\n","        }\n","    }\n","\n","    # Save with proper JSON serialization\n","    with open(training_history_path, 'w') as f:\n","        json.dump(training_summary, f, indent=2)\n","\n","    print(f\"Enhanced training documentation saved successfully: {training_history_path}\")\n","    print(f\"Experiment details: {training_summary['experiment_configuration']['loss_function']} loss\")\n","    if CASME2_POOLFORMER_CONFIG['use_focal_loss']:\n","        print(f\"  Gamma: {CASME2_POOLFORMER_CONFIG['focal_loss_gamma']}, Alpha Sum: {sum(CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","    print(f\"Model variant: {CASME2_POOLFORMER_CONFIG['poolformer_model']}\")\n","    print(f\"Dataset phase: {CASME2_POOLFORMER_CONFIG['dataset_phase']}\")\n","    print(f\"Frame strategy: {CASME2_POOLFORMER_CONFIG['frame_strategy']}\")\n","\n","except Exception as e:\n","    print(f\"Warning: Could not save training documentation: {e}\")\n","    print(\"Training completed successfully, but documentation export failed\")\n","\n","# Enhanced memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\nNext: Cell 3 - CASME II Multi-Frame PoolFormer Evaluation\")\n","print(\"Enhanced training pipeline with multi-frame temporal windows completed successfully!\")"],"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"_m_okj9tLt2l","outputId":"af9b4b03-f3d8-41ac-9817-111e6e2c6815","executionInfo":{"status":"ok","timestamp":1759716943321,"user_tz":-420,"elapsed":6498275,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II Multi-Frame PoolFormer Training Pipeline\n","======================================================================\n","Loss Function: Optimized Focal Loss\n","Focal Loss Parameters:\n","  Gamma: 2.0\n","  Per-class Alpha: [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","  Alpha Sum: 0.999\n","Dataset Phase: v3\n","Frame Strategy: multi_frame_sampling\n","Train Augmentation: temporal_windows\n","Training epochs: 50\n","Scheduler patience: 3\n","\n","Creating CASME II Multi-Frame training datasets...\n","Detected v2/v3 metadata format (with 'splits' key)\n","Loading CASME II train dataset for training...\n","Loaded 2613 CASME II train samples\n","  others: 1027 samples (39.3%)\n","  disgust: 650 samples (24.9%)\n","  happiness: 325 samples (12.4%)\n","  repression: 273 samples (10.4%)\n","  surprise: 260 samples (10.0%)\n","  sadness: 65 samples (2.5%)\n","  fear: 13 samples (0.5%)\n","Preloading 2613 train images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading train to RAM: 100%|██████████| 2613/2613 [00:27<00:00, 93.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TRAIN RAM caching completed: 2613/2613 images, ~4.62GB\n","Loading CASME II val dataset for training...\n","Loaded 78 CASME II val samples\n","  others: 30 samples (38.5%)\n","  disgust: 18 samples (23.1%)\n","  happiness: 9 samples (11.5%)\n","  repression: 9 samples (11.5%)\n","  surprise: 6 samples (7.7%)\n","  sadness: 3 samples (3.8%)\n","  fear: 3 samples (3.8%)\n","Preloading 78 val images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading val to RAM: 100%|██████████| 78/78 [00:01<00:00, 71.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["VAL RAM caching completed: 78/78 images, ~0.14GB\n","Training batches: 164 (samples: 2613)\n","Validation batches: 5 (samples: 78)\n","\n","Initializing CASME II Multi-Frame PoolFormer model...\n","PoolFormer feature dimension: 768\n","PoolFormer CASME II: 768 -> 512 -> 128 -> 7\n","Using Optimized Focal Loss with gamma=2.0\n","Per-class alpha weights: [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","Alpha sum: 0.999\n","Scheduler: ReduceLROnPlateau monitoring val_f1_macro\n","Optimizer: AdamW (LR=1e-05)\n","Scheduler: ReduceLROnPlateau (patience=3)\n","Criterion: Optimized Focal Loss\n","\n","Starting CASME II Multi-Frame PoolFormer training...\n","Training configuration: 50 epochs\n","Expected training time: 3-4x longer per epoch due to 10x more samples\n","======================================================================\n","\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 1/50: 100%|██████████| 164/164 [02:11<00:00,  1.25it/s, Loss=0.0783, LR=1.00e-05]\n","CASME II Validation Epoch 1/50: 100%|██████████| 5/5 [00:02<00:00,  1.69it/s, Val Loss=0.0621]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0779, F1: 0.2986, Acc: 0.4577\n","Val   - Loss: 0.1492, F1: 0.1551, Acc: 0.3077\n","Time  - Epoch: 134.4s, LR: 1.00e-05\n","Attempt 1: Saving checkpoint to temporary file...\n","Validating checkpoint integrity...\n","Checkpoint validation passed\n","Moving validated checkpoint to final location...\n","Checkpoint saved and validated successfully: casme2_poolformer_multiframe_best_f1.pth\n","  Epoch: 1\n","  Val F1: 0.1551\n","  Val Loss: 0.1492\n","  Val Acc: 0.3077\n","New best model: Higher F1 - F1: 0.1551\n","Progress: 2.0% | Best F1: 0.1551 | ETA: 112.0min\n","\n","Epoch 2/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 2/50: 100%|██████████| 164/164 [02:06<00:00,  1.29it/s, Loss=0.0403, LR=1.00e-05]\n","CASME II Validation Epoch 2/50: 100%|██████████| 5/5 [00:02<00:00,  2.44it/s, Val Loss=0.0617]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0402, F1: 0.5832, Acc: 0.7088\n","Val   - Loss: 0.1494, F1: 0.1366, Acc: 0.2436\n","Time  - Epoch: 129.1s, LR: 1.00e-05\n","Progress: 4.0% | Best F1: 0.1551 | ETA: 106.5min\n","\n","Epoch 3/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 3/50: 100%|██████████| 164/164 [02:06<00:00,  1.29it/s, Loss=0.0195, LR=1.00e-05]\n","CASME II Validation Epoch 3/50: 100%|██████████| 5/5 [00:02<00:00,  2.44it/s, Val Loss=0.0663]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0194, F1: 0.8597, Acc: 0.8913\n","Val   - Loss: 0.1542, F1: 0.1260, Acc: 0.2564\n","Time  - Epoch: 128.9s, LR: 1.00e-05\n","Progress: 6.0% | Best F1: 0.1551 | ETA: 103.2min\n","\n","Epoch 4/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 4/50: 100%|██████████| 164/164 [02:06<00:00,  1.29it/s, Loss=0.0085, LR=1.00e-05]\n","CASME II Validation Epoch 4/50: 100%|██████████| 5/5 [00:02<00:00,  2.44it/s, Val Loss=0.0717]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0085, F1: 0.9726, Acc: 0.9644\n","Val   - Loss: 0.1613, F1: 0.1541, Acc: 0.2692\n","Time  - Epoch: 129.1s, LR: 1.00e-05\n","Progress: 8.0% | Best F1: 0.1551 | ETA: 100.5min\n","\n","Epoch 5/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 5/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0043, LR=1.00e-05]\n","CASME II Validation Epoch 5/50: 100%|██████████| 5/5 [00:02<00:00,  2.47it/s, Val Loss=0.0750]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0042, F1: 0.9916, Acc: 0.9900\n","Val   - Loss: 0.1681, F1: 0.1734, Acc: 0.3205\n","Time  - Epoch: 129.0s, LR: 1.00e-05\n","Attempt 1: Saving checkpoint to temporary file...\n","Validating checkpoint integrity...\n","Checkpoint validation passed\n","Moving validated checkpoint to final location...\n","Checkpoint saved and validated successfully: casme2_poolformer_multiframe_best_f1.pth\n","  Epoch: 5\n","  Val F1: 0.1734\n","  Val Loss: 0.1681\n","  Val Acc: 0.3205\n","New best model: Higher F1 - F1: 0.1734\n","Progress: 10.0% | Best F1: 0.1734 | ETA: 98.4min\n","\n","Epoch 6/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 6/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0027, LR=1.00e-05]\n","CASME II Validation Epoch 6/50: 100%|██████████| 5/5 [00:02<00:00,  2.43it/s, Val Loss=0.0862]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0027, F1: 0.9941, Acc: 0.9935\n","Val   - Loss: 0.1838, F1: 0.1322, Acc: 0.2821\n","Time  - Epoch: 129.1s, LR: 1.00e-05\n","Progress: 12.0% | Best F1: 0.1734 | ETA: 96.0min\n","\n","Epoch 7/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 7/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0018, LR=1.00e-05]\n","CASME II Validation Epoch 7/50: 100%|██████████| 5/5 [00:02<00:00,  2.43it/s, Val Loss=0.0896]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0018, F1: 0.9968, Acc: 0.9969\n","Val   - Loss: 0.1866, F1: 0.1179, Acc: 0.2821\n","Time  - Epoch: 129.1s, LR: 1.00e-05\n","Progress: 14.0% | Best F1: 0.1734 | ETA: 93.6min\n","\n","Epoch 8/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 8/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0014, LR=1.00e-05]\n","CASME II Validation Epoch 8/50: 100%|██████████| 5/5 [00:02<00:00,  2.46it/s, Val Loss=0.0892]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0014, F1: 0.9967, Acc: 0.9969\n","Val   - Loss: 0.1949, F1: 0.1116, Acc: 0.2692\n","Time  - Epoch: 129.1s, LR: 1.00e-05\n","Progress: 16.0% | Best F1: 0.1734 | ETA: 91.3min\n","\n","Epoch 9/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 9/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0011, LR=1.00e-05]\n","CASME II Validation Epoch 9/50: 100%|██████████| 5/5 [00:02<00:00,  2.43it/s, Val Loss=0.0892]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0011, F1: 0.9977, Acc: 0.9981\n","Val   - Loss: 0.1963, F1: 0.1020, Acc: 0.2564\n","Time  - Epoch: 129.1s, LR: 5.00e-06\n","Progress: 18.0% | Best F1: 0.1734 | ETA: 89.0min\n","\n","Epoch 10/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 10/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0009, LR=5.00e-06]\n","CASME II Validation Epoch 10/50: 100%|██████████| 5/5 [00:02<00:00,  2.45it/s, Val Loss=0.0949]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9989, Acc: 0.9989\n","Val   - Loss: 0.2019, F1: 0.1035, Acc: 0.2564\n","Time  - Epoch: 129.1s, LR: 5.00e-06\n","Progress: 20.0% | Best F1: 0.1734 | ETA: 86.8min\n","\n","Epoch 11/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 11/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0007, LR=5.00e-06]\n","CASME II Validation Epoch 11/50: 100%|██████████| 5/5 [00:02<00:00,  2.42it/s, Val Loss=0.0957]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0007, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2028, F1: 0.1107, Acc: 0.2692\n","Time  - Epoch: 129.2s, LR: 5.00e-06\n","Progress: 22.0% | Best F1: 0.1734 | ETA: 84.6min\n","\n","Epoch 12/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 12/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0007, LR=5.00e-06]\n","CASME II Validation Epoch 12/50: 100%|██████████| 5/5 [00:02<00:00,  2.46it/s, Val Loss=0.0981]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0007, F1: 0.9987, Acc: 0.9989\n","Val   - Loss: 0.2061, F1: 0.1059, Acc: 0.2564\n","Time  - Epoch: 129.1s, LR: 5.00e-06\n","Progress: 24.0% | Best F1: 0.1734 | ETA: 82.3min\n","\n","Epoch 13/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 13/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0006, LR=5.00e-06]\n","CASME II Validation Epoch 13/50: 100%|██████████| 5/5 [00:02<00:00,  2.40it/s, Val Loss=0.0979]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 0.9995, Acc: 0.9996\n","Val   - Loss: 0.2079, F1: 0.1099, Acc: 0.2692\n","Time  - Epoch: 129.3s, LR: 2.50e-06\n","Progress: 26.0% | Best F1: 0.1734 | ETA: 80.1min\n","\n","Epoch 14/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 14/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0006, LR=2.50e-06]\n","CASME II Validation Epoch 14/50: 100%|██████████| 5/5 [00:02<00:00,  2.46it/s, Val Loss=0.0985]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2081, F1: 0.1047, Acc: 0.2564\n","Time  - Epoch: 129.2s, LR: 2.50e-06\n","Progress: 28.0% | Best F1: 0.1734 | ETA: 77.9min\n","\n","Epoch 15/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 15/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0005, LR=2.50e-06]\n","CASME II Validation Epoch 15/50: 100%|██████████| 5/5 [00:02<00:00,  2.42it/s, Val Loss=0.1009]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2117, F1: 0.1080, Acc: 0.2692\n","Time  - Epoch: 129.1s, LR: 2.50e-06\n","Progress: 30.0% | Best F1: 0.1734 | ETA: 75.7min\n","\n","Epoch 16/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 16/50: 100%|██████████| 164/164 [02:06<00:00,  1.29it/s, Loss=0.0005, LR=2.50e-06]\n","CASME II Validation Epoch 16/50: 100%|██████████| 5/5 [00:02<00:00,  2.45it/s, Val Loss=0.1001]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2100, F1: 0.1047, Acc: 0.2564\n","Time  - Epoch: 129.0s, LR: 2.50e-06\n","Progress: 32.0% | Best F1: 0.1734 | ETA: 73.5min\n","\n","Epoch 17/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 17/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0005, LR=2.50e-06]\n","CASME II Validation Epoch 17/50: 100%|██████████| 5/5 [00:02<00:00,  2.44it/s, Val Loss=0.1007]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2111, F1: 0.1047, Acc: 0.2564\n","Time  - Epoch: 129.3s, LR: 1.25e-06\n","Progress: 34.0% | Best F1: 0.1734 | ETA: 71.4min\n","\n","Epoch 18/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 18/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0005, LR=1.25e-06]\n","CASME II Validation Epoch 18/50: 100%|██████████| 5/5 [00:02<00:00,  2.43it/s, Val Loss=0.1010]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2122, F1: 0.1044, Acc: 0.2564\n","Time  - Epoch: 129.1s, LR: 1.25e-06\n","Progress: 36.0% | Best F1: 0.1734 | ETA: 69.2min\n","\n","Epoch 19/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 19/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0004, LR=1.25e-06]\n","CASME II Validation Epoch 19/50: 100%|██████████| 5/5 [00:02<00:00,  2.47it/s, Val Loss=0.1015]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2131, F1: 0.1044, Acc: 0.2564\n","Time  - Epoch: 129.1s, LR: 1.25e-06\n","Progress: 38.0% | Best F1: 0.1734 | ETA: 67.0min\n","\n","Epoch 20/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 20/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0004, LR=1.25e-06]\n","CASME II Validation Epoch 20/50: 100%|██████████| 5/5 [00:02<00:00,  2.47it/s, Val Loss=0.1028]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2145, F1: 0.1044, Acc: 0.2564\n","Time  - Epoch: 129.2s, LR: 1.25e-06\n","Progress: 40.0% | Best F1: 0.1734 | ETA: 64.8min\n","\n","Epoch 21/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 21/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0004, LR=1.25e-06]\n","CASME II Validation Epoch 21/50: 100%|██████████| 5/5 [00:02<00:00,  2.41it/s, Val Loss=0.1021]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2149, F1: 0.1044, Acc: 0.2564\n","Time  - Epoch: 129.2s, LR: 1.00e-06\n","Progress: 42.0% | Best F1: 0.1734 | ETA: 62.7min\n","\n","Epoch 22/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 22/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0004, LR=1.00e-06]\n","CASME II Validation Epoch 22/50: 100%|██████████| 5/5 [00:02<00:00,  2.43it/s, Val Loss=0.1026]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2153, F1: 0.1103, Acc: 0.2692\n","Time  - Epoch: 129.1s, LR: 1.00e-06\n","Progress: 44.0% | Best F1: 0.1734 | ETA: 60.5min\n","\n","Epoch 23/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 23/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0004, LR=1.00e-06]\n","CASME II Validation Epoch 23/50: 100%|██████████| 5/5 [00:02<00:00,  2.44it/s, Val Loss=0.1026]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2150, F1: 0.1044, Acc: 0.2564\n","Time  - Epoch: 129.1s, LR: 1.00e-06\n","Progress: 46.0% | Best F1: 0.1734 | ETA: 58.3min\n","\n","Epoch 24/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 24/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0004, LR=1.00e-06]\n","CASME II Validation Epoch 24/50: 100%|██████████| 5/5 [00:02<00:00,  2.45it/s, Val Loss=0.1034]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2165, F1: 0.1103, Acc: 0.2692\n","Time  - Epoch: 129.2s, LR: 1.00e-06\n","Progress: 48.0% | Best F1: 0.1734 | ETA: 56.2min\n","\n","Epoch 25/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 25/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0004, LR=1.00e-06]\n","CASME II Validation Epoch 25/50: 100%|██████████| 5/5 [00:02<00:00,  2.41it/s, Val Loss=0.1039]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2177, F1: 0.1044, Acc: 0.2564\n","Time  - Epoch: 129.1s, LR: 1.00e-06\n","Progress: 50.0% | Best F1: 0.1734 | ETA: 54.0min\n","\n","Epoch 26/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 26/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0004, LR=1.00e-06]\n","CASME II Validation Epoch 26/50: 100%|██████████| 5/5 [00:02<00:00,  2.40it/s, Val Loss=0.1045]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2183, F1: 0.1033, Acc: 0.2564\n","Time  - Epoch: 129.2s, LR: 1.00e-06\n","Progress: 52.0% | Best F1: 0.1734 | ETA: 51.8min\n","\n","Epoch 27/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 27/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0004, LR=1.00e-06]\n","CASME II Validation Epoch 27/50: 100%|██████████| 5/5 [00:02<00:00,  2.40it/s, Val Loss=0.1037]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2180, F1: 0.1091, Acc: 0.2692\n","Time  - Epoch: 129.3s, LR: 1.00e-06\n","Progress: 54.0% | Best F1: 0.1734 | ETA: 49.7min\n","\n","Epoch 28/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 28/50: 100%|██████████| 164/164 [02:06<00:00,  1.29it/s, Loss=0.0003, LR=1.00e-06]\n","CASME II Validation Epoch 28/50: 100%|██████████| 5/5 [00:02<00:00,  2.43it/s, Val Loss=0.1037]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2182, F1: 0.1103, Acc: 0.2692\n","Time  - Epoch: 129.1s, LR: 1.00e-06\n","Progress: 56.0% | Best F1: 0.1734 | ETA: 47.5min\n","\n","Epoch 29/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 29/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0003, LR=1.00e-06]\n","CASME II Validation Epoch 29/50: 100%|██████████| 5/5 [00:02<00:00,  2.46it/s, Val Loss=0.1043]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2190, F1: 0.1103, Acc: 0.2692\n","Time  - Epoch: 129.1s, LR: 1.00e-06\n","Progress: 58.0% | Best F1: 0.1734 | ETA: 45.3min\n","\n","Epoch 30/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 30/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0004, LR=1.00e-06]\n","CASME II Validation Epoch 30/50: 100%|██████████| 5/5 [00:02<00:00,  2.44it/s, Val Loss=0.1041]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2198, F1: 0.1091, Acc: 0.2692\n","Time  - Epoch: 129.2s, LR: 1.00e-06\n","Progress: 60.0% | Best F1: 0.1734 | ETA: 43.2min\n","\n","Epoch 31/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 31/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0003, LR=1.00e-06]\n","CASME II Validation Epoch 31/50: 100%|██████████| 5/5 [00:02<00:00,  2.43it/s, Val Loss=0.1068]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2213, F1: 0.1091, Acc: 0.2692\n","Time  - Epoch: 129.3s, LR: 1.00e-06\n","Progress: 62.0% | Best F1: 0.1734 | ETA: 41.0min\n","\n","Epoch 32/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 32/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0003, LR=1.00e-06]\n","CASME II Validation Epoch 32/50: 100%|██████████| 5/5 [00:02<00:00,  2.44it/s, Val Loss=0.1067]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2222, F1: 0.1033, Acc: 0.2564\n","Time  - Epoch: 129.1s, LR: 1.00e-06\n","Progress: 64.0% | Best F1: 0.1734 | ETA: 38.8min\n","\n","Epoch 33/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 33/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0003, LR=1.00e-06]\n","CASME II Validation Epoch 33/50: 100%|██████████| 5/5 [00:02<00:00,  2.41it/s, Val Loss=0.1076]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2229, F1: 0.1103, Acc: 0.2692\n","Time  - Epoch: 129.2s, LR: 1.00e-06\n","Progress: 66.0% | Best F1: 0.1734 | ETA: 36.7min\n","\n","Epoch 34/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 34/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0003, LR=1.00e-06]\n","CASME II Validation Epoch 34/50: 100%|██████████| 5/5 [00:02<00:00,  2.46it/s, Val Loss=0.1078]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2250, F1: 0.1103, Acc: 0.2692\n","Time  - Epoch: 129.2s, LR: 1.00e-06\n","Progress: 68.0% | Best F1: 0.1734 | ETA: 34.5min\n","\n","Epoch 35/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 35/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0002, LR=1.00e-06]\n","CASME II Validation Epoch 35/50: 100%|██████████| 5/5 [00:02<00:00,  2.47it/s, Val Loss=0.1075]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2254, F1: 0.1091, Acc: 0.2692\n","Time  - Epoch: 129.2s, LR: 1.00e-06\n","Progress: 70.0% | Best F1: 0.1734 | ETA: 32.4min\n","\n","Epoch 36/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 36/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0003, LR=1.00e-06]\n","CASME II Validation Epoch 36/50: 100%|██████████| 5/5 [00:02<00:00,  2.41it/s, Val Loss=0.1073]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2241, F1: 0.1103, Acc: 0.2692\n","Time  - Epoch: 129.2s, LR: 1.00e-06\n","Progress: 72.0% | Best F1: 0.1734 | ETA: 30.2min\n","\n","Epoch 37/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 37/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0003, LR=1.00e-06]\n","CASME II Validation Epoch 37/50: 100%|██████████| 5/5 [00:02<00:00,  2.44it/s, Val Loss=0.1067]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2264, F1: 0.1091, Acc: 0.2692\n","Time  - Epoch: 129.1s, LR: 1.00e-06\n","Progress: 74.0% | Best F1: 0.1734 | ETA: 28.0min\n","\n","Epoch 38/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 38/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0003, LR=1.00e-06]\n","CASME II Validation Epoch 38/50: 100%|██████████| 5/5 [00:02<00:00,  2.44it/s, Val Loss=0.1081]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2283, F1: 0.1091, Acc: 0.2692\n","Time  - Epoch: 129.2s, LR: 1.00e-06\n","Progress: 76.0% | Best F1: 0.1734 | ETA: 25.9min\n","\n","Epoch 39/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 39/50: 100%|██████████| 164/164 [02:06<00:00,  1.29it/s, Loss=0.0003, LR=1.00e-06]\n","CASME II Validation Epoch 39/50: 100%|██████████| 5/5 [00:02<00:00,  2.40it/s, Val Loss=0.1095]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2293, F1: 0.1080, Acc: 0.2692\n","Time  - Epoch: 129.1s, LR: 1.00e-06\n","Progress: 78.0% | Best F1: 0.1734 | ETA: 23.7min\n","\n","Epoch 40/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 40/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0002, LR=1.00e-06]\n","CASME II Validation Epoch 40/50: 100%|██████████| 5/5 [00:02<00:00,  2.44it/s, Val Loss=0.1102]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2291, F1: 0.1091, Acc: 0.2692\n","Time  - Epoch: 129.2s, LR: 1.00e-06\n","Progress: 80.0% | Best F1: 0.1734 | ETA: 21.6min\n","\n","Epoch 41/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 41/50: 100%|██████████| 164/164 [02:06<00:00,  1.29it/s, Loss=0.0002, LR=1.00e-06]\n","CASME II Validation Epoch 41/50: 100%|██████████| 5/5 [00:02<00:00,  2.41it/s, Val Loss=0.1127]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2324, F1: 0.1091, Acc: 0.2692\n","Time  - Epoch: 129.0s, LR: 1.00e-06\n","Progress: 82.0% | Best F1: 0.1734 | ETA: 19.4min\n","\n","Epoch 42/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 42/50: 100%|██████████| 164/164 [02:06<00:00,  1.29it/s, Loss=0.0002, LR=1.00e-06]\n","CASME II Validation Epoch 42/50: 100%|██████████| 5/5 [00:02<00:00,  2.41it/s, Val Loss=0.1117]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2322, F1: 0.1091, Acc: 0.2692\n","Time  - Epoch: 129.1s, LR: 1.00e-06\n","Progress: 84.0% | Best F1: 0.1734 | ETA: 17.3min\n","\n","Epoch 43/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 43/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0002, LR=1.00e-06]\n","CASME II Validation Epoch 43/50: 100%|██████████| 5/5 [00:02<00:00,  2.46it/s, Val Loss=0.1107]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2320, F1: 0.1080, Acc: 0.2692\n","Time  - Epoch: 129.2s, LR: 1.00e-06\n","Progress: 86.0% | Best F1: 0.1734 | ETA: 15.1min\n","\n","Epoch 44/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 44/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0002, LR=1.00e-06]\n","CASME II Validation Epoch 44/50: 100%|██████████| 5/5 [00:02<00:00,  2.43it/s, Val Loss=0.1139]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2354, F1: 0.1080, Acc: 0.2692\n","Time  - Epoch: 129.1s, LR: 1.00e-06\n","Progress: 88.0% | Best F1: 0.1734 | ETA: 12.9min\n","\n","Epoch 45/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 45/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0002, LR=1.00e-06]\n","CASME II Validation Epoch 45/50: 100%|██████████| 5/5 [00:02<00:00,  2.40it/s, Val Loss=0.1138]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2351, F1: 0.1080, Acc: 0.2692\n","Time  - Epoch: 129.1s, LR: 1.00e-06\n","Progress: 90.0% | Best F1: 0.1734 | ETA: 10.8min\n","\n","Epoch 46/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 46/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0002, LR=1.00e-06]\n","CASME II Validation Epoch 46/50: 100%|██████████| 5/5 [00:02<00:00,  2.43it/s, Val Loss=0.1141]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2355, F1: 0.1103, Acc: 0.2692\n","Time  - Epoch: 129.1s, LR: 1.00e-06\n","Progress: 92.0% | Best F1: 0.1734 | ETA: 8.6min\n","\n","Epoch 47/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 47/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0002, LR=1.00e-06]\n","CASME II Validation Epoch 47/50: 100%|██████████| 5/5 [00:02<00:00,  2.43it/s, Val Loss=0.1155]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2377, F1: 0.1112, Acc: 0.2692\n","Time  - Epoch: 129.2s, LR: 1.00e-06\n","Progress: 94.0% | Best F1: 0.1734 | ETA: 6.5min\n","\n","Epoch 48/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 48/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0002, LR=1.00e-06]\n","CASME II Validation Epoch 48/50: 100%|██████████| 5/5 [00:02<00:00,  2.38it/s, Val Loss=0.1155]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2383, F1: 0.1103, Acc: 0.2692\n","Time  - Epoch: 129.4s, LR: 1.00e-06\n","Progress: 96.0% | Best F1: 0.1734 | ETA: 4.3min\n","\n","Epoch 49/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 49/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0002, LR=1.00e-06]\n","CASME II Validation Epoch 49/50: 100%|██████████| 5/5 [00:02<00:00,  2.39it/s, Val Loss=0.1168]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2395, F1: 0.1103, Acc: 0.2692\n","Time  - Epoch: 129.1s, LR: 1.00e-06\n","Progress: 98.0% | Best F1: 0.1734 | ETA: 2.2min\n","\n","Epoch 50/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 50/50: 100%|██████████| 164/164 [02:07<00:00,  1.29it/s, Loss=0.0002, LR=1.00e-06]\n","CASME II Validation Epoch 50/50: 100%|██████████| 5/5 [00:02<00:00,  2.43it/s, Val Loss=0.1156]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2403, F1: 0.1080, Acc: 0.2692\n","Time  - Epoch: 129.2s, LR: 1.00e-06\n","Progress: 100.0% | Best F1: 0.1734 | ETA: 0.0min\n","\n","======================================================================\n","CASME II MULTI-FRAME POOLFORMER TRAINING COMPLETED\n","======================================================================\n","Training time: 107.8 minutes\n","Epochs completed: 50\n","Best validation F1: 0.1734 (epoch 5)\n","Final train F1: 1.0000\n","Final validation F1: 0.1080\n","\n","Exporting enhanced training documentation...\n","Enhanced training documentation saved successfully: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/04_03_poolformer_casme2_mfs/training_logs/casme2_poolformer_multiframe_training_history.json\n","Experiment details: Optimized Focal Loss loss\n","  Gamma: 2.0, Alpha Sum: 0.999\n","Model variant: sail/poolformer_m48\n","Dataset phase: v3\n","Frame strategy: multi_frame_sampling\n","\n","Next: Cell 3 - CASME II Multi-Frame PoolFormer Evaluation\n","Enhanced training pipeline with multi-frame temporal windows completed successfully!\n"]}]},{"cell_type":"code","source":["# @title Cell 3: CASME II Multi-Frame PoolFormer Evaluation with Test Version Selection\n","\n","# File: 04_03_PoolFormer_CASME2_MFS_Cell3.py\n","# Location: experiments/04_03_PoolFormer_CASME2-MFS.ipynb\n","# Purpose: Comprehensive evaluation framework with support for v1 (apex-only) or v2 (key-frames) test sets\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from datetime import datetime\n","\n","# Evaluation specific imports\n","from sklearn.metrics import (\n","    accuracy_score, precision_recall_fscore_support,\n","    classification_report, confusion_matrix,\n","    roc_curve, auc\n",")\n","from sklearn.preprocessing import label_binarize\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing as mp\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =====================================================\n","# TEST DATASET VERSION SELECTOR\n","# =====================================================\n","# Select which test dataset to use for evaluation:\n","# 'v1' = Phase 1 apex-only frames (28 samples from data_split/test/)\n","# 'v2' = Phase 2 key-frames (84 samples from data_split_v2/test/)\n","\n","TEST_DATASET_VERSION = 'v2'  # Default: Phase 2 key-frames\n","\n","print(\"CASME II Multi-Frame PoolFormer Evaluation Framework\")\n","print(\"=\" * 60)\n","print(f\"Test Dataset Version: {TEST_DATASET_VERSION}\")\n","print(\"=\" * 60)\n","\n","# =====================================================\n","# DYNAMIC TEST DATASET CONFIGURATION\n","# =====================================================\n","\n","def get_test_dataset_config(version, project_root):\n","    \"\"\"\n","    Get test dataset configuration based on version selection\n","\n","    Args:\n","        version: 'v1' or 'v2'\n","        project_root: Project root path\n","\n","    Returns:\n","        dict: Configuration for selected test dataset\n","    \"\"\"\n","    if version == 'v1':\n","        config = {\n","            'version': 'v1',\n","            'phase': 'Phase 1',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/data_split\",\n","            'metadata_file': 'split_metadata.json',\n","            'processing_summary': 'processing_summary.json',\n","            'description': 'Apex-only frames',\n","            'expected_samples': 28,\n","            'frame_types': ['apex']\n","        }\n","    elif version == 'v2':\n","        config = {\n","            'version': 'v2',\n","            'phase': 'Phase 2',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/data_split_v2\",\n","            'metadata_file': 'split_metadata_v2.json',\n","            'processing_summary': 'processing_summary_v2.json',\n","            'description': 'Key-frames (onset, apex, offset)',\n","            'expected_samples': 84,\n","            'frame_types': ['onset', 'apex', 'offset']\n","        }\n","    else:\n","        raise ValueError(f\"Invalid TEST_DATASET_VERSION: {version}. Must be 'v1' or 'v2'\")\n","\n","    return config\n","\n","# Get test dataset configuration\n","test_config = get_test_dataset_config(TEST_DATASET_VERSION, PROJECT_ROOT)\n","\n","print(f\"Test Dataset Configuration:\")\n","print(f\"  Version: {test_config['version']}\")\n","print(f\"  Phase: {test_config['phase']}\")\n","print(f\"  Description: {test_config['description']}\")\n","print(f\"  Expected samples: {test_config['expected_samples']}\")\n","print(f\"  Frame types: {test_config['frame_types']}\")\n","print(f\"  Dataset path: {test_config['dataset_path']}\")\n","\n","# Independent test dataset metadata loading\n","print(f\"\\nLoading test metadata independently from selected version...\")\n","test_metadata_path = f\"{test_config['dataset_path']}/{test_config['metadata_file']}\"\n","test_processing_path = f\"{test_config['dataset_path']}/{test_config['processing_summary']}\"\n","\n","if not os.path.exists(test_metadata_path):\n","    raise FileNotFoundError(f\"Test metadata not found: {test_metadata_path}\")\n","\n","print(f\"Test metadata path: {test_metadata_path}\")\n","\n","with open(test_metadata_path, 'r') as f:\n","    test_metadata = json.load(f)\n","\n","# Normalize metadata structure for v1/v2 compatibility\n","def normalize_metadata_structure(metadata):\n","    \"\"\"Normalize metadata structure to handle both v1 and v2 formats\"\"\"\n","    if 'splits' in metadata:\n","        print(\"  Metadata format: v2 (with 'splits' key)\")\n","        return metadata['splits']\n","    elif 'train' in metadata or 'test' in metadata:\n","        print(\"  Metadata format: v1 (direct split keys)\")\n","        return metadata\n","    else:\n","        raise ValueError(\"Unknown metadata format\")\n","\n","normalized_test_metadata = normalize_metadata_structure(test_metadata)\n","\n","# Verify test samples\n","if 'test' not in normalized_test_metadata:\n","    raise ValueError(\"Test split not found in metadata\")\n","\n","actual_test_samples = len(normalized_test_metadata['test']['samples'])\n","print(f\"Loaded {actual_test_samples} test samples (expected: {test_config['expected_samples']})\")\n","\n","if actual_test_samples != test_config['expected_samples']:\n","    print(f\"WARNING: Sample count mismatch! Expected {test_config['expected_samples']}, got {actual_test_samples}\")\n","\n","# Enhanced test dataset for CASME II evaluation\n","class CASME2DatasetEvaluation(Dataset):\n","    \"\"\"Enhanced CASME II test dataset with comprehensive evaluation support\"\"\"\n","\n","    def __init__(self, split_metadata, dataset_root, transform=None, split='test', use_ram_cache=True):\n","        self.metadata = split_metadata[split]['samples']\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","        self.split = split\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.sample_ids = []\n","        self.emotions = []\n","        self.subjects = []\n","        self.cached_images = []\n","\n","        print(f\"Loading CASME II {split} dataset for evaluation...\")\n","\n","        # Process metadata for evaluation\n","        for sample in self.metadata:\n","            image_path = os.path.join(dataset_root, split, sample['image_filename'])\n","            self.images.append(image_path)\n","            self.labels.append(CLASS_TO_IDX[sample['emotion']])\n","            self.sample_ids.append(sample['sample_id'])\n","            self.emotions.append(sample['emotion'])\n","            self.subjects.append(sample['subject'])\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples for evaluation\")\n","        self._print_evaluation_distribution()\n","\n","        # RAM caching for fast evaluation\n","        if self.use_ram_cache:\n","            self._preload_to_ram_evaluation()\n","\n","    def _print_evaluation_distribution(self):\n","        \"\"\"Print comprehensive distribution for evaluation analysis\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"No test samples found!\")\n","            return\n","\n","        label_counts = {}\n","        subject_counts = {}\n","\n","        for label, subject in zip(self.labels, self.subjects):\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","            subject_counts[subject] = subject_counts.get(subject, 0) + 1\n","\n","        print(\"Test set class distribution:\")\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","        print(f\"Test set covers {len(subject_counts)} subjects\")\n","\n","        # Check for missing classes\n","        missing_classes = []\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i not in label_counts:\n","                missing_classes.append(class_name)\n","\n","        if missing_classes:\n","            print(f\"Missing classes in test set: {missing_classes}\")\n","\n","    def _preload_to_ram_evaluation(self):\n","        \"\"\"RAM preloading with parallel loading optimized for evaluation\"\"\"\n","        print(f\"Preloading {len(self.images)} test images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            \"\"\"Load single image with error handling\"\"\"\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (384, 384):\n","                    image = image.resize((384, 384), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (384, 384), (128, 128, 128)), False\n","\n","        # Parallel loading with ThreadPoolExecutor\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            # Submit all loading tasks\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            # Collect results with progress bar\n","            for future in tqdm(futures, desc=\"Loading test images to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * 384 * 384 * 3 * 4 / 1e9\n","        print(f\"Test RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (384, 384):\n","                    image = image.resize((384, 384), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (384, 384), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return (image, self.labels[idx], self.sample_ids[idx],\n","                self.emotions[idx], self.subjects[idx], os.path.basename(self.images[idx]))\n","\n","# CASME II evaluation configuration with test version tracking\n","EVALUATION_CONFIG_CASME2 = {\n","    'model_type': 'PoolFormer_CASME2_MultiFrame_Baseline',\n","    'task_type': 'micro_expression_recognition',\n","    'num_classes': 7,\n","    'class_names': CASME2_CLASSES,\n","    'checkpoint_file': 'casme2_poolformer_multiframe_best_f1.pth',\n","    'dataset_name': 'CASME_II',\n","    'input_size': '384x384',\n","    'evaluation_protocol': 'stratified_split',\n","    'test_dataset_version': test_config['version'],\n","    'test_dataset_phase': test_config['phase'],\n","    'test_dataset_description': test_config['description'],\n","    'test_frame_types': test_config['frame_types']\n","}\n","\n","print(f\"\\nCASME II PoolFormer Evaluation Configuration:\")\n","print(f\"  Model: {EVALUATION_CONFIG_CASME2['model_type']}\")\n","print(f\"  Task: {EVALUATION_CONFIG_CASME2['task_type']}\")\n","print(f\"  Test Version: {EVALUATION_CONFIG_CASME2['test_dataset_version']}\")\n","print(f\"  Test Description: {EVALUATION_CONFIG_CASME2['test_dataset_description']}\")\n","print(f\"  Classes: {EVALUATION_CONFIG_CASME2['class_names']}\")\n","print(f\"  Input size: {EVALUATION_CONFIG_CASME2['input_size']}\")\n","\n","def extract_logits_safe_casme2(outputs_all):\n","    \"\"\"Robust logits extraction for CASME II PoolFormer model\"\"\"\n","    if isinstance(outputs_all, torch.Tensor):\n","        return outputs_all\n","    if isinstance(outputs_all, (tuple, list)):\n","        for item in outputs_all:\n","            if isinstance(item, torch.Tensor):\n","                return item\n","    if isinstance(outputs_all, dict):\n","        for key in ('logits', 'logit', 'predictions', 'outputs', 'scores'):\n","            value = outputs_all.get(key)\n","            if isinstance(value, torch.Tensor):\n","                return value\n","        # Fallback to first tensor value\n","        for value in outputs_all.values():\n","            if isinstance(value, torch.Tensor):\n","                return value\n","    raise RuntimeError(\"Unable to extract tensor logits from CASME II PoolFormer model output\")\n","\n","def load_trained_model_casme2(checkpoint_path, device):\n","    \"\"\"Load trained CASME II PoolFormer model with comprehensive compatibility\"\"\"\n","    print(f\"Loading trained CASME II Multi-Frame PoolFormer model from: {checkpoint_path}\")\n","\n","    if not os.path.exists(checkpoint_path):\n","        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n","\n","    # Multiple loading approaches for maximum compatibility\n","    checkpoint = None\n","    loading_method = \"unknown\"\n","\n","    try:\n","        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","        loading_method = \"standard\"\n","    except Exception as e1:\n","        try:\n","            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n","            loading_method = \"weights_only_false\"\n","        except Exception as e2:\n","            try:\n","                import pickle\n","                with open(checkpoint_path, 'rb') as f:\n","                    checkpoint = pickle.load(f)\n","                loading_method = \"pickle\"\n","            except Exception as e3:\n","                raise RuntimeError(f\"All loading methods failed: {e1}, {e2}, {e3}\")\n","\n","    print(f\"Checkpoint loaded using: {loading_method}\")\n","\n","    # Initialize CASME II PoolFormer model\n","    model = PoolFormerCASME2Baseline(\n","        num_classes=EVALUATION_CONFIG_CASME2['num_classes'],\n","        dropout_rate=CASME2_POOLFORMER_CONFIG['dropout_rate']\n","    ).to(device)\n","\n","    # Load state dict with fallback approaches\n","    state_dict = checkpoint.get('model_state_dict', checkpoint)\n","\n","    try:\n","        model.load_state_dict(state_dict, strict=True)\n","        print(\"Model state loaded with strict=True\")\n","    except Exception as e:\n","        print(f\"Strict loading failed, trying non-strict: {str(e)[:100]}...\")\n","        try:\n","            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n","            if missing_keys or unexpected_keys:\n","                print(f\"Non-strict loading: Missing {len(missing_keys)}, Unexpected {len(unexpected_keys)}\")\n","            else:\n","                print(\"Model state loaded with strict=False (no key mismatches)\")\n","        except Exception as e2:\n","            raise RuntimeError(f\"Both loading approaches failed: {e2}\")\n","\n","    model.eval()\n","\n","    # Extract training information\n","    training_info = {\n","        'best_val_f1': float(checkpoint.get('best_f1', 0.0)),\n","        'best_val_loss': float(checkpoint.get('best_loss', float('inf'))),\n","        'best_val_accuracy': float(checkpoint.get('best_acc', 0.0)),\n","        'best_epoch': int(checkpoint.get('epoch', 0)) + 1,\n","        'model_checkpoint': EVALUATION_CONFIG_CASME2['checkpoint_file'],\n","        'num_classes': EVALUATION_CONFIG_CASME2['num_classes'],\n","        'config': checkpoint.get('casme2_config', {})\n","    }\n","\n","    print(f\"Model loaded successfully:\")\n","    print(f\"  Best validation F1: {training_info['best_val_f1']:.4f}\")\n","    print(f\"  Best validation accuracy: {training_info['best_val_accuracy']:.4f}\")\n","    print(f\"  Best epoch: {training_info['best_epoch']}\")\n","    print(f\"  Model classes: {EVALUATION_CONFIG_CASME2['num_classes']}\")\n","\n","    return model, training_info\n","\n","def run_model_inference_casme2(model, test_loader, device):\n","    \"\"\"Run CASME II PoolFormer model inference with comprehensive tracking\"\"\"\n","    print(\"Running CASME II Multi-Frame PoolFormer model inference on test set...\")\n","\n","    model.eval()\n","    all_predictions = []\n","    all_probabilities = []\n","    all_labels = []\n","    all_sample_ids = []\n","    all_emotions = []\n","    all_subjects = []\n","    all_filenames = []\n","\n","    inference_start = time.time()\n","\n","    with torch.no_grad():\n","        for batch_idx, (images, labels, sample_ids, emotions, subjects, filenames) in enumerate(\n","            tqdm(test_loader, desc=\"CASME II Inference\")):\n","\n","            images = images.to(device)\n","\n","            # Forward pass with robust output extraction\n","            try:\n","                outputs_raw = model(images)\n","                outputs = extract_logits_safe_casme2(outputs_raw)\n","            except Exception as e:\n","                print(f\"Error in model forward pass: {e}\")\n","                outputs = model(images)\n","                if not isinstance(outputs, torch.Tensor):\n","                    outputs = outputs[0] if isinstance(outputs, (list, tuple)) else outputs\n","\n","            # Validate output shape for 7 CASME II classes\n","            if outputs.shape[1] != 7:\n","                print(f\"Warning: Expected 7 classes output, got {outputs.shape[1]}\")\n","\n","            # Get probabilities and predictions\n","            probabilities = torch.softmax(outputs, dim=1)\n","            predictions = torch.argmax(probabilities, dim=1)\n","\n","            # Store results (CPU for memory efficiency)\n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_probabilities.extend(probabilities.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","            all_sample_ids.extend(sample_ids)\n","            all_emotions.extend(emotions)\n","            all_subjects.extend(subjects)\n","            all_filenames.extend(filenames)\n","\n","    inference_time = time.time() - inference_start\n","\n","    print(f\"CASME II inference completed: {len(all_predictions)} samples in {inference_time:.2f}s\")\n","\n","    # Analyze prediction distribution\n","    predictions_array = np.array(all_predictions)\n","    labels_array = np.array(all_labels)\n","\n","    unique_predictions, pred_counts = np.unique(predictions_array, return_counts=True)\n","    print(f\"Predicted classes: {[CASME2_CLASSES[i] for i in unique_predictions]}\")\n","\n","    unique_labels, label_counts = np.unique(labels_array, return_counts=True)\n","    print(f\"True classes in test: {[CASME2_CLASSES[i] for i in unique_labels]}\")\n","\n","    return {\n","        'predictions': predictions_array,\n","        'probabilities': np.array(all_probabilities),\n","        'labels': labels_array,\n","        'sample_ids': all_sample_ids,\n","        'emotions': all_emotions,\n","        'subjects': all_subjects,\n","        'filenames': all_filenames,\n","        'inference_time': inference_time,\n","        'samples_count': len(predictions_array)\n","    }\n","\n","def analyze_wrong_predictions_casme2(inference_results):\n","    \"\"\"Comprehensive wrong predictions analysis for CASME II\"\"\"\n","    print(\"Analyzing wrong predictions for CASME II micro-expression recognition...\")\n","\n","    predictions = inference_results['predictions']\n","    labels = inference_results['labels']\n","    sample_ids = inference_results['sample_ids']\n","    emotions = inference_results['emotions']\n","    subjects = inference_results['subjects']\n","    filenames = inference_results['filenames']\n","\n","    # Find wrong predictions\n","    wrong_mask = predictions != labels\n","    wrong_indices = np.where(wrong_mask)[0]\n","\n","    # Organize by true emotion class\n","    wrong_predictions_by_class = {}\n","    subject_error_analysis = {}\n","\n","    for class_name in CASME2_CLASSES:\n","        wrong_predictions_by_class[class_name] = []\n","\n","    # Analyze wrong predictions\n","    for idx in wrong_indices:\n","        true_label = labels[idx]\n","        pred_label = predictions[idx]\n","        sample_id = sample_ids[idx]\n","        emotion = emotions[idx]\n","        subject = subjects[idx]\n","        filename = filenames[idx]\n","\n","        true_class = CASME2_CLASSES[true_label]\n","        pred_class = CASME2_CLASSES[pred_label]\n","\n","        wrong_info = {\n","            'sample_id': sample_id,\n","            'filename': filename,\n","            'subject': subject,\n","            'true_label': int(true_label),\n","            'true_class': true_class,\n","            'predicted_label': int(pred_label),\n","            'predicted_class': pred_class,\n","            'emotion': emotion\n","        }\n","\n","        wrong_predictions_by_class[true_class].append(wrong_info)\n","\n","        # Subject error tracking\n","        if subject not in subject_error_analysis:\n","            subject_error_analysis[subject] = {'total': 0, 'wrong': 0, 'errors': []}\n","        subject_error_analysis[subject]['wrong'] += 1\n","        subject_error_analysis[subject]['errors'].append(wrong_info)\n","\n","    # Count total samples per subject\n","    for subject in subjects:\n","        if subject in subject_error_analysis:\n","            subject_error_analysis[subject]['total'] += 1\n","        else:\n","            subject_error_analysis[subject] = {'total': 1, 'wrong': 0, 'errors': []}\n","\n","    # Calculate error rates per subject\n","    for subject in subject_error_analysis:\n","        total = subject_error_analysis[subject]['total']\n","        wrong = subject_error_analysis[subject]['wrong']\n","        subject_error_analysis[subject]['error_rate'] = wrong / total if total > 0 else 0.0\n","\n","    # Summary statistics\n","    total_wrong = len(wrong_indices)\n","    total_samples = len(predictions)\n","    error_rate = (total_wrong / total_samples) * 100\n","\n","    # Confusion patterns analysis\n","    confusion_patterns = {}\n","    for idx in wrong_indices:\n","        true_label = labels[idx]\n","        pred_label = predictions[idx]\n","        pattern = f\"{CASME2_CLASSES[true_label]}_to_{CASME2_CLASSES[pred_label]}\"\n","        confusion_patterns[pattern] = confusion_patterns.get(pattern, 0) + 1\n","\n","    analysis_results = {\n","        'analysis_metadata': {\n","            'evaluation_timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n","            'model_type': EVALUATION_CONFIG_CASME2['model_type'],\n","            'dataset': EVALUATION_CONFIG_CASME2['dataset_name'],\n","            'test_version': EVALUATION_CONFIG_CASME2['test_dataset_version'],\n","            'test_description': EVALUATION_CONFIG_CASME2['test_dataset_description'],\n","            'total_samples': int(total_samples),\n","            'total_wrong_predictions': int(total_wrong),\n","            'overall_error_rate': float(error_rate)\n","        },\n","        'wrong_predictions_by_class': wrong_predictions_by_class,\n","        'subject_error_analysis': subject_error_analysis,\n","        'confusion_patterns': confusion_patterns,\n","        'error_summary': {\n","            class_name: len(wrong_predictions_by_class[class_name])\n","            for class_name in CASME2_CLASSES\n","        }\n","    }\n","\n","    return analysis_results\n","\n","def calculate_comprehensive_metrics_casme2(inference_results):\n","    \"\"\"Calculate comprehensive evaluation metrics for CASME II micro-expression recognition\"\"\"\n","    print(\"Calculating comprehensive metrics for CASME II micro-expression recognition...\")\n","\n","    predictions = inference_results['predictions']\n","    probabilities = inference_results['probabilities']\n","    labels = inference_results['labels']\n","\n","    if len(predictions) == 0:\n","        raise ValueError(\"No predictions to evaluate!\")\n","\n","    # Identify available classes in test set\n","    unique_test_labels = sorted(np.unique(labels))\n","    unique_predictions = sorted(np.unique(predictions))\n","\n","    print(f\"Test set contains labels: {[CASME2_CLASSES[i] for i in unique_test_labels]}\")\n","    print(f\"Model predicted classes: {[CASME2_CLASSES[i] for i in unique_predictions]}\")\n","\n","    # Basic metrics\n","    accuracy = accuracy_score(labels, predictions)\n","\n","    # Macro metrics (only for available classes)\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions, labels=unique_test_labels, average='macro', zero_division=0\n","    )\n","\n","    print(f\"Macro F1 (available classes): {f1:.4f}\")\n","\n","    # Per-class metrics (all 7 classes)\n","    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n","        labels, predictions, labels=range(7), average=None, zero_division=0\n","    )\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(labels, predictions, labels=range(7))\n","\n","    # Multi-class AUC (only for classes with test samples)\n","    auc_scores = {}\n","    fpr_dict = {}\n","    tpr_dict = {}\n","\n","    try:\n","        labels_binarized = label_binarize(labels, classes=range(7))\n","\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i in unique_test_labels and len(np.unique(labels_binarized[:, i])) > 1:\n","                fpr, tpr, _ = roc_curve(labels_binarized[:, i], probabilities[:, i])\n","                auc_score = auc(fpr, tpr)\n","                auc_scores[class_name] = float(auc_score)\n","                fpr_dict[class_name] = fpr.tolist()\n","                tpr_dict[class_name] = tpr.tolist()\n","            else:\n","                auc_scores[class_name] = 0.0\n","                fpr_dict[class_name] = [0.0, 1.0]\n","                tpr_dict[class_name] = [0.0, 0.0]\n","\n","        # Macro AUC for available classes\n","        available_auc_scores = [auc_scores[CASME2_CLASSES[i]] for i in unique_test_labels]\n","        macro_auc = float(np.mean(available_auc_scores)) if available_auc_scores else 0.0\n","\n","    except Exception as e:\n","        print(f\"Warning: AUC calculation failed: {e}\")\n","        auc_scores = {class_name: 0.0 for class_name in CASME2_CLASSES}\n","        macro_auc = 0.0\n","\n","    # Subject-level analysis\n","    subjects = inference_results['subjects']\n","    subject_performance = {}\n","\n","    for subject in set(subjects):\n","        subject_mask = [s == subject for s in subjects]\n","        subject_predictions = predictions[subject_mask]\n","        subject_labels = labels[subject_mask]\n","\n","        if len(subject_predictions) > 0:\n","            subject_acc = accuracy_score(subject_labels, subject_predictions)\n","            subject_performance[subject] = {\n","                'accuracy': float(subject_acc),\n","                'samples': int(len(subject_predictions)),\n","                'correct': int(np.sum(subject_predictions == subject_labels))\n","            }\n","\n","    # Comprehensive results with test version tracking\n","    comprehensive_results = {\n","        'evaluation_metadata': {\n","            'model_type': EVALUATION_CONFIG_CASME2['model_type'],\n","            'dataset': EVALUATION_CONFIG_CASME2['dataset_name'],\n","            'test_version': EVALUATION_CONFIG_CASME2['test_dataset_version'],\n","            'test_phase': EVALUATION_CONFIG_CASME2['test_dataset_phase'],\n","            'test_description': EVALUATION_CONFIG_CASME2['test_dataset_description'],\n","            'test_frame_types': EVALUATION_CONFIG_CASME2['test_frame_types'],\n","            'evaluation_timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n","            'num_classes': EVALUATION_CONFIG_CASME2['num_classes'],\n","            'class_names': EVALUATION_CONFIG_CASME2['class_names'],\n","            'test_samples': int(len(labels)),\n","            'available_classes': [CASME2_CLASSES[i] for i in unique_test_labels],\n","            'missing_classes': [CASME2_CLASSES[i] for i in range(7) if i not in unique_test_labels]\n","        },\n","\n","        'overall_performance': {\n","            'accuracy': float(accuracy),\n","            'macro_precision': float(precision),\n","            'macro_recall': float(recall),\n","            'macro_f1': float(f1),\n","            'macro_auc': macro_auc\n","        },\n","\n","        'per_class_performance': {},\n","\n","        'confusion_matrix': cm.tolist(),\n","\n","        'subject_level_performance': subject_performance,\n","\n","        'roc_analysis': {\n","            'auc_scores': auc_scores,\n","            'fpr_curves': fpr_dict,\n","            'tpr_curves': tpr_dict\n","        },\n","\n","        'inference_performance': {\n","            'total_time_seconds': float(inference_results['inference_time']),\n","            'average_time_ms_per_sample': float(inference_results['inference_time'] * 1000 / len(labels))\n","        }\n","    }\n","\n","    # Per-class performance details\n","    for i, class_name in enumerate(CASME2_CLASSES):\n","        comprehensive_results['per_class_performance'][class_name] = {\n","            'precision': float(precision_per_class[i]),\n","            'recall': float(recall_per_class[i]),\n","            'f1_score': float(f1_per_class[i]),\n","            'support': int(support_per_class[i]),\n","            'auc': auc_scores[class_name],\n","            'in_test_set': i in unique_test_labels\n","        }\n","\n","    return comprehensive_results\n","\n","def save_evaluation_results_casme2(evaluation_results, wrong_predictions_results, results_dir, test_version):\n","    \"\"\"Save comprehensive evaluation results for CASME II with test version in filename\"\"\"\n","    os.makedirs(results_dir, exist_ok=True)\n","\n","    # Save main evaluation results with version suffix\n","    results_file = f\"{results_dir}/casme2_poolformer_multiframe_evaluation_results_{test_version}.json\"\n","    with open(results_file, 'w') as f:\n","        json.dump(evaluation_results, f, indent=2, default=str)\n","\n","    # Save wrong predictions analysis with version suffix\n","    wrong_predictions_file = f\"{results_dir}/casme2_poolformer_multiframe_wrong_predictions_{test_version}.json\"\n","    with open(wrong_predictions_file, 'w') as f:\n","        json.dump(wrong_predictions_results, f, indent=2, default=str)\n","\n","    print(f\"Evaluation results saved:\")\n","    print(f\"  Main results: {os.path.basename(results_file)}\")\n","    print(f\"  Wrong predictions: {os.path.basename(wrong_predictions_file)}\")\n","\n","    return results_file, wrong_predictions_file\n","\n","# Main evaluation execution\n","try:\n","    print(\"\\nStarting CASME II Multi-Frame PoolFormer comprehensive evaluation...\")\n","    print(f\"Using test dataset: {test_config['description']} ({test_config['version']})\")\n","\n","    # Create test dataset with selected version\n","    print(f\"\\nCreating CASME II test dataset from {test_config['phase']}...\")\n","    casme2_test_dataset = CASME2DatasetEvaluation(\n","        split_metadata=normalized_test_metadata,\n","        dataset_root=test_config['dataset_path'],\n","        transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","        split='test',\n","        use_ram_cache=True\n","    )\n","\n","    if len(casme2_test_dataset) == 0:\n","        raise ValueError(\"No test samples found! Check test data path.\")\n","\n","    casme2_test_loader = DataLoader(\n","        casme2_test_dataset,\n","        batch_size=CASME2_POOLFORMER_CONFIG['batch_size'],\n","        shuffle=False,\n","        num_workers=CASME2_POOLFORMER_CONFIG['num_workers'],\n","        pin_memory=True\n","    )\n","\n","    # Load trained model\n","    checkpoint_path = f\"{GLOBAL_CONFIG_CASME2['checkpoint_root']}/{EVALUATION_CONFIG_CASME2['checkpoint_file']}\"\n","    casme2_model, training_info = load_trained_model_casme2(checkpoint_path, GLOBAL_CONFIG_CASME2['device'])\n","\n","    # Run inference\n","    inference_results = run_model_inference_casme2(casme2_model, casme2_test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","    # Calculate comprehensive metrics\n","    evaluation_results = calculate_comprehensive_metrics_casme2(inference_results)\n","\n","    # Analyze wrong predictions\n","    wrong_predictions_results = analyze_wrong_predictions_casme2(inference_results)\n","\n","    # Add training information\n","    evaluation_results['training_information'] = training_info\n","\n","    # Save results with test version in filename\n","    results_dir = f\"{GLOBAL_CONFIG_CASME2['results_root']}/evaluation_results\"\n","    results_file, wrong_file = save_evaluation_results_casme2(\n","        evaluation_results, wrong_predictions_results, results_dir, test_config['version']\n","    )\n","\n","    # Display comprehensive results\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"CASME II MULTI-FRAME POOLFORMER EVALUATION RESULTS\")\n","    print(\"=\" * 60)\n","    print(f\"Test Dataset: {test_config['description']} ({test_config['version']})\")\n","\n","    # Overall performance\n","    overall = evaluation_results['overall_performance']\n","    print(f\"\\nOverall Performance (Macro - Available Classes):\")\n","    print(f\"  Accuracy:  {overall['accuracy']:.4f}\")\n","    print(f\"  Precision: {overall['macro_precision']:.4f}\")\n","    print(f\"  Recall:    {overall['macro_recall']:.4f}\")\n","    print(f\"  F1 Score:  {overall['macro_f1']:.4f}\")\n","    print(f\"  AUC:       {overall['macro_auc']:.4f}\")\n","\n","    # Per-class performance\n","    print(f\"\\nPer-Class Performance:\")\n","    for class_name, metrics in evaluation_results['per_class_performance'].items():\n","        in_test = \"Present\" if metrics['in_test_set'] else \"Missing\"\n","        print(f\"  {class_name} [{in_test}]: F1={metrics['f1_score']:.4f}, \"\n","              f\"AUC={metrics['auc']:.4f}, Support={metrics['support']}\")\n","\n","    # Training vs test comparison\n","    print(f\"\\nTraining vs Test Performance:\")\n","    training_f1 = training_info['best_val_f1']\n","    training_acc = training_info['best_val_accuracy']\n","    test_f1 = overall['macro_f1']\n","    test_acc = overall['accuracy']\n","\n","    print(f\"  Training Val F1:  {training_f1:.4f}\")\n","    print(f\"  Test F1:          {test_f1:.4f}\")\n","    print(f\"  F1 Difference:    {training_f1 - test_f1:+.4f}\")\n","    print(f\"  Training Val Acc: {training_acc:.4f}\")\n","    print(f\"  Test Accuracy:    {test_acc:.4f}\")\n","    print(f\"  Acc Difference:   {training_acc - test_acc:+.4f}\")\n","    print(f\"  Best Epoch:       {training_info['best_epoch']}\")\n","\n","    # Wrong predictions summary\n","    print(f\"\\n\" + \"=\" * 40)\n","    print(\"WRONG PREDICTIONS ANALYSIS\")\n","    print(\"=\" * 40)\n","\n","    wrong_meta = wrong_predictions_results['analysis_metadata']\n","    print(f\"Total wrong predictions: {wrong_meta['total_wrong_predictions']} / {wrong_meta['total_samples']}\")\n","    print(f\"Overall error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","    print(f\"\\nErrors by True Class:\")\n","    for class_name, error_count in wrong_predictions_results['error_summary'].items():\n","        if error_count > 0:\n","            wrong_samples = wrong_predictions_results['wrong_predictions_by_class'][class_name]\n","            print(f\"  {class_name}: {error_count} errors\")\n","            for sample in wrong_samples[:3]:  # Show first 3\n","                print(f\"    - {sample['filename']} -> predicted as {sample['predicted_class']}\")\n","            if len(wrong_samples) > 3:\n","                print(f\"    ... and {len(wrong_samples) - 3} more\")\n","\n","    # Subject-level analysis\n","    print(f\"\\nSubject-Level Performance:\")\n","    subject_perfs = list(evaluation_results['subject_level_performance'].items())\n","    subject_perfs.sort(key=lambda x: x[1]['accuracy'], reverse=True)\n","    for subject, perf in subject_perfs[:5]:  # Show top 5\n","        print(f\"  {subject}: {perf['accuracy']:.3f} ({perf['correct']}/{perf['samples']})\")\n","\n","    # Most common confusion patterns\n","    print(f\"\\nMost Common Confusion Patterns:\")\n","    patterns = sorted(wrong_predictions_results['confusion_patterns'].items(),\n","                     key=lambda x: x[1], reverse=True)\n","    for pattern, count in patterns[:3]:\n","        print(f\"  {pattern}: {count} cases\")\n","\n","    print(f\"\\nInference Performance:\")\n","    print(f\"  Total time: {inference_results['inference_time']:.2f}s\")\n","    print(f\"  Speed: {evaluation_results['inference_performance']['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","    print(f\"\\nTest Dataset Info:\")\n","    print(f\"  Version: {test_config['version']}\")\n","    print(f\"  Phase: {test_config['phase']}\")\n","    print(f\"  Frame types: {test_config['frame_types']}\")\n","    print(f\"  Missing classes: {evaluation_results['evaluation_metadata']['missing_classes']}\")\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"CASME II MULTI-FRAME POOLFORMER EVALUATION COMPLETED\")\n","    print(\"=\" * 60)\n","\n","except Exception as e:\n","    print(f\"Evaluation failed: {e}\")\n","    import traceback\n","    traceback.print_exc()\n","\n","finally:\n","    # Memory cleanup\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","        torch.cuda.empty_cache()\n","\n","print(f\"\\nEvaluation completed with test dataset {TEST_DATASET_VERSION}\")\n","print(\"Next: Cell 4 - Generate confusion matrix and comparative analysis\")"],"metadata":{"cellView":"form","collapsed":true,"id":"wUdFv0jWMK4T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759716950517,"user_tz":-420,"elapsed":7163,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"40cc5383-d61e-4a94-894a-2eb7352774d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II Multi-Frame PoolFormer Evaluation Framework\n","============================================================\n","Test Dataset Version: v2\n","============================================================\n","Test Dataset Configuration:\n","  Version: v2\n","  Phase: Phase 2\n","  Description: Key-frames (onset, apex, offset)\n","  Expected samples: 84\n","  Frame types: ['onset', 'apex', 'offset']\n","  Dataset path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v2\n","\n","Loading test metadata independently from selected version...\n","Test metadata path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v2/split_metadata_v2.json\n","  Metadata format: v2 (with 'splits' key)\n","Loaded 84 test samples (expected: 84)\n","\n","CASME II PoolFormer Evaluation Configuration:\n","  Model: PoolFormer_CASME2_MultiFrame_Baseline\n","  Task: micro_expression_recognition\n","  Test Version: v2\n","  Test Description: Key-frames (onset, apex, offset)\n","  Classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","  Input size: 384x384\n","\n","Starting CASME II Multi-Frame PoolFormer comprehensive evaluation...\n","Using test dataset: Key-frames (onset, apex, offset) (v2)\n","\n","Creating CASME II test dataset from Phase 2...\n","Loading CASME II test dataset for evaluation...\n","Loaded 84 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 30 samples (35.7%)\n","  disgust: 21 samples (25.0%)\n","  happiness: 12 samples (14.3%)\n","  repression: 9 samples (10.7%)\n","  surprise: 9 samples (10.7%)\n","  sadness: 3 samples (3.6%)\n","Test set covers 16 subjects\n","Missing classes in test set: ['fear']\n","Preloading 84 test images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test images to RAM: 100%|██████████| 84/84 [00:01<00:00, 58.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test RAM caching completed: 84/84 images, ~0.15GB\n","Loading trained CASME II Multi-Frame PoolFormer model from: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/04_03_poolformer_casme2_mfs/casme2_poolformer_multiframe_best_f1.pth\n","Checkpoint loaded using: standard\n","PoolFormer feature dimension: 768\n","PoolFormer CASME II: 768 -> 512 -> 128 -> 7\n","Model state loaded with strict=True\n","Model loaded successfully:\n","  Best validation F1: 0.1734\n","  Best validation accuracy: 0.3205\n","  Best epoch: 5\n","  Model classes: 7\n","Running CASME II Multi-Frame PoolFormer model inference on test set...\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Inference: 100%|██████████| 6/6 [00:03<00:00,  1.90it/s]"]},{"output_type":"stream","name":"stdout","text":["CASME II inference completed: 84 samples in 3.15s\n","Predicted classes: ['others', 'disgust', 'happiness', 'repression', 'surprise']\n","True classes in test: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness']\n","Calculating comprehensive metrics for CASME II micro-expression recognition...\n","Test set contains labels: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness']\n","Model predicted classes: ['others', 'disgust', 'happiness', 'repression', 'surprise']\n","Macro F1 (available classes): 0.3654\n","Analyzing wrong predictions for CASME II micro-expression recognition...\n","Evaluation results saved:\n","  Main results: casme2_poolformer_multiframe_evaluation_results_v2.json\n","  Wrong predictions: casme2_poolformer_multiframe_wrong_predictions_v2.json\n","\n","============================================================\n","CASME II MULTI-FRAME POOLFORMER EVALUATION RESULTS\n","============================================================\n","Test Dataset: Key-frames (onset, apex, offset) (v2)\n","\n","Overall Performance (Macro - Available Classes):\n","  Accuracy:  0.4881\n","  Precision: 0.3854\n","  Recall:    0.3639\n","  F1 Score:  0.3654\n","  AUC:       0.6396\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.5806, AUC=0.7296, Support=30\n","  disgust [Present]: F1=0.6829, AUC=0.7377, Support=21\n","  happiness [Present]: F1=0.2500, AUC=0.6736, Support=12\n","  repression [Present]: F1=0.2500, AUC=0.6919, Support=9\n","  surprise [Present]: F1=0.4286, AUC=0.8074, Support=9\n","  sadness [Present]: F1=0.0000, AUC=0.1975, Support=3\n","  fear [Missing]: F1=0.0000, AUC=0.0000, Support=0\n","\n","Training vs Test Performance:\n","  Training Val F1:  0.1734\n","  Test F1:          0.3654\n","  F1 Difference:    -0.1919\n","  Training Val Acc: 0.3205\n","  Test Accuracy:    0.4881\n","  Acc Difference:   -0.1676\n","  Best Epoch:       5\n","\n","========================================\n","WRONG PREDICTIONS ANALYSIS\n","========================================\n","Total wrong predictions: 43 / 84\n","Overall error rate: 51.19%\n","\n","Errors by True Class:\n","  others: 12 errors\n","    - sub14_EP04_04f_onset_others.jpg -> predicted as happiness\n","    - sub14_EP04_04f_apex_others.jpg -> predicted as happiness\n","    - sub14_EP04_04f_offset_others.jpg -> predicted as happiness\n","    ... and 9 more\n","  disgust: 7 errors\n","    - sub05_EP09_05f_onset_disgust.jpg -> predicted as others\n","    - sub05_EP09_05f_apex_disgust.jpg -> predicted as others\n","    - sub05_EP09_05f_offset_disgust.jpg -> predicted as others\n","    ... and 4 more\n","  happiness: 9 errors\n","    - sub23_EP02_01_onset_happiness.jpg -> predicted as repression\n","    - sub23_EP02_01_apex_happiness.jpg -> predicted as repression\n","    - sub23_EP02_01_offset_happiness.jpg -> predicted as repression\n","    ... and 6 more\n","  repression: 6 errors\n","    - sub17_EP05_03f_onset_repression.jpg -> predicted as happiness\n","    - sub17_EP05_03f_apex_repression.jpg -> predicted as happiness\n","    - sub17_EP05_03f_offset_repression.jpg -> predicted as happiness\n","    ... and 3 more\n","  surprise: 6 errors\n","    - sub17_EP01_13_onset_surprise.jpg -> predicted as happiness\n","    - sub17_EP01_13_apex_surprise.jpg -> predicted as happiness\n","    - sub17_EP01_13_offset_surprise.jpg -> predicted as happiness\n","    ... and 3 more\n","  sadness: 3 errors\n","    - sub17_EP15_03_onset_sadness.jpg -> predicted as repression\n","    - sub17_EP15_03_apex_sadness.jpg -> predicted as repression\n","    - sub17_EP15_03_offset_sadness.jpg -> predicted as repression\n","\n","Subject-Level Performance:\n","  sub09: 1.000 (3/3)\n","  sub01: 1.000 (6/6)\n","  sub24: 1.000 (3/3)\n","  sub10: 1.000 (3/3)\n","  sub20: 1.000 (3/3)\n","\n","Most Common Confusion Patterns:\n","  others_to_disgust: 6 cases\n","  happiness_to_repression: 6 cases\n","  disgust_to_others: 5 cases\n","\n","Inference Performance:\n","  Total time: 3.15s\n","  Speed: 37.5 ms/sample\n","\n","Test Dataset Info:\n","  Version: v2\n","  Phase: Phase 2\n","  Frame types: ['onset', 'apex', 'offset']\n","  Missing classes: ['fear']\n","\n","============================================================\n","CASME II MULTI-FRAME POOLFORMER EVALUATION COMPLETED\n","============================================================\n","\n","Evaluation completed with test dataset v2\n","Next: Cell 4 - Generate confusion matrix and comparative analysis\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title CASME II PoolFormer Confusion Matrix Generator for Paper\n","\n","import json\n","import os\n","import glob\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from datetime import datetime\n","\n","print(\"CASME II Multi-Frame PoolFormer - Clean Confusion Matrix Generation\")\n","print(\"=\" * 70)\n","\n","# Project paths configuration\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/04_03_poolformer_casme2_mfs\"\n","\n","def find_evaluation_files(results_path):\n","    \"\"\"Find evaluation JSON files with version detection\"\"\"\n","    json_files = {}\n","    eval_dir = f\"{results_path}/evaluation_results\"\n","\n","    if not os.path.exists(eval_dir):\n","        print(f\"ERROR: Evaluation directory not found: {eval_dir}\")\n","        return json_files\n","\n","    for version in ['v1', 'v2']:\n","        eval_pattern = f\"{eval_dir}/casme2_poolformer_multiframe_evaluation_results_{version}.json\"\n","        eval_files = glob.glob(eval_pattern)\n","\n","        if eval_files:\n","            json_files[version] = eval_files[0]\n","            print(f\"Found {version.upper()}: {os.path.basename(eval_files[0])}\")\n","\n","    if not json_files:\n","        print(\"WARNING: No evaluation results found\")\n","        print(\"Make sure Cell 3 (evaluation) has been executed first\")\n","\n","    return json_files\n","\n","def load_evaluation_data(json_path):\n","    \"\"\"Load evaluation results JSON\"\"\"\n","    try:\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","        return data\n","    except Exception as e:\n","        print(f\"ERROR loading {json_path}: {str(e)}\")\n","        return None\n","\n","def calculate_weighted_f1(per_class_performance):\n","    \"\"\"Calculate weighted F1 score\"\"\"\n","    total_support = sum([class_data['support'] for class_data in per_class_performance.values()\n","                        if class_data['support'] > 0])\n","\n","    if total_support == 0:\n","        return 0.0\n","\n","    weighted_f1 = 0.0\n","    for class_name, class_data in per_class_performance.items():\n","        if class_data['support'] > 0:\n","            weight = class_data['support'] / total_support\n","            weighted_f1 += class_data['f1_score'] * weight\n","\n","    return weighted_f1\n","\n","def calculate_balanced_accuracy(confusion_matrix):\n","    \"\"\"Calculate balanced accuracy\"\"\"\n","    cm = np.array(confusion_matrix)\n","    n_classes = cm.shape[0]\n","\n","    per_class_balanced_acc = []\n","\n","    for i in range(n_classes):\n","        if cm[i, :].sum() > 0:\n","            tp = cm[i, i]\n","            fn = cm[i, :].sum() - tp\n","            fp = cm[:, i].sum() - tp\n","            tn = cm.sum() - tp - fn - fp\n","\n","            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n","\n","            class_balanced_acc = (sensitivity + specificity) / 2\n","            per_class_balanced_acc.append(class_balanced_acc)\n","\n","    balanced_acc = np.mean(per_class_balanced_acc) if per_class_balanced_acc else 0.0\n","    return balanced_acc\n","\n","def determine_text_color(color_value, threshold=0.5):\n","    \"\"\"Determine optimal text color based on background intensity\"\"\"\n","    return 'white' if color_value > threshold else 'black'\n","\n","def create_clean_confusion_matrix(data, output_path, version):\n","    \"\"\"Create clean confusion matrix for academic paper\"\"\"\n","\n","    # Extract data\n","    meta = data['evaluation_metadata']\n","    class_names = meta['class_names']\n","    cm = np.array(data['confusion_matrix'], dtype=int)\n","    overall = data['overall_performance']\n","    per_class = data['per_class_performance']\n","\n","    print(f\"\\nProcessing {version.upper()} confusion matrix\")\n","    print(f\"  Classes: {class_names}\")\n","    print(f\"  Matrix shape: {cm.shape}\")\n","\n","    # Calculate metrics\n","    macro_f1 = overall.get('macro_f1', 0.0)\n","    accuracy = overall.get('accuracy', 0.0)\n","    weighted_f1 = calculate_weighted_f1(per_class)\n","    balanced_acc = calculate_balanced_accuracy(cm)\n","\n","    print(f\"  Accuracy: {accuracy:.4f} | Macro F1: {macro_f1:.4f} | \"\n","          f\"Weighted F1: {weighted_f1:.4f} | Balanced Acc: {balanced_acc:.4f}\")\n","\n","    # Row-wise normalization for percentage display\n","    row_sums = cm.sum(axis=1, keepdims=True)\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        cm_pct = np.divide(cm, row_sums, where=(row_sums!=0))\n","        cm_pct = np.nan_to_num(cm_pct)\n","\n","    # Create figure\n","    fig, ax = plt.subplots(figsize=(10, 8.5))\n","\n","    # Color scheme\n","    cmap = 'Blues'\n","\n","    # Create heatmap\n","    im = ax.imshow(cm_pct, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=0.8)\n","\n","    # Add colorbar\n","    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","    cbar.set_label('Percentage', rotation=270, labelpad=20, fontsize=11)\n","\n","    # Annotate cells with count and percentage\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            count = cm[i, j]\n","\n","            if row_sums[i, 0] > 0:\n","                percentage = cm_pct[i, j] * 100\n","                text = f\"{count}\\n{percentage:.1f}%\"\n","            else:\n","                text = f\"{count}\\n(N/A)\"\n","\n","            # Determine text color\n","            cell_value = cm_pct[i, j]\n","            text_color = determine_text_color(cell_value, threshold=0.4)\n","\n","            ax.text(j, i, text, ha=\"center\", va=\"center\",\n","                   color=text_color, fontsize=11, fontweight='bold')\n","\n","    # Configure axes\n","    ax.set_xticks(np.arange(len(class_names)))\n","    ax.set_yticks(np.arange(len(class_names)))\n","    ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=11)\n","    ax.set_yticklabels(class_names, fontsize=11)\n","    ax.set_xlabel(\"Predicted Label\", fontsize=12)\n","    ax.set_ylabel(\"True Label\", fontsize=12)\n","\n","    # Adjust layout and save\n","    plt.tight_layout()\n","    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n","    plt.close(fig)\n","\n","    print(f\"  Saved: {os.path.basename(output_path)}\")\n","\n","    return {\n","        'version': version,\n","        'accuracy': accuracy,\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        'balanced_accuracy': balanced_acc\n","    }\n","\n","# Main execution\n","print(\"\\nSearching for evaluation files...\")\n","json_files = find_evaluation_files(RESULTS_ROOT)\n","\n","if not json_files:\n","    print(\"\\nERROR: No evaluation files found\")\n","    print(\"Please ensure Cell 3 (evaluation) has been executed first\")\n","else:\n","    # Create output directory\n","    output_dir = f\"{RESULTS_ROOT}/confusion_matrix_paper\"\n","    Path(output_dir).mkdir(parents=True, exist_ok=True)\n","    print(f\"\\nOutput directory: {output_dir}\")\n","\n","    # Process each version\n","    results = []\n","\n","    for version in ['v1', 'v2']:\n","        if version in json_files:\n","            print(f\"\\n{'='*70}\")\n","            print(f\"Processing {version.upper()}\")\n","            print(f\"{'='*70}\")\n","\n","            # Load data\n","            eval_data = load_evaluation_data(json_files[version])\n","\n","            if eval_data is not None:\n","                try:\n","                    # Generate confusion matrix\n","                    output_filename = f\"confusion_matrix_{version}.png\"\n","                    output_path = os.path.join(output_dir, output_filename)\n","\n","                    metrics = create_clean_confusion_matrix(eval_data, output_path, version)\n","                    results.append(metrics)\n","\n","                except Exception as e:\n","                    print(f\"ERROR generating {version.upper()} confusion matrix: {str(e)}\")\n","                    import traceback\n","                    traceback.print_exc()\n","            else:\n","                print(f\"ERROR: Could not load {version.upper()} evaluation data\")\n","\n","    # Final summary\n","    if results:\n","        print(f\"\\n{'='*70}\")\n","        print(\"CONFUSION MATRIX GENERATION COMPLETED\")\n","        print(f\"{'='*70}\")\n","\n","        print(f\"\\nGenerated files in: {output_dir}\")\n","        for result in results:\n","            print(f\"\\n{result['version'].upper()} Performance:\")\n","            print(f\"  Accuracy:       {result['accuracy']:.4f}\")\n","            print(f\"  Macro F1:       {result['macro_f1']:.4f}\")\n","            print(f\"  Weighted F1:    {result['weighted_f1']:.4f}\")\n","            print(f\"  Balanced Acc:   {result['balanced_accuracy']:.4f}\")\n","\n","        print(f\"\\nCompleted at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","    else:\n","        print(\"\\nERROR: No confusion matrices were generated\")\n","\n","print(\"\\nScript execution completed\")"],"metadata":{"collapsed":true,"id":"8Ko2qFc9MxyD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761801068864,"user_tz":-420,"elapsed":2507,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"657a77d9-a39e-434c-f080-53011439f038"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II Multi-Frame PoolFormer - Clean Confusion Matrix Generation\n","======================================================================\n","\n","Searching for evaluation files...\n","Found V1: casme2_poolformer_multiframe_evaluation_results_v1.json\n","Found V2: casme2_poolformer_multiframe_evaluation_results_v2.json\n","\n","Output directory: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/04_03_poolformer_casme2_mfs/confusion_matrix_paper\n","\n","======================================================================\n","Processing V1\n","======================================================================\n","\n","Processing V1 confusion matrix\n","  Classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","  Matrix shape: (7, 7)\n","  Accuracy: 0.5357 | Macro F1: 0.4762 | Weighted F1: 0.5354 | Balanced Acc: 0.6772\n","  Saved: confusion_matrix_v1.png\n","\n","======================================================================\n","Processing V2\n","======================================================================\n","\n","Processing V2 confusion matrix\n","  Classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","  Matrix shape: (7, 7)\n","  Accuracy: 0.5119 | Macro F1: 0.4641 | Weighted F1: 0.5141 | Balanced Acc: 0.6694\n","  Saved: confusion_matrix_v2.png\n","\n","======================================================================\n","CONFUSION MATRIX GENERATION COMPLETED\n","======================================================================\n","\n","Generated files in: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/04_03_poolformer_casme2_mfs/confusion_matrix_paper\n","\n","V1 Performance:\n","  Accuracy:       0.5357\n","  Macro F1:       0.4762\n","  Weighted F1:    0.5354\n","  Balanced Acc:   0.6772\n","\n","V2 Performance:\n","  Accuracy:       0.5119\n","  Macro F1:       0.4641\n","  Weighted F1:    0.5141\n","  Balanced Acc:   0.6694\n","\n","Completed at: 2025-10-30 05:11:08\n","\n","Script execution completed\n"]}]}]}