{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyM7LBVjorE0wGREYFazYhPY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5a5ca8c471514578a31b460a588f7ef3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ca226e192ea5453eb952af3855ca5893","IPY_MODEL_836b7f682ba94929b52c9cfbf7526c77","IPY_MODEL_85334d7743014c2a9cefe66ca6bd94c2"],"layout":"IPY_MODEL_99113ece2dc24a5d8ed11d7ce28233c5"}},"ca226e192ea5453eb952af3855ca5893":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dff1c48abfbe4f5e8f439ee47bbd3dae","placeholder":"​","style":"IPY_MODEL_fe55d2aadfc44481aa1619dcf3365b40","value":"preprocessor_config.json: 100%"}},"836b7f682ba94929b52c9cfbf7526c77":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db6c523e29654076b38dd221ce7fb275","max":255,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b02949652d5447618ab855c5f5950d54","value":255}},"85334d7743014c2a9cefe66ca6bd94c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e44a93da6f1f411ea3b3155d5b97f4c7","placeholder":"​","style":"IPY_MODEL_c1117cf33e1a4340957588b3644699fb","value":" 255/255 [00:00&lt;00:00, 30.1kB/s]"}},"99113ece2dc24a5d8ed11d7ce28233c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dff1c48abfbe4f5e8f439ee47bbd3dae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe55d2aadfc44481aa1619dcf3365b40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db6c523e29654076b38dd221ce7fb275":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b02949652d5447618ab855c5f5950d54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e44a93da6f1f411ea3b3155d5b97f4c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1117cf33e1a4340957588b3644699fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b45dbd98b79448cfb3554a1c9bf71c2b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3852932a861f48ca8e461917cd0b7b38","IPY_MODEL_4e0d0944a280433b9cdb55290ae6f31d","IPY_MODEL_26ec771d15904c64a08e5e847e997aab"],"layout":"IPY_MODEL_22b73b55d9bf4f809ca44701f9b2bcdd"}},"3852932a861f48ca8e461917cd0b7b38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46f4b34baa9340ba8aba19ef356ee775","placeholder":"​","style":"IPY_MODEL_00d0fad465584d44a1285d3b1a45be40","value":"config.json: "}},"4e0d0944a280433b9cdb55290ae6f31d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec0c891f13034ad5bc6515ad6b403b9f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a788acf2e41642b68b0e7501370ef4cc","value":1}},"26ec771d15904c64a08e5e847e997aab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ccab1307d6f484ca6c418b93e0b6423","placeholder":"​","style":"IPY_MODEL_4e002b095453498390f9513cad84d695","value":" 71.8k/? [00:00&lt;00:00, 8.50MB/s]"}},"22b73b55d9bf4f809ca44701f9b2bcdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46f4b34baa9340ba8aba19ef356ee775":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00d0fad465584d44a1285d3b1a45be40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec0c891f13034ad5bc6515ad6b403b9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a788acf2e41642b68b0e7501370ef4cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ccab1307d6f484ca6c418b93e0b6423":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e002b095453498390f9513cad84d695":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c45e1aa7d41a444a8b9a694b38a73249":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07faf3ad61ea4e6780a0fdd5263a1f43","IPY_MODEL_38834d018b55435eb1704d9969387440","IPY_MODEL_566a7c0fc22b4e60b9a87f61287f525e"],"layout":"IPY_MODEL_aa56fa64f4e34b60ab615b41b75f062e"}},"07faf3ad61ea4e6780a0fdd5263a1f43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_479bb4160aa54cd481510190eaf520f3","placeholder":"​","style":"IPY_MODEL_425d1f0c0d57495aa7ba23009e1c73e1","value":"model.safetensors: 100%"}},"38834d018b55435eb1704d9969387440":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_85ec554bee3c406fa7d9eac8b9370f3a","max":351590690,"min":0,"orientation":"horizontal","style":"IPY_MODEL_95403ff1cf3b4ac5874af21175c77e2d","value":351590690}},"566a7c0fc22b4e60b9a87f61287f525e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a6e12f6fcac435d8882da1a3a7f3fb6","placeholder":"​","style":"IPY_MODEL_c9a672b8268a49308ef334639bb6b7a4","value":" 352M/352M [01:17&lt;00:00, 52.3kB/s]"}},"aa56fa64f4e34b60ab615b41b75f062e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"479bb4160aa54cd481510190eaf520f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"425d1f0c0d57495aa7ba23009e1c73e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85ec554bee3c406fa7d9eac8b9370f3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95403ff1cf3b4ac5874af21175c77e2d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a6e12f6fcac435d8882da1a3a7f3fb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9a672b8268a49308ef334639bb6b7a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5a5ca8c471514578a31b460a588f7ef3","ca226e192ea5453eb952af3855ca5893","836b7f682ba94929b52c9cfbf7526c77","85334d7743014c2a9cefe66ca6bd94c2","99113ece2dc24a5d8ed11d7ce28233c5","dff1c48abfbe4f5e8f439ee47bbd3dae","fe55d2aadfc44481aa1619dcf3365b40","db6c523e29654076b38dd221ce7fb275","b02949652d5447618ab855c5f5950d54","e44a93da6f1f411ea3b3155d5b97f4c7","c1117cf33e1a4340957588b3644699fb","b45dbd98b79448cfb3554a1c9bf71c2b","3852932a861f48ca8e461917cd0b7b38","4e0d0944a280433b9cdb55290ae6f31d","26ec771d15904c64a08e5e847e997aab","22b73b55d9bf4f809ca44701f9b2bcdd","46f4b34baa9340ba8aba19ef356ee775","00d0fad465584d44a1285d3b1a45be40","ec0c891f13034ad5bc6515ad6b403b9f","a788acf2e41642b68b0e7501370ef4cc","1ccab1307d6f484ca6c418b93e0b6423","4e002b095453498390f9513cad84d695","c45e1aa7d41a444a8b9a694b38a73249","07faf3ad61ea4e6780a0fdd5263a1f43","38834d018b55435eb1704d9969387440","566a7c0fc22b4e60b9a87f61287f525e","aa56fa64f4e34b60ab615b41b75f062e","479bb4160aa54cd481510190eaf520f3","425d1f0c0d57495aa7ba23009e1c73e1","85ec554bee3c406fa7d9eac8b9370f3a","95403ff1cf3b4ac5874af21175c77e2d","1a6e12f6fcac435d8882da1a3a7f3fb6","c9a672b8268a49308ef334639bb6b7a4"]},"collapsed":true,"cellView":"form","id":"eAl2z6VbS_8o","executionInfo":{"status":"ok","timestamp":1760944087499,"user_tz":-420,"elapsed":125281,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"08419081-ae06-4e08-f554-d6eaf779d8cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","CASME II APEX FRAME SWIN TRANSFORMER WITH FACE-AWARE PREPROCESSING\n","============================================================\n","\n","[1] Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully\n","\n","[2] Importing required libraries...\n","CASME II Apex Frame Swin Transformer - Face-Aware Preprocessing Infrastructure\n","============================================================\n","Loading CASME II v7 preprocessing metadata...\n","Dataset variant: AF\n","Processing date: 2025-10-19T08:16:29.397012\n","Preprocessing method: face_bbox_expansion_all_directions\n","Total images processed: 255\n","Face detection rate: 100.00%\n","Target size: 224x224px\n","BBox expansion: 20px (all directions)\n","Using Swin-Base for advanced hierarchical micro-expression recognition\n","\n","==================================================\n","EXPERIMENT CONFIGURATION - APEX FRAME FACE-AWARE WITH SWIN\n","==================================================\n","Dataset: v7 Apex Frame with Face-Aware Preprocessing\n","Loss Function: Focal Loss\n","  Gamma: 2.0\n","  Alpha Weights (per-class): [0.052, 0.066, 0.093, 0.101, 0.104, 0.208, 0.376]\n","  Alpha Sum Validation: 1.000\n","Swin Model: microsoft/swin-base-patch4-window7-224\n","Window Size: 7x7\n","Patch Size: 4x4\n","Input Resolution: 224x224px (native Swin 224 model)\n","Image Format: Grayscale converted to RGB (3-channel)\n","==================================================\n","\n","Device: cuda\n","GPU: NVIDIA L4 (23.8 GB)\n","GPU optimization enabled for NVIDIA L4\n","Small dataset configuration: Batch size 4 (optimal for 201 samples)\n","Iterations per epoch: 50 (more frequent weight updates)\n","RAM preload workers: 32 (parallel image loading)\n","\n","Loading v7 class distribution...\n","\n","v7 Train distribution: [79, 50, 25, 21, 20, 5, 1]\n","v7 Val distribution: [10, 6, 3, 3, 2, 1, 1]\n","v7 Test distribution: [10, 7, 4, 3, 3, 1, 0]\n","Applied Focal Loss alpha weights: [0.052 0.066 0.093 0.101 0.104 0.208 0.376]\n","Alpha weights sum: 1.000\n","\n","Swin Transformer Configuration Summary:\n","  Model: microsoft/swin-base-patch4-window7-224\n","  Variant: base\n","  Input size: 224px (native model)\n","  Window size: 7\n","  Learning rate: 2e-05 (optimized for small batches)\n","  Batch size: 4 (optimal for small dataset)\n","  Dropout rate: 0.3 (increased regularization)\n","  Dataset version: v7\n","  Frame strategy: apex_frame\n","  Preprocessing: face_aware_bbox_expansion\n","  Train images: 201\n","\n","Setting up Swin Transformer Image Processor for 224px input...\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a5ca8c471514578a31b460a588f7ef3"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"]},{"output_type":"stream","name":"stdout","text":["Swin Transformer Image Processor configured for 224px\n","Grayscale images will be converted to RGB via channel repetition\n","\n","Dataset paths:\n","Train: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7/train\n","Validation: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7/val\n","Test: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7/test\n","\n","Swin CASME II architecture validation...\n"]},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b45dbd98b79448cfb3554a1c9bf71c2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/352M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c45e1aa7d41a444a8b9a694b38a73249"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Swin feature dimension (final stage): 1024\n","Base embed_dim: 128, Stages: 4\n","Swin CASME II Simplified: 1024 -> GAP -> 256 -> 7\n","Dropout rate: 0.3 (increased for small dataset regularization)\n","Validation successful: Output shape torch.Size([1, 7])\n","Initial patches (Stage 1): 3136\n","Window size: 7x7\n","Hierarchical processing: Multi-stage feature extraction with shifted windows\n","Simplified architecture: 1024 -> 256 -> 7 (reduced parameters for small dataset)\n","\n","============================================================\n","CASME II APEX FRAME SWIN TRANSFORMER CONFIGURATION COMPLETE\n","============================================================\n","Loss Configuration:\n","  Function: Optimized Focal Loss\n","  Gamma: 2.0\n","  Per-class Alpha: [0.052, 0.066, 0.093, 0.101, 0.104, 0.208, 0.376]\n","  Alpha Sum: 1.000\n","\n","Model Configuration:\n","  Architecture: microsoft/swin-base-patch4-window7-224\n","  Variant: base\n","  Window Size: 7x7\n","  Patch Size: 4x4\n","  Input Resolution: 224px (native Swin model)\n","  Hidden Dimension: 1024\n","  Classification Head: Simplified 1024->256->7\n","  Dropout: 0.3 (increased regularization)\n","\n","Dataset Configuration:\n","  Version: v7\n","  Frame strategy: apex_frame\n","  Preprocessing: face_aware_bbox_expansion\n","  Classes: 7\n","  Train samples: 201\n","  Val samples: 26\n","  Test samples: 28\n","  Face detection rate: 100.00%\n","\n","Training Configuration (Optimized for Small Dataset):\n","  Batch size: 4 (optimal for 201 samples)\n","  Learning rate: 2e-05 (adjusted for small batches)\n","  Iterations/epoch: ~50 (frequent updates)\n","  Strategy: Simplified architecture + stronger regularization\n","  Config source: Proven from ViT v7 AF experiments\n","\n","Next: Cell 2 - Dataset Loading and Training Pipeline\n"]}],"source":["# @title Cell 1: CASME II Apex Frame Swin Transformer Infrastructure Configuration\n","\n","# File: 05_02_SwinT_CASME2_AF_Cell1.py\n","# Location: experiments/05_02_SwinT_CASME2-AF-PREP.ipynb\n","# Purpose: Swin Transformer for CASME II micro-expression recognition with apex frame and face-aware preprocessing\n","\n","# Mount Google Drive\n","from google.colab import drive\n","print(\"=\" * 60)\n","print(\"CASME II APEX FRAME SWIN TRANSFORMER WITH FACE-AWARE PREPROCESSING\")\n","print(\"=\" * 60)\n","print(\"\\n[1] Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Google Drive mounted successfully\")\n","\n","print(\"\\n[2] Importing required libraries...\")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import timm\n","import json\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import time\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Project paths configuration\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","DATASET_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/preprocessed_v7\"\n","CHECKPOINT_ROOT = f\"{PROJECT_ROOT}/models/05_02_swint_casme2_af_prep\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/05_02_swint_casme2_af_prep\"\n","\n","# Load CASME II v7 preprocessing metadata\n","PREPROCESSING_SUMMARY = f\"{DATASET_ROOT}/preprocessing_summary.json\"\n","\n","print(\"CASME II Apex Frame Swin Transformer - Face-Aware Preprocessing Infrastructure\")\n","print(\"=\" * 60)\n","\n","# Validate preprocessing metadata exists\n","if not os.path.exists(PREPROCESSING_SUMMARY):\n","    raise FileNotFoundError(f\"v7 preprocessing summary not found: {PREPROCESSING_SUMMARY}\")\n","\n","# Load v7 preprocessing metadata\n","print(\"Loading CASME II v7 preprocessing metadata...\")\n","with open(PREPROCESSING_SUMMARY, 'r') as f:\n","    preprocessing_info = json.load(f)\n","\n","print(f\"Dataset variant: {preprocessing_info['variant']}\")\n","print(f\"Processing date: {preprocessing_info['processing_date']}\")\n","print(f\"Preprocessing method: {preprocessing_info['preprocessing_method']}\")\n","print(f\"Total images processed: {preprocessing_info['total_processed']}\")\n","print(f\"Face detection rate: {preprocessing_info['face_detection_stats']['detection_rate']:.2%}\")\n","\n","# Extract preprocessing parameters\n","preproc_params = preprocessing_info['preprocessing_parameters']\n","print(f\"Target size: {preproc_params['target_size']}x{preproc_params['target_size']}px\")\n","print(f\"BBox expansion: {preproc_params['bbox_expansion']}px (all directions)\")\n","\n","# =====================================================\n","# EXPERIMENT CONFIGURATION - Apex Frame with Face-Aware Preprocessing\n","# Optimized based on ViT experiment results (Batch 4, LR 2e-5, Dropout 0.3)\n","# =====================================================\n","\n","# FOCAL LOSS CONFIGURATION - Toggle for experimentation\n","USE_FOCAL_LOSS = True  # Default: CrossEntropy, Set True to enable Focal Loss\n","FOCAL_LOSS_GAMMA = 2.0  # Focal loss focusing parameter (if enabled)\n","\n","# OPTIMIZED CLASS WEIGHTS CONFIGURATION\n","# CASME II classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","# v7 Train distribution: [79, 50, 25, 21, 20, 5, 1] - inverse sqrt frequency approach\n","\n","# CrossEntropy Loss - Optimized inverse square root frequency weights\n","CROSSENTROPY_CLASS_WEIGHTS = [1.00, 1.26, 1.78, 1.94, 1.99, 3.98, 8.89]\n","\n","# Focal Loss - Normalized per-class alpha values (sum = 1.0)\n","FOCAL_LOSS_ALPHA_WEIGHTS = [0.052, 0.066, 0.093, 0.101, 0.104, 0.208, 0.376]\n","\n","# SWIN TRANSFORMER MODEL CONFIGURATION\n","SWIN_MODEL_VARIANT = 'base'  # Using Base for consistency with ViT-Base experiments\n","\n","# Swin model selection - Native 224px support!\n","if SWIN_MODEL_VARIANT == 'tiny':\n","    SWIN_MODEL_NAME = 'microsoft/swin-tiny-patch4-window7-224'\n","    EXPECTED_HIDDEN_DIM = 768\n","    WINDOW_SIZE = 7\n","    PATCH_SIZE = 4\n","    print(\"Using Swin-Tiny for efficient hierarchical micro-expression analysis\")\n","elif SWIN_MODEL_VARIANT == 'base':\n","    SWIN_MODEL_NAME = 'microsoft/swin-base-patch4-window7-224'\n","    EXPECTED_HIDDEN_DIM = 1024\n","    WINDOW_SIZE = 7\n","    PATCH_SIZE = 4\n","    print(\"Using Swin-Base for advanced hierarchical micro-expression recognition\")\n","else:\n","    raise ValueError(f\"Unsupported SWIN_MODEL_VARIANT: {SWIN_MODEL_VARIANT}\")\n","\n","# Display experiment configuration\n","print(\"\\n\" + \"=\" * 50)\n","print(\"EXPERIMENT CONFIGURATION - APEX FRAME FACE-AWARE WITH SWIN\")\n","print(\"=\" * 50)\n","print(f\"Dataset: v7 Apex Frame with Face-Aware Preprocessing\")\n","print(f\"Loss Function: {'Focal Loss' if USE_FOCAL_LOSS else 'CrossEntropy Loss'}\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Alpha Weights (per-class): {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum Validation: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Class Weights (inverse sqrt freq): {CROSSENTROPY_CLASS_WEIGHTS}\")\n","print(f\"Swin Model: {SWIN_MODEL_NAME}\")\n","print(f\"Window Size: {WINDOW_SIZE}x{WINDOW_SIZE}\")\n","print(f\"Patch Size: {PATCH_SIZE}x{PATCH_SIZE}\")\n","print(f\"Input Resolution: 224x224px (native Swin 224 model)\")\n","print(f\"Image Format: Grayscale converted to RGB (3-channel)\")\n","print(\"=\" * 50)\n","\n","# Enhanced GPU configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n","gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n","\n","print(f\"\\nDevice: {device}\")\n","print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n","\n","# Optimized batch size for small dataset (201 train samples)\n","# Using proven config from ViT experiment: Batch 4\n","BATCH_SIZE = 4\n","NUM_WORKERS = 4\n","\n","if 'A100' in gpu_name or 'L4' in gpu_name:\n","    torch.backends.cudnn.benchmark = True\n","    print(f\"GPU optimization enabled for {gpu_name}\")\n","\n","print(f\"Small dataset configuration: Batch size {BATCH_SIZE} (optimal for 201 samples)\")\n","print(f\"Iterations per epoch: {201 // BATCH_SIZE} (more frequent weight updates)\")\n","\n","# RAM preloading workers (separate from DataLoader workers)\n","RAM_PRELOAD_WORKERS = 32\n","print(f\"RAM preload workers: {RAM_PRELOAD_WORKERS} (parallel image loading)\")\n","\n","# CASME II class mapping and analysis\n","CASME2_CLASSES = ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CASME2_CLASSES)}\n","\n","# Extract class distribution from v7 preprocessing metadata\n","print(\"\\nLoading v7 class distribution...\")\n","train_dist = preprocessing_info['splits']['train']['emotion_distribution']\n","val_dist = preprocessing_info['splits']['val']['emotion_distribution']\n","test_dist = preprocessing_info['splits']['test']['emotion_distribution']\n","\n","# Convert to ordered list matching CASME2_CLASSES\n","def emotion_dist_to_list(emotion_dict, class_names):\n","    \"\"\"Convert emotion distribution dict to ordered list\"\"\"\n","    return [emotion_dict.get(cls, 0) for cls in class_names]\n","\n","train_dist_list = emotion_dist_to_list(train_dist, CASME2_CLASSES)\n","val_dist_list = emotion_dist_to_list(val_dist, CASME2_CLASSES)\n","test_dist_list = emotion_dist_to_list(test_dist, CASME2_CLASSES)\n","\n","print(f\"\\nv7 Train distribution: {train_dist_list}\")\n","print(f\"v7 Val distribution: {val_dist_list}\")\n","print(f\"v7 Test distribution: {test_dist_list}\")\n","\n","# Apply optimized class weights based on loss function selection\n","if USE_FOCAL_LOSS:\n","    class_weights = torch.tensor(FOCAL_LOSS_ALPHA_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied Focal Loss alpha weights: {class_weights.cpu().numpy()}\")\n","    print(f\"Alpha weights sum: {class_weights.sum().item():.3f}\")\n","else:\n","    class_weights = torch.tensor(CROSSENTROPY_CLASS_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied CrossEntropy class weights: {class_weights.cpu().numpy()}\")\n","\n","# CASME II Swin Transformer Configuration for Apex Frame with Face-Aware Preprocessing\n","# Optimized for small dataset (201 train samples) with proven config from ViT experiments\n","CASME2_SWIN_CONFIG = {\n","    # Architecture configuration - Swin Transformer specific\n","    'swin_model': SWIN_MODEL_NAME,\n","    'swin_variant': SWIN_MODEL_VARIANT,\n","    'window_size': WINDOW_SIZE,\n","    'patch_size': PATCH_SIZE,\n","    'input_size': 224,  # Native 224px Swin model\n","    'num_classes': 7,\n","    'dropout_rate': 0.3,  # Increased from 0.2 for stronger regularization (proven in ViT)\n","    'expected_hidden_dim': EXPECTED_HIDDEN_DIM,\n","    'interpolate_pos_encoding': False,  # No interpolation needed for native 224px\n","\n","    # Training configuration - optimized for small dataset (proven from ViT experiments)\n","    'learning_rate': 2e-5,  # Slightly higher for small batch size (4)\n","    'weight_decay': 1e-5,\n","    'gradient_clip': 1.0,\n","    'num_epochs': 50,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'device': device,\n","\n","    # Scheduler configuration\n","    'scheduler_type': 'plateau',\n","    'scheduler_mode': 'max',\n","    'scheduler_factor': 0.5,\n","    'scheduler_patience': 3,\n","    'scheduler_min_lr': 1e-6,\n","    'scheduler_monitor': 'val_f1_macro',\n","\n","    # Loss configuration\n","    'use_focal_loss': USE_FOCAL_LOSS,\n","    'focal_loss_gamma': FOCAL_LOSS_GAMMA,\n","    'focal_loss_alpha_weights': FOCAL_LOSS_ALPHA_WEIGHTS,\n","    'crossentropy_class_weights': CROSSENTROPY_CLASS_WEIGHTS,\n","    'class_weights': class_weights,\n","\n","    # Evaluation configuration\n","    'use_macro_avg': True,\n","    'early_stopping': False,\n","    'save_best_f1': True,\n","    'save_strategy': 'best_only',\n","\n","    # v7 specific configuration\n","    'dataset_version': 'v7',\n","    'preprocessing_method': 'face_aware_bbox_expansion',\n","    'frame_strategy': 'apex_frame',\n","    'image_format': 'grayscale_to_rgb',\n","    'bbox_expansion': preproc_params['bbox_expansion'],\n","    'face_detection_rate': preprocessing_info['face_detection_stats']['detection_rate']\n","}\n","\n","print(f\"\\nSwin Transformer Configuration Summary:\")\n","print(f\"  Model: {CASME2_SWIN_CONFIG['swin_model']}\")\n","print(f\"  Variant: {CASME2_SWIN_CONFIG['swin_variant']}\")\n","print(f\"  Input size: {CASME2_SWIN_CONFIG['input_size']}px (native model)\")\n","print(f\"  Window size: {CASME2_SWIN_CONFIG['window_size']}\")\n","print(f\"  Learning rate: {CASME2_SWIN_CONFIG['learning_rate']} (optimized for small batches)\")\n","print(f\"  Batch size: {BATCH_SIZE} (optimal for small dataset)\")\n","print(f\"  Dropout rate: {CASME2_SWIN_CONFIG['dropout_rate']} (increased regularization)\")\n","print(f\"  Dataset version: {CASME2_SWIN_CONFIG['dataset_version']}\")\n","print(f\"  Frame strategy: {CASME2_SWIN_CONFIG['frame_strategy']}\")\n","print(f\"  Preprocessing: {CASME2_SWIN_CONFIG['preprocessing_method']}\")\n","print(f\"  Train images: {preprocessing_info['splits']['train']['total_images']}\")\n","\n","# =====================================================\n","# FOCAL LOSS IMPLEMENTATION\n","# =====================================================\n","\n","class OptimizedFocalLoss(nn.Module):\n","    \"\"\"\n","    Advanced Focal Loss implementation with per-class alpha support\n","    Paper: Focal Loss for Dense Object Detection (Lin et al., 2017)\n","    \"\"\"\n","\n","    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n","        super(OptimizedFocalLoss, self).__init__()\n","\n","        if alpha is not None:\n","            if isinstance(alpha, list):\n","                self.alpha = torch.tensor(alpha, dtype=torch.float32)\n","            else:\n","                self.alpha = alpha\n","\n","            alpha_sum = self.alpha.sum().item()\n","            if abs(alpha_sum - 1.0) > 0.01:\n","                print(f\"Warning: Alpha weights sum to {alpha_sum:.3f}, expected 1.0\")\n","        else:\n","            self.alpha = None\n","\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","\n","        if self.alpha is not None:\n","            if self.alpha.device != targets.device:\n","                self.alpha = self.alpha.to(targets.device)\n","            alpha_t = self.alpha.gather(0, targets)\n","        else:\n","            alpha_t = 1.0\n","\n","        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","\n","# Swin Transformer Architecture for CASME II\n","class SwinCASME2Baseline(nn.Module):\n","    \"\"\"Swin Transformer baseline for CASME II micro-expression recognition - Simplified for small dataset\"\"\"\n","\n","    def __init__(self, num_classes, dropout_rate=0.3):\n","        super(SwinCASME2Baseline, self).__init__()\n","\n","        from transformers import SwinModel\n","\n","        self.swin = SwinModel.from_pretrained(\n","            CASME2_SWIN_CONFIG['swin_model'],\n","            add_pooling_layer=False\n","        )\n","\n","        # Enable fine-tuning\n","        for param in self.swin.parameters():\n","            param.requires_grad = True\n","\n","        # Get Swin feature dimensions from hierarchical structure\n","        # Swin uses embed_dim with hierarchical scaling: embed_dim * (2^(num_stages-1))\n","        base_embed_dim = self.swin.config.embed_dim\n","        num_stages = len(self.swin.config.depths)\n","        self.swin_feature_dim = base_embed_dim * (2 ** (num_stages - 1))\n","\n","        print(f\"Swin feature dimension (final stage): {self.swin_feature_dim}\")\n","        print(f\"Base embed_dim: {base_embed_dim}, Stages: {num_stages}\")\n","\n","        # Verify expected dimensions\n","        if self.swin_feature_dim != CASME2_SWIN_CONFIG['expected_hidden_dim']:\n","            print(f\"Warning: Expected {CASME2_SWIN_CONFIG['expected_hidden_dim']}, got {self.swin_feature_dim}\")\n","\n","        # Global Average Pooling for hierarchical features\n","        self.global_pool = nn.AdaptiveAvgPool1d(1)\n","\n","        # Simplified classification head for small dataset (201 samples)\n","        # Mirroring ViT success: Reduced complexity 1024 -> 256 -> 7 (instead of 1024 -> 512 -> 128 -> 7)\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.swin_feature_dim, 256),\n","            nn.LayerNorm(256),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","        )\n","\n","        self.classifier = nn.Linear(256, num_classes)\n","\n","        print(f\"Swin CASME II Simplified: {self.swin_feature_dim} -> GAP -> 256 -> {num_classes}\")\n","        print(f\"Dropout rate: {dropout_rate} (increased for small dataset regularization)\")\n","\n","    def forward(self, pixel_values):\n","        # Swin forward pass with hierarchical feature extraction\n","        swin_outputs = self.swin(\n","            pixel_values=pixel_values,\n","            interpolate_pos_encoding=CASME2_SWIN_CONFIG['interpolate_pos_encoding']\n","        )\n","\n","        # Extract hierarchical features from last layer\n","        hierarchical_features = swin_outputs.last_hidden_state\n","\n","        # Global Average Pooling across spatial dimensions\n","        pooled_features = self.global_pool(hierarchical_features.transpose(1, 2)).squeeze(-1)\n","\n","        # Classification pipeline\n","        processed_features = self.classifier_layers(pooled_features)\n","        output = self.classifier(processed_features)\n","\n","        return output\n","\n","# Optimizer and scheduler factory\n","def create_optimizer_scheduler_casme2(model, config):\n","    \"\"\"Create optimizer and scheduler for CASME II Swin Transformer training\"\"\"\n","\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=config['learning_rate'],\n","        weight_decay=config['weight_decay'],\n","        betas=(0.9, 0.999)\n","    )\n","\n","    if config['scheduler_type'] == 'plateau':\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer,\n","            mode=config['scheduler_mode'],\n","            factor=config['scheduler_factor'],\n","            patience=config['scheduler_patience'],\n","            min_lr=config['scheduler_min_lr']\n","        )\n","        print(f\"Scheduler: ReduceLROnPlateau monitoring {config['scheduler_monitor']}\")\n","    else:\n","        scheduler = None\n","\n","    return optimizer, scheduler\n","\n","# Swin Transformer Image Processor setup for 224px input\n","from transformers import AutoImageProcessor\n","\n","print(\"\\nSetting up Swin Transformer Image Processor for 224px input...\")\n","\n","swin_processor = AutoImageProcessor.from_pretrained(\n","    CASME2_SWIN_CONFIG['swin_model'],\n","    do_resize=True,\n","    size={'height': 224, 'width': 224},\n","    do_normalize=True,\n","    do_rescale=True,\n","    do_center_crop=False\n",")\n","\n","# Transform functions for Swin Transformer with grayscale to RGB conversion\n","def swint_transform_train(image):\n","    \"\"\"\n","    Training transform with Swin Transformer Image Processor\n","    Handles grayscale to RGB conversion via channel repetition\n","    \"\"\"\n","    # Ensure RGB format (grayscale images will be converted via channel repetition)\n","    if image.mode != 'RGB':\n","        image = image.convert('RGB')\n","\n","    inputs = swin_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","def swint_transform_val(image):\n","    \"\"\"\n","    Validation transform with Swin Transformer Image Processor\n","    Handles grayscale to RGB conversion via channel repetition\n","    \"\"\"\n","    # Ensure RGB format (grayscale images will be converted via channel repetition)\n","    if image.mode != 'RGB':\n","        image = image.convert('RGB')\n","\n","    inputs = swin_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","print(\"Swin Transformer Image Processor configured for 224px\")\n","print(\"Grayscale images will be converted to RGB via channel repetition\")\n","\n","# Custom Dataset class for CASME II v7\n","class CASME2Dataset(Dataset):\n","    \"\"\"Custom dataset class for CASME II v7 with flat file structure\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.images = []\n","        self.labels = []\n","        self.emotions = []\n","\n","        split_dir = os.path.join(dataset_root, split)\n","\n","        if not os.path.exists(split_dir):\n","            raise ValueError(f\"Split directory not found: {split_dir}\")\n","\n","        # Load all images from flat directory structure\n","        # Filename pattern: sub01_EP02_01f_happiness.jpg\n","        for img_file in os.listdir(split_dir):\n","            if img_file.endswith(('.jpg', '.jpeg', '.png')):\n","                # Extract emotion from filename (last part before extension)\n","                emotion = img_file.rsplit('_', 1)[-1].split('.')[0]\n","\n","                if emotion in CASME2_CLASSES:\n","                    self.images.append(os.path.join(split_dir, img_file))\n","                    self.labels.append(CLASS_TO_IDX[emotion])\n","                    self.emotions.append(emotion)\n","                else:\n","                    print(f\"Warning: Unknown emotion '{emotion}' in file {img_file}\")\n","\n","        print(f\"Loaded {len(self.images)} samples for {split} split\")\n","        self._print_distribution()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.images[idx]\n","\n","        try:\n","            # Load image (will be grayscale from v7 preprocessing)\n","            image = Image.open(image_path)\n","\n","            # Verify size is 224x224\n","            if image.size != (224, 224):\n","                print(f\"Warning: Image {os.path.basename(image_path)} has unexpected size {image.size}\")\n","                image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","        except Exception as e:\n","            print(f\"Error loading {image_path}: {e}\")\n","            # Fallback to gray dummy image\n","            image = Image.new('L', (224, 224), 128)\n","\n","        # Apply transform (will handle grayscale to RGB conversion)\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        label = self.labels[idx]\n","        sample_id = os.path.basename(self.images[idx])\n","\n","        return image, label, sample_id\n","\n","# Create directories\n","os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/training_logs\", exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/evaluation_results\", exist_ok=True)\n","\n","# Dataset paths - v7 structure\n","TRAIN_PATH = f\"{DATASET_ROOT}/train\"\n","VAL_PATH = f\"{DATASET_ROOT}/val\"\n","TEST_PATH = f\"{DATASET_ROOT}/test\"\n","\n","print(f\"\\nDataset paths:\")\n","print(f\"Train: {TRAIN_PATH}\")\n","print(f\"Validation: {VAL_PATH}\")\n","print(f\"Test: {TEST_PATH}\")\n","\n","# Architecture validation\n","print(\"\\nSwin CASME II architecture validation...\")\n","\n","try:\n","    test_model = SwinCASME2Baseline(num_classes=7, dropout_rate=0.3).to(device)\n","    test_input = torch.randn(1, 3, 224, 224).to(device)\n","    test_output = test_model(test_input)\n","\n","    # Calculate expected hierarchical patches\n","    stage1_patches = (224 // CASME2_SWIN_CONFIG['patch_size']) ** 2\n","\n","    print(f\"Validation successful: Output shape {test_output.shape}\")\n","    print(f\"Initial patches (Stage 1): {stage1_patches}\")\n","    print(f\"Window size: {CASME2_SWIN_CONFIG['window_size']}x{CASME2_SWIN_CONFIG['window_size']}\")\n","    print(f\"Hierarchical processing: Multi-stage feature extraction with shifted windows\")\n","    print(f\"Simplified architecture: 1024 -> 256 -> 7 (reduced parameters for small dataset)\")\n","\n","    del test_model, test_input, test_output\n","    torch.cuda.empty_cache()\n","\n","except Exception as e:\n","    print(f\"Validation failed: {e}\")\n","\n","# Loss function factory\n","def create_criterion_casme2(weights, use_focal_loss=False, alpha_weights=None, gamma=2.0):\n","    \"\"\"Factory function to create loss criterion\"\"\"\n","    if use_focal_loss:\n","        print(f\"Using Optimized Focal Loss with gamma={gamma}\")\n","        if alpha_weights:\n","            print(f\"Per-class alpha weights: {alpha_weights}\")\n","            print(f\"Alpha sum: {sum(alpha_weights):.3f}\")\n","        return OptimizedFocalLoss(alpha=alpha_weights, gamma=gamma)\n","    else:\n","        print(f\"Using CrossEntropy Loss with optimized class weights\")\n","        print(f\"Class weights: {weights.cpu().numpy()}\")\n","        return nn.CrossEntropyLoss(weight=weights)\n","\n","# Global configuration\n","GLOBAL_CONFIG_CASME2 = {\n","    'device': device,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'num_classes': 7,\n","    'class_weights': class_weights,\n","    'class_names': CASME2_CLASSES,\n","    'class_to_idx': CLASS_TO_IDX,\n","    'transform_train': swint_transform_train,\n","    'transform_val': swint_transform_val,\n","    'swin_config': CASME2_SWIN_CONFIG,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'results_root': RESULTS_ROOT,\n","    'train_path': TRAIN_PATH,\n","    'val_path': VAL_PATH,\n","    'test_path': TEST_PATH,\n","    'preprocessing_info': preprocessing_info,\n","    'optimizer_scheduler_factory': create_optimizer_scheduler_casme2,\n","    'criterion_factory': create_criterion_casme2\n","}\n","\n","# Configuration validation and summary\n","print(\"\\n\" + \"=\" * 60)\n","print(\"CASME II APEX FRAME SWIN TRANSFORMER CONFIGURATION COMPLETE\")\n","print(\"=\" * 60)\n","\n","print(f\"Loss Configuration:\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Function: Optimized Focal Loss\")\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Per-class Alpha: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Function: CrossEntropy with Optimized Weights\")\n","    print(f\"  Class Weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","\n","print(f\"\\nModel Configuration:\")\n","print(f\"  Architecture: {SWIN_MODEL_NAME}\")\n","print(f\"  Variant: {SWIN_MODEL_VARIANT}\")\n","print(f\"  Window Size: {WINDOW_SIZE}x{WINDOW_SIZE}\")\n","print(f\"  Patch Size: {PATCH_SIZE}x{PATCH_SIZE}\")\n","print(f\"  Input Resolution: 224px (native Swin model)\")\n","print(f\"  Hidden Dimension: {EXPECTED_HIDDEN_DIM}\")\n","print(f\"  Classification Head: Simplified 1024->256->7\")\n","print(f\"  Dropout: {CASME2_SWIN_CONFIG['dropout_rate']} (increased regularization)\")\n","\n","print(f\"\\nDataset Configuration:\")\n","print(f\"  Version: {CASME2_SWIN_CONFIG['dataset_version']}\")\n","print(f\"  Frame strategy: {CASME2_SWIN_CONFIG['frame_strategy']}\")\n","print(f\"  Preprocessing: {CASME2_SWIN_CONFIG['preprocessing_method']}\")\n","print(f\"  Classes: {len(CASME2_CLASSES)}\")\n","print(f\"  Train samples: {preprocessing_info['splits']['train']['total_images']}\")\n","print(f\"  Val samples: {preprocessing_info['splits']['val']['total_images']}\")\n","print(f\"  Test samples: {preprocessing_info['splits']['test']['total_images']}\")\n","print(f\"  Face detection rate: {CASME2_SWIN_CONFIG['face_detection_rate']:.2%}\")\n","\n","print(f\"\\nTraining Configuration (Optimized for Small Dataset):\")\n","print(f\"  Batch size: {BATCH_SIZE} (optimal for 201 samples)\")\n","print(f\"  Learning rate: {CASME2_SWIN_CONFIG['learning_rate']} (adjusted for small batches)\")\n","print(f\"  Iterations/epoch: ~{201 // BATCH_SIZE} (frequent updates)\")\n","print(f\"  Strategy: Simplified architecture + stronger regularization\")\n","print(f\"  Config source: Proven from ViT v7 AF experiments\")\n","\n","print(\"\\nNext: Cell 2 - Dataset Loading and Training Pipeline\")"]},{"cell_type":"code","source":["# @title Cell 2: CASME II Apex Frame Swin Transformer Training Pipeline\n","\n","# File: 05_02_SwinT_CASME2_AF_Cell2.py\n","# Location: experiments/05_02_SwinT_CASME2-AF-PREP.ipynb\n","# Purpose: Training pipeline for CASME II Apex Frame Swin Transformer with face-aware preprocessing\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing as mp\n","import shutil\n","import tempfile\n","\n","print(\"CASME II Apex Frame Swin Transformer Training Pipeline\")\n","print(\"=\" * 70)\n","print(f\"Loss Function: {'Optimized Focal Loss' if CASME2_SWIN_CONFIG['use_focal_loss'] else 'CrossEntropy Loss'}\")\n","if CASME2_SWIN_CONFIG['use_focal_loss']:\n","    print(f\"Focal Loss Parameters:\")\n","    print(f\"  Gamma: {CASME2_SWIN_CONFIG['focal_loss_gamma']}\")\n","    print(f\"  Per-class Alpha: {CASME2_SWIN_CONFIG['focal_loss_alpha_weights']}\")\n","    print(f\"  Alpha Sum: {sum(CASME2_SWIN_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","else:\n","    print(f\"CrossEntropy Parameters:\")\n","    print(f\"  Optimized Class Weights: {CASME2_SWIN_CONFIG['crossentropy_class_weights']}\")\n","print(f\"Dataset Version: {CASME2_SWIN_CONFIG['dataset_version']}\")\n","print(f\"Frame Strategy: {CASME2_SWIN_CONFIG['frame_strategy']}\")\n","print(f\"Preprocessing: {CASME2_SWIN_CONFIG['preprocessing_method']}\")\n","print(f\"Training epochs: {CASME2_SWIN_CONFIG['num_epochs']}\")\n","print(f\"Scheduler patience: {CASME2_SWIN_CONFIG['scheduler_patience']}\")\n","\n","# Enhanced CASME II Dataset with RAM caching\n","class CASME2DatasetTraining(Dataset):\n","    \"\"\"Enhanced CASME II dataset for training with RAM caching optimization\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.emotions = []\n","        self.cached_images = []\n","\n","        split_dir = os.path.join(dataset_root, split)\n","\n","        if not os.path.exists(split_dir):\n","            raise ValueError(f\"Split directory not found: {split_dir}\")\n","\n","        print(f\"Loading CASME II {split} dataset for training...\")\n","\n","        # Load all images from flat directory structure\n","        # Filename pattern: sub01_EP02_01f_happiness.jpg\n","        for img_file in os.listdir(split_dir):\n","            if img_file.endswith(('.jpg', '.jpeg', '.png')):\n","                # Extract emotion from filename\n","                emotion = img_file.rsplit('_', 1)[-1].split('.')[0]\n","\n","                if emotion in CASME2_CLASSES:\n","                    self.images.append(os.path.join(split_dir, img_file))\n","                    self.labels.append(CLASS_TO_IDX[emotion])\n","                    self.emotions.append(emotion)\n","                else:\n","                    print(f\"Warning: Unknown emotion '{emotion}' in file {img_file}\")\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples\")\n","        self._print_distribution()\n","\n","        # RAM caching for training efficiency\n","        if self.use_ram_cache:\n","            self._preload_to_ram()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def _preload_to_ram(self):\n","        \"\"\"RAM preloading with parallel loading for training efficiency\"\"\"\n","        print(f\"Preloading {len(self.images)} {self.split} images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            \"\"\"Load single image with error handling\"\"\"\n","            try:\n","                # Load image (grayscale from v7 preprocessing)\n","                image = Image.open(img_path)\n","\n","                # Verify size is 224x224\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('L', (224, 224), 128), False\n","\n","        # Parallel loading with ThreadPoolExecutor\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=f\"Loading {self.split} to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_mb = len(self.cached_images) * 224 * 224 * 1 / 1e6\n","        print(f\"{self.split.upper()} RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_mb:.1f}MB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx])\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('L', (224, 224), 128)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], os.path.basename(self.images[idx])\n","\n","# Metrics calculation with comprehensive error handling\n","def calculate_metrics_safe_robust(outputs, labels, class_names, average='macro'):\n","    \"\"\"Calculate metrics with enhanced error handling and validation\"\"\"\n","    try:\n","        if outputs.size(0) != labels.size(0):\n","            raise ValueError(f\"Batch size mismatch: outputs {outputs.size(0)} vs labels {labels.size(0)}\")\n","\n","        if isinstance(outputs, torch.Tensor):\n","            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n","        else:\n","            predictions = np.array(outputs)\n","\n","        if isinstance(labels, torch.Tensor):\n","            labels = labels.cpu().numpy()\n","        else:\n","            labels = np.array(labels)\n","\n","        accuracy = accuracy_score(labels, predictions)\n","        precision, recall, f1, _ = precision_recall_fscore_support(\n","            labels, predictions,\n","            average=average,\n","            zero_division=0,\n","            labels=list(range(len(class_names)))\n","        )\n","\n","        return {\n","            'accuracy': float(accuracy),\n","            'precision': float(precision),\n","            'recall': float(recall),\n","            'f1_score': float(f1)\n","        }\n","    except Exception as e:\n","        print(f\"Warning: Metrics calculation error: {e}\")\n","        return {\n","            'accuracy': 0.0,\n","            'precision': 0.0,\n","            'recall': 0.0,\n","            'f1_score': 0.0\n","        }\n","\n","# Training epoch function\n","def train_epoch(model, dataloader, criterion, optimizer, device, epoch, total_epochs):\n","    \"\"\"Training epoch with robust error handling\"\"\"\n","    model.train()\n","    running_loss = 0.0\n","    all_outputs = []\n","    all_labels = []\n","\n","    progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch+1}/{total_epochs}\")\n","\n","    for batch_idx, (images, labels, sample_ids) in enumerate(progress_bar):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Model forward pass\n","        model_output = model(images)\n","\n","        # Robust output extraction\n","        if isinstance(model_output, (tuple, list)):\n","            outputs = model_output[0]\n","        elif isinstance(model_output, dict):\n","            outputs = model_output.get('logits', model_output.get('prediction', model_output))\n","        else:\n","            outputs = model_output\n","\n","        # Validate output shape for 7 CASME II classes\n","        if outputs.dim() != 2 or outputs.size(1) != 7:\n","            raise ValueError(f\"Invalid output shape: {outputs.shape}, expected [batch_size, 7]\")\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), CASME2_SWIN_CONFIG['gradient_clip'])\n","\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        # Memory optimized: Move to CPU before accumulating\n","        all_outputs.append(outputs.detach().cpu())\n","        all_labels.append(labels.detach().cpu())\n","\n","        # Update progress\n","        if batch_idx % 5 == 0:\n","            avg_loss = running_loss / (batch_idx + 1)\n","            current_lr = optimizer.param_groups[0]['lr']\n","            progress_bar.set_postfix({\n","                'Loss': f'{avg_loss:.4f}',\n","                'LR': f'{current_lr:.2e}'\n","            })\n","\n","    # Metrics calculation\n","    try:\n","        epoch_outputs = torch.cat(all_outputs, dim=0)\n","        epoch_labels = torch.cat(all_labels, dim=0)\n","        metrics = calculate_metrics_safe_robust(epoch_outputs, epoch_labels, CASME2_CLASSES, average='macro')\n","    except Exception as e:\n","        print(f\"Warning: Training metrics calculation failed: {e}\")\n","        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n","\n","    avg_loss = running_loss / len(dataloader)\n","    return avg_loss, metrics\n","\n","# Validation epoch function\n","def validate_epoch(model, dataloader, criterion, device, epoch, total_epochs):\n","    \"\"\"Validation epoch with robust error handling\"\"\"\n","    model.eval()\n","    running_loss = 0.0\n","    all_outputs = []\n","    all_labels = []\n","    all_sample_ids = []\n","\n","    with torch.no_grad():\n","        progress_bar = tqdm(dataloader, desc=f\"Validation Epoch {epoch+1}/{total_epochs}\")\n","\n","        for batch_idx, (images, labels, sample_ids) in enumerate(progress_bar):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            # Model forward pass\n","            model_output = model(images)\n","\n","            # Robust output extraction\n","            if isinstance(model_output, (tuple, list)):\n","                outputs = model_output[0]\n","            elif isinstance(model_output, dict):\n","                outputs = model_output.get('logits', model_output.get('prediction', model_output))\n","            else:\n","                outputs = model_output\n","\n","            loss = criterion(outputs, labels)\n","            running_loss += loss.item()\n","\n","            # Memory optimized: Move to CPU before accumulating\n","            all_outputs.append(outputs.detach().cpu())\n","            all_labels.append(labels.detach().cpu())\n","            all_sample_ids.extend(sample_ids)\n","\n","            if batch_idx % 3 == 0:\n","                avg_loss = running_loss / (batch_idx + 1)\n","                progress_bar.set_postfix({'Val Loss': f'{avg_loss:.4f}'})\n","\n","    # Metrics calculation\n","    try:\n","        epoch_outputs = torch.cat(all_outputs, dim=0)\n","        epoch_labels = torch.cat(all_labels, dim=0)\n","        metrics = calculate_metrics_safe_robust(epoch_outputs, epoch_labels, CASME2_CLASSES, average='macro')\n","    except Exception as e:\n","        print(f\"Warning: Validation metrics calculation failed: {e}\")\n","        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n","\n","    avg_loss = running_loss / len(dataloader)\n","    return avg_loss, metrics, all_sample_ids\n","\n","# Hardened checkpoint saving function\n","def save_checkpoint_robust(model, optimizer, scheduler, epoch, train_metrics, val_metrics,\n","                         checkpoint_dir, best_metrics, config, max_retries=3):\n","    \"\"\"Hardened checkpoint saving with atomic write and validation\"\"\"\n","\n","    def make_serializable_cpu(obj):\n","        if isinstance(obj, torch.Tensor):\n","            cpu_obj = obj.detach().cpu()\n","            return cpu_obj.item() if cpu_obj.numel() == 1 else cpu_obj.tolist()\n","        elif isinstance(obj, np.ndarray):\n","            return obj.tolist()\n","        elif isinstance(obj, np.integer):\n","            return int(obj)\n","        elif isinstance(obj, np.floating):\n","            return float(obj)\n","        elif isinstance(obj, dict):\n","            return {k: make_serializable_cpu(v) for k, v in obj.items()}\n","        elif isinstance(obj, (list, tuple)):\n","            return [make_serializable_cpu(item) for item in obj]\n","        else:\n","            return obj\n","\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","        'train_metrics': make_serializable_cpu(train_metrics),\n","        'val_metrics': make_serializable_cpu(val_metrics),\n","        'casme2_config': make_serializable_cpu(config),\n","        'best_f1': float(best_metrics['f1']),\n","        'best_loss': float(best_metrics['loss']),\n","        'best_acc': float(best_metrics['accuracy']),\n","        'class_names': CASME2_CLASSES,\n","        'num_classes': 7\n","    }\n","\n","    final_path = f\"{checkpoint_dir}/casme2_swint_apex_frame_best_f1.pth\"\n","\n","    for attempt in range(max_retries):\n","        try:\n","            # Save to temporary file\n","            temp_fd, temp_path = tempfile.mkstemp(dir=checkpoint_dir, suffix='.pth.tmp')\n","            os.close(temp_fd)\n","\n","            print(f\"Attempt {attempt + 1}: Saving checkpoint to temporary file...\")\n","            torch.save(checkpoint, temp_path)\n","\n","            # Validate checkpoint\n","            print(\"Validating checkpoint integrity...\")\n","            validation_checkpoint = torch.load(temp_path, map_location='cpu')\n","\n","            required_keys = ['model_state_dict', 'epoch', 'best_f1', 'num_classes']\n","            for key in required_keys:\n","                if key not in validation_checkpoint:\n","                    raise ValueError(f\"Checkpoint validation failed: missing key '{key}'\")\n","\n","            if validation_checkpoint['epoch'] != epoch:\n","                raise ValueError(f\"Checkpoint epoch mismatch: saved {epoch}, loaded {validation_checkpoint['epoch']}\")\n","\n","            print(\"Checkpoint validation passed\")\n","\n","            # Atomic rename\n","            print(f\"Moving validated checkpoint to final location...\")\n","            shutil.move(temp_path, final_path)\n","\n","            print(f\"Checkpoint saved and validated successfully: {os.path.basename(final_path)}\")\n","            print(f\"  Epoch: {epoch + 1}\")\n","            print(f\"  Val F1: {best_metrics['f1']:.4f}\")\n","            print(f\"  Val Loss: {best_metrics['loss']:.4f}\")\n","            print(f\"  Val Acc: {best_metrics['accuracy']:.4f}\")\n","\n","            return final_path\n","\n","        except Exception as e:\n","            print(f\"Checkpoint save attempt {attempt + 1}/{max_retries} failed: {e}\")\n","\n","            if os.path.exists(temp_path):\n","                try:\n","                    os.remove(temp_path)\n","                except:\n","                    pass\n","\n","            if attempt < max_retries - 1:\n","                wait_time = 2 ** attempt\n","                print(f\"Retrying in {wait_time} seconds...\")\n","                time.sleep(wait_time)\n","            else:\n","                print(f\"All {max_retries} checkpoint save attempts failed\")\n","                return None\n","\n","    return None\n","\n","# Safe JSON serialization\n","def safe_json_serialize(obj):\n","    \"\"\"Convert objects to JSON-serializable format\"\"\"\n","    if isinstance(obj, torch.Tensor):\n","        return obj.cpu().item() if obj.numel() == 1 else obj.cpu().numpy().tolist()\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    elif isinstance(obj, (np.integer, np.int32, np.int64)):\n","        return int(obj)\n","    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n","        return float(obj)\n","    elif isinstance(obj, dict):\n","        return {k: safe_json_serialize(v) for k, v in obj.items()}\n","    elif isinstance(obj, (list, tuple)):\n","        return [safe_json_serialize(item) for item in obj]\n","    elif hasattr(obj, '__dict__'):\n","        return safe_json_serialize(obj.__dict__)\n","    else:\n","        try:\n","            return float(obj) if isinstance(obj, (int, float)) else str(obj)\n","        except:\n","            return str(obj)\n","\n","# Create training datasets\n","print(\"\\nCreating CASME II Apex Frame Swin Transformer training datasets...\")\n","\n","train_dataset = CASME2DatasetTraining(\n","    dataset_root=DATASET_ROOT,\n","    split='train',\n","    transform=GLOBAL_CONFIG_CASME2['transform_train'],\n","    use_ram_cache=True\n",")\n","\n","val_dataset = CASME2DatasetTraining(\n","    dataset_root=DATASET_ROOT,\n","    split='val',\n","    transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","    use_ram_cache=True\n",")\n","\n","# Create data loaders\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=CASME2_SWIN_CONFIG['batch_size'],\n","    shuffle=True,\n","    num_workers=CASME2_SWIN_CONFIG['num_workers'],\n","    pin_memory=True,\n","    prefetch_factor=2\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=CASME2_SWIN_CONFIG['batch_size'],\n","    shuffle=False,\n","    num_workers=CASME2_SWIN_CONFIG['num_workers'],\n","    pin_memory=True,\n","    prefetch_factor=2\n",")\n","\n","print(f\"Training batches: {len(train_loader)} (samples: {len(train_dataset)})\")\n","print(f\"Validation batches: {len(val_loader)} (samples: {len(val_dataset)})\")\n","\n","# Initialize model, criterion, optimizer, scheduler\n","print(\"\\nInitializing CASME II Apex Frame Swin Transformer model...\")\n","model = SwinCASME2Baseline(\n","    num_classes=GLOBAL_CONFIG_CASME2['num_classes'],\n","    dropout_rate=CASME2_SWIN_CONFIG['dropout_rate']\n",").to(GLOBAL_CONFIG_CASME2['device'])\n","\n","# Create criterion\n","if CASME2_SWIN_CONFIG['use_focal_loss']:\n","    criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","        weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","        use_focal_loss=True,\n","        alpha_weights=CASME2_SWIN_CONFIG['focal_loss_alpha_weights'],\n","        gamma=CASME2_SWIN_CONFIG['focal_loss_gamma']\n","    )\n","else:\n","    criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","        weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","        use_focal_loss=False,\n","        alpha_weights=None,\n","        gamma=2.0\n","    )\n","\n","optimizer, scheduler = GLOBAL_CONFIG_CASME2['optimizer_scheduler_factory'](\n","    model, CASME2_SWIN_CONFIG\n",")\n","\n","print(f\"Optimizer: AdamW (LR={CASME2_SWIN_CONFIG['learning_rate']})\")\n","print(f\"Scheduler: ReduceLROnPlateau (patience={CASME2_SWIN_CONFIG['scheduler_patience']})\")\n","print(f\"Criterion: {'Optimized Focal Loss' if CASME2_SWIN_CONFIG['use_focal_loss'] else 'CrossEntropy'}\")\n","\n","# Training history tracking\n","training_history = {\n","    'train_loss': [],\n","    'val_loss': [],\n","    'train_f1': [],\n","    'val_f1': [],\n","    'train_acc': [],\n","    'val_acc': [],\n","    'learning_rate': [],\n","    'epoch_time': []\n","}\n","\n","# Best metrics tracking\n","best_metrics = {\n","    'f1': 0.0,\n","    'loss': float('inf'),\n","    'accuracy': 0.0,\n","    'epoch': 0\n","}\n","\n","print(\"\\nStarting CASME II Apex Frame Swin Transformer training...\")\n","print(f\"Training configuration: {CASME2_SWIN_CONFIG['num_epochs']} epochs\")\n","print(f\"Small dataset size: {len(train_dataset)} train samples (overfitting risk)\")\n","print(\"=\" * 70)\n","\n","# Main training loop\n","start_time = time.time()\n","\n","for epoch in range(CASME2_SWIN_CONFIG['num_epochs']):\n","    epoch_start_time = time.time()\n","    print(f\"\\nEpoch {epoch+1}/{CASME2_SWIN_CONFIG['num_epochs']}\")\n","\n","    # Training phase\n","    train_loss, train_metrics = train_epoch(\n","        model, train_loader, criterion, optimizer,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_SWIN_CONFIG['num_epochs']\n","    )\n","\n","    # Validation phase\n","    val_loss, val_metrics, val_sample_ids = validate_epoch(\n","        model, val_loader, criterion,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_SWIN_CONFIG['num_epochs']\n","    )\n","\n","    # Update scheduler\n","    if scheduler:\n","        scheduler.step(val_metrics['f1_score'])\n","\n","    # Record training history\n","    epoch_time = time.time() - epoch_start_time\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    training_history['train_loss'].append(float(train_loss))\n","    training_history['val_loss'].append(float(val_loss))\n","    training_history['train_f1'].append(float(train_metrics['f1_score']))\n","    training_history['val_f1'].append(float(val_metrics['f1_score']))\n","    training_history['train_acc'].append(float(train_metrics['accuracy']))\n","    training_history['val_acc'].append(float(val_metrics['accuracy']))\n","    training_history['learning_rate'].append(float(current_lr))\n","    training_history['epoch_time'].append(float(epoch_time))\n","\n","    # Print epoch summary\n","    print(f\"Train - Loss: {train_loss:.4f}, F1: {train_metrics['f1_score']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n","    print(f\"Val   - Loss: {val_loss:.4f}, F1: {val_metrics['f1_score']:.4f}, Acc: {val_metrics['accuracy']:.4f}\")\n","    print(f\"Time  - Epoch: {epoch_time:.1f}s, LR: {current_lr:.2e}\")\n","\n","    # Multi-criteria checkpoint saving\n","    save_model = False\n","    improvement_reason = \"\"\n","\n","    if val_metrics['f1_score'] > best_metrics['f1']:\n","        save_model = True\n","        improvement_reason = \"Higher F1\"\n","    elif val_metrics['f1_score'] == best_metrics['f1']:\n","        if val_loss < best_metrics['loss']:\n","            save_model = True\n","            improvement_reason = \"Same F1, Lower Loss\"\n","        elif val_loss == best_metrics['loss'] and val_metrics['accuracy'] > best_metrics['accuracy']:\n","            save_model = True\n","            improvement_reason = \"Same F1&Loss, Higher Accuracy\"\n","\n","    if save_model:\n","        best_metrics['f1'] = val_metrics['f1_score']\n","        best_metrics['loss'] = val_loss\n","        best_metrics['accuracy'] = val_metrics['accuracy']\n","        best_metrics['epoch'] = epoch + 1\n","\n","        best_model_path = save_checkpoint_robust(\n","            model, optimizer, scheduler, epoch,\n","            train_metrics, val_metrics, GLOBAL_CONFIG_CASME2['checkpoint_root'],\n","            best_metrics, CASME2_SWIN_CONFIG\n","        )\n","\n","        if best_model_path:\n","            print(f\"New best model: {improvement_reason} - F1: {best_metrics['f1']:.4f}\")\n","        else:\n","            print(f\"Warning: Failed to save checkpoint despite improvement\")\n","\n","    # Progress tracking\n","    elapsed_time = time.time() - start_time\n","    estimated_total = (elapsed_time / (epoch + 1)) * CASME2_SWIN_CONFIG['num_epochs']\n","    remaining_time = estimated_total - elapsed_time\n","    progress_pct = ((epoch + 1) / CASME2_SWIN_CONFIG['num_epochs']) * 100\n","\n","    print(f\"Progress: {progress_pct:.1f}% | Best F1: {best_metrics['f1']:.4f} | ETA: {remaining_time/60:.1f}min\")\n","\n","# Training completion\n","total_time = time.time() - start_time\n","actual_epochs = CASME2_SWIN_CONFIG['num_epochs']\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II APEX FRAME SWIN TRANSFORMER TRAINING COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Training time: {total_time/60:.1f} minutes\")\n","print(f\"Epochs completed: {actual_epochs}\")\n","print(f\"Best validation F1: {best_metrics['f1']:.4f} (epoch {best_metrics['epoch']})\")\n","print(f\"Final train F1: {training_history['train_f1'][-1]:.4f}\")\n","print(f\"Final validation F1: {training_history['val_f1'][-1]:.4f}\")\n","\n","# Export training documentation\n","results_dir = GLOBAL_CONFIG_CASME2['results_root']\n","os.makedirs(f\"{results_dir}/training_logs\", exist_ok=True)\n","\n","training_history_path = f\"{results_dir}/training_logs/casme2_swint_apex_frame_training_history.json\"\n","\n","print(\"\\nExporting training documentation...\")\n","\n","try:\n","    training_summary = {\n","        'experiment_type': 'CASME2_Swin_Apex_Frame_Face_Aware',\n","        'experiment_configuration': {\n","            'dataset_version': CASME2_SWIN_CONFIG['dataset_version'],\n","            'frame_strategy': CASME2_SWIN_CONFIG['frame_strategy'],\n","            'preprocessing_method': CASME2_SWIN_CONFIG['preprocessing_method'],\n","            'image_format': CASME2_SWIN_CONFIG['image_format'],\n","            'bbox_expansion': CASME2_SWIN_CONFIG['bbox_expansion'],\n","            'loss_function': 'Optimized Focal Loss' if CASME2_SWIN_CONFIG['use_focal_loss'] else 'CrossEntropy',\n","            'weight_approach': 'Per-class Alpha (sum=1.0)' if CASME2_SWIN_CONFIG['use_focal_loss'] else 'Inverse Sqrt Frequency',\n","            'focal_loss_gamma': CASME2_SWIN_CONFIG['focal_loss_gamma'],\n","            'focal_loss_alpha_weights': CASME2_SWIN_CONFIG['focal_loss_alpha_weights'],\n","            'crossentropy_class_weights': CASME2_SWIN_CONFIG['crossentropy_class_weights'],\n","            'swin_model': CASME2_SWIN_CONFIG['swin_model'],\n","            'swin_variant': CASME2_SWIN_CONFIG['swin_variant'],\n","            'window_size': CASME2_SWIN_CONFIG['window_size'],\n","            'patch_size': CASME2_SWIN_CONFIG['patch_size']\n","        },\n","        'training_history': safe_json_serialize(training_history),\n","        'best_val_f1': float(best_metrics['f1']),\n","        'best_val_loss': float(best_metrics['loss']),\n","        'best_val_accuracy': float(best_metrics['accuracy']),\n","        'best_epoch': int(best_metrics['epoch']),\n","        'total_epochs': int(actual_epochs),\n","        'total_time_minutes': float(total_time / 60),\n","        'average_epoch_time_seconds': float(np.mean(training_history['epoch_time'])),\n","        'config': safe_json_serialize(CASME2_SWIN_CONFIG),\n","        'final_train_f1': float(training_history['train_f1'][-1]),\n","        'final_val_f1': float(training_history['val_f1'][-1]),\n","        'model_checkpoint': 'casme2_swint_apex_frame_best_f1.pth',\n","        'dataset_info': {\n","            'name': 'CASME_II',\n","            'version': CASME2_SWIN_CONFIG['dataset_version'],\n","            'frame_strategy': CASME2_SWIN_CONFIG['frame_strategy'],\n","            'preprocessing': CASME2_SWIN_CONFIG['preprocessing_method'],\n","            'train_samples': len(train_dataset),\n","            'val_samples': len(val_dataset),\n","            'num_classes': 7,\n","            'class_names': CASME2_CLASSES,\n","            'face_detection_rate': CASME2_SWIN_CONFIG['face_detection_rate']\n","        },\n","        'architecture_info': {\n","            'model_type': 'SwinCASME2Baseline',\n","            'backbone': CASME2_SWIN_CONFIG['swin_model'],\n","            'variant': CASME2_SWIN_CONFIG['swin_variant'],\n","            'input_size': f\"{CASME2_SWIN_CONFIG['input_size']}x{CASME2_SWIN_CONFIG['input_size']}\",\n","            'window_size': CASME2_SWIN_CONFIG['window_size'],\n","            'patch_size': CASME2_SWIN_CONFIG['patch_size'],\n","            'hidden_dimension': CASME2_SWIN_CONFIG['expected_hidden_dim'],\n","            'classification_head': '1024->GAP->256->7'\n","        }\n","    }\n","\n","    with open(training_history_path, 'w') as f:\n","        json.dump(training_summary, f, indent=2)\n","\n","    print(f\"Training documentation saved successfully: {training_history_path}\")\n","    print(f\"Loss function: {training_summary['experiment_configuration']['loss_function']}\")\n","    print(f\"Model variant: {CASME2_SWIN_CONFIG['swin_model']}\")\n","    print(f\"Dataset version: {CASME2_SWIN_CONFIG['dataset_version']}\")\n","\n","except Exception as e:\n","    print(f\"Warning: Could not save training documentation: {e}\")\n","    print(\"Training completed successfully, but documentation export failed\")\n","\n","# Memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\nNext: Cell 3 - CASME II Apex Frame Swin Transformer Evaluation\")\n","print(\"Training pipeline with hierarchical features completed successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","collapsed":true,"id":"fAjcDg7kTxSy","executionInfo":{"status":"ok","timestamp":1760944492377,"user_tz":-420,"elapsed":404492,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"8a10a8e8-a069-4294-d604-162d9b23fd81"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II Apex Frame Swin Transformer Training Pipeline\n","======================================================================\n","Loss Function: Optimized Focal Loss\n","Focal Loss Parameters:\n","  Gamma: 2.0\n","  Per-class Alpha: [0.052, 0.066, 0.093, 0.101, 0.104, 0.208, 0.376]\n","  Alpha Sum: 1.000\n","Dataset Version: v7\n","Frame Strategy: apex_frame\n","Preprocessing: face_aware_bbox_expansion\n","Training epochs: 50\n","Scheduler patience: 3\n","\n","Creating CASME II Apex Frame Swin Transformer training datasets...\n","Loading CASME II train dataset for training...\n","Loaded 201 CASME II train samples\n","  others: 79 samples (39.3%)\n","  disgust: 50 samples (24.9%)\n","  happiness: 25 samples (12.4%)\n","  repression: 21 samples (10.4%)\n","  surprise: 20 samples (10.0%)\n","  sadness: 5 samples (2.5%)\n","  fear: 1 samples (0.5%)\n","Preloading 201 train images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading train to RAM: 100%|██████████| 201/201 [00:02<00:00, 73.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TRAIN RAM caching completed: 201/201 images, ~10.1MB\n","Loading CASME II val dataset for training...\n","Loaded 26 CASME II val samples\n","  others: 10 samples (38.5%)\n","  disgust: 6 samples (23.1%)\n","  happiness: 3 samples (11.5%)\n","  repression: 3 samples (11.5%)\n","  surprise: 2 samples (7.7%)\n","  sadness: 1 samples (3.8%)\n","  fear: 1 samples (3.8%)\n","Preloading 26 val images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading val to RAM: 100%|██████████| 26/26 [00:00<00:00, 33.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["VAL RAM caching completed: 26/26 images, ~1.3MB\n","Training batches: 51 (samples: 201)\n","Validation batches: 7 (samples: 26)\n","\n","Initializing CASME II Apex Frame Swin Transformer model...\n","Swin feature dimension (final stage): 1024\n","Base embed_dim: 128, Stages: 4\n","Swin CASME II Simplified: 1024 -> GAP -> 256 -> 7\n","Dropout rate: 0.3 (increased for small dataset regularization)\n","Using Optimized Focal Loss with gamma=2.0\n","Per-class alpha weights: [0.052, 0.066, 0.093, 0.101, 0.104, 0.208, 0.376]\n","Alpha sum: 1.000\n","Scheduler: ReduceLROnPlateau monitoring val_f1_macro\n","Optimizer: AdamW (LR=2e-05)\n","Scheduler: ReduceLROnPlateau (patience=3)\n","Criterion: Optimized Focal Loss\n","\n","Starting CASME II Apex Frame Swin Transformer training...\n","Training configuration: 50 epochs\n","Small dataset size: 201 train samples (overfitting risk)\n","======================================================================\n","\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1/50: 100%|██████████| 51/51 [00:07<00:00,  6.55it/s, Loss=0.0994, LR=2.00e-05]\n","Validation Epoch 1/50: 100%|██████████| 7/7 [00:00<00:00, 16.05it/s, Val Loss=0.1234]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0994, F1: 0.1465, Acc: 0.3383\n","Val   - Loss: 0.1234, F1: 0.1652, Acc: 0.3462\n","Time  - Epoch: 8.2s, LR: 2.00e-05\n","Attempt 1: Saving checkpoint to temporary file...\n","Validating checkpoint integrity...\n","Checkpoint validation passed\n","Moving validated checkpoint to final location...\n","Checkpoint saved and validated successfully: casme2_swint_apex_frame_best_f1.pth\n","  Epoch: 1\n","  Val F1: 0.1652\n","  Val Loss: 0.1234\n","  Val Acc: 0.3462\n","New best model: Higher F1 - F1: 0.1652\n","Progress: 2.0% | Best F1: 0.1652 | ETA: 9.8min\n","\n","Epoch 2/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 2/50: 100%|██████████| 51/51 [00:06<00:00,  7.33it/s, Loss=0.0829, LR=2.00e-05]\n","Validation Epoch 2/50: 100%|██████████| 7/7 [00:00<00:00, 15.54it/s, Val Loss=0.1257]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0829, F1: 0.2387, Acc: 0.4328\n","Val   - Loss: 0.1257, F1: 0.1231, Acc: 0.2692\n","Time  - Epoch: 7.4s, LR: 2.00e-05\n","Progress: 4.0% | Best F1: 0.1652 | ETA: 7.8min\n","\n","Epoch 3/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 3/50: 100%|██████████| 51/51 [00:07<00:00,  7.05it/s, Loss=0.0642, LR=2.00e-05]\n","Validation Epoch 3/50: 100%|██████████| 7/7 [00:00<00:00, 15.77it/s, Val Loss=0.1348]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0642, F1: 0.3738, Acc: 0.5473\n","Val   - Loss: 0.1348, F1: 0.1698, Acc: 0.3077\n","Time  - Epoch: 7.7s, LR: 2.00e-05\n","Attempt 1: Saving checkpoint to temporary file...\n","Validating checkpoint integrity...\n","Checkpoint validation passed\n","Moving validated checkpoint to final location...\n","Checkpoint saved and validated successfully: casme2_swint_apex_frame_best_f1.pth\n","  Epoch: 3\n","  Val F1: 0.1698\n","  Val Loss: 0.1348\n","  Val Acc: 0.3077\n","New best model: Higher F1 - F1: 0.1698\n","Progress: 6.0% | Best F1: 0.1698 | ETA: 8.0min\n","\n","Epoch 4/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 4/50: 100%|██████████| 51/51 [00:07<00:00,  6.92it/s, Loss=0.0492, LR=2.00e-05]\n","Validation Epoch 4/50: 100%|██████████| 7/7 [00:00<00:00, 16.40it/s, Val Loss=0.1421]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0492, F1: 0.4660, Acc: 0.6617\n","Val   - Loss: 0.1421, F1: 0.1460, Acc: 0.2692\n","Time  - Epoch: 7.8s, LR: 2.00e-05\n","Progress: 8.0% | Best F1: 0.1698 | ETA: 7.4min\n","\n","Epoch 5/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 5/50: 100%|██████████| 51/51 [00:06<00:00,  7.33it/s, Loss=0.0351, LR=2.00e-05]\n","Validation Epoch 5/50: 100%|██████████| 7/7 [00:00<00:00, 15.81it/s, Val Loss=0.1446]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0351, F1: 0.6213, Acc: 0.7811\n","Val   - Loss: 0.1446, F1: 0.1701, Acc: 0.3462\n","Time  - Epoch: 7.4s, LR: 2.00e-05\n","Attempt 1: Saving checkpoint to temporary file...\n","Validating checkpoint integrity...\n","Checkpoint validation passed\n","Moving validated checkpoint to final location...\n","Checkpoint saved and validated successfully: casme2_swint_apex_frame_best_f1.pth\n","  Epoch: 5\n","  Val F1: 0.1701\n","  Val Loss: 0.1446\n","  Val Acc: 0.3462\n","New best model: Higher F1 - F1: 0.1701\n","Progress: 10.0% | Best F1: 0.1701 | ETA: 10.0min\n","\n","Epoch 6/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 6/50: 100%|██████████| 51/51 [00:07<00:00,  7.25it/s, Loss=0.0214, LR=2.00e-05]\n","Validation Epoch 6/50: 100%|██████████| 7/7 [00:00<00:00, 15.29it/s, Val Loss=0.1532]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0214, F1: 0.7476, Acc: 0.8756\n","Val   - Loss: 0.1532, F1: 0.1619, Acc: 0.3077\n","Time  - Epoch: 7.5s, LR: 2.00e-05\n","Progress: 12.0% | Best F1: 0.1701 | ETA: 9.1min\n","\n","Epoch 7/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 7/50: 100%|██████████| 51/51 [00:07<00:00,  7.24it/s, Loss=0.0153, LR=2.00e-05]\n","Validation Epoch 7/50: 100%|██████████| 7/7 [00:00<00:00, 16.04it/s, Val Loss=0.1485]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0153, F1: 0.7760, Acc: 0.9353\n","Val   - Loss: 0.1485, F1: 0.1541, Acc: 0.3077\n","Time  - Epoch: 7.5s, LR: 2.00e-05\n","Progress: 14.0% | Best F1: 0.1701 | ETA: 8.4min\n","\n","Epoch 8/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 8/50: 100%|██████████| 51/51 [00:07<00:00,  7.11it/s, Loss=0.0091, LR=2.00e-05]\n","Validation Epoch 8/50: 100%|██████████| 7/7 [00:00<00:00, 15.28it/s, Val Loss=0.1667]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0091, F1: 0.8036, Acc: 0.9502\n","Val   - Loss: 0.1667, F1: 0.1076, Acc: 0.2692\n","Time  - Epoch: 7.7s, LR: 2.00e-05\n","Progress: 16.0% | Best F1: 0.1701 | ETA: 7.8min\n","\n","Epoch 9/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 9/50: 100%|██████████| 51/51 [00:06<00:00,  7.42it/s, Loss=0.0072, LR=2.00e-05]\n","Validation Epoch 9/50: 100%|██████████| 7/7 [00:00<00:00, 16.67it/s, Val Loss=0.1719]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0072, F1: 0.8976, Acc: 0.9453\n","Val   - Loss: 0.1719, F1: 0.1505, Acc: 0.3077\n","Time  - Epoch: 7.3s, LR: 1.00e-05\n","Progress: 18.0% | Best F1: 0.1701 | ETA: 7.3min\n","\n","Epoch 10/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 10/50: 100%|██████████| 51/51 [00:06<00:00,  7.35it/s, Loss=0.0026, LR=1.00e-05]\n","Validation Epoch 10/50: 100%|██████████| 7/7 [00:00<00:00, 16.37it/s, Val Loss=0.1817]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0026, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1817, F1: 0.1505, Acc: 0.3077\n","Time  - Epoch: 7.4s, LR: 1.00e-05\n","Progress: 20.0% | Best F1: 0.1701 | ETA: 6.9min\n","\n","Epoch 11/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 11/50: 100%|██████████| 51/51 [00:06<00:00,  7.44it/s, Loss=0.0017, LR=1.00e-05]\n","Validation Epoch 11/50: 100%|██████████| 7/7 [00:00<00:00, 14.90it/s, Val Loss=0.1817]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0017, F1: 0.9958, Acc: 0.9950\n","Val   - Loss: 0.1817, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 1.00e-05\n","Progress: 22.0% | Best F1: 0.1701 | ETA: 6.6min\n","\n","Epoch 12/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 12/50: 100%|██████████| 51/51 [00:06<00:00,  7.43it/s, Loss=0.0011, LR=1.00e-05]\n","Validation Epoch 12/50: 100%|██████████| 7/7 [00:00<00:00, 16.72it/s, Val Loss=0.1896]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0011, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1896, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 1.00e-05\n","Progress: 24.0% | Best F1: 0.1701 | ETA: 6.3min\n","\n","Epoch 13/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 13/50: 100%|██████████| 51/51 [00:07<00:00,  7.22it/s, Loss=0.0008, LR=1.00e-05]\n","Validation Epoch 13/50: 100%|██████████| 7/7 [00:00<00:00, 16.55it/s, Val Loss=0.1902]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0008, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1902, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.5s, LR: 5.00e-06\n","Progress: 26.0% | Best F1: 0.1701 | ETA: 6.0min\n","\n","Epoch 14/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 14/50: 100%|██████████| 51/51 [00:07<00:00,  7.07it/s, Loss=0.0009, LR=5.00e-06]\n","Validation Epoch 14/50: 100%|██████████| 7/7 [00:00<00:00, 16.00it/s, Val Loss=0.1936]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1936, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.7s, LR: 5.00e-06\n","Progress: 28.0% | Best F1: 0.1701 | ETA: 5.7min\n","\n","Epoch 15/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 15/50: 100%|██████████| 51/51 [00:07<00:00,  7.26it/s, Loss=0.0009, LR=5.00e-06]\n","Validation Epoch 15/50: 100%|██████████| 7/7 [00:00<00:00, 15.86it/s, Val Loss=0.1956]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1956, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.5s, LR: 5.00e-06\n","Progress: 30.0% | Best F1: 0.1701 | ETA: 5.5min\n","\n","Epoch 16/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 16/50: 100%|██████████| 51/51 [00:07<00:00,  7.17it/s, Loss=0.0005, LR=5.00e-06]\n","Validation Epoch 16/50: 100%|██████████| 7/7 [00:00<00:00, 16.44it/s, Val Loss=0.1953]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1953, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.6s, LR: 5.00e-06\n","Progress: 32.0% | Best F1: 0.1701 | ETA: 5.3min\n","\n","Epoch 17/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 17/50: 100%|██████████| 51/51 [00:06<00:00,  7.40it/s, Loss=0.0006, LR=5.00e-06]\n","Validation Epoch 17/50: 100%|██████████| 7/7 [00:00<00:00, 17.14it/s, Val Loss=0.1959]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1959, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 2.50e-06\n","Progress: 34.0% | Best F1: 0.1701 | ETA: 5.1min\n","\n","Epoch 18/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 18/50: 100%|██████████| 51/51 [00:06<00:00,  7.44it/s, Loss=0.0004, LR=2.50e-06]\n","Validation Epoch 18/50: 100%|██████████| 7/7 [00:00<00:00, 17.30it/s, Val Loss=0.1973]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1973, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 2.50e-06\n","Progress: 36.0% | Best F1: 0.1701 | ETA: 4.8min\n","\n","Epoch 19/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 19/50: 100%|██████████| 51/51 [00:06<00:00,  7.46it/s, Loss=0.0004, LR=2.50e-06]\n","Validation Epoch 19/50: 100%|██████████| 7/7 [00:00<00:00, 16.71it/s, Val Loss=0.1985]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1985, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 2.50e-06\n","Progress: 38.0% | Best F1: 0.1701 | ETA: 4.6min\n","\n","Epoch 20/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 20/50: 100%|██████████| 51/51 [00:06<00:00,  7.41it/s, Loss=0.0006, LR=2.50e-06]\n","Validation Epoch 20/50: 100%|██████████| 7/7 [00:00<00:00, 16.80it/s, Val Loss=0.1993]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1993, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 2.50e-06\n","Progress: 40.0% | Best F1: 0.1701 | ETA: 4.5min\n","\n","Epoch 21/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 21/50: 100%|██████████| 51/51 [00:06<00:00,  7.48it/s, Loss=0.0005, LR=2.50e-06]\n","Validation Epoch 21/50: 100%|██████████| 7/7 [00:00<00:00, 16.77it/s, Val Loss=0.2003]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2003, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 1.25e-06\n","Progress: 42.0% | Best F1: 0.1701 | ETA: 4.3min\n","\n","Epoch 22/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 22/50: 100%|██████████| 51/51 [00:06<00:00,  7.42it/s, Loss=0.0004, LR=1.25e-06]\n","Validation Epoch 22/50: 100%|██████████| 7/7 [00:00<00:00, 16.94it/s, Val Loss=0.2000]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2000, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 1.25e-06\n","Progress: 44.0% | Best F1: 0.1701 | ETA: 4.1min\n","\n","Epoch 23/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 23/50: 100%|██████████| 51/51 [00:06<00:00,  7.50it/s, Loss=0.0004, LR=1.25e-06]\n","Validation Epoch 23/50: 100%|██████████| 7/7 [00:00<00:00, 16.92it/s, Val Loss=0.2004]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2004, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.2s, LR: 1.25e-06\n","Progress: 46.0% | Best F1: 0.1701 | ETA: 3.9min\n","\n","Epoch 24/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 24/50: 100%|██████████| 51/51 [00:06<00:00,  7.47it/s, Loss=0.0004, LR=1.25e-06]\n","Validation Epoch 24/50: 100%|██████████| 7/7 [00:00<00:00, 16.75it/s, Val Loss=0.2005]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2005, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 1.25e-06\n","Progress: 48.0% | Best F1: 0.1701 | ETA: 3.7min\n","\n","Epoch 25/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 25/50: 100%|██████████| 51/51 [00:06<00:00,  7.42it/s, Loss=0.0004, LR=1.25e-06]\n","Validation Epoch 25/50: 100%|██████████| 7/7 [00:00<00:00, 16.54it/s, Val Loss=0.2008]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2008, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 1.00e-06\n","Progress: 50.0% | Best F1: 0.1701 | ETA: 3.6min\n","\n","Epoch 26/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 26/50: 100%|██████████| 51/51 [00:06<00:00,  7.52it/s, Loss=0.0004, LR=1.00e-06]\n","Validation Epoch 26/50: 100%|██████████| 7/7 [00:00<00:00, 15.96it/s, Val Loss=0.2010]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2010, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.2s, LR: 1.00e-06\n","Progress: 52.0% | Best F1: 0.1701 | ETA: 3.4min\n","\n","Epoch 27/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 27/50: 100%|██████████| 51/51 [00:06<00:00,  7.42it/s, Loss=0.0004, LR=1.00e-06]\n","Validation Epoch 27/50: 100%|██████████| 7/7 [00:00<00:00, 16.42it/s, Val Loss=0.2011]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2011, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 1.00e-06\n","Progress: 54.0% | Best F1: 0.1701 | ETA: 3.3min\n","\n","Epoch 28/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 28/50: 100%|██████████| 51/51 [00:06<00:00,  7.43it/s, Loss=0.0004, LR=1.00e-06]\n","Validation Epoch 28/50: 100%|██████████| 7/7 [00:00<00:00, 16.47it/s, Val Loss=0.2018]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2018, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 1.00e-06\n","Progress: 56.0% | Best F1: 0.1701 | ETA: 3.1min\n","\n","Epoch 29/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 29/50: 100%|██████████| 51/51 [00:06<00:00,  7.42it/s, Loss=0.0004, LR=1.00e-06]\n","Validation Epoch 29/50: 100%|██████████| 7/7 [00:00<00:00, 16.80it/s, Val Loss=0.2021]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2021, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 1.00e-06\n","Progress: 58.0% | Best F1: 0.1701 | ETA: 2.9min\n","\n","Epoch 30/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 30/50: 100%|██████████| 51/51 [00:06<00:00,  7.43it/s, Loss=0.0004, LR=1.00e-06]\n","Validation Epoch 30/50: 100%|██████████| 7/7 [00:00<00:00, 17.16it/s, Val Loss=0.2025]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2025, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 1.00e-06\n","Progress: 60.0% | Best F1: 0.1701 | ETA: 2.8min\n","\n","Epoch 31/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 31/50: 100%|██████████| 51/51 [00:06<00:00,  7.42it/s, Loss=0.0004, LR=1.00e-06]\n","Validation Epoch 31/50: 100%|██████████| 7/7 [00:00<00:00, 16.64it/s, Val Loss=0.2026]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2026, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.3s, LR: 1.00e-06\n","Progress: 62.0% | Best F1: 0.1701 | ETA: 2.6min\n","\n","Epoch 32/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 32/50: 100%|██████████| 51/51 [00:07<00:00,  7.27it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 32/50: 100%|██████████| 7/7 [00:00<00:00, 16.66it/s, Val Loss=0.2029]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2029, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.4s, LR: 1.00e-06\n","Progress: 64.0% | Best F1: 0.1701 | ETA: 2.5min\n","\n","Epoch 33/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 33/50: 100%|██████████| 51/51 [00:06<00:00,  7.30it/s, Loss=0.0004, LR=1.00e-06]\n","Validation Epoch 33/50: 100%|██████████| 7/7 [00:00<00:00, 16.38it/s, Val Loss=0.2036]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2036, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.4s, LR: 1.00e-06\n","Progress: 66.0% | Best F1: 0.1701 | ETA: 2.3min\n","\n","Epoch 34/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 34/50: 100%|██████████| 51/51 [00:06<00:00,  7.38it/s, Loss=0.0004, LR=1.00e-06]\n","Validation Epoch 34/50: 100%|██████████| 7/7 [00:00<00:00, 16.49it/s, Val Loss=0.2037]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2037, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.4s, LR: 1.00e-06\n","Progress: 68.0% | Best F1: 0.1701 | ETA: 2.2min\n","\n","Epoch 35/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 35/50: 100%|██████████| 51/51 [00:07<00:00,  7.28it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 35/50: 100%|██████████| 7/7 [00:00<00:00, 16.73it/s, Val Loss=0.2039]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2039, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.4s, LR: 1.00e-06\n","Progress: 70.0% | Best F1: 0.1701 | ETA: 2.1min\n","\n","Epoch 36/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 36/50: 100%|██████████| 51/51 [00:07<00:00,  6.80it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 36/50: 100%|██████████| 7/7 [00:00<00:00, 15.66it/s, Val Loss=0.2042]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2042, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 8.0s, LR: 1.00e-06\n","Progress: 72.0% | Best F1: 0.1701 | ETA: 1.9min\n","\n","Epoch 37/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 37/50: 100%|██████████| 51/51 [00:07<00:00,  7.25it/s, Loss=0.0005, LR=1.00e-06]\n","Validation Epoch 37/50: 100%|██████████| 7/7 [00:00<00:00, 16.59it/s, Val Loss=0.2042]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2042, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.5s, LR: 1.00e-06\n","Progress: 74.0% | Best F1: 0.1701 | ETA: 1.8min\n","\n","Epoch 38/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 38/50: 100%|██████████| 51/51 [00:06<00:00,  7.33it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 38/50: 100%|██████████| 7/7 [00:00<00:00, 16.33it/s, Val Loss=0.2036]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2036, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.4s, LR: 1.00e-06\n","Progress: 76.0% | Best F1: 0.1701 | ETA: 1.6min\n","\n","Epoch 39/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 39/50: 100%|██████████| 51/51 [00:06<00:00,  7.31it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 39/50: 100%|██████████| 7/7 [00:00<00:00, 15.66it/s, Val Loss=0.2036]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2036, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.4s, LR: 1.00e-06\n","Progress: 78.0% | Best F1: 0.1701 | ETA: 1.5min\n","\n","Epoch 40/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 40/50: 100%|██████████| 51/51 [00:07<00:00,  7.24it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 40/50: 100%|██████████| 7/7 [00:00<00:00, 16.01it/s, Val Loss=0.2045]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2045, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.5s, LR: 1.00e-06\n","Progress: 80.0% | Best F1: 0.1701 | ETA: 1.4min\n","\n","Epoch 41/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 41/50: 100%|██████████| 51/51 [00:06<00:00,  7.32it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 41/50: 100%|██████████| 7/7 [00:00<00:00, 15.54it/s, Val Loss=0.2045]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2045, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.4s, LR: 1.00e-06\n","Progress: 82.0% | Best F1: 0.1701 | ETA: 1.2min\n","\n","Epoch 42/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 42/50: 100%|██████████| 51/51 [00:06<00:00,  7.29it/s, Loss=0.0005, LR=1.00e-06]\n","Validation Epoch 42/50: 100%|██████████| 7/7 [00:00<00:00, 16.13it/s, Val Loss=0.2024]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2024, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.4s, LR: 1.00e-06\n","Progress: 84.0% | Best F1: 0.1701 | ETA: 1.1min\n","\n","Epoch 43/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 43/50: 100%|██████████| 51/51 [00:07<00:00,  7.25it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 43/50: 100%|██████████| 7/7 [00:00<00:00, 16.45it/s, Val Loss=0.2032]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2032, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.5s, LR: 1.00e-06\n","Progress: 86.0% | Best F1: 0.1701 | ETA: 0.9min\n","\n","Epoch 44/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 44/50: 100%|██████████| 51/51 [00:06<00:00,  7.35it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 44/50: 100%|██████████| 7/7 [00:00<00:00, 15.83it/s, Val Loss=0.2042]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2042, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.4s, LR: 1.00e-06\n","Progress: 88.0% | Best F1: 0.1701 | ETA: 0.8min\n","\n","Epoch 45/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 45/50: 100%|██████████| 51/51 [00:07<00:00,  7.28it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 45/50: 100%|██████████| 7/7 [00:00<00:00, 16.08it/s, Val Loss=0.2049]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2049, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.5s, LR: 1.00e-06\n","Progress: 90.0% | Best F1: 0.1701 | ETA: 0.7min\n","\n","Epoch 46/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 46/50: 100%|██████████| 51/51 [00:06<00:00,  7.33it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 46/50: 100%|██████████| 7/7 [00:00<00:00, 15.95it/s, Val Loss=0.2054]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2054, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.4s, LR: 1.00e-06\n","Progress: 92.0% | Best F1: 0.1701 | ETA: 0.5min\n","\n","Epoch 47/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 47/50: 100%|██████████| 51/51 [00:06<00:00,  7.31it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 47/50: 100%|██████████| 7/7 [00:00<00:00, 15.64it/s, Val Loss=0.2064]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2064, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.4s, LR: 1.00e-06\n","Progress: 94.0% | Best F1: 0.1701 | ETA: 0.4min\n","\n","Epoch 48/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 48/50: 100%|██████████| 51/51 [00:07<00:00,  7.22it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 48/50: 100%|██████████| 7/7 [00:00<00:00, 16.07it/s, Val Loss=0.2077]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2077, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.5s, LR: 1.00e-06\n","Progress: 96.0% | Best F1: 0.1701 | ETA: 0.3min\n","\n","Epoch 49/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 49/50: 100%|██████████| 51/51 [00:06<00:00,  7.32it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 49/50: 100%|██████████| 7/7 [00:00<00:00, 15.94it/s, Val Loss=0.2078]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2078, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.4s, LR: 1.00e-06\n","Progress: 98.0% | Best F1: 0.1701 | ETA: 0.1min\n","\n","Epoch 50/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 50/50: 100%|██████████| 51/51 [00:07<00:00,  7.28it/s, Loss=0.0003, LR=1.00e-06]\n","Validation Epoch 50/50: 100%|██████████| 7/7 [00:00<00:00, 15.95it/s, Val Loss=0.2081]"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2081, F1: 0.1058, Acc: 0.2692\n","Time  - Epoch: 7.5s, LR: 1.00e-06\n","Progress: 100.0% | Best F1: 0.1701 | ETA: 0.0min\n","\n","======================================================================\n","CASME II APEX FRAME SWIN TRANSFORMER TRAINING COMPLETED\n","======================================================================\n","Training time: 6.7 minutes\n","Epochs completed: 50\n","Best validation F1: 0.1701 (epoch 5)\n","Final train F1: 1.0000\n","Final validation F1: 0.1058\n","\n","Exporting training documentation...\n","Training documentation saved successfully: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/05_02_swint_casme2_af_prep/training_logs/casme2_swint_apex_frame_training_history.json\n","Loss function: Optimized Focal Loss\n","Model variant: microsoft/swin-base-patch4-window7-224\n","Dataset version: v7\n","\n","Next: Cell 3 - CASME II Apex Frame Swin Transformer Evaluation\n","Training pipeline with hierarchical features completed successfully!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title Cell 3: CASME II Apex Frame Swin Transformer Evaluation\n","\n","# File: 05_02_SwinT_CASME2_AF_Cell3.py\n","# Location: experiments/05_02_SwinT_CASME2-AF-PREP.ipynb\n","# Purpose: Comprehensive evaluation framework for Swin Transformer with apex frame and face-aware preprocessing\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from datetime import datetime\n","\n","# Evaluation specific imports\n","from sklearn.metrics import (\n","    accuracy_score, precision_recall_fscore_support,\n","    classification_report, confusion_matrix,\n","    roc_curve, auc\n",")\n","from sklearn.preprocessing import label_binarize\n","from concurrent.futures import ThreadPoolExecutor\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"CASME II Apex Frame Swin Transformer Evaluation Framework\")\n","print(\"=\" * 60)\n","print(f\"Dataset Version: v7 - Face-Aware Preprocessing\")\n","print(f\"Model: Swin Transformer Base\")\n","print(f\"Frame Strategy: Apex Frame\")\n","print(\"=\" * 60)\n","\n","# CASME II evaluation configuration\n","EVALUATION_CONFIG_CASME2 = {\n","    'model_type': 'Swin_CASME2_Apex_Frame_Face_Aware',\n","    'task_type': 'micro_expression_recognition',\n","    'num_classes': 7,\n","    'class_names': CASME2_CLASSES,\n","    'checkpoint_file': 'casme2_swint_apex_frame_best_f1.pth',\n","    'dataset_name': 'CASME_II',\n","    'dataset_version': 'v7',\n","    'preprocessing_method': 'face_aware_bbox_expansion',\n","    'input_size': '224x224',\n","    'evaluation_protocol': 'stratified_split'\n","}\n","\n","print(f\"\\nCASME II Swin Transformer Evaluation Configuration:\")\n","print(f\"  Model: {EVALUATION_CONFIG_CASME2['model_type']}\")\n","print(f\"  Task: {EVALUATION_CONFIG_CASME2['task_type']}\")\n","print(f\"  Dataset Version: {EVALUATION_CONFIG_CASME2['dataset_version']}\")\n","print(f\"  Preprocessing: {EVALUATION_CONFIG_CASME2['preprocessing_method']}\")\n","print(f\"  Classes: {EVALUATION_CONFIG_CASME2['class_names']}\")\n","print(f\"  Input size: {EVALUATION_CONFIG_CASME2['input_size']}\")\n","\n","# Enhanced test dataset with RAM caching\n","class CASME2DatasetEvaluation(Dataset):\n","    \"\"\"Enhanced CASME II test dataset with comprehensive evaluation support\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.emotions = []\n","        self.subjects = []\n","        self.filenames = []\n","        self.cached_images = []\n","\n","        split_dir = os.path.join(dataset_root, split)\n","\n","        if not os.path.exists(split_dir):\n","            raise ValueError(f\"Split directory not found: {split_dir}\")\n","\n","        print(f\"Loading CASME II {split} dataset for evaluation...\")\n","\n","        # Load all images from flat directory structure\n","        # Filename pattern: sub01_EP02_01f_happiness.jpg\n","        for img_file in os.listdir(split_dir):\n","            if img_file.endswith(('.jpg', '.jpeg', '.png')):\n","                # Extract emotion from filename\n","                emotion = img_file.rsplit('_', 1)[-1].split('.')[0]\n","\n","                # Extract subject from filename (sub01, sub02, etc)\n","                subject = img_file.split('_')[0]\n","\n","                if emotion in CASME2_CLASSES:\n","                    self.images.append(os.path.join(split_dir, img_file))\n","                    self.labels.append(CLASS_TO_IDX[emotion])\n","                    self.emotions.append(emotion)\n","                    self.subjects.append(subject)\n","                    self.filenames.append(img_file)\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples for evaluation\")\n","        self._print_evaluation_distribution()\n","\n","        # RAM caching for fast evaluation\n","        if self.use_ram_cache:\n","            self._preload_to_ram_evaluation()\n","\n","    def _print_evaluation_distribution(self):\n","        \"\"\"Print comprehensive distribution for evaluation analysis\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"No test samples found!\")\n","            return\n","\n","        label_counts = {}\n","        subject_counts = {}\n","\n","        for label, subject in zip(self.labels, self.subjects):\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","            subject_counts[subject] = subject_counts.get(subject, 0) + 1\n","\n","        print(\"Test set class distribution:\")\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","        print(f\"Test set covers {len(subject_counts)} subjects\")\n","\n","        # Check for missing classes\n","        missing_classes = []\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i not in label_counts:\n","                missing_classes.append(class_name)\n","\n","        if missing_classes:\n","            print(f\"Missing classes in test set: {missing_classes}\")\n","\n","    def _preload_to_ram_evaluation(self):\n","        \"\"\"RAM preloading with parallel loading optimized for evaluation\"\"\"\n","        print(f\"Preloading {len(self.images)} test images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            \"\"\"Load single image with error handling\"\"\"\n","            try:\n","                image = Image.open(img_path)\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('L', (224, 224), 128), False\n","\n","        # Parallel loading with ThreadPoolExecutor\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=\"Loading test images to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_mb = len(self.cached_images) * 224 * 224 * 1 / 1e6\n","        print(f\"Test RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_mb:.1f}MB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx])\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('L', (224, 224), 128)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return (image, self.labels[idx], self.filenames[idx],\n","                self.emotions[idx], self.subjects[idx])\n","\n","def extract_logits_safe_casme2(outputs_all):\n","    \"\"\"Robust logits extraction for CASME II Swin Transformer model\"\"\"\n","    if isinstance(outputs_all, torch.Tensor):\n","        return outputs_all\n","    if isinstance(outputs_all, (tuple, list)):\n","        for item in outputs_all:\n","            if isinstance(item, torch.Tensor):\n","                return item\n","    if isinstance(outputs_all, dict):\n","        for key in ('logits', 'logit', 'predictions', 'outputs', 'scores'):\n","            value = outputs_all.get(key)\n","            if isinstance(value, torch.Tensor):\n","                return value\n","        for value in outputs_all.values():\n","            if isinstance(value, torch.Tensor):\n","                return value\n","    raise RuntimeError(\"Unable to extract tensor logits from CASME II Swin Transformer model output\")\n","\n","def load_trained_model_casme2(checkpoint_path, device):\n","    \"\"\"Load trained CASME II Swin Transformer model with comprehensive compatibility\"\"\"\n","    print(f\"Loading trained CASME II Apex Frame Swin Transformer model from: {checkpoint_path}\")\n","\n","    if not os.path.exists(checkpoint_path):\n","        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n","\n","    # Multiple loading approaches for maximum compatibility\n","    checkpoint = None\n","    loading_method = \"unknown\"\n","\n","    try:\n","        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","        loading_method = \"standard\"\n","    except Exception as e1:\n","        try:\n","            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n","            loading_method = \"weights_only_false\"\n","        except Exception as e2:\n","            try:\n","                import pickle\n","                with open(checkpoint_path, 'rb') as f:\n","                    checkpoint = pickle.load(f)\n","                loading_method = \"pickle\"\n","            except Exception as e3:\n","                raise RuntimeError(f\"All loading methods failed: {e1}, {e2}, {e3}\")\n","\n","    print(f\"Checkpoint loaded using: {loading_method}\")\n","\n","    # Initialize CASME II Swin Transformer model\n","    model = SwinCASME2Baseline(\n","        num_classes=EVALUATION_CONFIG_CASME2['num_classes'],\n","        dropout_rate=CASME2_SWIN_CONFIG['dropout_rate']\n","    ).to(device)\n","\n","    # Load state dict with fallback approaches\n","    state_dict = checkpoint.get('model_state_dict', checkpoint)\n","\n","    try:\n","        model.load_state_dict(state_dict, strict=True)\n","        print(\"Model state loaded with strict=True\")\n","    except Exception as e:\n","        print(f\"Strict loading failed, trying non-strict: {str(e)[:100]}...\")\n","        try:\n","            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n","            if missing_keys or unexpected_keys:\n","                print(f\"Non-strict loading: Missing {len(missing_keys)}, Unexpected {len(unexpected_keys)}\")\n","            else:\n","                print(\"Model state loaded with strict=False (no key mismatches)\")\n","        except Exception as e2:\n","            raise RuntimeError(f\"Both loading approaches failed: {e2}\")\n","\n","    model.eval()\n","\n","    # Extract training information\n","    training_info = {\n","        'best_val_f1': float(checkpoint.get('best_f1', 0.0)),\n","        'best_val_loss': float(checkpoint.get('best_loss', float('inf'))),\n","        'best_val_accuracy': float(checkpoint.get('best_acc', 0.0)),\n","        'best_epoch': int(checkpoint.get('epoch', 0)) + 1,\n","        'model_checkpoint': EVALUATION_CONFIG_CASME2['checkpoint_file'],\n","        'num_classes': EVALUATION_CONFIG_CASME2['num_classes'],\n","        'config': checkpoint.get('casme2_config', {})\n","    }\n","\n","    print(f\"Model loaded successfully:\")\n","    print(f\"  Best validation F1: {training_info['best_val_f1']:.4f}\")\n","    print(f\"  Best validation accuracy: {training_info['best_val_accuracy']:.4f}\")\n","    print(f\"  Best epoch: {training_info['best_epoch']}\")\n","    print(f\"  Model classes: {EVALUATION_CONFIG_CASME2['num_classes']}\")\n","\n","    return model, training_info\n","\n","def run_model_inference_casme2(model, test_loader, device):\n","    \"\"\"Run CASME II Swin Transformer model inference with comprehensive tracking\"\"\"\n","    print(\"Running CASME II Apex Frame Swin Transformer model inference on test set...\")\n","\n","    model.eval()\n","    all_predictions = []\n","    all_probabilities = []\n","    all_labels = []\n","    all_filenames = []\n","    all_emotions = []\n","    all_subjects = []\n","\n","    inference_start = time.time()\n","\n","    with torch.no_grad():\n","        for batch_idx, (images, labels, filenames, emotions, subjects) in enumerate(\n","            tqdm(test_loader, desc=\"CASME II Swin Inference\")):\n","\n","            images = images.to(device)\n","\n","            # Forward pass with robust output extraction\n","            try:\n","                outputs_raw = model(images)\n","                outputs = extract_logits_safe_casme2(outputs_raw)\n","            except Exception as e:\n","                print(f\"Error in model forward pass: {e}\")\n","                outputs = model(images)\n","                if not isinstance(outputs, torch.Tensor):\n","                    outputs = outputs[0] if isinstance(outputs, (list, tuple)) else outputs\n","\n","            # Validate output shape for 7 CASME II classes\n","            if outputs.shape[1] != 7:\n","                print(f\"Warning: Expected 7 classes output, got {outputs.shape[1]}\")\n","\n","            # Get probabilities and predictions\n","            probabilities = torch.softmax(outputs, dim=1)\n","            predictions = torch.argmax(probabilities, dim=1)\n","\n","            # Store results (CPU for memory efficiency)\n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_probabilities.extend(probabilities.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","            all_filenames.extend(filenames)\n","            all_emotions.extend(emotions)\n","            all_subjects.extend(subjects)\n","\n","    inference_time = time.time() - inference_start\n","\n","    print(f\"CASME II Swin inference completed: {len(all_predictions)} samples in {inference_time:.2f}s\")\n","\n","    # Analyze prediction distribution\n","    predictions_array = np.array(all_predictions)\n","    labels_array = np.array(all_labels)\n","\n","    unique_predictions, pred_counts = np.unique(predictions_array, return_counts=True)\n","    print(f\"Predicted classes: {[CASME2_CLASSES[i] for i in unique_predictions]}\")\n","\n","    unique_labels, label_counts = np.unique(labels_array, return_counts=True)\n","    print(f\"True classes in test: {[CASME2_CLASSES[i] for i in unique_labels]}\")\n","\n","    return {\n","        'predictions': predictions_array,\n","        'probabilities': np.array(all_probabilities),\n","        'labels': labels_array,\n","        'filenames': all_filenames,\n","        'emotions': all_emotions,\n","        'subjects': all_subjects,\n","        'inference_time': inference_time,\n","        'samples_count': len(predictions_array)\n","    }\n","\n","def analyze_wrong_predictions_casme2(inference_results):\n","    \"\"\"Comprehensive wrong predictions analysis for CASME II\"\"\"\n","    print(\"Analyzing wrong predictions for CASME II micro-expression recognition...\")\n","\n","    predictions = inference_results['predictions']\n","    labels = inference_results['labels']\n","    filenames = inference_results['filenames']\n","    emotions = inference_results['emotions']\n","    subjects = inference_results['subjects']\n","\n","    # Find wrong predictions\n","    wrong_mask = predictions != labels\n","    wrong_indices = np.where(wrong_mask)[0]\n","\n","    # Organize by true emotion class\n","    wrong_predictions_by_class = {}\n","    subject_error_analysis = {}\n","\n","    for class_name in CASME2_CLASSES:\n","        wrong_predictions_by_class[class_name] = []\n","\n","    # Analyze wrong predictions\n","    for idx in wrong_indices:\n","        true_label = labels[idx]\n","        pred_label = predictions[idx]\n","        filename = filenames[idx]\n","        emotion = emotions[idx]\n","        subject = subjects[idx]\n","\n","        true_class = CASME2_CLASSES[true_label]\n","        pred_class = CASME2_CLASSES[pred_label]\n","\n","        wrong_info = {\n","            'filename': filename,\n","            'subject': subject,\n","            'true_label': int(true_label),\n","            'true_class': true_class,\n","            'predicted_label': int(pred_label),\n","            'predicted_class': pred_class,\n","            'emotion': emotion\n","        }\n","\n","        wrong_predictions_by_class[true_class].append(wrong_info)\n","\n","        # Subject error tracking\n","        if subject not in subject_error_analysis:\n","            subject_error_analysis[subject] = {'total': 0, 'wrong': 0, 'errors': []}\n","        subject_error_analysis[subject]['wrong'] += 1\n","        subject_error_analysis[subject]['errors'].append(wrong_info)\n","\n","    # Count total samples per subject\n","    for subject in subjects:\n","        if subject in subject_error_analysis:\n","            subject_error_analysis[subject]['total'] += 1\n","        else:\n","            subject_error_analysis[subject] = {'total': 1, 'wrong': 0, 'errors': []}\n","\n","    # Calculate error rates per subject\n","    for subject in subject_error_analysis:\n","        total = subject_error_analysis[subject]['total']\n","        wrong = subject_error_analysis[subject]['wrong']\n","        subject_error_analysis[subject]['error_rate'] = wrong / total if total > 0 else 0.0\n","\n","    # Summary statistics\n","    total_wrong = len(wrong_indices)\n","    total_samples = len(predictions)\n","    error_rate = (total_wrong / total_samples) * 100\n","\n","    # Confusion patterns analysis\n","    confusion_patterns = {}\n","    for idx in wrong_indices:\n","        true_label = labels[idx]\n","        pred_label = predictions[idx]\n","        pattern = f\"{CASME2_CLASSES[true_label]}_to_{CASME2_CLASSES[pred_label]}\"\n","        confusion_patterns[pattern] = confusion_patterns.get(pattern, 0) + 1\n","\n","    analysis_results = {\n","        'analysis_metadata': {\n","            'evaluation_timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n","            'model_type': EVALUATION_CONFIG_CASME2['model_type'],\n","            'dataset': EVALUATION_CONFIG_CASME2['dataset_name'],\n","            'dataset_version': EVALUATION_CONFIG_CASME2['dataset_version'],\n","            'preprocessing': EVALUATION_CONFIG_CASME2['preprocessing_method'],\n","            'total_samples': int(total_samples),\n","            'total_wrong_predictions': int(total_wrong),\n","            'overall_error_rate': float(error_rate)\n","        },\n","        'wrong_predictions_by_class': wrong_predictions_by_class,\n","        'subject_error_analysis': subject_error_analysis,\n","        'confusion_patterns': confusion_patterns,\n","        'error_summary': {\n","            class_name: len(wrong_predictions_by_class[class_name])\n","            for class_name in CASME2_CLASSES\n","        }\n","    }\n","\n","    return analysis_results\n","\n","def calculate_comprehensive_metrics_casme2(inference_results):\n","    \"\"\"Calculate comprehensive evaluation metrics for CASME II micro-expression recognition\"\"\"\n","    print(\"Calculating comprehensive metrics for CASME II micro-expression recognition...\")\n","\n","    predictions = inference_results['predictions']\n","    probabilities = inference_results['probabilities']\n","    labels = inference_results['labels']\n","\n","    if len(predictions) == 0:\n","        raise ValueError(\"No predictions to evaluate!\")\n","\n","    # Identify available classes in test set\n","    unique_test_labels = sorted(np.unique(labels))\n","    unique_predictions = sorted(np.unique(predictions))\n","\n","    print(f\"Test set contains labels: {[CASME2_CLASSES[i] for i in unique_test_labels]}\")\n","    print(f\"Model predicted classes: {[CASME2_CLASSES[i] for i in unique_predictions]}\")\n","\n","    # Basic metrics\n","    accuracy = accuracy_score(labels, predictions)\n","\n","    # Macro metrics (only for available classes)\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions, labels=unique_test_labels, average='macro', zero_division=0\n","    )\n","\n","    print(f\"Macro F1 (available classes): {f1:.4f}\")\n","\n","    # Per-class metrics (all 7 classes)\n","    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n","        labels, predictions, labels=range(7), average=None, zero_division=0\n","    )\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(labels, predictions, labels=range(7))\n","\n","    # Multi-class AUC (only for classes with test samples)\n","    auc_scores = {}\n","    fpr_dict = {}\n","    tpr_dict = {}\n","\n","    try:\n","        labels_binarized = label_binarize(labels, classes=range(7))\n","\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i in unique_test_labels and len(np.unique(labels_binarized[:, i])) > 1:\n","                fpr, tpr, _ = roc_curve(labels_binarized[:, i], probabilities[:, i])\n","                auc_score = auc(fpr, tpr)\n","                auc_scores[class_name] = float(auc_score)\n","                fpr_dict[class_name] = fpr.tolist()\n","                tpr_dict[class_name] = tpr.tolist()\n","            else:\n","                auc_scores[class_name] = 0.0\n","                fpr_dict[class_name] = [0.0, 1.0]\n","                tpr_dict[class_name] = [0.0, 0.0]\n","\n","        # Macro AUC for available classes\n","        available_auc_scores = [auc_scores[CASME2_CLASSES[i]] for i in unique_test_labels]\n","        macro_auc = float(np.mean(available_auc_scores)) if available_auc_scores else 0.0\n","\n","    except Exception as e:\n","        print(f\"Warning: AUC calculation failed: {e}\")\n","        auc_scores = {class_name: 0.0 for class_name in CASME2_CLASSES}\n","        macro_auc = 0.0\n","\n","    # Subject-level analysis\n","    subjects = inference_results['subjects']\n","    subject_performance = {}\n","\n","    for subject in set(subjects):\n","        subject_mask = [s == subject for s in subjects]\n","        subject_predictions = predictions[subject_mask]\n","        subject_labels = labels[subject_mask]\n","\n","        if len(subject_predictions) > 0:\n","            subject_acc = accuracy_score(subject_labels, subject_predictions)\n","            subject_performance[subject] = {\n","                'accuracy': float(subject_acc),\n","                'samples': int(len(subject_predictions)),\n","                'correct': int(np.sum(subject_predictions == subject_labels))\n","            }\n","\n","    # Comprehensive results\n","    comprehensive_results = {\n","        'evaluation_metadata': {\n","            'model_type': EVALUATION_CONFIG_CASME2['model_type'],\n","            'dataset': EVALUATION_CONFIG_CASME2['dataset_name'],\n","            'dataset_version': EVALUATION_CONFIG_CASME2['dataset_version'],\n","            'preprocessing_method': EVALUATION_CONFIG_CASME2['preprocessing_method'],\n","            'evaluation_timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n","            'num_classes': EVALUATION_CONFIG_CASME2['num_classes'],\n","            'class_names': EVALUATION_CONFIG_CASME2['class_names'],\n","            'test_samples': int(len(labels)),\n","            'available_classes': [CASME2_CLASSES[i] for i in unique_test_labels],\n","            'missing_classes': [CASME2_CLASSES[i] for i in range(7) if i not in unique_test_labels]\n","        },\n","\n","        'overall_performance': {\n","            'accuracy': float(accuracy),\n","            'macro_precision': float(precision),\n","            'macro_recall': float(recall),\n","            'macro_f1': float(f1),\n","            'macro_auc': macro_auc\n","        },\n","\n","        'per_class_performance': {},\n","\n","        'confusion_matrix': cm.tolist(),\n","\n","        'subject_level_performance': subject_performance,\n","\n","        'roc_analysis': {\n","            'auc_scores': auc_scores,\n","            'fpr_curves': fpr_dict,\n","            'tpr_curves': tpr_dict\n","        },\n","\n","        'inference_performance': {\n","            'total_time_seconds': float(inference_results['inference_time']),\n","            'average_time_ms_per_sample': float(inference_results['inference_time'] * 1000 / len(labels))\n","        }\n","    }\n","\n","    # Per-class performance details\n","    for i, class_name in enumerate(CASME2_CLASSES):\n","        comprehensive_results['per_class_performance'][class_name] = {\n","            'precision': float(precision_per_class[i]),\n","            'recall': float(recall_per_class[i]),\n","            'f1_score': float(f1_per_class[i]),\n","            'support': int(support_per_class[i]),\n","            'auc': auc_scores[class_name],\n","            'in_test_set': i in unique_test_labels\n","        }\n","\n","    return comprehensive_results\n","\n","def save_evaluation_results_casme2(evaluation_results, wrong_predictions_results, results_dir):\n","    \"\"\"Save comprehensive evaluation results for CASME II Swin Transformer\"\"\"\n","    os.makedirs(results_dir, exist_ok=True)\n","\n","    # Save main evaluation results\n","    results_file = f\"{results_dir}/casme2_swint_apex_frame_evaluation_results.json\"\n","    with open(results_file, 'w') as f:\n","        json.dump(evaluation_results, f, indent=2, default=str)\n","\n","    # Save wrong predictions analysis\n","    wrong_predictions_file = f\"{results_dir}/casme2_swint_apex_frame_wrong_predictions.json\"\n","    with open(wrong_predictions_file, 'w') as f:\n","        json.dump(wrong_predictions_results, f, indent=2, default=str)\n","\n","    print(f\"Evaluation results saved:\")\n","    print(f\"  Main results: {os.path.basename(results_file)}\")\n","    print(f\"  Wrong predictions: {os.path.basename(wrong_predictions_file)}\")\n","\n","    return results_file, wrong_predictions_file\n","\n","# Main evaluation execution\n","try:\n","    print(\"\\nStarting CASME II Apex Frame Swin Transformer comprehensive evaluation...\")\n","    print(f\"Using test dataset: v7 Face-Aware Preprocessing\")\n","\n","    # Create test dataset\n","    print(f\"\\nCreating CASME II test dataset from v7...\")\n","    casme2_test_dataset = CASME2DatasetEvaluation(\n","        dataset_root=DATASET_ROOT,\n","        split='test',\n","        transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","        use_ram_cache=True\n","    )\n","\n","    if len(casme2_test_dataset) == 0:\n","        raise ValueError(\"No test samples found! Check test data path.\")\n","\n","    casme2_test_loader = DataLoader(\n","        casme2_test_dataset,\n","        batch_size=CASME2_SWIN_CONFIG['batch_size'],\n","        shuffle=False,\n","        num_workers=CASME2_SWIN_CONFIG['num_workers'],\n","        pin_memory=True\n","    )\n","\n","    # Load trained model\n","    checkpoint_path = f\"{GLOBAL_CONFIG_CASME2['checkpoint_root']}/{EVALUATION_CONFIG_CASME2['checkpoint_file']}\"\n","    casme2_model, training_info = load_trained_model_casme2(checkpoint_path, GLOBAL_CONFIG_CASME2['device'])\n","\n","    # Run inference\n","    inference_results = run_model_inference_casme2(casme2_model, casme2_test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","    # Calculate comprehensive metrics\n","    evaluation_results = calculate_comprehensive_metrics_casme2(inference_results)\n","\n","    # Analyze wrong predictions\n","    wrong_predictions_results = analyze_wrong_predictions_casme2(inference_results)\n","\n","    # Add training information\n","    evaluation_results['training_information'] = training_info\n","\n","    # Save results\n","    results_dir = f\"{GLOBAL_CONFIG_CASME2['results_root']}/evaluation_results\"\n","    results_file, wrong_file = save_evaluation_results_casme2(\n","        evaluation_results, wrong_predictions_results, results_dir\n","    )\n","\n","    # Display comprehensive results\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"CASME II APEX FRAME SWIN TRANSFORMER EVALUATION RESULTS\")\n","    print(\"=\" * 60)\n","    print(f\"Dataset: v7 Face-Aware Preprocessing (Apex Frame)\")\n","    print(f\"Model: Swin Transformer Base\")\n","\n","    # Overall performance\n","    overall = evaluation_results['overall_performance']\n","    print(f\"\\nOverall Performance (Macro - Available Classes):\")\n","    print(f\"  Accuracy:  {overall['accuracy']:.4f}\")\n","    print(f\"  Precision: {overall['macro_precision']:.4f}\")\n","    print(f\"  Recall:    {overall['macro_recall']:.4f}\")\n","    print(f\"  F1 Score:  {overall['macro_f1']:.4f}\")\n","    print(f\"  AUC:       {overall['macro_auc']:.4f}\")\n","\n","    # Per-class performance\n","    print(f\"\\nPer-Class Performance:\")\n","    for class_name, metrics in evaluation_results['per_class_performance'].items():\n","        in_test = \"Present\" if metrics['in_test_set'] else \"Missing\"\n","        print(f\"  {class_name} [{in_test}]: F1={metrics['f1_score']:.4f}, \"\n","              f\"AUC={metrics['auc']:.4f}, Support={metrics['support']}\")\n","\n","    # Training vs test comparison\n","    print(f\"\\nTraining vs Test Performance:\")\n","    training_f1 = training_info['best_val_f1']\n","    training_acc = training_info['best_val_accuracy']\n","    test_f1 = overall['macro_f1']\n","    test_acc = overall['accuracy']\n","\n","    print(f\"  Training Val F1:  {training_f1:.4f}\")\n","    print(f\"  Test F1:          {test_f1:.4f}\")\n","    print(f\"  F1 Difference:    {training_f1 - test_f1:+.4f}\")\n","    print(f\"  Training Val Acc: {training_acc:.4f}\")\n","    print(f\"  Test Accuracy:    {test_acc:.4f}\")\n","    print(f\"  Acc Difference:   {training_acc - test_acc:+.4f}\")\n","    print(f\"  Best Epoch:       {training_info['best_epoch']}\")\n","\n","    # Wrong predictions summary\n","    print(f\"\\n\" + \"=\" * 40)\n","    print(\"WRONG PREDICTIONS ANALYSIS\")\n","    print(\"=\" * 40)\n","\n","    wrong_meta = wrong_predictions_results['analysis_metadata']\n","    print(f\"Total wrong predictions: {wrong_meta['total_wrong_predictions']} / {wrong_meta['total_samples']}\")\n","    print(f\"Overall error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","    print(f\"\\nErrors by True Class:\")\n","    for class_name, error_count in wrong_predictions_results['error_summary'].items():\n","        if error_count > 0:\n","            wrong_samples = wrong_predictions_results['wrong_predictions_by_class'][class_name]\n","            print(f\"  {class_name}: {error_count} errors\")\n","            for sample in wrong_samples[:3]:\n","                print(f\"    - {sample['filename']} -> predicted as {sample['predicted_class']}\")\n","            if len(wrong_samples) > 3:\n","                print(f\"    ... and {len(wrong_samples) - 3} more\")\n","\n","    # Subject-level analysis\n","    print(f\"\\nSubject-Level Performance:\")\n","    subject_perfs = list(evaluation_results['subject_level_performance'].items())\n","    subject_perfs.sort(key=lambda x: x[1]['accuracy'], reverse=True)\n","    for subject, perf in subject_perfs[:5]:\n","        print(f\"  {subject}: {perf['accuracy']:.3f} ({perf['correct']}/{perf['samples']})\")\n","\n","    # Most common confusion patterns\n","    print(f\"\\nMost Common Confusion Patterns:\")\n","    patterns = sorted(wrong_predictions_results['confusion_patterns'].items(),\n","                     key=lambda x: x[1], reverse=True)\n","    for pattern, count in patterns[:3]:\n","        print(f\"  {pattern}: {count} cases\")\n","\n","    print(f\"\\nInference Performance:\")\n","    print(f\"  Total time: {inference_results['inference_time']:.2f}s\")\n","    print(f\"  Speed: {evaluation_results['inference_performance']['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","    print(f\"\\nTest Dataset Info:\")\n","    print(f\"  Version: {EVALUATION_CONFIG_CASME2['dataset_version']}\")\n","    print(f\"  Preprocessing: {EVALUATION_CONFIG_CASME2['preprocessing_method']}\")\n","    print(f\"  Missing classes: {evaluation_results['evaluation_metadata']['missing_classes']}\")\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"CASME II APEX FRAME SWIN TRANSFORMER EVALUATION COMPLETED\")\n","    print(\"=\" * 60)\n","\n","except Exception as e:\n","    print(f\"Evaluation failed: {e}\")\n","    import traceback\n","    traceback.print_exc()\n","\n","finally:\n","    # Memory cleanup\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","        torch.cuda.empty_cache()\n","\n","print(f\"\\nEvaluation completed successfully\")\n","print(\"Next: Cell 4 - Generate confusion matrix visualization\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"cellView":"form","id":"vjKRWapyVtAi","executionInfo":{"status":"ok","timestamp":1760944496814,"user_tz":-420,"elapsed":4423,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"bc87208f-15a4-477b-9cfe-7dbf8f757c67"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II Apex Frame Swin Transformer Evaluation Framework\n","============================================================\n","Dataset Version: v7 - Face-Aware Preprocessing\n","Model: Swin Transformer Base\n","Frame Strategy: Apex Frame\n","============================================================\n","\n","CASME II Swin Transformer Evaluation Configuration:\n","  Model: Swin_CASME2_Apex_Frame_Face_Aware\n","  Task: micro_expression_recognition\n","  Dataset Version: v7\n","  Preprocessing: face_aware_bbox_expansion\n","  Classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","  Input size: 224x224\n","\n","Starting CASME II Apex Frame Swin Transformer comprehensive evaluation...\n","Using test dataset: v7 Face-Aware Preprocessing\n","\n","Creating CASME II test dataset from v7...\n","Loading CASME II test dataset for evaluation...\n","Loaded 28 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 10 samples (35.7%)\n","  disgust: 7 samples (25.0%)\n","  happiness: 4 samples (14.3%)\n","  repression: 3 samples (10.7%)\n","  surprise: 3 samples (10.7%)\n","  sadness: 1 samples (3.6%)\n","Test set covers 16 subjects\n","Missing classes in test set: ['fear']\n","Preloading 28 test images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test images to RAM: 100%|██████████| 28/28 [00:01<00:00, 14.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test RAM caching completed: 28/28 images, ~1.4MB\n","Loading trained CASME II Apex Frame Swin Transformer model from: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/05_02_swint_casme2_af_prep/casme2_swint_apex_frame_best_f1.pth\n","Checkpoint loaded using: standard\n","Swin feature dimension (final stage): 1024\n","Base embed_dim: 128, Stages: 4\n","Swin CASME II Simplified: 1024 -> GAP -> 256 -> 7\n","Dropout rate: 0.3 (increased for small dataset regularization)\n","Model state loaded with strict=True\n","Model loaded successfully:\n","  Best validation F1: 0.1701\n","  Best validation accuracy: 0.3462\n","  Best epoch: 5\n","  Model classes: 7\n","Running CASME II Apex Frame Swin Transformer model inference on test set...\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Swin Inference: 100%|██████████| 7/7 [00:00<00:00, 13.11it/s]"]},{"output_type":"stream","name":"stdout","text":["CASME II Swin inference completed: 28 samples in 0.54s\n","Predicted classes: ['others', 'disgust', 'happiness', 'repression', 'surprise']\n","True classes in test: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness']\n","Calculating comprehensive metrics for CASME II micro-expression recognition...\n","Test set contains labels: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness']\n","Model predicted classes: ['others', 'disgust', 'happiness', 'repression', 'surprise']\n","Macro F1 (available classes): 0.2869\n","Analyzing wrong predictions for CASME II micro-expression recognition...\n","Evaluation results saved:\n","  Main results: casme2_swint_apex_frame_evaluation_results.json\n","  Wrong predictions: casme2_swint_apex_frame_wrong_predictions.json\n","\n","============================================================\n","CASME II APEX FRAME SWIN TRANSFORMER EVALUATION RESULTS\n","============================================================\n","Dataset: v7 Face-Aware Preprocessing (Apex Frame)\n","Model: Swin Transformer Base\n","\n","Overall Performance (Macro - Available Classes):\n","  Accuracy:  0.3929\n","  Precision: 0.3036\n","  Recall:    0.2758\n","  F1 Score:  0.2869\n","  AUC:       0.5832\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.5000, AUC=0.6889, Support=10\n","  disgust [Present]: F1=0.5714, AUC=0.7619, Support=7\n","  happiness [Present]: F1=0.2500, AUC=0.5208, Support=4\n","  repression [Present]: F1=0.0000, AUC=0.7067, Support=3\n","  surprise [Present]: F1=0.4000, AUC=0.7467, Support=3\n","  sadness [Present]: F1=0.0000, AUC=0.0741, Support=1\n","  fear [Missing]: F1=0.0000, AUC=0.0000, Support=0\n","\n","Training vs Test Performance:\n","  Training Val F1:  0.1701\n","  Test F1:          0.2869\n","  F1 Difference:    -0.1168\n","  Training Val Acc: 0.3462\n","  Test Accuracy:    0.3929\n","  Acc Difference:   -0.0467\n","  Best Epoch:       5\n","\n","========================================\n","WRONG PREDICTIONS ANALYSIS\n","========================================\n","Total wrong predictions: 17 / 28\n","Overall error rate: 60.71%\n","\n","Errors by True Class:\n","  others: 5 errors\n","    - sub03_EP07_03_others.jpg -> predicted as disgust\n","    - sub04_EP13_02f_others.jpg -> predicted as disgust\n","    - sub14_EP04_04f_others.jpg -> predicted as happiness\n","    ... and 2 more\n","  disgust: 3 errors\n","    - sub01_EP19_05f_disgust.jpg -> predicted as others\n","    - sub05_EP09_05f_disgust.jpg -> predicted as others\n","    - sub15_EP08_02_disgust.jpg -> predicted as others\n","  happiness: 3 errors\n","    - sub12_EP03_04_happiness.jpg -> predicted as disgust\n","    - sub17_EP06_07_happiness.jpg -> predicted as repression\n","    - sub23_EP02_01_happiness.jpg -> predicted as repression\n","  repression: 3 errors\n","    - sub09_EP09_05_repression.jpg -> predicted as others\n","    - sub16_EP01_08_repression.jpg -> predicted as others\n","    - sub17_EP05_03f_repression.jpg -> predicted as happiness\n","  surprise: 2 errors\n","    - sub02_EP11_01_surprise.jpg -> predicted as repression\n","    - sub17_EP01_13_surprise.jpg -> predicted as happiness\n","  sadness: 1 errors\n","    - sub17_EP15_03_sadness.jpg -> predicted as repression\n","\n","Subject-Level Performance:\n","  sub26: 1.000 (3/3)\n","  sub10: 1.000 (1/1)\n","  sub20: 1.000 (1/1)\n","  sub05: 0.667 (2/3)\n","  sub14: 0.500 (1/2)\n","\n","Most Common Confusion Patterns:\n","  disgust_to_others: 3 cases\n","  others_to_disgust: 2 cases\n","  repression_to_others: 2 cases\n","\n","Inference Performance:\n","  Total time: 0.54s\n","  Speed: 19.2 ms/sample\n","\n","Test Dataset Info:\n","  Version: v7\n","  Preprocessing: face_aware_bbox_expansion\n","  Missing classes: ['fear']\n","\n","============================================================\n","CASME II APEX FRAME SWIN TRANSFORMER EVALUATION COMPLETED\n","============================================================\n","\n","Evaluation completed successfully\n","Next: Cell 4 - Generate confusion matrix visualization\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title Cell 4: CASME II Apex Frame Swin Transformer Confusion Matrix Generation\n","\n","# File: 05_02_SwinT_CASME2_AF_Cell4.py\n","# Location: experiments/05_02_SwinT_CASME2-AF-PREP.ipynb\n","# Purpose: Generate professional confusion matrix visualization with comprehensive metrics\n","\n","import json\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import seaborn as sns\n","from datetime import datetime\n","\n","print(\"CASME II Apex Frame Swin Transformer Confusion Matrix Generation\")\n","print(\"=\" * 60)\n","\n","# Project paths configuration\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/05_02_swint_casme2_af_prep\"\n","\n","def find_evaluation_json_files_casme2(results_path):\n","    \"\"\"Find CASME II evaluation JSON files\"\"\"\n","    json_files = {}\n","    eval_dir = f\"{results_path}/evaluation_results\"\n","\n","    if os.path.exists(eval_dir):\n","        # Look for evaluation results\n","        eval_pattern = f\"{eval_dir}/casme2_swint_apex_frame_evaluation_results.json\"\n","        eval_files = glob.glob(eval_pattern)\n","\n","        if eval_files:\n","            json_files['main'] = eval_files[0]\n","            print(f\"Found evaluation file: {os.path.basename(eval_files[0])}\")\n","\n","        # Look for wrong predictions\n","        wrong_pattern = f\"{eval_dir}/casme2_swint_apex_frame_wrong_predictions.json\"\n","        wrong_files = glob.glob(wrong_pattern)\n","\n","        if wrong_files:\n","            json_files['wrong'] = wrong_files[0]\n","            print(f\"Found wrong predictions: {os.path.basename(wrong_files[0])}\")\n","\n","        if not json_files:\n","            print(f\"WARNING: No evaluation results found in {eval_dir}\")\n","            print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","    else:\n","        print(f\"ERROR: Evaluation directory not found: {eval_dir}\")\n","\n","    return json_files\n","\n","def load_evaluation_results_casme2(json_path):\n","    \"\"\"Load and parse CASME II evaluation results JSON\"\"\"\n","    try:\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"Successfully loaded: {os.path.basename(json_path)}\")\n","        return data\n","    except Exception as e:\n","        print(f\"ERROR loading {json_path}: {str(e)}\")\n","        return None\n","\n","def calculate_weighted_f1_casme2(per_class_performance):\n","    \"\"\"Calculate weighted F1 score for CASME II micro-expression classes\"\"\"\n","    total_support = sum([class_data['support'] for class_data in per_class_performance.values()\n","                        if class_data['support'] > 0])\n","\n","    if total_support == 0:\n","        return 0.0\n","\n","    weighted_f1 = 0.0\n","\n","    for class_name, class_data in per_class_performance.items():\n","        if class_data['support'] > 0:\n","            weight = class_data['support'] / total_support\n","            weighted_f1 += class_data['f1_score'] * weight\n","\n","    return weighted_f1\n","\n","def calculate_balanced_accuracy_casme2(confusion_matrix):\n","    \"\"\"Calculate balanced accuracy for CASME II 7-class micro-expression recognition\"\"\"\n","    cm = np.array(confusion_matrix)\n","    n_classes = cm.shape[0]\n","\n","    per_class_balanced_acc = []\n","\n","    # Find classes with actual test samples\n","    classes_with_samples = []\n","    for i in range(n_classes):\n","        if cm[i, :].sum() > 0:\n","            classes_with_samples.append(i)\n","\n","    for i in classes_with_samples:\n","        tp = cm[i, i]\n","        fn = cm[i, :].sum() - tp\n","        fp = cm[:, i].sum() - tp\n","        tn = cm.sum() - tp - fn - fp\n","\n","        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n","\n","        class_balanced_acc = (sensitivity + specificity) / 2\n","        per_class_balanced_acc.append(class_balanced_acc)\n","\n","    balanced_acc = np.mean(per_class_balanced_acc) if per_class_balanced_acc else 0.0\n","\n","    return balanced_acc\n","\n","def determine_text_color_casme2(color_value, threshold=0.5):\n","    \"\"\"Determine optimal text color based on background intensity\"\"\"\n","    return 'white' if color_value > threshold else 'black'\n","\n","def create_confusion_matrix_plot_casme2(data, output_path):\n","    \"\"\"Create professional confusion matrix visualization for CASME II micro-expression recognition\"\"\"\n","\n","    # Extract data\n","    meta = data['evaluation_metadata']\n","    class_names = meta['class_names']\n","    cm = np.array(data['confusion_matrix'], dtype=int)\n","    overall = data['overall_performance']\n","    per_class = data['per_class_performance']\n","\n","    print(f\"Processing confusion matrix for CASME II classes: {class_names}\")\n","    print(f\"Dataset version: {meta['dataset_version']}\")\n","    print(f\"Preprocessing: {meta['preprocessing_method']}\")\n","    print(f\"Confusion matrix shape: {cm.shape}\")\n","\n","    # Calculate comprehensive metrics\n","    macro_f1 = overall.get('macro_f1', 0.0)\n","    accuracy = overall.get('accuracy', 0.0)\n","    weighted_f1 = calculate_weighted_f1_casme2(per_class)\n","    balanced_acc = calculate_balanced_accuracy_casme2(cm)\n","\n","    print(f\"Calculated metrics - Macro F1: {macro_f1:.4f}, Weighted F1: {weighted_f1:.4f}, \"\n","          f\"Balanced Acc: {balanced_acc:.4f}, Accuracy: {accuracy:.4f}\")\n","\n","    # Row-wise normalization for percentage display\n","    row_sums = cm.sum(axis=1, keepdims=True)\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        cm_pct = np.divide(cm, row_sums, where=(row_sums!=0))\n","        cm_pct = np.nan_to_num(cm_pct)\n","\n","    # Create visualization\n","    fig, ax = plt.subplots(figsize=(12, 10))\n","\n","    # Color scheme optimized for micro-expression research\n","    cmap = 'Blues'\n","\n","    # Create heatmap with improved color scaling\n","    im = ax.imshow(cm_pct, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=0.8)\n","\n","    # Add colorbar\n","    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","    cbar.set_label('True Class Percentage', rotation=270, labelpad=15, fontsize=11)\n","\n","    # Annotate cells with count and percentage\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            count = cm[i, j]\n","\n","            # Handle percentage calculation for classes with 0 samples\n","            if row_sums[i, 0] > 0:\n","                percentage = cm_pct[i, j] * 100\n","                text = f\"{count}\\n{percentage:.1f}%\"\n","            else:\n","                text = f\"{count}\\n(N/A)\"\n","\n","            # Determine text color based on cell intensity\n","            cell_value = cm_pct[i, j]\n","            text_color = determine_text_color_casme2(cell_value, threshold=0.4)\n","\n","            ax.text(j, i, text, ha=\"center\", va=\"center\",\n","                   color=text_color, fontsize=9, fontweight='bold')\n","\n","    # Configure axes\n","    ax.set_xticks(np.arange(len(class_names)))\n","    ax.set_yticks(np.arange(len(class_names)))\n","    ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=10)\n","    ax.set_yticklabels(class_names, fontsize=10)\n","    ax.set_xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n","    ax.set_ylabel(\"True Label\", fontsize=12, fontweight='bold')\n","\n","    # Add preprocessing info note\n","    preprocessing_note = f\"Model: Swin Transformer\\nPreprocessing: {meta['preprocessing_method']}\\nDataset: {meta['dataset_version']}\"\n","    missing_classes = meta.get('missing_classes', [])\n","    if missing_classes:\n","        preprocessing_note += f\"\\nMissing: {', '.join(missing_classes)}\"\n","\n","    ax.text(0.02, 0.98, preprocessing_note, transform=ax.transAxes, fontsize=8,\n","            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n","\n","    # Create comprehensive title with all metrics\n","    title = f\"CASME II Apex Frame Micro-Expression Recognition - Swin Transformer\\n\"\n","    title += f\"Acc: {accuracy:.4f}  |  Macro F1: {macro_f1:.4f}  |  Weighted F1: {weighted_f1:.4f}  |  Balanced Acc: {balanced_acc:.4f}\"\n","    ax.set_title(title, fontsize=12, pad=25, fontweight='bold')\n","\n","    # Adjust layout and save\n","    plt.tight_layout()\n","    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n","    plt.close(fig)\n","\n","    print(f\"Confusion matrix saved to: {os.path.basename(output_path)}\")\n","\n","    return {\n","        'accuracy': accuracy,\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        'balanced_accuracy': balanced_acc,\n","        'missing_classes': missing_classes\n","    }\n","\n","def generate_performance_summary_casme2(evaluation_data, wrong_predictions_data=None):\n","    \"\"\"Generate comprehensive performance summary for CASME II\"\"\"\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"CASME II APEX FRAME SWIN TRANSFORMER PERFORMANCE SUMMARY\")\n","    print(\"=\" * 60)\n","\n","    # Overall performance\n","    overall = evaluation_data['overall_performance']\n","    meta = evaluation_data['evaluation_metadata']\n","\n","    print(f\"Dataset: {meta['dataset']}\")\n","    print(f\"Dataset version: {meta.get('dataset_version', 'N/A')}\")\n","    print(f\"Preprocessing: {meta.get('preprocessing_method', 'N/A')}\")\n","    print(f\"Test samples: {meta['test_samples']}\")\n","    print(f\"Model: {meta['model_type']}\")\n","    print(f\"Evaluation date: {meta['evaluation_timestamp']}\")\n","\n","    print(f\"\\nOverall Performance:\")\n","    print(f\"  Accuracy:         {overall['accuracy']:.4f}\")\n","    print(f\"  Macro Precision:  {overall['macro_precision']:.4f}\")\n","    print(f\"  Macro Recall:     {overall['macro_recall']:.4f}\")\n","    print(f\"  Macro F1:         {overall['macro_f1']:.4f}\")\n","    print(f\"  Macro AUC:        {overall['macro_auc']:.4f}\")\n","\n","    # Per-class performance\n","    print(f\"\\nPer-Class Performance:\")\n","    per_class = evaluation_data['per_class_performance']\n","\n","    print(f\"{'Class':<12} {'F1':<8} {'Precision':<10} {'Recall':<8} {'AUC':<8} {'Support':<8} {'In Test'}\")\n","    print(\"-\" * 65)\n","\n","    for class_name, metrics in per_class.items():\n","        in_test = \"Yes\" if metrics['in_test_set'] else \"No\"\n","        print(f\"{class_name:<12} {metrics['f1_score']:<8.4f} {metrics['precision']:<10.4f} \"\n","              f\"{metrics['recall']:<8.4f} {metrics['auc']:<8.4f} {metrics['support']:<8} {in_test}\")\n","\n","    # Training vs test performance\n","    if 'training_information' in evaluation_data:\n","        training = evaluation_data['training_information']\n","        print(f\"\\nTraining vs Test Comparison:\")\n","        print(f\"  Training Val F1:  {training['best_val_f1']:.4f}\")\n","        print(f\"  Test F1:          {overall['macro_f1']:.4f}\")\n","        print(f\"  Performance Gap:  {training['best_val_f1'] - overall['macro_f1']:+.4f}\")\n","        print(f\"  Best Epoch:       {training['best_epoch']}\")\n","\n","    # Class availability analysis\n","    missing_classes = meta.get('missing_classes', [])\n","    available_classes = meta.get('available_classes', [])\n","\n","    print(f\"\\nClass Availability Analysis:\")\n","    print(f\"  Available classes: {len(available_classes)}/7\")\n","    print(f\"  Missing classes: {missing_classes if missing_classes else 'None'}\")\n","\n","    # Wrong predictions summary if available\n","    if wrong_predictions_data:\n","        wrong_meta = wrong_predictions_data['analysis_metadata']\n","        print(f\"\\nError Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']}/{wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        # Top confusion patterns\n","        patterns = wrong_predictions_data.get('confusion_patterns', {})\n","        if patterns:\n","            print(f\"\\nTop Confusion Patterns:\")\n","            sorted_patterns = sorted(patterns.items(), key=lambda x: x[1], reverse=True)[:3]\n","            for pattern, count in sorted_patterns:\n","                print(f\"  {pattern}: {count} cases\")\n","\n","    print(f\"\\nInference Performance:\")\n","    inference = evaluation_data['inference_performance']\n","    print(f\"  Total time: {inference['total_time_seconds']:.2f}s\")\n","    print(f\"  Speed: {inference['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","# Find evaluation JSON files\n","json_files = find_evaluation_json_files_casme2(RESULTS_ROOT)\n","\n","if not json_files:\n","    print(f\"ERROR: No evaluation JSON files found in {RESULTS_ROOT}\")\n","    print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","else:\n","    print(f\"\\nFound evaluation results\")\n","\n","# Create output directory\n","output_dir = f\"{RESULTS_ROOT}/confusion_matrix_analysis\"\n","Path(output_dir).mkdir(parents=True, exist_ok=True)\n","\n","# Process evaluation results\n","if 'main' in json_files:\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Processing Evaluation Results\")\n","    print(f\"{'='*60}\")\n","\n","    # Load evaluation data\n","    eval_data = load_evaluation_results_casme2(json_files['main'])\n","\n","    # Load wrong predictions data if available\n","    wrong_data = None\n","    if 'wrong' in json_files:\n","        wrong_data = load_evaluation_results_casme2(json_files['wrong'])\n","\n","    if eval_data is not None:\n","        try:\n","            # Generate confusion matrix\n","            cm_output_path = os.path.join(output_dir, \"confusion_matrix_CASME2_Swin_Apex_Frame_v7.png\")\n","            metrics = create_confusion_matrix_plot_casme2(eval_data, cm_output_path)\n","\n","            print(f\"\\nSUCCESS: Confusion matrix generated successfully\")\n","            print(f\"Output file: {os.path.basename(cm_output_path)}\")\n","\n","            # Display metrics summary\n","            print(f\"\\nPerformance Metrics Summary:\")\n","            print(f\"  Accuracy:        {metrics['accuracy']:.4f}\")\n","            print(f\"  Macro F1:        {metrics['macro_f1']:.4f}\")\n","            print(f\"  Weighted F1:     {metrics['weighted_f1']:.4f}\")\n","            print(f\"  Balanced Acc:    {metrics['balanced_accuracy']:.4f}\")\n","\n","            if metrics['missing_classes']:\n","                print(f\"  Missing classes: {metrics['missing_classes']}\")\n","\n","        except Exception as e:\n","            print(f\"ERROR: Failed to generate confusion matrix: {str(e)}\")\n","            import traceback\n","            traceback.print_exc()\n","\n","        # Generate comprehensive summary\n","        generate_performance_summary_casme2(eval_data, wrong_data)\n","    else:\n","        print(f\"ERROR: Could not load evaluation data\")\n","\n","    # Final summary\n","    print(f\"\\n\" + \"=\" * 60)\n","    print(\"CASME II APEX FRAME SWIN TRANSFORMER CM GENERATION COMPLETED\")\n","    print(\"=\" * 60)\n","\n","    print(f\"\\nGenerated file:\")\n","    print(f\"  confusion_matrix_CASME2_Swin_Apex_Frame_v7.png\")\n","\n","    print(f\"\\nFiles saved in: {output_dir}\")\n","    print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","else:\n","    print(f\"\\nERROR: No evaluation results found\")\n","    print(\"Please run Cell 3 (evaluation) first to generate evaluation JSON files\")\n","\n","print(\"\\nCell 4 completed - CASME II Apex Frame Swin Transformer confusion matrix generated\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","collapsed":true,"id":"LXD9B5U-WA-s","executionInfo":{"status":"ok","timestamp":1760944498693,"user_tz":-420,"elapsed":1829,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"b60dc808-0eb5-445a-eb34-59ec2fa149a5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II Apex Frame Swin Transformer Confusion Matrix Generation\n","============================================================\n","Found evaluation file: casme2_swint_apex_frame_evaluation_results.json\n","Found wrong predictions: casme2_swint_apex_frame_wrong_predictions.json\n","\n","Found evaluation results\n","\n","============================================================\n","Processing Evaluation Results\n","============================================================\n","Successfully loaded: casme2_swint_apex_frame_evaluation_results.json\n","Successfully loaded: casme2_swint_apex_frame_wrong_predictions.json\n","Processing confusion matrix for CASME II classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","Dataset version: v7\n","Preprocessing: face_aware_bbox_expansion\n","Confusion matrix shape: (7, 7)\n","Calculated metrics - Macro F1: 0.2869, Weighted F1: 0.4000, Balanced Acc: 0.5724, Accuracy: 0.3929\n","Confusion matrix saved to: confusion_matrix_CASME2_Swin_Apex_Frame_v7.png\n","\n","SUCCESS: Confusion matrix generated successfully\n","Output file: confusion_matrix_CASME2_Swin_Apex_Frame_v7.png\n","\n","Performance Metrics Summary:\n","  Accuracy:        0.3929\n","  Macro F1:        0.2869\n","  Weighted F1:     0.4000\n","  Balanced Acc:    0.5724\n","  Missing classes: ['fear']\n","\n","============================================================\n","CASME II APEX FRAME SWIN TRANSFORMER PERFORMANCE SUMMARY\n","============================================================\n","Dataset: CASME_II\n","Dataset version: v7\n","Preprocessing: face_aware_bbox_expansion\n","Test samples: 28\n","Model: Swin_CASME2_Apex_Frame_Face_Aware\n","Evaluation date: 20251020_071456\n","\n","Overall Performance:\n","  Accuracy:         0.3929\n","  Macro Precision:  0.3036\n","  Macro Recall:     0.2758\n","  Macro F1:         0.2869\n","  Macro AUC:        0.5832\n","\n","Per-Class Performance:\n","Class        F1       Precision  Recall   AUC      Support  In Test\n","-----------------------------------------------------------------\n","others       0.5000   0.5000     0.5000   0.6889   10       Yes\n","disgust      0.5714   0.5714     0.5714   0.7619   7        Yes\n","happiness    0.2500   0.2500     0.2500   0.5208   4        Yes\n","repression   0.0000   0.0000     0.0000   0.7067   3        Yes\n","surprise     0.4000   0.5000     0.3333   0.7467   3        Yes\n","sadness      0.0000   0.0000     0.0000   0.0741   1        Yes\n","fear         0.0000   0.0000     0.0000   0.0000   0        No\n","\n","Training vs Test Comparison:\n","  Training Val F1:  0.1701\n","  Test F1:          0.2869\n","  Performance Gap:  -0.1168\n","  Best Epoch:       5\n","\n","Class Availability Analysis:\n","  Available classes: 6/7\n","  Missing classes: ['fear']\n","\n","Error Analysis:\n","  Total errors: 17/28\n","  Error rate: 60.71%\n","\n","Top Confusion Patterns:\n","  disgust_to_others: 3 cases\n","  others_to_disgust: 2 cases\n","  repression_to_others: 2 cases\n","\n","Inference Performance:\n","  Total time: 0.54s\n","  Speed: 19.2 ms/sample\n","\n","============================================================\n","CASME II APEX FRAME SWIN TRANSFORMER CM GENERATION COMPLETED\n","============================================================\n","\n","Generated file:\n","  confusion_matrix_CASME2_Swin_Apex_Frame_v7.png\n","\n","Files saved in: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/05_02_swint_casme2_af_prep/confusion_matrix_analysis\n","Analysis completed at: 2025-10-20 07:14:58\n","\n","Cell 4 completed - CASME II Apex Frame Swin Transformer confusion matrix generated\n"]}]}]}