{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyPGFfkTYn/HJOa29RKRtLOv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d2313132e6674b529dba59fe3370ad2e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2fdbdef4b2404ae195e204912e062796","IPY_MODEL_a756a705cd1b49699933d2b1479f698d","IPY_MODEL_5204cdf393d34fd8b37cd81fc7e11965"],"layout":"IPY_MODEL_fa424d9dcba84653b469fa51a643e724"}},"2fdbdef4b2404ae195e204912e062796":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7342536762574bb493687d446153df56","placeholder":"​","style":"IPY_MODEL_89759abfdf79458e95790bf93827f7ba","value":"preprocessor_config.json: 100%"}},"a756a705cd1b49699933d2b1479f698d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d14a438c372346cfaf0288e215815c25","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_267f52717cdc48cbb037d447b45170c7","value":160}},"5204cdf393d34fd8b37cd81fc7e11965":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc2f7d317bd24802a171f04dadd68671","placeholder":"​","style":"IPY_MODEL_1fc9f46cc5da4ae4a92c87aeebd2ed6d","value":" 160/160 [00:00&lt;00:00, 20.4kB/s]"}},"fa424d9dcba84653b469fa51a643e724":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7342536762574bb493687d446153df56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89759abfdf79458e95790bf93827f7ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d14a438c372346cfaf0288e215815c25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"267f52717cdc48cbb037d447b45170c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc2f7d317bd24802a171f04dadd68671":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fc9f46cc5da4ae4a92c87aeebd2ed6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1e9e523f88142c7938337e53c272393":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c3e4fa9b2c84511af9aa25e482b792a","IPY_MODEL_99bdd1baeb3b4648970e6c86ce10704f","IPY_MODEL_ca961b8cc3da412ea4cb3a3c89a4c907"],"layout":"IPY_MODEL_9adf2f29417c44c5bd1d40758ad97318"}},"2c3e4fa9b2c84511af9aa25e482b792a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03296dff503a44549f4be3e05078e95f","placeholder":"​","style":"IPY_MODEL_35b10d5327e34d8b837dbe5b080079e2","value":"config.json: 100%"}},"99bdd1baeb3b4648970e6c86ce10704f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cb95f39afcf4dc086dce5d3cbd0da3e","max":502,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2bf09bf99cbf424da63a273d34b3f9e1","value":502}},"ca961b8cc3da412ea4cb3a3c89a4c907":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acdc3902c659406881f15c1f94883b82","placeholder":"​","style":"IPY_MODEL_bc91935fbdd548249ad9fc30e2fee5ee","value":" 502/502 [00:00&lt;00:00, 62.7kB/s]"}},"9adf2f29417c44c5bd1d40758ad97318":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03296dff503a44549f4be3e05078e95f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35b10d5327e34d8b837dbe5b080079e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cb95f39afcf4dc086dce5d3cbd0da3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bf09bf99cbf424da63a273d34b3f9e1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"acdc3902c659406881f15c1f94883b82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc91935fbdd548249ad9fc30e2fee5ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"632cc96d3e4d4485aaee1862b82de679":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b5330b136d184040b640f16d61111a96","IPY_MODEL_fa6229819206404393b2cdc40e1769a0","IPY_MODEL_9b73eef89b9b4220b4fb74a950e05864"],"layout":"IPY_MODEL_d48e4e49a60e434c9c7844c821ce01dd"}},"b5330b136d184040b640f16d61111a96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94f253b8d5e446b4bbd2c788749f131a","placeholder":"​","style":"IPY_MODEL_d4140f71d202449989a36d349bf27683","value":"model.safetensors: 100%"}},"fa6229819206404393b2cdc40e1769a0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae9c892aca4e4439be0d73e33f7ff90b","max":345579424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97e239ff3f5649b4af95678d1f7aa7bc","value":345579424}},"9b73eef89b9b4220b4fb74a950e05864":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_096838738a714e85b314e109fea2c04b","placeholder":"​","style":"IPY_MODEL_7e1b4e6ff71c45bb962acb5ddd7f3bf6","value":" 346M/346M [00:01&lt;00:00, 461MB/s]"}},"d48e4e49a60e434c9c7844c821ce01dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94f253b8d5e446b4bbd2c788749f131a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4140f71d202449989a36d349bf27683":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae9c892aca4e4439be0d73e33f7ff90b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97e239ff3f5649b4af95678d1f7aa7bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"096838738a714e85b314e109fea2c04b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e1b4e6ff71c45bb962acb5ddd7f3bf6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d2313132e6674b529dba59fe3370ad2e","2fdbdef4b2404ae195e204912e062796","a756a705cd1b49699933d2b1479f698d","5204cdf393d34fd8b37cd81fc7e11965","fa424d9dcba84653b469fa51a643e724","7342536762574bb493687d446153df56","89759abfdf79458e95790bf93827f7ba","d14a438c372346cfaf0288e215815c25","267f52717cdc48cbb037d447b45170c7","dc2f7d317bd24802a171f04dadd68671","1fc9f46cc5da4ae4a92c87aeebd2ed6d","f1e9e523f88142c7938337e53c272393","2c3e4fa9b2c84511af9aa25e482b792a","99bdd1baeb3b4648970e6c86ce10704f","ca961b8cc3da412ea4cb3a3c89a4c907","9adf2f29417c44c5bd1d40758ad97318","03296dff503a44549f4be3e05078e95f","35b10d5327e34d8b837dbe5b080079e2","2cb95f39afcf4dc086dce5d3cbd0da3e","2bf09bf99cbf424da63a273d34b3f9e1","acdc3902c659406881f15c1f94883b82","bc91935fbdd548249ad9fc30e2fee5ee","632cc96d3e4d4485aaee1862b82de679","b5330b136d184040b640f16d61111a96","fa6229819206404393b2cdc40e1769a0","9b73eef89b9b4220b4fb74a950e05864","d48e4e49a60e434c9c7844c821ce01dd","94f253b8d5e446b4bbd2c788749f131a","d4140f71d202449989a36d349bf27683","ae9c892aca4e4439be0d73e33f7ff90b","97e239ff3f5649b4af95678d1f7aa7bc","096838738a714e85b314e109fea2c04b","7e1b4e6ff71c45bb962acb5ddd7f3bf6"]},"collapsed":true,"cellView":"form","id":"l1VSXMG3OMLn","executionInfo":{"status":"ok","timestamp":1761145230133,"user_tz":-420,"elapsed":47630,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"efce2b06-c2b0-45aa-a133-2ea619eededa"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","CASME II KEY FRAME SEQUENCE ViT WITH FACE-AWARE PREPROCESSING\n","============================================================\n","\n","[1] Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully\n","\n","[2] Importing required libraries...\n","CASME II Key Frame Sequence ViT - Face-Aware Preprocessing Infrastructure\n","============================================================\n","Loading CASME II v8 preprocessing metadata...\n","Dataset variant: KFS\n","Processing date: 2025-10-19T08:17:23.905019\n","Preprocessing method: face_bbox_expansion_all_directions\n","Total images processed: 765\n","Face detection rate: 100.00%\n","Target size: 224x224px\n","BBox expansion: 20px (all directions)\n","\n","Dataset split information:\n","  Train samples: 603\n","  Validation samples: 78\n","  Test samples: 84\n","Using ViT-Base Patch16 for fine-grained micro-expression analysis\n","\n","==================================================\n","EXPERIMENT CONFIGURATION - KEY FRAME SEQUENCE FACE-AWARE\n","==================================================\n","Dataset: v8 Key Frame Sequence with Face-Aware Preprocessing\n","Frame strategy: onset, apex, offset (3 frames per video)\n","Training approach: Frame-level independent learning for late fusion\n","Loss Function: Focal Loss\n","  Gamma: 2.0\n","  Alpha Weights (per-class): [0.055, 0.069, 0.098, 0.107, 0.109, 0.219, 0.343]\n","  Alpha Sum Validation: 1.000\n","ViT Model: google/vit-base-patch16-224-in21k\n","Patch Size: 16px\n","Input Resolution: 224x224px (face-centered with bbox expansion)\n","Image Format: Grayscale converted to RGB (3-channel)\n","==================================================\n","\n","Device: cuda\n","GPU: NVIDIA L4 (23.8 GB)\n","GPU optimization enabled for NVIDIA L4\n","Moderate dataset configuration: Batch size 8 (optimal for 603 samples)\n","Iterations per epoch: 75 (~75 iterations per epoch)\n","RAM preload workers: 32 (parallel image loading)\n","\n","Loading v8 class distribution...\n","\n","v8 Train distribution: [237, 150, 75, 63, 60, 15, 3]\n","v8 Val distribution: [30, 18, 9, 9, 6, 3, 3]\n","v8 Test distribution: [30, 21, 12, 9, 9, 3, 0]\n","Applied Focal Loss alpha weights: [0.055 0.069 0.098 0.107 0.109 0.219 0.343]\n","Alpha weights sum: 1.000\n","\n","ViT Configuration Summary:\n","  Model: google/vit-base-patch16-224-in21k\n","  Input size: 224px\n","  Learning rate: 2e-05 (optimized for small batches)\n","  Batch size: 8 (optimal for moderate dataset)\n","  Dropout rate: 0.3 (increased regularization)\n","  Dataset version: v8\n","  Frame strategy: key_frame_sequence\n","  Frame types: ['onset', 'apex', 'offset']\n","  Training approach: frame_level_independent\n","  Inference strategy: late_fusion\n","  Preprocessing: face_aware_bbox_expansion\n","  Train frames: 603\n","  Architecture: Simplified 768->256->7 for regularization\n","\n","Setting up ViT Image Processor for 224px input...\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2313132e6674b529dba59fe3370ad2e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ViT Image Processor configured for 224px native resolution\n","\n","Dataset paths:\n","Train: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8/train\n","Validation: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8/val\n","Test: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8/test\n","\n","ViT CASME II architecture validation...\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1e9e523f88142c7938337e53c272393"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"632cc96d3e4d4485aaee1862b82de679"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["ViT feature dimension: 768\n","ViT CASME II Simplified: 768 -> 256 -> 7\n","Dropout rate: 0.3 (increased for regularization)\n","Validation successful: Output shape torch.Size([1, 7])\n","Expected tokens for 224px with patch16: 196 tokens\n","\n","============================================================\n","CASME II KEY FRAME SEQUENCE ViT CONFIGURATION COMPLETE\n","============================================================\n","Loss Configuration:\n","  Function: Optimized Focal Loss\n","  Gamma: 2.0\n","  Per-class Alpha: [0.055, 0.069, 0.098, 0.107, 0.109, 0.219, 0.343]\n","  Alpha Sum: 1.000\n","\n","Model Configuration:\n","  Architecture: google/vit-base-patch16-224-in21k\n","  Patch Size: 16px\n","  Input Resolution: 224px (native)\n","  Expected Tokens: 196\n","  Classification Head: 768 -> 256 -> 7 (simplified)\n","\n","Dataset Configuration:\n","  Version: v8\n","  Classes: 7\n","  Frame strategy: key_frame_sequence\n","  Frame types: ['onset', 'apex', 'offset']\n","  Training approach: frame_level_independent\n","  Inference strategy: late_fusion\n","  Weight Optimization: Per-class Alpha\n","\n","Training Configuration:\n","  Train samples: 603 frames\n","  Validation samples: 78 frames\n","  Test samples: 84 frames\n","  Batch size: 8\n","  Learning rate: 2e-05\n","  Dropout rate: 0.3\n","\n","Next: Cell 2 - Dataset Loading and Training Pipeline\n"]}],"source":["# @title Cell 1: CASME II Key Frame Sequence ViT Infrastructure Configuration\n","\n","# File: 06_01_ViT_CASME2_KFS_Cell1.py\n","# Location: experiments/06_01_ViT_CASME2-KFS-PREP.ipynb\n","# Purpose: ViT-Base for CASME II micro-expression recognition with key frame sequence strategy and face-aware preprocessing\n","\n","from google.colab import drive\n","print(\"=\" * 60)\n","print(\"CASME II KEY FRAME SEQUENCE ViT WITH FACE-AWARE PREPROCESSING\")\n","print(\"=\" * 60)\n","print(\"\\n[1] Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Google Drive mounted successfully\")\n","\n","print(\"\\n[2] Importing required libraries...\")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import timm\n","import json\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import time\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Project paths configuration\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","DATASET_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/preprocessed_v8\"\n","CHECKPOINT_ROOT = f\"{PROJECT_ROOT}/models/06_01_vit_casme2_kfs_prep\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/06_01_vit_casme2_kfs_prep\"\n","\n","# Load CASME II v8 preprocessing metadata\n","PREPROCESSING_SUMMARY = f\"{DATASET_ROOT}/preprocessing_summary.json\"\n","\n","print(\"CASME II Key Frame Sequence ViT - Face-Aware Preprocessing Infrastructure\")\n","print(\"=\" * 60)\n","\n","# Validate preprocessing metadata exists\n","if not os.path.exists(PREPROCESSING_SUMMARY):\n","    raise FileNotFoundError(f\"v8 preprocessing summary not found: {PREPROCESSING_SUMMARY}\")\n","\n","# Load v8 preprocessing metadata\n","print(\"Loading CASME II v8 preprocessing metadata...\")\n","with open(PREPROCESSING_SUMMARY, 'r') as f:\n","    preprocessing_info = json.load(f)\n","\n","print(f\"Dataset variant: {preprocessing_info['variant']}\")\n","print(f\"Processing date: {preprocessing_info['processing_date']}\")\n","print(f\"Preprocessing method: {preprocessing_info['preprocessing_method']}\")\n","print(f\"Total images processed: {preprocessing_info['total_processed']}\")\n","print(f\"Face detection rate: {preprocessing_info['face_detection_stats']['detection_rate']:.2%}\")\n","\n","# Extract preprocessing parameters\n","preproc_params = preprocessing_info['preprocessing_parameters']\n","print(f\"Target size: {preproc_params['target_size']}x{preproc_params['target_size']}px\")\n","print(f\"BBox expansion: {preproc_params['bbox_expansion']}px (all directions)\")\n","\n","# Display split information\n","print(f\"\\nDataset split information:\")\n","print(f\"  Train samples: {preprocessing_info['splits']['train']['total_images']}\")\n","print(f\"  Validation samples: {preprocessing_info['splits']['val']['total_images']}\")\n","print(f\"  Test samples: {preprocessing_info['splits']['test']['total_images']}\")\n","\n","# =====================================================\n","# EXPERIMENT CONFIGURATION - Key Frame Sequence with Face-Aware Preprocessing\n","# =====================================================\n","\n","# FOCAL LOSS CONFIGURATION - Toggle for experimentation\n","USE_FOCAL_LOSS = True  # Default: CrossEntropy, Set True to enable Focal Loss\n","FOCAL_LOSS_GAMMA = 2.0  # Focal loss focusing parameter (if enabled)\n","\n","# OPTIMIZED CLASS WEIGHTS CONFIGURATION\n","# CASME II classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","# v8 Train distribution: [237, 150, 75, 63, 60, 15, 3] - inverse sqrt frequency approach\n","\n","# CrossEntropy Loss - Optimized inverse square root frequency weights\n","CROSSENTROPY_CLASS_WEIGHTS = [1.00, 1.26, 1.78, 1.94, 1.99, 3.98, 8.90]\n","\n","# Focal Loss - Normalized per-class alpha values (sum = 1.0)\n","FOCAL_LOSS_ALPHA_WEIGHTS = [0.055, 0.069, 0.098, 0.107, 0.109, 0.219, 0.343]\n","\n","# ViT MODEL CONFIGURATION - Support patch16 and patch32\n","VIT_MODEL_VARIANT = 'patch16'  # Default: patch32, Options: 'patch16' or 'patch32'\n","\n","# Dynamic ViT model selection based on patch size\n","if VIT_MODEL_VARIANT == 'patch16':\n","    VIT_MODEL_NAME = 'google/vit-base-patch16-224-in21k'\n","    PATCH_SIZE = 16\n","    print(\"Using ViT-Base Patch16 for fine-grained micro-expression analysis\")\n","elif VIT_MODEL_VARIANT == 'patch32':\n","    VIT_MODEL_NAME = 'google/vit-base-patch32-224-in21k'\n","    PATCH_SIZE = 32\n","    print(\"Using ViT-Base Patch32 for efficient micro-expression recognition\")\n","else:\n","    raise ValueError(f\"Unsupported VIT_MODEL_VARIANT: {VIT_MODEL_VARIANT}\")\n","\n","# Display experiment configuration\n","print(\"\\n\" + \"=\" * 50)\n","print(\"EXPERIMENT CONFIGURATION - KEY FRAME SEQUENCE FACE-AWARE\")\n","print(\"=\" * 50)\n","print(f\"Dataset: v8 Key Frame Sequence with Face-Aware Preprocessing\")\n","print(f\"Frame strategy: onset, apex, offset (3 frames per video)\")\n","print(f\"Training approach: Frame-level independent learning for late fusion\")\n","print(f\"Loss Function: {'Focal Loss' if USE_FOCAL_LOSS else 'CrossEntropy Loss'}\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Alpha Weights (per-class): {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum Validation: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Class Weights (inverse sqrt freq): {CROSSENTROPY_CLASS_WEIGHTS}\")\n","print(f\"ViT Model: {VIT_MODEL_NAME}\")\n","print(f\"Patch Size: {PATCH_SIZE}px\")\n","print(f\"Input Resolution: 224x224px (face-centered with bbox expansion)\")\n","print(f\"Image Format: Grayscale converted to RGB (3-channel)\")\n","print(\"=\" * 50)\n","\n","# Enhanced GPU configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n","gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n","\n","print(f\"\\nDevice: {device}\")\n","print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n","\n","# Optimized batch size for moderate dataset (603 train samples)\n","# Using batch size 8 for better training stability with 603 samples\n","BATCH_SIZE = 8\n","NUM_WORKERS = 4\n","\n","if 'A100' in gpu_name or 'L4' in gpu_name:\n","    torch.backends.cudnn.benchmark = True\n","    print(f\"GPU optimization enabled for {gpu_name}\")\n","\n","print(f\"Moderate dataset configuration: Batch size {BATCH_SIZE} (optimal for 603 samples)\")\n","print(f\"Iterations per epoch: {603 // BATCH_SIZE} (~75 iterations per epoch)\")\n","\n","# RAM preloading workers (separate from DataLoader workers)\n","RAM_PRELOAD_WORKERS = 32\n","print(f\"RAM preload workers: {RAM_PRELOAD_WORKERS} (parallel image loading)\")\n","\n","# CASME II class mapping and analysis\n","CASME2_CLASSES = ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CASME2_CLASSES)}\n","\n","# Extract class distribution from v8 preprocessing metadata\n","print(\"\\nLoading v8 class distribution...\")\n","train_dist = preprocessing_info['splits']['train']['emotion_distribution']\n","val_dist = preprocessing_info['splits']['val']['emotion_distribution']\n","test_dist = preprocessing_info['splits']['test']['emotion_distribution']\n","\n","# Convert to ordered list matching CASME2_CLASSES\n","def emotion_dist_to_list(emotion_dict, class_names):\n","    \"\"\"Convert emotion distribution dict to ordered list\"\"\"\n","    return [emotion_dict.get(cls, 0) for cls in class_names]\n","\n","train_dist_list = emotion_dist_to_list(train_dist, CASME2_CLASSES)\n","val_dist_list = emotion_dist_to_list(val_dist, CASME2_CLASSES)\n","test_dist_list = emotion_dist_to_list(test_dist, CASME2_CLASSES)\n","\n","print(f\"\\nv8 Train distribution: {train_dist_list}\")\n","print(f\"v8 Val distribution: {val_dist_list}\")\n","print(f\"v8 Test distribution: {test_dist_list}\")\n","\n","# Apply optimized class weights based on loss function selection\n","if USE_FOCAL_LOSS:\n","    class_weights = torch.tensor(FOCAL_LOSS_ALPHA_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied Focal Loss alpha weights: {class_weights.cpu().numpy()}\")\n","    print(f\"Alpha weights sum: {class_weights.sum().item():.3f}\")\n","else:\n","    class_weights = torch.tensor(CROSSENTROPY_CLASS_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied CrossEntropy class weights: {class_weights.cpu().numpy()}\")\n","\n","# CASME II ViT Configuration for Key Frame Sequence with Face-Aware Preprocessing\n","# Optimized for moderate dataset (603 train samples) with class imbalance\n","CASME2_VIT_CONFIG = {\n","    # Architecture configuration - simplified for regularization\n","    'vit_model': VIT_MODEL_NAME,\n","    'patch_size': PATCH_SIZE,\n","    'input_size': 224,\n","    'num_classes': 7,\n","    'dropout_rate': 0.3,  # Increased for stronger regularization\n","    'expected_feature_dim': 768,\n","    'interpolate_pos_encoding': False,  # No interpolation needed for native 224px\n","\n","    # Training configuration - optimized for moderate dataset\n","    'learning_rate': 2e-5,  # Optimized for small batch size\n","    'weight_decay': 1e-5,\n","    'gradient_clip': 1.0,\n","    'num_epochs': 50,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'device': device,\n","\n","    # Scheduler configuration\n","    'scheduler_type': 'plateau',\n","    'scheduler_mode': 'max',\n","    'scheduler_factor': 0.5,\n","    'scheduler_patience': 5,  # Increased for stability\n","    'scheduler_min_lr': 1e-6,\n","    'scheduler_monitor': 'val_f1_macro',\n","\n","    # Loss configuration\n","    'use_focal_loss': USE_FOCAL_LOSS,\n","    'focal_loss_gamma': FOCAL_LOSS_GAMMA,\n","    'focal_loss_alpha_weights': FOCAL_LOSS_ALPHA_WEIGHTS,\n","    'crossentropy_class_weights': CROSSENTROPY_CLASS_WEIGHTS,\n","    'class_weights': class_weights,\n","\n","    # Evaluation configuration\n","    'use_macro_avg': True,\n","    'early_stopping': False,\n","    'save_best_f1': True,\n","    'save_strategy': 'best_only',\n","\n","    # v8 specific configuration\n","    'dataset_version': 'v8',\n","    'preprocessing_method': 'face_aware_bbox_expansion',\n","    'frame_strategy': 'key_frame_sequence',\n","    'frame_types': ['onset', 'apex', 'offset'],\n","    'image_format': 'grayscale_to_rgb',\n","    'bbox_expansion': preproc_params['bbox_expansion'],\n","    'face_detection_rate': preprocessing_info['face_detection_stats']['detection_rate'],\n","    'training_approach': 'frame_level_independent',\n","    'inference_strategy': 'late_fusion'\n","}\n","\n","print(f\"\\nViT Configuration Summary:\")\n","print(f\"  Model: {CASME2_VIT_CONFIG['vit_model']}\")\n","print(f\"  Input size: {CASME2_VIT_CONFIG['input_size']}px\")\n","print(f\"  Learning rate: {CASME2_VIT_CONFIG['learning_rate']} (optimized for small batches)\")\n","print(f\"  Batch size: {BATCH_SIZE} (optimal for moderate dataset)\")\n","print(f\"  Dropout rate: {CASME2_VIT_CONFIG['dropout_rate']} (increased regularization)\")\n","print(f\"  Dataset version: {CASME2_VIT_CONFIG['dataset_version']}\")\n","print(f\"  Frame strategy: {CASME2_VIT_CONFIG['frame_strategy']}\")\n","print(f\"  Frame types: {CASME2_VIT_CONFIG['frame_types']}\")\n","print(f\"  Training approach: {CASME2_VIT_CONFIG['training_approach']}\")\n","print(f\"  Inference strategy: {CASME2_VIT_CONFIG['inference_strategy']}\")\n","print(f\"  Preprocessing: {CASME2_VIT_CONFIG['preprocessing_method']}\")\n","print(f\"  Train frames: {preprocessing_info['splits']['train']['total_images']}\")\n","print(f\"  Architecture: Simplified 768->256->7 for regularization\")\n","\n","# =====================================================\n","# FOCAL LOSS IMPLEMENTATION\n","# =====================================================\n","\n","class OptimizedFocalLoss(nn.Module):\n","    \"\"\"\n","    Advanced Focal Loss implementation with per-class alpha support\n","    Paper: Focal Loss for Dense Object Detection (Lin et al., 2017)\n","    \"\"\"\n","\n","    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n","        super(OptimizedFocalLoss, self).__init__()\n","\n","        if alpha is not None:\n","            if isinstance(alpha, list):\n","                self.alpha = torch.tensor(alpha, dtype=torch.float32)\n","            else:\n","                self.alpha = alpha\n","\n","            alpha_sum = self.alpha.sum().item()\n","            if abs(alpha_sum - 1.0) > 0.01:\n","                print(f\"Warning: Alpha weights sum to {alpha_sum:.3f}, expected 1.0\")\n","        else:\n","            self.alpha = None\n","\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","\n","        if self.alpha is not None:\n","            if self.alpha.device != targets.device:\n","                self.alpha = self.alpha.to(targets.device)\n","            alpha_t = self.alpha.gather(0, targets)\n","        else:\n","            alpha_t = 1.0\n","\n","        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","\n","# ViT Architecture for CASME II Key Frame Sequence\n","class ViTCASME2Baseline(nn.Module):\n","    \"\"\"ViT baseline for CASME II micro-expression recognition - Simplified for regularization\"\"\"\n","\n","    def __init__(self, num_classes, dropout_rate=0.3):\n","        super(ViTCASME2Baseline, self).__init__()\n","\n","        from transformers import ViTModel\n","\n","        self.vit = ViTModel.from_pretrained(\n","            CASME2_VIT_CONFIG['vit_model'],\n","            add_pooling_layer=False\n","        )\n","\n","        # Enable fine-tuning\n","        for param in self.vit.parameters():\n","            param.requires_grad = True\n","\n","        self.vit_feature_dim = self.vit.config.hidden_size\n","\n","        print(f\"ViT feature dimension: {self.vit_feature_dim}\")\n","\n","        # Simplified classification head for regularization\n","        # Reduced complexity: 768 -> 256 -> 7 (instead of 768 -> 512 -> 128 -> 7)\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.vit_feature_dim, 256),\n","            nn.LayerNorm(256),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","        )\n","\n","        self.classifier = nn.Linear(256, num_classes)\n","\n","        print(f\"ViT CASME II Simplified: {self.vit_feature_dim} -> 256 -> {num_classes}\")\n","        print(f\"Dropout rate: {dropout_rate} (increased for regularization)\")\n","\n","    def forward(self, pixel_values):\n","        vit_outputs = self.vit(\n","            pixel_values=pixel_values,\n","            interpolate_pos_encoding=CASME2_VIT_CONFIG['interpolate_pos_encoding']\n","        )\n","\n","        vit_features = vit_outputs.last_hidden_state[:, 0]\n","        processed_features = self.classifier_layers(vit_features)\n","        output = self.classifier(processed_features)\n","\n","        return output\n","\n","# Optimizer and scheduler factory\n","def create_optimizer_scheduler_casme2(model, config):\n","    \"\"\"Create optimizer and scheduler for CASME II ViT training\"\"\"\n","\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=config['learning_rate'],\n","        weight_decay=config['weight_decay'],\n","        betas=(0.9, 0.999)\n","    )\n","\n","    if config['scheduler_type'] == 'plateau':\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer,\n","            mode=config['scheduler_mode'],\n","            factor=config['scheduler_factor'],\n","            patience=config['scheduler_patience'],\n","            min_lr=config['scheduler_min_lr']\n","        )\n","        print(f\"Scheduler: ReduceLROnPlateau monitoring {config['scheduler_monitor']}\")\n","    else:\n","        scheduler = None\n","\n","    return optimizer, scheduler\n","\n","# ViT Image Processor setup for 224px input\n","from transformers import ViTImageProcessor\n","\n","print(\"\\nSetting up ViT Image Processor for 224px input...\")\n","\n","vit_processor = ViTImageProcessor.from_pretrained(\n","    CASME2_VIT_CONFIG['vit_model'],\n","    do_resize=True,\n","    size={'height': 224, 'width': 224},\n","    do_normalize=True,\n","    do_rescale=True,\n","    do_center_crop=False\n",")\n","\n","# Transform functions for ViT\n","def vit_transform_train(image):\n","    \"\"\"Training transform with ViT Image Processor\"\"\"\n","    inputs = vit_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","def vit_transform_val(image):\n","    \"\"\"Validation transform with ViT Image Processor\"\"\"\n","    inputs = vit_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","print(\"ViT Image Processor configured for 224px native resolution\")\n","\n","# Custom Dataset class for CASME II v8 preprocessing\n","class CASME2Dataset(Dataset):\n","    \"\"\"Custom dataset class for CASME II v8 preprocessing with flat directory structure\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        # Load all images from flat directory structure\n","        print(f\"Loading {split} dataset from {split_path}...\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        if len(all_files) > 0:\n","            print(f\"Sample filename: {all_files[0]}\")\n","\n","        loaded_count = 0\n","        skipped_count = 0\n","\n","        for filename in sorted(all_files):\n","            # Extract emotion from filename - flexible approach\n","            # Try multiple patterns to handle different filename formats\n","            emotion_found = None\n","\n","            # Remove file extension first\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            # Pattern 1: Check if any emotion class appears in filename\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                loaded_count += 1\n","            else:\n","                skipped_count += 1\n","                if skipped_count <= 3:\n","                    print(f\"  Skipped (no emotion found): {filename}\")\n","\n","        print(f\"Loaded {len(self.images)} samples for {split} split\")\n","        if skipped_count > 0:\n","            print(f\"  Skipped {skipped_count} files (no recognizable emotion)\")\n","\n","        if len(self.images) == 0:\n","            print(f\"ERROR: No samples loaded! Check filename format and emotion labels.\")\n","            print(f\"Expected emotions in filenames: {CASME2_CLASSES}\")\n","\n","        self._print_distribution()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"  No samples to display distribution\")\n","            return\n","\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.images[idx]\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","# Create directories\n","os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/training_logs\", exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/evaluation_results\", exist_ok=True)\n","\n","print(f\"\\nDataset paths:\")\n","print(f\"Train: {DATASET_ROOT}/train\")\n","print(f\"Validation: {DATASET_ROOT}/val\")\n","print(f\"Test: {DATASET_ROOT}/test\")\n","\n","# Architecture validation\n","print(\"\\nViT CASME II architecture validation...\")\n","\n","try:\n","    test_model = ViTCASME2Baseline(num_classes=7, dropout_rate=0.3).to(device)\n","    test_input = torch.randn(1, 3, 224, 224).to(device)\n","    test_output = test_model(test_input)\n","\n","    expected_tokens = (224 // CASME2_VIT_CONFIG['patch_size']) ** 2\n","\n","    print(f\"Validation successful: Output shape {test_output.shape}\")\n","    print(f\"Expected tokens for 224px with patch{CASME2_VIT_CONFIG['patch_size']}: {expected_tokens} tokens\")\n","\n","    del test_model, test_input, test_output\n","    torch.cuda.empty_cache()\n","\n","except Exception as e:\n","    print(f\"Validation failed: {e}\")\n","\n","# Loss function factory\n","def create_criterion_casme2(weights, use_focal_loss=False, alpha_weights=None, gamma=2.0):\n","    \"\"\"\n","    Factory function to create loss criterion based on configuration\n","\n","    Args:\n","        weights: Class weights for CrossEntropy\n","        use_focal_loss: Whether to use Focal Loss or CrossEntropy\n","        alpha_weights: Per-class alpha weights for Focal Loss\n","        gamma: Focal loss gamma parameter\n","\n","    Returns:\n","        Loss function\n","    \"\"\"\n","    if use_focal_loss:\n","        print(f\"Using Optimized Focal Loss with gamma={gamma}\")\n","        if alpha_weights:\n","            print(f\"Per-class alpha weights: {alpha_weights}\")\n","            print(f\"Alpha sum: {sum(alpha_weights):.3f}\")\n","        return OptimizedFocalLoss(alpha=alpha_weights, gamma=gamma)\n","    else:\n","        print(f\"Using CrossEntropy Loss with optimized class weights\")\n","        print(f\"Class weights: {weights.cpu().numpy()}\")\n","        return nn.CrossEntropyLoss(weight=weights)\n","\n","# Global configuration for training pipeline\n","GLOBAL_CONFIG_CASME2 = {\n","    'device': device,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'num_classes': 7,\n","    'class_weights': class_weights,\n","    'class_names': CASME2_CLASSES,\n","    'class_to_idx': CLASS_TO_IDX,\n","    'transform_train': vit_transform_train,\n","    'transform_val': vit_transform_val,\n","    'vit_config': CASME2_VIT_CONFIG,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'results_root': RESULTS_ROOT,\n","    'dataset_root': DATASET_ROOT,\n","    'optimizer_scheduler_factory': create_optimizer_scheduler_casme2,\n","    'criterion_factory': create_criterion_casme2\n","}\n","\n","# Configuration validation and summary\n","print(\"\\n\" + \"=\" * 60)\n","print(\"CASME II KEY FRAME SEQUENCE ViT CONFIGURATION COMPLETE\")\n","print(\"=\" * 60)\n","\n","print(f\"Loss Configuration:\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Function: Optimized Focal Loss\")\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Per-class Alpha: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Function: CrossEntropy with Optimized Weights\")\n","    print(f\"  Class Weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","\n","print(f\"\\nModel Configuration:\")\n","print(f\"  Architecture: {VIT_MODEL_NAME}\")\n","print(f\"  Patch Size: {PATCH_SIZE}px\")\n","print(f\"  Input Resolution: 224px (native)\")\n","print(f\"  Expected Tokens: {(224 // PATCH_SIZE) ** 2}\")\n","print(f\"  Classification Head: 768 -> 256 -> 7 (simplified)\")\n","\n","print(f\"\\nDataset Configuration:\")\n","print(f\"  Version: {CASME2_VIT_CONFIG['dataset_version']}\")\n","print(f\"  Classes: {len(CASME2_CLASSES)}\")\n","print(f\"  Frame strategy: {CASME2_VIT_CONFIG['frame_strategy']}\")\n","print(f\"  Frame types: {CASME2_VIT_CONFIG['frame_types']}\")\n","print(f\"  Training approach: {CASME2_VIT_CONFIG['training_approach']}\")\n","print(f\"  Inference strategy: {CASME2_VIT_CONFIG['inference_strategy']}\")\n","print(f\"  Weight Optimization: {'Per-class Alpha' if USE_FOCAL_LOSS else 'Inverse Sqrt Frequency'}\")\n","\n","print(f\"\\nTraining Configuration:\")\n","print(f\"  Train samples: {preprocessing_info['splits']['train']['total_images']} frames\")\n","print(f\"  Validation samples: {preprocessing_info['splits']['val']['total_images']} frames\")\n","print(f\"  Test samples: {preprocessing_info['splits']['test']['total_images']} frames\")\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","print(f\"  Learning rate: {CASME2_VIT_CONFIG['learning_rate']}\")\n","print(f\"  Dropout rate: {CASME2_VIT_CONFIG['dropout_rate']}\")\n","\n","print(\"\\nNext: Cell 2 - Dataset Loading and Training Pipeline\")"]},{"cell_type":"code","source":["# @title Cell 2: CASME II Key Frame Sequence ViT Training Pipeline\n","\n","# File: 06_01_ViT_CASME2_KFS_Cell2.py\n","# Location: experiments/06_01_ViT_CASME2-KFS-PREP.ipynb\n","# Purpose: Enhanced training pipeline for CASME II Key Frame Sequence ViT with hardened checkpoint system\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing as mp\n","import shutil\n","import tempfile\n","\n","print(\"CASME II Key Frame Sequence ViT Training Pipeline\")\n","print(\"=\" * 70)\n","print(f\"Loss Function: {'Optimized Focal Loss' if CASME2_VIT_CONFIG['use_focal_loss'] else 'CrossEntropy Loss'}\")\n","if CASME2_VIT_CONFIG['use_focal_loss']:\n","    print(f\"Focal Loss Parameters:\")\n","    print(f\"  Gamma: {CASME2_VIT_CONFIG['focal_loss_gamma']}\")\n","    print(f\"  Per-class Alpha: {CASME2_VIT_CONFIG['focal_loss_alpha_weights']}\")\n","    print(f\"  Alpha Sum: {sum(CASME2_VIT_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","else:\n","    print(f\"CrossEntropy Parameters:\")\n","    print(f\"  Optimized Class Weights: {CASME2_VIT_CONFIG['crossentropy_class_weights']}\")\n","print(f\"Dataset version: {CASME2_VIT_CONFIG['dataset_version']}\")\n","print(f\"Frame strategy: {CASME2_VIT_CONFIG['frame_strategy']}\")\n","print(f\"Training approach: {CASME2_VIT_CONFIG['training_approach']}\")\n","print(f\"Training epochs: {CASME2_VIT_CONFIG['num_epochs']}\")\n","print(f\"Scheduler patience: {CASME2_VIT_CONFIG['scheduler_patience']}\")\n","\n","# Enhanced CASME II Dataset with optimized RAM caching\n","class CASME2DatasetTraining(Dataset):\n","    \"\"\"Enhanced CASME II dataset for training with RAM caching optimization\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","        self.cached_images = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading CASME II {split} dataset for training...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        if len(all_files) > 0:\n","            print(f\"Sample filename: {all_files[0]}\")\n","\n","        loaded_count = 0\n","        skipped_count = 0\n","\n","        for filename in sorted(all_files):\n","            # Extract emotion from filename - flexible approach\n","            emotion_found = None\n","\n","            # Remove file extension first\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            # Check if any emotion class appears in filename\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                loaded_count += 1\n","            else:\n","                skipped_count += 1\n","                if skipped_count <= 3:\n","                    print(f\"  Skipped (no emotion found): {filename}\")\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples\")\n","        if skipped_count > 0:\n","            print(f\"  Skipped {skipped_count} files (no recognizable emotion)\")\n","\n","        if len(self.images) == 0:\n","            print(f\"ERROR: No samples loaded! Check filename format and emotion labels.\")\n","            print(f\"Expected emotions in filenames: {CASME2_CLASSES}\")\n","\n","        self._print_distribution()\n","\n","        if self.use_ram_cache and len(self.images) > 0:\n","            self._preload_to_ram()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"  No samples to display distribution\")\n","            return\n","\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def _preload_to_ram(self):\n","        \"\"\"RAM preloading with parallel loading for training efficiency\"\"\"\n","        if len(self.images) == 0:\n","            print(f\"Skipping RAM preload: No images to load\")\n","            return\n","\n","        print(f\"Preloading {len(self.images)} {self.split} images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            \"\"\"Load single image with error handling\"\"\"\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (224, 224), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=f\"Loading {self.split} to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * 224 * 224 * 3 * 4 / 1e9\n","        print(f\"{self.split.upper()} RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (224, 224), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","# Enhanced metrics calculation with comprehensive error handling\n","def calculate_metrics_safe_robust(predictions, labels, class_names, average='macro'):\n","    \"\"\"\n","    Calculate metrics with enhanced error handling and validation\n","\n","    Args:\n","        predictions: numpy array or list of predicted class indices (NOT logits)\n","        labels: numpy array or list of true class indices\n","        class_names: list of class names\n","        average: averaging method for metrics\n","    \"\"\"\n","    try:\n","        # Convert to numpy arrays if needed\n","        if isinstance(predictions, torch.Tensor):\n","            predictions = predictions.cpu().numpy()\n","        else:\n","            predictions = np.array(predictions)\n","\n","        if isinstance(labels, torch.Tensor):\n","            labels = labels.cpu().numpy()\n","        else:\n","            labels = np.array(labels)\n","\n","        # Validate shapes\n","        if predictions.shape[0] != labels.shape[0]:\n","            raise ValueError(f\"Size mismatch: predictions {predictions.shape[0]} vs labels {labels.shape[0]}\")\n","\n","        # Ensure predictions are 1D (class indices, not one-hot)\n","        if len(predictions.shape) > 1:\n","            if predictions.shape[1] > 1:\n","                predictions = np.argmax(predictions, axis=1)\n","            else:\n","                predictions = predictions.flatten()\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(labels, predictions)\n","        precision, recall, f1, _ = precision_recall_fscore_support(\n","            labels, predictions,\n","            average=average,\n","            zero_division=0,\n","            labels=list(range(len(class_names)))\n","        )\n","\n","        return {\n","            'accuracy': float(accuracy),\n","            'precision': float(precision),\n","            'recall': float(recall),\n","            'f1_score': float(f1)\n","        }\n","    except Exception as e:\n","        print(f\"Warning: Metrics calculation error: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        return {\n","            'accuracy': 0.0,\n","            'precision': 0.0,\n","            'recall': 0.0,\n","            'f1_score': 0.0\n","        }\n","\n","# Enhanced training function with comprehensive error handling\n","def train_epoch(model, dataloader, criterion, optimizer, device, epoch, total_epochs):\n","    \"\"\"Enhanced training epoch with robust error handling\"\"\"\n","    model.train()\n","\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_labels = []\n","\n","    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs} [Train]\")\n","\n","    for batch_idx, (images, labels, filenames) in enumerate(progress_bar):\n","        try:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(images)\n","\n","            if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n","                print(f\"Warning: Invalid outputs detected at batch {batch_idx}\")\n","                continue\n","\n","            loss = criterion(outputs, labels)\n","\n","            if torch.isnan(loss) or torch.isinf(loss):\n","                print(f\"Warning: Invalid loss detected at batch {batch_idx}\")\n","                continue\n","\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), CASME2_VIT_CONFIG['gradient_clip'])\n","\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            predictions = torch.argmax(outputs, dim=1)\n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n","\n","        except Exception as e:\n","            print(f\"Error in training batch {batch_idx}: {e}\")\n","            continue\n","\n","    epoch_loss = running_loss / len(dataloader)\n","\n","    # Convert to numpy arrays for metrics calculation\n","    all_predictions = np.array(all_predictions)\n","    all_labels = np.array(all_labels)\n","\n","    # Calculate metrics - predictions are already class indices\n","    metrics = calculate_metrics_safe_robust(\n","        all_predictions,\n","        all_labels,\n","        CASME2_CLASSES,\n","        average='macro'\n","    )\n","\n","    return epoch_loss, metrics\n","\n","# Enhanced validation function\n","def validate_epoch(model, dataloader, criterion, device, epoch, total_epochs):\n","    \"\"\"Enhanced validation epoch with comprehensive metrics\"\"\"\n","    model.eval()\n","\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_labels = []\n","    all_filenames = []\n","\n","    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs} [Val]\")\n","\n","    with torch.no_grad():\n","        for batch_idx, (images, labels, filenames) in enumerate(progress_bar):\n","            try:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","\n","                outputs = model(images)\n","\n","                if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n","                    print(f\"Warning: Invalid outputs in validation batch {batch_idx}\")\n","                    continue\n","\n","                loss = criterion(outputs, labels)\n","\n","                if torch.isnan(loss) or torch.isinf(loss):\n","                    print(f\"Warning: Invalid loss in validation batch {batch_idx}\")\n","                    continue\n","\n","                running_loss += loss.item()\n","\n","                predictions = torch.argmax(outputs, dim=1)\n","                all_predictions.extend(predictions.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","                all_filenames.extend(filenames)\n","\n","                progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n","\n","            except Exception as e:\n","                print(f\"Error in validation batch {batch_idx}: {e}\")\n","                continue\n","\n","    epoch_loss = running_loss / len(dataloader)\n","\n","    # Convert to numpy arrays for metrics calculation\n","    all_predictions = np.array(all_predictions)\n","    all_labels = np.array(all_labels)\n","\n","    # Calculate metrics - predictions are already class indices\n","    metrics = calculate_metrics_safe_robust(\n","        all_predictions,\n","        all_labels,\n","        CASME2_CLASSES,\n","        average='macro'\n","    )\n","\n","    return epoch_loss, metrics, all_filenames\n","\n","# Hardened checkpoint saving with atomic writes\n","def save_checkpoint_robust(model, optimizer, scheduler, epoch, train_metrics, val_metrics,\n","                          checkpoint_root, best_metrics, config):\n","    \"\"\"Hardened checkpoint saving with atomic writes and validation\"\"\"\n","    try:\n","        checkpoint_data = {\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","            'train_metrics': train_metrics,\n","            'val_metrics': val_metrics,\n","            'best_metrics': best_metrics,\n","            'config': config\n","        }\n","\n","        temp_path = os.path.join(checkpoint_root, 'checkpoint_temp.pth')\n","        final_path = os.path.join(checkpoint_root, 'casme2_vit_kfs_best_f1.pth')\n","\n","        torch.save(checkpoint_data, temp_path)\n","\n","        test_load = torch.load(temp_path, map_location='cpu')\n","        if 'model_state_dict' not in test_load:\n","            raise ValueError(\"Checkpoint validation failed: missing model_state_dict\")\n","\n","        shutil.move(temp_path, final_path)\n","\n","        return final_path\n","\n","    except Exception as e:\n","        print(f\"Error saving checkpoint: {e}\")\n","        if os.path.exists(temp_path):\n","            os.remove(temp_path)\n","        return None\n","\n","# Safe JSON serialization helper\n","def safe_json_serialize(obj):\n","    \"\"\"Convert numpy/torch types to native Python types for JSON serialization\"\"\"\n","    if isinstance(obj, dict):\n","        return {k: safe_json_serialize(v) for k, v in obj.items()}\n","    elif isinstance(obj, list):\n","        return [safe_json_serialize(item) for item in obj]\n","    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n","        return int(obj)\n","    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n","        return float(obj)\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    elif torch.is_tensor(obj):\n","        return obj.cpu().numpy().tolist()\n","    else:\n","        return obj\n","\n","# Create enhanced datasets with RAM caching\n","print(\"\\nCreating CASME II training and validation datasets...\")\n","\n","train_dataset = CASME2DatasetTraining(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='train',\n","    transform=GLOBAL_CONFIG_CASME2['transform_train'],\n","    use_ram_cache=True\n",")\n","\n","val_dataset = CASME2DatasetTraining(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='val',\n","    transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","    use_ram_cache=True\n",")\n","\n","# Create data loaders\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=CASME2_VIT_CONFIG['batch_size'],\n","    shuffle=True,\n","    num_workers=CASME2_VIT_CONFIG['num_workers'],\n","    pin_memory=True,\n","    drop_last=False\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=CASME2_VIT_CONFIG['batch_size'],\n","    shuffle=False,\n","    num_workers=CASME2_VIT_CONFIG['num_workers'],\n","    pin_memory=True\n",")\n","\n","print(f\"\\nDataLoader configuration:\")\n","print(f\"  Train batches per epoch: {len(train_loader)}\")\n","print(f\"  Validation batches per epoch: {len(val_loader)}\")\n","print(f\"  Batch size: {CASME2_VIT_CONFIG['batch_size']}\")\n","print(f\"  Workers: {CASME2_VIT_CONFIG['num_workers']}\")\n","\n","# Initialize model, optimizer, criterion\n","print(\"\\nInitializing ViT model for CASME II Key Frame Sequence...\")\n","\n","model = ViTCASME2Baseline(\n","    num_classes=CASME2_VIT_CONFIG['num_classes'],\n","    dropout_rate=CASME2_VIT_CONFIG['dropout_rate']\n",").to(GLOBAL_CONFIG_CASME2['device'])\n","\n","optimizer, scheduler = GLOBAL_CONFIG_CASME2['optimizer_scheduler_factory'](\n","    model, CASME2_VIT_CONFIG\n",")\n","\n","criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","    CASME2_VIT_CONFIG['class_weights'],\n","    CASME2_VIT_CONFIG['use_focal_loss'],\n","    CASME2_VIT_CONFIG['focal_loss_alpha_weights'],\n","    CASME2_VIT_CONFIG['focal_loss_gamma']\n",")\n","\n","print(f\"Model initialized on {GLOBAL_CONFIG_CASME2['device']}\")\n","print(f\"Optimizer: AdamW with LR={CASME2_VIT_CONFIG['learning_rate']}\")\n","print(f\"Scheduler: ReduceLROnPlateau with patience={CASME2_VIT_CONFIG['scheduler_patience']}\")\n","\n","# Training history tracking\n","training_history = {\n","    'train_loss': [],\n","    'val_loss': [],\n","    'train_f1': [],\n","    'val_f1': [],\n","    'train_acc': [],\n","    'val_acc': [],\n","    'learning_rate': [],\n","    'epoch_time': []\n","}\n","\n","# Best metrics tracking\n","best_metrics = {\n","    'f1': 0.0,\n","    'loss': float('inf'),\n","    'accuracy': 0.0,\n","    'epoch': 0\n","}\n","\n","print(\"\\nStarting CASME II Key Frame Sequence ViT training...\")\n","print(f\"Training configuration: {CASME2_VIT_CONFIG['num_epochs']} epochs\")\n","print(\"=\" * 70)\n","\n","# Main training loop with hardened checkpoint system\n","start_time = time.time()\n","\n","for epoch in range(CASME2_VIT_CONFIG['num_epochs']):\n","    epoch_start_time = time.time()\n","    print(f\"\\nEpoch {epoch+1}/{CASME2_VIT_CONFIG['num_epochs']}\")\n","\n","    # Training phase\n","    train_loss, train_metrics = train_epoch(\n","        model, train_loader, criterion, optimizer,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_VIT_CONFIG['num_epochs']\n","    )\n","\n","    # Validation phase\n","    val_loss, val_metrics, val_filenames = validate_epoch(\n","        model, val_loader, criterion,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_VIT_CONFIG['num_epochs']\n","    )\n","\n","    # Update scheduler\n","    if scheduler:\n","        scheduler.step(val_metrics['f1_score'])\n","\n","    # Record training history\n","    epoch_time = time.time() - epoch_start_time\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    training_history['train_loss'].append(float(train_loss))\n","    training_history['val_loss'].append(float(val_loss))\n","    training_history['train_f1'].append(float(train_metrics['f1_score']))\n","    training_history['val_f1'].append(float(val_metrics['f1_score']))\n","    training_history['train_acc'].append(float(train_metrics['accuracy']))\n","    training_history['val_acc'].append(float(val_metrics['accuracy']))\n","    training_history['learning_rate'].append(float(current_lr))\n","    training_history['epoch_time'].append(float(epoch_time))\n","\n","    # Print epoch summary\n","    print(f\"Train - Loss: {train_loss:.4f}, F1: {train_metrics['f1_score']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n","    print(f\"Val   - Loss: {val_loss:.4f}, F1: {val_metrics['f1_score']:.4f}, Acc: {val_metrics['accuracy']:.4f}\")\n","    print(f\"Time  - Epoch: {epoch_time:.1f}s, LR: {current_lr:.2e}\")\n","\n","    # Enhanced multi-criteria checkpoint saving logic\n","    save_model = False\n","    improvement_reason = \"\"\n","\n","    if val_metrics['f1_score'] > best_metrics['f1']:\n","        save_model = True\n","        improvement_reason = \"Higher F1\"\n","    elif val_metrics['f1_score'] == best_metrics['f1']:\n","        if val_loss < best_metrics['loss']:\n","            save_model = True\n","            improvement_reason = \"Same F1, Lower Loss\"\n","        elif val_loss == best_metrics['loss'] and val_metrics['accuracy'] > best_metrics['accuracy']:\n","            save_model = True\n","            improvement_reason = \"Same F1&Loss, Higher Accuracy\"\n","\n","    if save_model:\n","        best_metrics['f1'] = val_metrics['f1_score']\n","        best_metrics['loss'] = val_loss\n","        best_metrics['accuracy'] = val_metrics['accuracy']\n","        best_metrics['epoch'] = epoch + 1\n","\n","        best_model_path = save_checkpoint_robust(\n","            model, optimizer, scheduler, epoch,\n","            train_metrics, val_metrics, GLOBAL_CONFIG_CASME2['checkpoint_root'],\n","            best_metrics, CASME2_VIT_CONFIG\n","        )\n","\n","        if best_model_path:\n","            print(f\"New best model: {improvement_reason} - F1: {best_metrics['f1']:.4f}\")\n","        else:\n","            print(f\"Warning: Failed to save checkpoint despite improvement\")\n","\n","    # Progress tracking\n","    elapsed_time = time.time() - start_time\n","    estimated_total = (elapsed_time / (epoch + 1)) * CASME2_VIT_CONFIG['num_epochs']\n","    remaining_time = estimated_total - elapsed_time\n","    progress_pct = ((epoch + 1) / CASME2_VIT_CONFIG['num_epochs']) * 100\n","\n","    print(f\"Progress: {progress_pct:.1f}% | Best F1: {best_metrics['f1']:.4f} | ETA: {remaining_time/60:.1f}min\")\n","\n","# Training completion\n","total_time = time.time() - start_time\n","actual_epochs = CASME2_VIT_CONFIG['num_epochs']\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II KEY FRAME SEQUENCE ViT TRAINING COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Training time: {total_time/60:.1f} minutes\")\n","print(f\"Epochs completed: {actual_epochs}\")\n","print(f\"Best validation F1: {best_metrics['f1']:.4f} (epoch {best_metrics['epoch']})\")\n","print(f\"Final train F1: {training_history['train_f1'][-1]:.4f}\")\n","print(f\"Final validation F1: {training_history['val_f1'][-1]:.4f}\")\n","\n","# Enhanced training documentation export\n","results_dir = GLOBAL_CONFIG_CASME2['results_root']\n","os.makedirs(f\"{results_dir}/training_logs\", exist_ok=True)\n","\n","training_history_path = f\"{results_dir}/training_logs/casme2_vit_kfs_training_history.json\"\n","\n","print(\"\\nExporting enhanced training documentation...\")\n","\n","try:\n","    training_summary = {\n","        'experiment_type': 'CASME2_ViT_KeyFrameSequence',\n","        'experiment_configuration': {\n","            'dataset_version': CASME2_VIT_CONFIG['dataset_version'],\n","            'frame_strategy': CASME2_VIT_CONFIG['frame_strategy'],\n","            'frame_types': CASME2_VIT_CONFIG['frame_types'],\n","            'training_approach': CASME2_VIT_CONFIG['training_approach'],\n","            'inference_strategy': CASME2_VIT_CONFIG['inference_strategy'],\n","            'loss_function': 'Optimized Focal Loss' if CASME2_VIT_CONFIG['use_focal_loss'] else 'CrossEntropy',\n","            'weight_approach': 'Per-class Alpha (sum=1.0)' if CASME2_VIT_CONFIG['use_focal_loss'] else 'Inverse Sqrt Frequency',\n","            'focal_loss_gamma': CASME2_VIT_CONFIG['focal_loss_gamma'],\n","            'focal_loss_alpha_weights': CASME2_VIT_CONFIG['focal_loss_alpha_weights'],\n","            'crossentropy_class_weights': CASME2_VIT_CONFIG['crossentropy_class_weights'],\n","            'vit_model': CASME2_VIT_CONFIG['vit_model'],\n","            'patch_size': CASME2_VIT_CONFIG['patch_size']\n","        },\n","        'training_history': safe_json_serialize(training_history),\n","        'best_val_f1': float(best_metrics['f1']),\n","        'best_val_loss': float(best_metrics['loss']),\n","        'best_val_accuracy': float(best_metrics['accuracy']),\n","        'best_epoch': int(best_metrics['epoch']),\n","        'total_epochs': int(actual_epochs),\n","        'total_time_minutes': float(total_time / 60),\n","        'average_epoch_time_seconds': float(np.mean(training_history['epoch_time'])),\n","        'config': safe_json_serialize(CASME2_VIT_CONFIG),\n","        'final_train_f1': float(training_history['train_f1'][-1]),\n","        'final_val_f1': float(training_history['val_f1'][-1]),\n","        'model_checkpoint': 'casme2_vit_kfs_best_f1.pth',\n","        'dataset_info': {\n","            'name': 'CASME_II',\n","            'version': CASME2_VIT_CONFIG['dataset_version'],\n","            'frame_strategy': CASME2_VIT_CONFIG['frame_strategy'],\n","            'frame_types': CASME2_VIT_CONFIG['frame_types'],\n","            'train_samples': len(train_dataset),\n","            'val_samples': len(val_dataset),\n","            'num_classes': 7,\n","            'class_names': CASME2_CLASSES\n","        },\n","        'architecture_info': {\n","            'model_type': 'ViTCASME2Baseline',\n","            'backbone': CASME2_VIT_CONFIG['vit_model'],\n","            'input_size': f\"{CASME2_VIT_CONFIG['input_size']}x{CASME2_VIT_CONFIG['input_size']}\",\n","            'patch_size': CASME2_VIT_CONFIG['patch_size'],\n","            'expected_tokens': (CASME2_VIT_CONFIG['input_size'] // CASME2_VIT_CONFIG['patch_size']) ** 2,\n","            'classification_head': '768->256->7'\n","        },\n","        'enhanced_features': {\n","            'hardened_checkpoint_system': True,\n","            'atomic_checkpoint_save': True,\n","            'checkpoint_validation': True,\n","            'model_output_validation': True,\n","            'enhanced_error_handling': True,\n","            'multi_criteria_checkpoint_logic': True,\n","            'memory_optimized_training': True,\n","            'ram_caching': True\n","        }\n","    }\n","\n","    with open(training_history_path, 'w') as f:\n","        json.dump(training_summary, f, indent=2)\n","\n","    print(f\"Enhanced training documentation saved successfully: {training_history_path}\")\n","    print(f\"Experiment details: {training_summary['experiment_configuration']['loss_function']} loss\")\n","    if CASME2_VIT_CONFIG['use_focal_loss']:\n","        print(f\"  Gamma: {CASME2_VIT_CONFIG['focal_loss_gamma']}, Alpha Sum: {sum(CASME2_VIT_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","    print(f\"Model variant: {CASME2_VIT_CONFIG['vit_model']}\")\n","    print(f\"Dataset version: {CASME2_VIT_CONFIG['dataset_version']}\")\n","\n","except Exception as e:\n","    print(f\"Warning: Could not save training documentation: {e}\")\n","    print(\"Training completed successfully, but documentation export failed\")\n","\n","# Enhanced memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\nNext: Cell 3 - CASME II Key Frame Sequence ViT Evaluation with Late Fusion\")\n","print(\"Enhanced training pipeline with hardened checkpoint system completed successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"cellView":"form","id":"ZGMlq5QUFl1k","executionInfo":{"status":"ok","timestamp":1761145853817,"user_tz":-420,"elapsed":623681,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"df7ee28d-119f-45e3-a72b-86591e089a25"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II Key Frame Sequence ViT Training Pipeline\n","======================================================================\n","Loss Function: Optimized Focal Loss\n","Focal Loss Parameters:\n","  Gamma: 2.0\n","  Per-class Alpha: [0.055, 0.069, 0.098, 0.107, 0.109, 0.219, 0.343]\n","  Alpha Sum: 1.000\n","Dataset version: v8\n","Frame strategy: key_frame_sequence\n","Training approach: frame_level_independent\n","Training epochs: 50\n","Scheduler patience: 5\n","\n","Creating CASME II training and validation datasets...\n","Loading CASME II train dataset for training...\n","Found 603 image files in directory\n","Sample filename: sub01_EP02_01f_onset_happiness.jpg\n","Loaded 603 CASME II train samples\n","  others: 237 samples (39.3%)\n","  disgust: 150 samples (24.9%)\n","  happiness: 75 samples (12.4%)\n","  repression: 63 samples (10.4%)\n","  surprise: 60 samples (10.0%)\n","  sadness: 15 samples (2.5%)\n","  fear: 3 samples (0.5%)\n","Preloading 603 train images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading train to RAM: 100%|██████████| 603/603 [00:08<00:00, 69.11it/s] \n"]},{"output_type":"stream","name":"stdout","text":["TRAIN RAM caching completed: 603/603 images, ~0.36GB\n","Loading CASME II val dataset for training...\n","Found 78 image files in directory\n","Sample filename: sub01_EP03_02_apex_others.jpg\n","Loaded 78 CASME II val samples\n","  others: 30 samples (38.5%)\n","  disgust: 18 samples (23.1%)\n","  happiness: 9 samples (11.5%)\n","  repression: 9 samples (11.5%)\n","  surprise: 6 samples (7.7%)\n","  sadness: 3 samples (3.8%)\n","  fear: 3 samples (3.8%)\n","Preloading 78 val images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading val to RAM: 100%|██████████| 78/78 [00:01<00:00, 47.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["VAL RAM caching completed: 78/78 images, ~0.05GB\n","\n","DataLoader configuration:\n","  Train batches per epoch: 76\n","  Validation batches per epoch: 10\n","  Batch size: 8\n","  Workers: 4\n","\n","Initializing ViT model for CASME II Key Frame Sequence...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["ViT feature dimension: 768\n","ViT CASME II Simplified: 768 -> 256 -> 7\n","Dropout rate: 0.3 (increased for regularization)\n","Scheduler: ReduceLROnPlateau monitoring val_f1_macro\n","Using Optimized Focal Loss with gamma=2.0\n","Per-class alpha weights: [0.055, 0.069, 0.098, 0.107, 0.109, 0.219, 0.343]\n","Alpha sum: 1.000\n","Model initialized on cuda\n","Optimizer: AdamW with LR=2e-05\n","Scheduler: ReduceLROnPlateau with patience=5\n","\n","Starting CASME II Key Frame Sequence ViT training...\n","Training configuration: 50 epochs\n","======================================================================\n","\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.35it/s, loss=0.1395]\n","Epoch 1/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.79it/s, loss=0.0443]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0919, F1: 0.2604, Acc: 0.4345\n","Val   - Loss: 0.1381, F1: 0.1691, Acc: 0.3333\n","Time  - Epoch: 12.6s, LR: 2.00e-05\n","New best model: Higher F1 - F1: 0.1691\n","Progress: 2.0% | Best F1: 0.1691 | ETA: 12.8min\n","\n","Epoch 2/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.74it/s, loss=0.0343]\n","Epoch 2/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.72it/s, loss=0.0560]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0505, F1: 0.4819, Acc: 0.6484\n","Val   - Loss: 0.1507, F1: 0.1358, Acc: 0.2949\n","Time  - Epoch: 11.9s, LR: 2.00e-05\n","Progress: 4.0% | Best F1: 0.1691 | ETA: 11.0min\n","\n","Epoch 3/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.65it/s, loss=0.0182]\n","Epoch 3/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.97it/s, loss=0.0541]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0235, F1: 0.7861, Acc: 0.8358\n","Val   - Loss: 0.1626, F1: 0.1211, Acc: 0.2821\n","Time  - Epoch: 12.0s, LR: 2.00e-05\n","Progress: 6.0% | Best F1: 0.1691 | ETA: 10.3min\n","\n","Epoch 4/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.52it/s, loss=0.0039]\n","Epoch 4/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.96it/s, loss=0.0479]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0113, F1: 0.9219, Acc: 0.9171\n","Val   - Loss: 0.1741, F1: 0.1307, Acc: 0.2949\n","Time  - Epoch: 12.3s, LR: 2.00e-05\n","Progress: 8.0% | Best F1: 0.1691 | ETA: 9.9min\n","\n","Epoch 5/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.44it/s, loss=0.0016]\n","Epoch 5/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.78it/s, loss=0.0742]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0048, F1: 0.9709, Acc: 0.9668\n","Val   - Loss: 0.1946, F1: 0.1070, Acc: 0.2564\n","Time  - Epoch: 12.4s, LR: 2.00e-05\n","Progress: 10.0% | Best F1: 0.1691 | ETA: 9.6min\n","\n","Epoch 6/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.45it/s, loss=0.0016]\n","Epoch 6/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.75it/s, loss=0.0662]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0031, F1: 0.9850, Acc: 0.9801\n","Val   - Loss: 0.1924, F1: 0.1082, Acc: 0.2564\n","Time  - Epoch: 12.4s, LR: 2.00e-05\n","Progress: 12.0% | Best F1: 0.1691 | ETA: 9.4min\n","\n","Epoch 7/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.52it/s, loss=0.0142]\n","Epoch 7/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.86it/s, loss=0.0782]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0029, F1: 0.9792, Acc: 0.9751\n","Val   - Loss: 0.1963, F1: 0.1206, Acc: 0.2821\n","Time  - Epoch: 12.3s, LR: 1.00e-05\n","Progress: 14.0% | Best F1: 0.1691 | ETA: 9.1min\n","\n","Epoch 8/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.59it/s, loss=0.0009]\n","Epoch 8/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.92it/s, loss=0.0789]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0016, F1: 0.9943, Acc: 0.9934\n","Val   - Loss: 0.1976, F1: 0.1290, Acc: 0.2949\n","Time  - Epoch: 12.1s, LR: 1.00e-05\n","Progress: 16.0% | Best F1: 0.1691 | ETA: 8.8min\n","\n","Epoch 9/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.63it/s, loss=0.0003]\n","Epoch 9/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.04it/s, loss=0.0671]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0010, F1: 0.9936, Acc: 0.9934\n","Val   - Loss: 0.1962, F1: 0.1398, Acc: 0.3205\n","Time  - Epoch: 12.1s, LR: 1.00e-05\n","Progress: 18.0% | Best F1: 0.1691 | ETA: 8.6min\n","\n","Epoch 10/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.63it/s, loss=0.0007]\n","Epoch 10/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.88it/s, loss=0.0771]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9958, Acc: 0.9967\n","Val   - Loss: 0.2048, F1: 0.1354, Acc: 0.2949\n","Time  - Epoch: 12.1s, LR: 1.00e-05\n","Progress: 20.0% | Best F1: 0.1691 | ETA: 8.3min\n","\n","Epoch 11/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.60it/s, loss=0.0005]\n","Epoch 11/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.98it/s, loss=0.0782]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9930, Acc: 0.9934\n","Val   - Loss: 0.2083, F1: 0.1311, Acc: 0.2949\n","Time  - Epoch: 12.1s, LR: 1.00e-05\n","Progress: 22.0% | Best F1: 0.1691 | ETA: 8.1min\n","\n","Epoch 12/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.57it/s, loss=0.0004]\n","Epoch 12/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.96it/s, loss=0.0726]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0007, F1: 0.9924, Acc: 0.9934\n","Val   - Loss: 0.2084, F1: 0.1365, Acc: 0.3205\n","Time  - Epoch: 12.2s, LR: 1.00e-05\n","Progress: 24.0% | Best F1: 0.1691 | ETA: 7.9min\n","\n","Epoch 13/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.55it/s, loss=0.0008]\n","Epoch 13/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.72it/s, loss=0.0743]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0011, F1: 0.9916, Acc: 0.9917\n","Val   - Loss: 0.2083, F1: 0.1200, Acc: 0.2821\n","Time  - Epoch: 12.2s, LR: 5.00e-06\n","Progress: 26.0% | Best F1: 0.1691 | ETA: 7.7min\n","\n","Epoch 14/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.54it/s, loss=0.0003]\n","Epoch 14/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.87it/s, loss=0.0783]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 0.9979, Acc: 0.9983\n","Val   - Loss: 0.2120, F1: 0.1365, Acc: 0.3205\n","Time  - Epoch: 12.2s, LR: 5.00e-06\n","Progress: 28.0% | Best F1: 0.1691 | ETA: 7.5min\n","\n","Epoch 15/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.54it/s, loss=0.0002]\n","Epoch 15/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.53it/s, loss=0.0834]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 0.9986, Acc: 0.9983\n","Val   - Loss: 0.2155, F1: 0.1362, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 5.00e-06\n","Progress: 30.0% | Best F1: 0.1691 | ETA: 7.2min\n","\n","Epoch 16/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.57it/s, loss=0.0001]\n","Epoch 16/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.68it/s, loss=0.0794]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 0.9986, Acc: 0.9983\n","Val   - Loss: 0.2141, F1: 0.1386, Acc: 0.3333\n","Time  - Epoch: 12.2s, LR: 5.00e-06\n","Progress: 32.0% | Best F1: 0.1691 | ETA: 7.0min\n","\n","Epoch 17/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.60it/s, loss=0.0003]\n","Epoch 17/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.81it/s, loss=0.0804]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 0.9986, Acc: 0.9983\n","Val   - Loss: 0.2155, F1: 0.1313, Acc: 0.3077\n","Time  - Epoch: 12.1s, LR: 5.00e-06\n","Progress: 34.0% | Best F1: 0.1691 | ETA: 6.8min\n","\n","Epoch 18/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.58it/s, loss=0.0001]\n","Epoch 18/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.19it/s, loss=0.0797]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2152, F1: 0.1313, Acc: 0.3077\n","Time  - Epoch: 12.1s, LR: 5.00e-06\n","Progress: 36.0% | Best F1: 0.1691 | ETA: 6.6min\n","\n","Epoch 19/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.58it/s, loss=0.0001]\n","Epoch 19/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.21it/s, loss=0.0843]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2172, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.1s, LR: 2.50e-06\n","Progress: 38.0% | Best F1: 0.1691 | ETA: 6.4min\n","\n","Epoch 20/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 20/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.58it/s, loss=0.0001]\n","Epoch 20/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.35it/s, loss=0.0825]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2166, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.1s, LR: 2.50e-06\n","Progress: 40.0% | Best F1: 0.1691 | ETA: 6.2min\n","\n","Epoch 21/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 21/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.55it/s, loss=0.0000]\n","Epoch 21/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.68it/s, loss=0.0829]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2173, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 2.50e-06\n","Progress: 42.0% | Best F1: 0.1691 | ETA: 6.0min\n","\n","Epoch 22/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 22/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.55it/s, loss=0.0001]\n","Epoch 22/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.51it/s, loss=0.0822]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2172, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 2.50e-06\n","Progress: 44.0% | Best F1: 0.1691 | ETA: 5.8min\n","\n","Epoch 23/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 23/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.54it/s, loss=0.0003]\n","Epoch 23/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.90it/s, loss=0.0836]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2179, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 2.50e-06\n","Progress: 46.0% | Best F1: 0.1691 | ETA: 5.5min\n","\n","Epoch 24/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 24/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.57it/s, loss=0.0001]\n","Epoch 24/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.78it/s, loss=0.0820]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2187, F1: 0.1313, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 2.50e-06\n","Progress: 48.0% | Best F1: 0.1691 | ETA: 5.3min\n","\n","Epoch 25/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 25/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.57it/s, loss=0.0001]\n","Epoch 25/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.88it/s, loss=0.0837]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2189, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.25e-06\n","Progress: 50.0% | Best F1: 0.1691 | ETA: 5.1min\n","\n","Epoch 26/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 26/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.56it/s, loss=0.0001]\n","Epoch 26/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.24it/s, loss=0.0836]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2193, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.25e-06\n","Progress: 52.0% | Best F1: 0.1691 | ETA: 4.9min\n","\n","Epoch 27/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 27/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.56it/s, loss=0.0001]\n","Epoch 27/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.14it/s, loss=0.0830]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2194, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.25e-06\n","Progress: 54.0% | Best F1: 0.1691 | ETA: 4.7min\n","\n","Epoch 28/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 28/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.56it/s, loss=0.0004]\n","Epoch 28/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.68it/s, loss=0.0832]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2198, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.25e-06\n","Progress: 56.0% | Best F1: 0.1691 | ETA: 4.5min\n","\n","Epoch 29/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 29/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.56it/s, loss=0.0001]\n","Epoch 29/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.30it/s, loss=0.0834]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2201, F1: 0.1344, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.25e-06\n","Progress: 58.0% | Best F1: 0.1691 | ETA: 4.3min\n","\n","Epoch 30/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 30/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.54it/s, loss=0.0003]\n","Epoch 30/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.78it/s, loss=0.0835]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2204, F1: 0.1313, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.25e-06\n","Progress: 60.0% | Best F1: 0.1691 | ETA: 4.1min\n","\n","Epoch 31/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 31/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.57it/s, loss=0.0000]\n","Epoch 31/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.06it/s, loss=0.0832]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2206, F1: 0.1313, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 62.0% | Best F1: 0.1691 | ETA: 3.9min\n","\n","Epoch 32/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 32/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.55it/s, loss=0.0007]\n","Epoch 32/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.90it/s, loss=0.0831]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2207, F1: 0.1313, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 64.0% | Best F1: 0.1691 | ETA: 3.7min\n","\n","Epoch 33/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 33/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.57it/s, loss=0.0001]\n","Epoch 33/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.76it/s, loss=0.0830]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2211, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 66.0% | Best F1: 0.1691 | ETA: 3.5min\n","\n","Epoch 34/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 34/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.55it/s, loss=0.0001]\n","Epoch 34/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.05it/s, loss=0.0835]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2216, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 68.0% | Best F1: 0.1691 | ETA: 3.3min\n","\n","Epoch 35/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 35/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.57it/s, loss=0.0002]\n","Epoch 35/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.04it/s, loss=0.0841]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2216, F1: 0.1344, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 70.0% | Best F1: 0.1691 | ETA: 3.1min\n","\n","Epoch 36/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 36/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.57it/s, loss=0.0001]\n","Epoch 36/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.16it/s, loss=0.0840]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2217, F1: 0.1344, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 72.0% | Best F1: 0.1691 | ETA: 2.9min\n","\n","Epoch 37/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 37/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.58it/s, loss=0.0001]\n","Epoch 37/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.08it/s, loss=0.0840]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2218, F1: 0.1344, Acc: 0.3077\n","Time  - Epoch: 12.1s, LR: 1.00e-06\n","Progress: 74.0% | Best F1: 0.1691 | ETA: 2.7min\n","\n","Epoch 38/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 38/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.57it/s, loss=0.0024]\n","Epoch 38/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.11it/s, loss=0.0845]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2223, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 76.0% | Best F1: 0.1691 | ETA: 2.5min\n","\n","Epoch 39/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 39/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.55it/s, loss=0.0004]\n","Epoch 39/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.78it/s, loss=0.0851]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2231, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 78.0% | Best F1: 0.1691 | ETA: 2.2min\n","\n","Epoch 40/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 40/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.54it/s, loss=0.0002]\n","Epoch 40/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.23it/s, loss=0.0849]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2231, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 80.0% | Best F1: 0.1691 | ETA: 2.0min\n","\n","Epoch 41/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 41/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.55it/s, loss=0.0000]\n","Epoch 41/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.91it/s, loss=0.0847]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2230, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 82.0% | Best F1: 0.1691 | ETA: 1.8min\n","\n","Epoch 42/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 42/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.55it/s, loss=0.0001]\n","Epoch 42/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.66it/s, loss=0.0848]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2235, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 84.0% | Best F1: 0.1691 | ETA: 1.6min\n","\n","Epoch 43/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 43/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.56it/s, loss=0.0001]\n","Epoch 43/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.86it/s, loss=0.0850]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2237, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 86.0% | Best F1: 0.1691 | ETA: 1.4min\n","\n","Epoch 44/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 44/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.56it/s, loss=0.0001]\n","Epoch 44/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.17it/s, loss=0.0856]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2243, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 88.0% | Best F1: 0.1691 | ETA: 1.2min\n","\n","Epoch 45/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 45/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.56it/s, loss=0.0002]\n","Epoch 45/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.06it/s, loss=0.0859]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2247, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 90.0% | Best F1: 0.1691 | ETA: 1.0min\n","\n","Epoch 46/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 46/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.59it/s, loss=0.0000]\n","Epoch 46/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.01it/s, loss=0.0864]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2248, F1: 0.1313, Acc: 0.3077\n","Time  - Epoch: 12.1s, LR: 1.00e-06\n","Progress: 92.0% | Best F1: 0.1691 | ETA: 0.8min\n","\n","Epoch 47/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 47/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.59it/s, loss=0.0001]\n","Epoch 47/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.18it/s, loss=0.0862]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2250, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.1s, LR: 1.00e-06\n","Progress: 94.0% | Best F1: 0.1691 | ETA: 0.6min\n","\n","Epoch 48/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 48/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.59it/s, loss=0.0000]\n","Epoch 48/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.61it/s, loss=0.0869]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2254, F1: 0.1313, Acc: 0.3077\n","Time  - Epoch: 12.1s, LR: 1.00e-06\n","Progress: 96.0% | Best F1: 0.1691 | ETA: 0.4min\n","\n","Epoch 49/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 49/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.56it/s, loss=0.0000]\n","Epoch 49/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 16.88it/s, loss=0.0869]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2259, F1: 0.1313, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 98.0% | Best F1: 0.1691 | ETA: 0.2min\n","\n","Epoch 50/50\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 50/50 [Train]: 100%|██████████| 76/76 [00:11<00:00,  6.56it/s, loss=0.0000]\n","Epoch 50/50 [Val]: 100%|██████████| 10/10 [00:00<00:00, 17.03it/s, loss=0.0865]"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2263, F1: 0.1328, Acc: 0.3077\n","Time  - Epoch: 12.2s, LR: 1.00e-06\n","Progress: 100.0% | Best F1: 0.1691 | ETA: 0.0min\n","\n","======================================================================\n","CASME II KEY FRAME SEQUENCE ViT TRAINING COMPLETED\n","======================================================================\n","Training time: 10.2 minutes\n","Epochs completed: 50\n","Best validation F1: 0.1691 (epoch 1)\n","Final train F1: 1.0000\n","Final validation F1: 0.1328\n","\n","Exporting enhanced training documentation...\n","Warning: Could not save training documentation: Object of type device is not JSON serializable\n","Training completed successfully, but documentation export failed\n","\n","Next: Cell 3 - CASME II Key Frame Sequence ViT Evaluation with Late Fusion\n","Enhanced training pipeline with hardened checkpoint system completed successfully!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title Cell 3: CASME II ViT Evaluation with Dual Dataset Support\n","\n","# File: 06_01_ViT_CASME2_KFS_Cell3.py\n","# Location: experiments/06_01_ViT_CASME2-KFS-PREP.ipynb\n","# Purpose: Comprehensive evaluation framework with support for AF (v7) and KFS (v8) with late fusion\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from datetime import datetime\n","from collections import defaultdict\n","\n","from sklearn.metrics import (\n","    accuracy_score, precision_recall_fscore_support,\n","    classification_report, confusion_matrix,\n","    roc_curve, auc\n",")\n","from sklearn.preprocessing import label_binarize\n","from concurrent.futures import ThreadPoolExecutor\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =====================================================\n","# DUAL DATASET EVALUATION CONFIGURATION\n","# =====================================================\n","# Configure which test datasets to evaluate:\n","# 'v7' = Apex Frame preprocessing (28 samples, frame-level evaluation)\n","# 'v8' = Key Frame Sequence preprocessing (84 frames -> 28 videos with late fusion)\n","\n","EVALUATE_DATASETS = ['v7', 'v8']  # Can be ['v7'], ['v8'], or ['v7', 'v8']\n","\n","print(\"CASME II ViT Evaluation Framework with Dual Dataset Support\")\n","print(\"=\" * 60)\n","print(f\"Datasets to evaluate: {EVALUATE_DATASETS}\")\n","print(\"=\" * 60)\n","\n","# =====================================================\n","# DATASET CONFIGURATION FUNCTION\n","# =====================================================\n","\n","def get_test_dataset_config(version, project_root):\n","    \"\"\"\n","    Get test dataset configuration based on version\n","\n","    Args:\n","        version: 'v7' (AF) or 'v8' (KFS)\n","        project_root: Project root path\n","\n","    Returns:\n","        dict: Configuration for selected test dataset\n","    \"\"\"\n","    if version == 'v7':\n","        config = {\n","            'version': 'v7',\n","            'variant': 'AF',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/preprocessed_v7\",\n","            'preprocessing_summary': 'preprocessing_summary.json',\n","            'description': 'Apex Frame with Face-Aware Preprocessing',\n","            'expected_samples': 28,\n","            'frame_strategy': 'apex_frame',\n","            'evaluation_mode': 'frame_level',\n","            'aggregation': None\n","        }\n","    elif version == 'v8':\n","        config = {\n","            'version': 'v8',\n","            'variant': 'KFS',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/preprocessed_v8\",\n","            'preprocessing_summary': 'preprocessing_summary.json',\n","            'description': 'Key Frame Sequence with Face-Aware Preprocessing',\n","            'expected_frames': 84,\n","            'expected_videos': 28,\n","            'frame_strategy': 'key_frame_sequence',\n","            'frame_types': ['onset', 'apex', 'offset'],\n","            'evaluation_mode': 'video_level',\n","            'aggregation': 'late_fusion'\n","        }\n","    else:\n","        raise ValueError(f\"Invalid version: {version}. Must be 'v7' or 'v8'\")\n","\n","    return config\n","\n","# =====================================================\n","# VIDEO ID EXTRACTION FOR KFS LATE FUSION\n","# =====================================================\n","\n","def extract_video_id_from_filename(filename):\n","    \"\"\"\n","    Extract video ID from KFS filename by removing frame type suffix\n","\n","    Expected format: sub01_EP02_01f_happiness_onset.jpg\n","    Video ID: sub01_EP02_01f_happiness\n","\n","    Args:\n","        filename: Image filename with frame type\n","\n","    Returns:\n","        str: Video ID without frame type\n","    \"\"\"\n","    # Remove file extension\n","    name_without_ext = filename.rsplit('.', 1)[0]\n","\n","    # Remove frame type suffix (onset, apex, offset)\n","    for frame_type in ['onset', 'apex', 'offset']:\n","        if name_without_ext.endswith(f'_{frame_type}'):\n","            video_id = name_without_ext.rsplit(f'_{frame_type}', 1)[0]\n","            return video_id\n","\n","    # If no frame type found, return as is\n","    return name_without_ext\n","\n","# =====================================================\n","# ENHANCED TEST DATASET CLASS\n","# =====================================================\n","\n","class CASME2DatasetEvaluation(Dataset):\n","    \"\"\"Enhanced CASME II test dataset with comprehensive evaluation support\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","        self.emotions = []\n","        self.video_ids = []\n","        self.cached_images = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading CASME II {split} dataset for evaluation...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        loaded_count = 0\n","\n","        for filename in sorted(all_files):\n","            emotion_found = None\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                video_id = extract_video_id_from_filename(filename)\n","\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                self.emotions.append(emotion_found)\n","                self.video_ids.append(video_id)\n","                loaded_count += 1\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples for evaluation\")\n","        self._print_evaluation_distribution()\n","\n","        if self.use_ram_cache and len(self.images) > 0:\n","            self._preload_to_ram_evaluation()\n","\n","    def _print_evaluation_distribution(self):\n","        \"\"\"Print comprehensive distribution for evaluation analysis\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"No test samples found!\")\n","            return\n","\n","        label_counts = {}\n","        unique_videos = set(self.video_ids)\n","\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        print(\"Test set class distribution:\")\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","        print(f\"Unique video IDs: {len(unique_videos)}\")\n","\n","        missing_classes = []\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i not in label_counts:\n","                missing_classes.append(class_name)\n","\n","        if missing_classes:\n","            print(f\"Missing classes in test set: {missing_classes}\")\n","\n","    def _preload_to_ram_evaluation(self):\n","        \"\"\"RAM preloading optimized for evaluation\"\"\"\n","        if len(self.images) == 0:\n","            return\n","\n","        print(f\"Preloading {len(self.images)} test images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (224, 224), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=\"Loading test to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * 224 * 224 * 3 * 4 / 1e9\n","        print(f\"TEST RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (224, 224), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx], self.video_ids[idx]\n","\n","# =====================================================\n","# MODEL LOADING FUNCTION\n","# =====================================================\n","\n","def load_trained_model_casme2(checkpoint_path, device):\n","    \"\"\"Load trained model from checkpoint\"\"\"\n","    print(f\"Loading trained model from: {checkpoint_path}\")\n","\n","    if not os.path.exists(checkpoint_path):\n","        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n","\n","    checkpoint = torch.load(checkpoint_path, map_location=device)\n","\n","    model = ViTCASME2Baseline(\n","        num_classes=7,\n","        dropout_rate=CASME2_VIT_CONFIG['dropout_rate']\n","    ).to(device)\n","\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","\n","    training_info = {\n","        'best_epoch': checkpoint.get('epoch', 0),\n","        'best_val_f1': checkpoint.get('val_metrics', {}).get('f1_score', 0.0),\n","        'best_val_accuracy': checkpoint.get('val_metrics', {}).get('accuracy', 0.0),\n","        'config': checkpoint.get('config', {})\n","    }\n","\n","    print(f\"Model loaded successfully from epoch {training_info['best_epoch']}\")\n","    print(f\"Training best val F1: {training_info['best_val_f1']:.4f}\")\n","\n","    return model, training_info\n","\n","# =====================================================\n","# FRAME-LEVEL INFERENCE (for AF v7)\n","# =====================================================\n","\n","def run_frame_level_inference(model, dataloader, device):\n","    \"\"\"\n","    Run frame-level inference for Apex Frame evaluation\n","    Returns predictions, probabilities, labels for each frame\n","    \"\"\"\n","    model.eval()\n","\n","    all_predictions = []\n","    all_probabilities = []\n","    all_labels = []\n","    all_filenames = []\n","    all_video_ids = []\n","\n","    print(\"Running frame-level inference...\")\n","\n","    inference_start = time.time()\n","\n","    with torch.no_grad():\n","        for images, labels, filenames, video_ids in tqdm(dataloader, desc=\"Frame-level inference\"):\n","            images = images.to(device)\n","\n","            outputs = model(images)\n","            probabilities = torch.softmax(outputs, dim=1)\n","            predictions = torch.argmax(probabilities, dim=1)\n","\n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_probabilities.extend(probabilities.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","            all_filenames.extend(filenames)\n","            all_video_ids.extend(video_ids)\n","\n","    inference_time = time.time() - inference_start\n","\n","    results = {\n","        'predictions': np.array(all_predictions),\n","        'probabilities': np.array(all_probabilities),\n","        'labels': np.array(all_labels),\n","        'filenames': all_filenames,\n","        'video_ids': all_video_ids,\n","        'inference_time': inference_time,\n","        'evaluation_mode': 'frame_level'\n","    }\n","\n","    print(f\"Frame-level inference completed: {len(all_predictions)} predictions in {inference_time:.2f}s\")\n","\n","    return results\n","\n","# =====================================================\n","# VIDEO-LEVEL INFERENCE WITH LATE FUSION (for KFS v8)\n","# =====================================================\n","\n","def run_video_level_inference_late_fusion(model, dataloader, device):\n","    \"\"\"\n","    Run video-level inference with late fusion for Key Frame Sequence\n","\n","    Process:\n","    1. Get frame-level probabilities for all frames (onset, apex, offset)\n","    2. Group frames by video ID\n","    3. Average probabilities across frames per video\n","    4. Get final video-level prediction from averaged probabilities\n","\n","    Returns video-level predictions (28 videos from 84 frames)\n","    \"\"\"\n","    model.eval()\n","\n","    # Step 1: Get frame-level probabilities\n","    frame_data = defaultdict(lambda: {\n","        'probabilities': [],\n","        'filenames': [],\n","        'label': None\n","    })\n","\n","    print(\"Running frame-level inference for late fusion...\")\n","\n","    inference_start = time.time()\n","\n","    with torch.no_grad():\n","        for images, labels, filenames, video_ids in tqdm(dataloader, desc=\"Processing frames\"):\n","            images = images.to(device)\n","\n","            outputs = model(images)\n","            probabilities = torch.softmax(outputs, dim=1)\n","\n","            # Group by video ID\n","            for i in range(len(video_ids)):\n","                video_id = video_ids[i]\n","                prob = probabilities[i].cpu().numpy()\n","                label = labels[i].item()\n","                filename = filenames[i]\n","\n","                frame_data[video_id]['probabilities'].append(prob)\n","                frame_data[video_id]['filenames'].append(filename)\n","                frame_data[video_id]['label'] = label\n","\n","    # Step 2: Late fusion - average probabilities per video\n","    print(f\"Applying late fusion across {len(frame_data)} videos...\")\n","\n","    video_predictions = []\n","    video_probabilities = []\n","    video_labels = []\n","    video_ids = []\n","    video_frame_counts = []\n","\n","    for video_id, data in sorted(frame_data.items()):\n","        # Average probabilities across all frames of this video\n","        frame_probs = np.array(data['probabilities'])\n","        avg_prob = np.mean(frame_probs, axis=0)\n","\n","        # Final prediction from averaged probabilities\n","        video_pred = np.argmax(avg_prob)\n","\n","        video_predictions.append(video_pred)\n","        video_probabilities.append(avg_prob)\n","        video_labels.append(data['label'])\n","        video_ids.append(video_id)\n","        video_frame_counts.append(len(data['probabilities']))\n","\n","    inference_time = time.time() - inference_start\n","\n","    results = {\n","        'predictions': np.array(video_predictions),\n","        'probabilities': np.array(video_probabilities),\n","        'labels': np.array(video_labels),\n","        'video_ids': video_ids,\n","        'frame_counts': video_frame_counts,\n","        'frame_data': dict(frame_data),\n","        'inference_time': inference_time,\n","        'evaluation_mode': 'video_level_late_fusion',\n","        'total_frames': sum(video_frame_counts),\n","        'total_videos': len(video_ids)\n","    }\n","\n","    print(f\"Late fusion completed: {len(video_ids)} video predictions from {sum(video_frame_counts)} frames\")\n","    print(f\"Inference time: {inference_time:.2f}s\")\n","\n","    return results\n","\n","# =====================================================\n","# COMPREHENSIVE METRICS CALCULATION\n","# =====================================================\n","\n","def calculate_comprehensive_metrics(inference_results):\n","    \"\"\"Calculate comprehensive metrics for evaluation\"\"\"\n","\n","    predictions = inference_results['predictions']\n","    labels = inference_results['labels']\n","    probabilities = inference_results['probabilities']\n","\n","    unique_labels = np.unique(labels)\n","    available_classes = [CASME2_CLASSES[i] for i in unique_labels]\n","    missing_classes = [cls for i, cls in enumerate(CASME2_CLASSES) if i not in unique_labels]\n","\n","    # Overall metrics\n","    accuracy = accuracy_score(labels, predictions)\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions,\n","        average='macro',\n","        zero_division=0,\n","        labels=list(range(len(CASME2_CLASSES)))\n","    )\n","\n","    # Per-class metrics\n","    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n","        labels, predictions,\n","        average=None,\n","        zero_division=0,\n","        labels=list(range(len(CASME2_CLASSES)))\n","    )\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(labels, predictions, labels=list(range(len(CASME2_CLASSES))))\n","\n","    # ROC and AUC calculation\n","    labels_binarized = label_binarize(labels, classes=list(range(len(CASME2_CLASSES))))\n","\n","    auc_scores = {}\n","    fpr_dict = {}\n","    tpr_dict = {}\n","\n","    for i, class_name in enumerate(CASME2_CLASSES):\n","        if i in unique_labels and np.sum(labels_binarized[:, i]) > 0:\n","            fpr, tpr, _ = roc_curve(labels_binarized[:, i], probabilities[:, i])\n","            auc_score = auc(fpr, tpr)\n","            auc_scores[class_name] = float(auc_score)\n","            fpr_dict[class_name] = fpr.tolist()\n","            tpr_dict[class_name] = tpr.tolist()\n","        else:\n","            auc_scores[class_name] = 0.0\n","            fpr_dict[class_name] = []\n","            tpr_dict[class_name] = []\n","\n","    macro_auc = np.mean([score for score in auc_scores.values() if score > 0])\n","\n","    # Compile results\n","    evaluation_results = {\n","        'evaluation_metadata': {\n","            'dataset': 'CASME_II',\n","            'model_type': 'ViTCASME2Baseline',\n","            'evaluation_timestamp': datetime.now().isoformat(),\n","            'evaluation_mode': inference_results['evaluation_mode'],\n","            'test_samples': len(predictions),\n","            'class_names': CASME2_CLASSES,\n","            'available_classes': available_classes,\n","            'missing_classes': missing_classes\n","        },\n","\n","        'overall_performance': {\n","            'accuracy': float(accuracy),\n","            'macro_precision': float(precision),\n","            'macro_recall': float(recall),\n","            'macro_f1': float(f1),\n","            'macro_auc': float(macro_auc)\n","        },\n","\n","        'per_class_performance': {},\n","\n","        'confusion_matrix': cm.tolist(),\n","\n","        'roc_analysis': {\n","            'auc_scores': auc_scores,\n","            'fpr_curves': fpr_dict,\n","            'tpr_curves': tpr_dict\n","        },\n","\n","        'inference_performance': {\n","            'total_time_seconds': float(inference_results['inference_time']),\n","            'average_time_ms_per_sample': float(inference_results['inference_time'] * 1000 / len(predictions))\n","        }\n","    }\n","\n","    # Add video-level specific info if applicable\n","    if 'total_frames' in inference_results:\n","        evaluation_results['kfs_late_fusion_info'] = {\n","            'total_frames': inference_results['total_frames'],\n","            'total_videos': inference_results['total_videos'],\n","            'aggregation_method': 'probability_averaging',\n","            'frames_per_video': inference_results['frame_counts']\n","        }\n","\n","    # Per-class performance details\n","    for i, class_name in enumerate(CASME2_CLASSES):\n","        evaluation_results['per_class_performance'][class_name] = {\n","            'precision': float(precision_per_class[i]),\n","            'recall': float(recall_per_class[i]),\n","            'f1_score': float(f1_per_class[i]),\n","            'support': int(support_per_class[i]),\n","            'auc': auc_scores[class_name],\n","            'in_test_set': i in unique_labels\n","        }\n","\n","    return evaluation_results\n","\n","# =====================================================\n","# WRONG PREDICTIONS ANALYSIS\n","# =====================================================\n","\n","def analyze_wrong_predictions(inference_results):\n","    \"\"\"Analyze wrong predictions for error analysis\"\"\"\n","\n","    predictions = inference_results['predictions']\n","    labels = inference_results['labels']\n","\n","    if 'filenames' in inference_results:\n","        identifiers = inference_results['filenames']\n","    elif 'video_ids' in inference_results:\n","        identifiers = inference_results['video_ids']\n","    else:\n","        identifiers = [f\"sample_{i}\" for i in range(len(predictions))]\n","\n","    wrong_predictions = []\n","    wrong_by_class = defaultdict(list)\n","    confusion_patterns = defaultdict(int)\n","\n","    for i in range(len(predictions)):\n","        if predictions[i] != labels[i]:\n","            true_class = CASME2_CLASSES[labels[i]]\n","            pred_class = CASME2_CLASSES[predictions[i]]\n","\n","            wrong_pred = {\n","                'identifier': identifiers[i],\n","                'true_class': true_class,\n","                'predicted_class': pred_class,\n","                'true_label': int(labels[i]),\n","                'predicted_label': int(predictions[i])\n","            }\n","\n","            wrong_predictions.append(wrong_pred)\n","            wrong_by_class[true_class].append(wrong_pred)\n","\n","            pattern = f\"{true_class} -> {pred_class}\"\n","            confusion_patterns[pattern] += 1\n","\n","    error_summary = {cls: len(wrong_by_class[cls]) for cls in CASME2_CLASSES}\n","\n","    results = {\n","        'analysis_metadata': {\n","            'total_samples': len(predictions),\n","            'total_wrong_predictions': len(wrong_predictions),\n","            'overall_error_rate': (len(wrong_predictions) / len(predictions)) * 100\n","        },\n","        'wrong_predictions': wrong_predictions,\n","        'wrong_predictions_by_class': dict(wrong_by_class),\n","        'error_summary': error_summary,\n","        'confusion_patterns': dict(confusion_patterns)\n","    }\n","\n","    return results\n","\n","# =====================================================\n","# SAVE EVALUATION RESULTS\n","# =====================================================\n","\n","def save_evaluation_results(evaluation_results, wrong_predictions_results, results_dir, test_version):\n","    \"\"\"Save comprehensive evaluation results\"\"\"\n","    os.makedirs(results_dir, exist_ok=True)\n","\n","    results_file = f\"{results_dir}/casme2_vit_evaluation_results_{test_version}.json\"\n","    with open(results_file, 'w') as f:\n","        json.dump(evaluation_results, f, indent=2, default=str)\n","\n","    wrong_predictions_file = f\"{results_dir}/casme2_vit_wrong_predictions_{test_version}.json\"\n","    with open(wrong_predictions_file, 'w') as f:\n","        json.dump(wrong_predictions_results, f, indent=2, default=str)\n","\n","    print(f\"Evaluation results saved:\")\n","    print(f\"  Main results: {os.path.basename(results_file)}\")\n","    print(f\"  Wrong predictions: {os.path.basename(wrong_predictions_file)}\")\n","\n","    return results_file, wrong_predictions_file\n","\n","# =====================================================\n","# MAIN EVALUATION EXECUTION\n","# =====================================================\n","\n","all_evaluation_results = {}\n","\n","for dataset_version in EVALUATE_DATASETS:\n","    print(\"\\n\" + \"=\" * 70)\n","    print(f\"EVALUATING DATASET: {dataset_version.upper()}\")\n","    print(\"=\" * 70)\n","\n","    try:\n","        # Get dataset configuration\n","        test_config = get_test_dataset_config(dataset_version, PROJECT_ROOT)\n","\n","        print(f\"\\nTest Dataset Configuration:\")\n","        print(f\"  Version: {test_config['version']}\")\n","        print(f\"  Variant: {test_config['variant']}\")\n","        print(f\"  Description: {test_config['description']}\")\n","        print(f\"  Frame strategy: {test_config['frame_strategy']}\")\n","        print(f\"  Evaluation mode: {test_config['evaluation_mode']}\")\n","        if 'aggregation' in test_config and test_config['aggregation']:\n","            print(f\"  Aggregation: {test_config['aggregation']}\")\n","        print(f\"  Dataset path: {test_config['dataset_path']}\")\n","\n","        # Create test dataset\n","        print(f\"\\nCreating CASME II test dataset from {test_config['variant']}...\")\n","        test_dataset = CASME2DatasetEvaluation(\n","            dataset_root=test_config['dataset_path'],\n","            split='test',\n","            transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","            use_ram_cache=True\n","        )\n","\n","        if len(test_dataset) == 0:\n","            raise ValueError(f\"No test samples found for {dataset_version}!\")\n","\n","        test_loader = DataLoader(\n","            test_dataset,\n","            batch_size=CASME2_VIT_CONFIG['batch_size'],\n","            shuffle=False,\n","            num_workers=CASME2_VIT_CONFIG['num_workers'],\n","            pin_memory=True\n","        )\n","\n","        # Load trained model\n","        checkpoint_path = f\"{GLOBAL_CONFIG_CASME2['checkpoint_root']}/casme2_vit_kfs_best_f1.pth\"\n","        model, training_info = load_trained_model_casme2(checkpoint_path, GLOBAL_CONFIG_CASME2['device'])\n","\n","        # Run inference based on evaluation mode\n","        if test_config['evaluation_mode'] == 'frame_level':\n","            print(f\"\\nRunning frame-level evaluation for {test_config['variant']}...\")\n","            inference_results = run_frame_level_inference(model, test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","        elif test_config['evaluation_mode'] == 'video_level':\n","            print(f\"\\nRunning video-level evaluation with late fusion for {test_config['variant']}...\")\n","            inference_results = run_video_level_inference_late_fusion(model, test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","        else:\n","            raise ValueError(f\"Unknown evaluation mode: {test_config['evaluation_mode']}\")\n","\n","        # Calculate comprehensive metrics\n","        evaluation_results = calculate_comprehensive_metrics(inference_results)\n","\n","        # Analyze wrong predictions\n","        wrong_predictions_results = analyze_wrong_predictions(inference_results)\n","\n","        # Add training information\n","        evaluation_results['training_information'] = training_info\n","        evaluation_results['test_configuration'] = test_config\n","\n","        # Save results\n","        results_dir = f\"{GLOBAL_CONFIG_CASME2['results_root']}/evaluation_results\"\n","        save_evaluation_results(\n","            evaluation_results, wrong_predictions_results, results_dir, test_config['version']\n","        )\n","\n","        # Store for comparison\n","        all_evaluation_results[dataset_version] = {\n","            'evaluation': evaluation_results,\n","            'wrong_predictions': wrong_predictions_results,\n","            'config': test_config\n","        }\n","\n","        # Display results\n","        print(\"\\n\" + \"=\" * 60)\n","        print(f\"EVALUATION RESULTS - {test_config['variant']} ({dataset_version})\")\n","        print(\"=\" * 60)\n","\n","        overall = evaluation_results['overall_performance']\n","        print(f\"\\nOverall Performance:\")\n","        print(f\"  Accuracy:  {overall['accuracy']:.4f}\")\n","        print(f\"  Precision: {overall['macro_precision']:.4f}\")\n","        print(f\"  Recall:    {overall['macro_recall']:.4f}\")\n","        print(f\"  F1 Score:  {overall['macro_f1']:.4f}\")\n","        print(f\"  AUC:       {overall['macro_auc']:.4f}\")\n","\n","        if 'kfs_late_fusion_info' in evaluation_results:\n","            fusion_info = evaluation_results['kfs_late_fusion_info']\n","            print(f\"\\nLate Fusion Info:\")\n","            print(f\"  Total frames processed: {fusion_info['total_frames']}\")\n","            print(f\"  Video-level predictions: {fusion_info['total_videos']}\")\n","            print(f\"  Aggregation method: {fusion_info['aggregation_method']}\")\n","\n","        print(f\"\\nPer-Class Performance:\")\n","        for class_name, metrics in evaluation_results['per_class_performance'].items():\n","            in_test = \"Present\" if metrics['in_test_set'] else \"Missing\"\n","            print(f\"  {class_name} [{in_test}]: F1={metrics['f1_score']:.4f}, \"\n","                  f\"Support={metrics['support']}\")\n","\n","        wrong_meta = wrong_predictions_results['analysis_metadata']\n","        print(f\"\\nWrong Predictions Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']} / {wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        print(f\"\\nInference Performance:\")\n","        print(f\"  Total time: {inference_results['inference_time']:.2f}s\")\n","        print(f\"  Speed: {evaluation_results['inference_performance']['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","    except Exception as e:\n","        print(f\"Evaluation failed for {dataset_version}: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        continue\n","\n","# =====================================================\n","# COMPARATIVE ANALYSIS (if both datasets evaluated)\n","# =====================================================\n","\n","if len(all_evaluation_results) == 2 and 'v7' in all_evaluation_results and 'v8' in all_evaluation_results:\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"COMPARATIVE ANALYSIS: AF (v7) vs KFS (v8)\")\n","    print(\"=\" * 70)\n","\n","    v7_results = all_evaluation_results['v7']['evaluation']\n","    v8_results = all_evaluation_results['v8']['evaluation']\n","\n","    print(\"\\nOverall Performance Comparison:\")\n","    print(f\"{'Metric':<20} {'AF (v7)':<15} {'KFS (v8)':<15} {'Difference':<15}\")\n","    print(\"-\" * 65)\n","\n","    metrics_to_compare = ['accuracy', 'macro_precision', 'macro_recall', 'macro_f1', 'macro_auc']\n","\n","    for metric in metrics_to_compare:\n","        v7_val = v7_results['overall_performance'][metric]\n","        v8_val = v8_results['overall_performance'][metric]\n","        diff = v8_val - v7_val\n","\n","        print(f\"{metric:<20} {v7_val:<15.4f} {v8_val:<15.4f} {diff:+.4f}\")\n","\n","    print(f\"\\nEvaluation Modes:\")\n","    print(f\"  AF (v7): {v7_results['evaluation_metadata']['evaluation_mode']}\")\n","    print(f\"  KFS (v8): {v8_results['evaluation_metadata']['evaluation_mode']}\")\n","\n","    if 'kfs_late_fusion_info' in v8_results:\n","        print(f\"\\nKFS Late Fusion Strategy:\")\n","        print(f\"  Frames used: {v8_results['kfs_late_fusion_info']['total_frames']}\")\n","        print(f\"  Video predictions: {v8_results['kfs_late_fusion_info']['total_videos']}\")\n","        print(f\"  Aggregation: {v8_results['kfs_late_fusion_info']['aggregation_method']}\")\n","\n","# Memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II ViT EVALUATION COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Evaluated datasets: {EVALUATE_DATASETS}\")\n","print(\"Next: Cell 4 - Generate confusion matrices and visualization\")"],"metadata":{"cellView":"form","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"kmUkZvxMRaSK","executionInfo":{"status":"ok","timestamp":1761145862142,"user_tz":-420,"elapsed":8311,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"c77ef414-1af0-4b2d-eeac-99b3379800e8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II ViT Evaluation Framework with Dual Dataset Support\n","============================================================\n","Datasets to evaluate: ['v7', 'v8']\n","============================================================\n","\n","======================================================================\n","EVALUATING DATASET: V7\n","======================================================================\n","\n","Test Dataset Configuration:\n","  Version: v7\n","  Variant: AF\n","  Description: Apex Frame with Face-Aware Preprocessing\n","  Frame strategy: apex_frame\n","  Evaluation mode: frame_level\n","  Dataset path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7\n","\n","Creating CASME II test dataset from AF...\n","Loading CASME II test dataset for evaluation...\n","Found 28 image files in directory\n","Loaded 28 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 10 samples (35.7%)\n","  disgust: 7 samples (25.0%)\n","  happiness: 4 samples (14.3%)\n","  repression: 3 samples (10.7%)\n","  surprise: 3 samples (10.7%)\n","  sadness: 1 samples (3.6%)\n","Unique video IDs: 28\n","Missing classes in test set: ['fear']\n","Preloading 28 test images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test to RAM: 100%|██████████| 28/28 [00:01<00:00, 23.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TEST RAM caching completed: 28/28 images, ~0.02GB\n","Loading trained model from: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/06_01_vit_casme2_kfs_prep/casme2_vit_kfs_best_f1.pth\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["ViT feature dimension: 768\n","ViT CASME II Simplified: 768 -> 256 -> 7\n","Dropout rate: 0.3 (increased for regularization)\n","Model loaded successfully from epoch 0\n","Training best val F1: 0.1691\n","\n","Running frame-level evaluation for AF...\n","Running frame-level inference...\n"]},{"output_type":"stream","name":"stderr","text":["Frame-level inference: 100%|██████████| 4/4 [00:00<00:00, 10.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Frame-level inference completed: 28 predictions in 0.40s\n","Evaluation results saved:\n","  Main results: casme2_vit_evaluation_results_v7.json\n","  Wrong predictions: casme2_vit_wrong_predictions_v7.json\n","\n","============================================================\n","EVALUATION RESULTS - AF (v7)\n","============================================================\n","\n","Overall Performance:\n","  Accuracy:  0.3571\n","  Precision: 0.1020\n","  Recall:    0.1612\n","  F1 Score:  0.1241\n","  AUC:       0.5185\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.5833, Support=10\n","  disgust [Present]: F1=0.2857, Support=7\n","  happiness [Present]: F1=0.0000, Support=4\n","  repression [Present]: F1=0.0000, Support=3\n","  surprise [Present]: F1=0.0000, Support=3\n","  sadness [Present]: F1=0.0000, Support=1\n","  fear [Missing]: F1=0.0000, Support=0\n","\n","Wrong Predictions Analysis:\n","  Total errors: 18 / 28\n","  Error rate: 64.29%\n","\n","Inference Performance:\n","  Total time: 0.40s\n","  Speed: 14.2 ms/sample\n","\n","======================================================================\n","EVALUATING DATASET: V8\n","======================================================================\n","\n","Test Dataset Configuration:\n","  Version: v8\n","  Variant: KFS\n","  Description: Key Frame Sequence with Face-Aware Preprocessing\n","  Frame strategy: key_frame_sequence\n","  Evaluation mode: video_level\n","  Aggregation: late_fusion\n","  Dataset path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8\n","\n","Creating CASME II test dataset from KFS...\n","Loading CASME II test dataset for evaluation...\n","Found 84 image files in directory\n","Loaded 84 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 30 samples (35.7%)\n","  disgust: 21 samples (25.0%)\n","  happiness: 12 samples (14.3%)\n","  repression: 9 samples (10.7%)\n","  surprise: 9 samples (10.7%)\n","  sadness: 3 samples (3.6%)\n","Unique video IDs: 84\n","Missing classes in test set: ['fear']\n","Preloading 84 test images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test to RAM: 100%|██████████| 84/84 [00:01<00:00, 56.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TEST RAM caching completed: 84/84 images, ~0.05GB\n","Loading trained model from: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/06_01_vit_casme2_kfs_prep/casme2_vit_kfs_best_f1.pth\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["ViT feature dimension: 768\n","ViT CASME II Simplified: 768 -> 256 -> 7\n","Dropout rate: 0.3 (increased for regularization)\n","Model loaded successfully from epoch 0\n","Training best val F1: 0.1691\n","\n","Running video-level evaluation with late fusion for KFS...\n","Running frame-level inference for late fusion...\n"]},{"output_type":"stream","name":"stderr","text":["Processing frames: 100%|██████████| 11/11 [00:00<00:00, 16.31it/s]"]},{"output_type":"stream","name":"stdout","text":["Applying late fusion across 84 videos...\n","Late fusion completed: 84 video predictions from 84 frames\n","Inference time: 0.68s\n","Evaluation results saved:\n","  Main results: casme2_vit_evaluation_results_v8.json\n","  Wrong predictions: casme2_vit_wrong_predictions_v8.json\n","\n","============================================================\n","EVALUATION RESULTS - KFS (v8)\n","============================================================\n","\n","Overall Performance:\n","  Accuracy:  0.3690\n","  Precision: 0.1062\n","  Recall:    0.1701\n","  F1 Score:  0.1296\n","  AUC:       0.5390\n","\n","Late Fusion Info:\n","  Total frames processed: 84\n","  Video-level predictions: 84\n","  Aggregation method: probability_averaging\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.5634, Support=30\n","  disgust [Present]: F1=0.3438, Support=21\n","  happiness [Present]: F1=0.0000, Support=12\n","  repression [Present]: F1=0.0000, Support=9\n","  surprise [Present]: F1=0.0000, Support=9\n","  sadness [Present]: F1=0.0000, Support=3\n","  fear [Missing]: F1=0.0000, Support=0\n","\n","Wrong Predictions Analysis:\n","  Total errors: 53 / 84\n","  Error rate: 63.10%\n","\n","Inference Performance:\n","  Total time: 0.68s\n","  Speed: 8.1 ms/sample\n","\n","======================================================================\n","COMPARATIVE ANALYSIS: AF (v7) vs KFS (v8)\n","======================================================================\n","\n","Overall Performance Comparison:\n","Metric               AF (v7)         KFS (v8)        Difference     \n","-----------------------------------------------------------------\n","accuracy             0.3571          0.3690          +0.0119\n","macro_precision      0.1020          0.1062          +0.0042\n","macro_recall         0.1612          0.1701          +0.0088\n","macro_f1             0.1241          0.1296          +0.0054\n","macro_auc            0.5185          0.5390          +0.0205\n","\n","Evaluation Modes:\n","  AF (v7): frame_level\n","  KFS (v8): video_level_late_fusion\n","\n","KFS Late Fusion Strategy:\n","  Frames used: 84\n","  Video predictions: 84\n","  Aggregation: probability_averaging\n","\n","======================================================================\n","CASME II ViT EVALUATION COMPLETED\n","======================================================================\n","Evaluated datasets: ['v7', 'v8']\n","Next: Cell 4 - Generate confusion matrices and visualization\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title Cell 4: CASME II ViT Confusion Matrix Generation\n","\n","# File: 06_01_ViT_CASME2_KFS_Cell4.py\n","# Location: experiments/06_01_ViT_CASME2-KFS-PREP.ipynb\n","# Purpose: Generate professional confusion matrix visualization for AF and KFS evaluations\n","\n","import json\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import seaborn as sns\n","from datetime import datetime\n","\n","print(\"CASME II ViT Confusion Matrix Generation\")\n","print(\"=\" * 60)\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/06_01_vit_casme2_kfs_prep\"\n","\n","def find_evaluation_json_files(results_path):\n","    \"\"\"Find evaluation JSON files with version detection\"\"\"\n","    json_files = {}\n","    eval_dir = f\"{results_path}/evaluation_results\"\n","\n","    if os.path.exists(eval_dir):\n","        for version in ['v7', 'v8']:\n","            eval_pattern = f\"{eval_dir}/casme2_vit_evaluation_results_{version}.json\"\n","            eval_files = glob.glob(eval_pattern)\n","\n","            if eval_files:\n","                json_files[f'main_{version}'] = eval_files[0]\n","                print(f\"Found {version.upper()} evaluation file: {os.path.basename(eval_files[0])}\")\n","\n","            wrong_pattern = f\"{eval_dir}/casme2_vit_wrong_predictions_{version}.json\"\n","            wrong_files = glob.glob(wrong_pattern)\n","\n","            if wrong_files:\n","                json_files[f'wrong_{version}'] = wrong_files[0]\n","                print(f\"Found {version.upper()} wrong predictions: {os.path.basename(wrong_files[0])}\")\n","\n","        if not json_files:\n","            print(f\"WARNING: No evaluation results found in {eval_dir}\")\n","            print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","    else:\n","        print(f\"ERROR: Evaluation directory not found: {eval_dir}\")\n","\n","    return json_files\n","\n","def load_evaluation_results(json_path):\n","    \"\"\"Load and parse evaluation results JSON\"\"\"\n","    try:\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"Successfully loaded: {os.path.basename(json_path)}\")\n","        return data\n","    except Exception as e:\n","        print(f\"ERROR loading {json_path}: {str(e)}\")\n","        return None\n","\n","def calculate_weighted_f1(per_class_performance):\n","    \"\"\"Calculate weighted F1 score\"\"\"\n","    total_support = sum([class_data['support'] for class_data in per_class_performance.values()\n","                        if class_data['support'] > 0])\n","\n","    if total_support == 0:\n","        return 0.0\n","\n","    weighted_f1 = 0.0\n","\n","    for class_name, class_data in per_class_performance.items():\n","        if class_data['support'] > 0:\n","            weight = class_data['support'] / total_support\n","            weighted_f1 += class_data['f1_score'] * weight\n","\n","    return weighted_f1\n","\n","def calculate_balanced_accuracy(confusion_matrix):\n","    \"\"\"Calculate balanced accuracy handling classes with zero support\"\"\"\n","    cm = np.array(confusion_matrix)\n","    n_classes = cm.shape[0]\n","\n","    per_class_balanced_acc = []\n","    classes_with_samples = []\n","\n","    for i in range(n_classes):\n","        if cm[i, :].sum() > 0:\n","            classes_with_samples.append(i)\n","\n","    for i in classes_with_samples:\n","        tp = cm[i, i]\n","        fn = cm[i, :].sum() - tp\n","        fp = cm[:, i].sum() - tp\n","        tn = cm.sum() - tp - fn - fp\n","\n","        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n","\n","        class_balanced_acc = (sensitivity + specificity) / 2\n","        per_class_balanced_acc.append(class_balanced_acc)\n","\n","    balanced_acc = np.mean(per_class_balanced_acc) if per_class_balanced_acc else 0.0\n","\n","    return balanced_acc\n","\n","def determine_text_color(color_value, threshold=0.5):\n","    \"\"\"Determine optimal text color based on background intensity\"\"\"\n","    return 'white' if color_value > threshold else 'black'\n","\n","def create_confusion_matrix_plot(data, output_path, test_version):\n","    \"\"\"Create professional confusion matrix visualization for CASME II micro-expression recognition\"\"\"\n","\n","    meta = data['evaluation_metadata']\n","    class_names = meta['class_names']\n","    cm = np.array(data['confusion_matrix'], dtype=int)\n","    overall = data['overall_performance']\n","    per_class = data['per_class_performance']\n","\n","    test_config = data.get('test_configuration', {})\n","    variant = test_config.get('variant', test_version.upper())\n","    description = test_config.get('description', f'{test_version} preprocessing')\n","    eval_mode = meta.get('evaluation_mode', 'frame_level')\n","\n","    print(f\"Processing confusion matrix for {variant} ({test_version})\")\n","    print(f\"Dataset: {description}\")\n","    print(f\"Evaluation mode: {eval_mode}\")\n","    print(f\"Confusion matrix shape: {cm.shape}\")\n","\n","    macro_f1 = overall.get('macro_f1', 0.0)\n","    accuracy = overall.get('accuracy', 0.0)\n","    weighted_f1 = calculate_weighted_f1(per_class)\n","    balanced_acc = calculate_balanced_accuracy(cm)\n","\n","    print(f\"Calculated metrics - Macro F1: {macro_f1:.4f}, Weighted F1: {weighted_f1:.4f}, \"\n","          f\"Balanced Acc: {balanced_acc:.4f}, Accuracy: {accuracy:.4f}\")\n","\n","    row_sums = cm.sum(axis=1, keepdims=True)\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        cm_pct = np.divide(cm, row_sums, where=(row_sums!=0))\n","        cm_pct = np.nan_to_num(cm_pct)\n","\n","    fig, ax = plt.subplots(figsize=(12, 10))\n","\n","    cmap = 'Blues'\n","    im = ax.imshow(cm_pct, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=0.8)\n","\n","    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","    cbar.set_label('True Class Percentage', rotation=270, labelpad=15, fontsize=11)\n","\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            count = cm[i, j]\n","\n","            if row_sums[i, 0] > 0:\n","                percentage = cm_pct[i, j] * 100\n","                text = f\"{count}\\n{percentage:.1f}%\"\n","            else:\n","                text = f\"{count}\\n(N/A)\"\n","\n","            cell_value = cm_pct[i, j]\n","            text_color = determine_text_color(cell_value, threshold=0.4)\n","\n","            ax.text(j, i, text, ha=\"center\", va=\"center\",\n","                   color=text_color, fontsize=9, fontweight='bold')\n","\n","    ax.set_xticks(np.arange(len(class_names)))\n","    ax.set_yticks(np.arange(len(class_names)))\n","    ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=10)\n","    ax.set_yticklabels(class_names, fontsize=10)\n","    ax.set_xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n","    ax.set_ylabel(\"True Label\", fontsize=12, fontweight='bold')\n","\n","    preprocessing_note = f\"Preprocessing: {description}\\n\"\n","    preprocessing_note += f\"Dataset: {test_version}\\n\"\n","    preprocessing_note += f\"Evaluation: {eval_mode.replace('_', ' ').title()}\"\n","\n","    if 'kfs_late_fusion_info' in data:\n","        fusion_info = data['kfs_late_fusion_info']\n","        preprocessing_note += f\"\\nFrames: {fusion_info['total_frames']}, Videos: {fusion_info['total_videos']}\"\n","\n","    missing_classes = meta.get('missing_classes', [])\n","    if missing_classes:\n","        preprocessing_note += f\"\\nMissing: {', '.join(missing_classes)}\"\n","\n","    ax.text(0.02, 0.98, preprocessing_note, transform=ax.transAxes, fontsize=8,\n","            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n","\n","    title = f\"CASME II {variant} Micro-Expression Recognition - Face-Aware Preprocessing\\n\"\n","    title += f\"Acc: {accuracy:.4f}  |  Macro F1: {macro_f1:.4f}  |  Weighted F1: {weighted_f1:.4f}  |  Balanced Acc: {balanced_acc:.4f}\"\n","    ax.set_title(title, fontsize=12, pad=25, fontweight='bold')\n","\n","    plt.tight_layout()\n","    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n","    plt.close(fig)\n","\n","    print(f\"Confusion matrix saved to: {os.path.basename(output_path)}\")\n","\n","    return {\n","        'accuracy': accuracy,\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        'balanced_accuracy': balanced_acc,\n","        'missing_classes': missing_classes\n","    }\n","\n","def generate_performance_summary(evaluation_data, wrong_predictions_data=None):\n","    \"\"\"Generate comprehensive performance summary\"\"\"\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\")\n","    print(\"=\" * 60)\n","\n","    overall = evaluation_data['overall_performance']\n","    meta = evaluation_data['evaluation_metadata']\n","    test_config = evaluation_data.get('test_configuration', {})\n","\n","    variant = test_config.get('variant', 'N/A')\n","\n","    print(f\"Dataset: {meta['dataset']}\")\n","    print(f\"Variant: {variant}\")\n","    print(f\"Dataset version: {test_config.get('version', 'N/A')}\")\n","    print(f\"Preprocessing: {test_config.get('description', 'N/A')}\")\n","    print(f\"Test samples: {meta['test_samples']}\")\n","    print(f\"Model: {meta['model_type']}\")\n","    print(f\"Evaluation date: {meta['evaluation_timestamp']}\")\n","\n","    if 'kfs_late_fusion_info' in evaluation_data:\n","        fusion_info = evaluation_data['kfs_late_fusion_info']\n","        print(f\"\\nLate Fusion Information:\")\n","        print(f\"  Total frames: {fusion_info['total_frames']}\")\n","        print(f\"  Video predictions: {fusion_info['total_videos']}\")\n","        print(f\"  Aggregation: {fusion_info['aggregation_method']}\")\n","\n","    print(f\"\\nOverall Performance:\")\n","    print(f\"  Accuracy:         {overall['accuracy']:.4f}\")\n","    print(f\"  Macro Precision:  {overall['macro_precision']:.4f}\")\n","    print(f\"  Macro Recall:     {overall['macro_recall']:.4f}\")\n","    print(f\"  Macro F1:         {overall['macro_f1']:.4f}\")\n","    print(f\"  Macro AUC:        {overall['macro_auc']:.4f}\")\n","\n","    print(f\"\\nPer-Class Performance:\")\n","    per_class = evaluation_data['per_class_performance']\n","\n","    print(f\"{'Class':<12} {'F1':<8} {'Precision':<10} {'Recall':<8} {'AUC':<8} {'Support':<8} {'In Test'}\")\n","    print(\"-\" * 65)\n","\n","    for class_name, metrics in per_class.items():\n","        in_test = \"Yes\" if metrics['in_test_set'] else \"No\"\n","        print(f\"{class_name:<12} {metrics['f1_score']:<8.4f} {metrics['precision']:<10.4f} \"\n","              f\"{metrics['recall']:<8.4f} {metrics['auc']:<8.4f} {metrics['support']:<8} {in_test}\")\n","\n","    if 'training_information' in evaluation_data:\n","        training = evaluation_data['training_information']\n","        print(f\"\\nTraining vs Test Comparison:\")\n","        print(f\"  Training Val F1:  {training['best_val_f1']:.4f}\")\n","        print(f\"  Test F1:          {overall['macro_f1']:.4f}\")\n","        print(f\"  Performance Gap:  {training['best_val_f1'] - overall['macro_f1']:+.4f}\")\n","        print(f\"  Best Epoch:       {training['best_epoch']}\")\n","\n","    missing_classes = meta.get('missing_classes', [])\n","    available_classes = meta.get('available_classes', [])\n","\n","    print(f\"\\nClass Availability Analysis:\")\n","    print(f\"  Available classes: {len(available_classes)}/7\")\n","    print(f\"  Missing classes: {missing_classes if missing_classes else 'None'}\")\n","\n","    if wrong_predictions_data:\n","        wrong_meta = wrong_predictions_data['analysis_metadata']\n","        print(f\"\\nError Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']}/{wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        patterns = wrong_predictions_data.get('confusion_patterns', {})\n","        if patterns:\n","            print(f\"\\nTop Confusion Patterns:\")\n","            sorted_patterns = sorted(patterns.items(), key=lambda x: x[1], reverse=True)[:3]\n","            for pattern, count in sorted_patterns:\n","                print(f\"  {pattern}: {count} cases\")\n","\n","    print(f\"\\nInference Performance:\")\n","    inference = evaluation_data['inference_performance']\n","    print(f\"  Total time: {inference['total_time_seconds']:.2f}s\")\n","    print(f\"  Speed: {inference['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","json_files = find_evaluation_json_files(RESULTS_ROOT)\n","\n","if not json_files:\n","    print(f\"ERROR: No evaluation JSON files found in {RESULTS_ROOT}\")\n","    print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","else:\n","    print(f\"\\nFound {len([k for k in json_files.keys() if k.startswith('main_')])} evaluation result(s)\")\n","\n","output_dir = f\"{RESULTS_ROOT}/confusion_matrix_analysis\"\n","Path(output_dir).mkdir(parents=True, exist_ok=True)\n","\n","results_summary = {}\n","generated_files = []\n","\n","for version in ['v7', 'v8']:\n","    main_key = f'main_{version}'\n","    wrong_key = f'wrong_{version}'\n","\n","    if main_key in json_files:\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Processing {version.upper()} Evaluation Results\")\n","        print(f\"{'='*60}\")\n","\n","        eval_data = load_evaluation_results(json_files[main_key])\n","\n","        wrong_data = None\n","        if wrong_key in json_files:\n","            wrong_data = load_evaluation_results(json_files[wrong_key])\n","\n","        if eval_data is not None:\n","            try:\n","                cm_output_path = os.path.join(output_dir, f\"confusion_matrix_CASME2_ViT_{version}.png\")\n","                metrics = create_confusion_matrix_plot(eval_data, cm_output_path, version)\n","                generated_files.append(cm_output_path)\n","\n","                results_summary[version] = metrics\n","\n","                print(f\"\\nSUCCESS: {version.upper()} confusion matrix generated successfully\")\n","                print(f\"Output file: {os.path.basename(cm_output_path)}\")\n","\n","                print(f\"\\nPerformance Metrics Summary:\")\n","                print(f\"  Accuracy:        {metrics['accuracy']:.4f}\")\n","                print(f\"  Macro F1:        {metrics['macro_f1']:.4f}\")\n","                print(f\"  Weighted F1:     {metrics['weighted_f1']:.4f}\")\n","                print(f\"  Balanced Acc:    {metrics['balanced_accuracy']:.4f}\")\n","\n","                if metrics['missing_classes']:\n","                    print(f\"  Missing classes: {metrics['missing_classes']}\")\n","\n","            except Exception as e:\n","                print(f\"ERROR: Failed to generate {version.upper()} confusion matrix: {str(e)}\")\n","                import traceback\n","                traceback.print_exc()\n","\n","            generate_performance_summary(eval_data, wrong_data)\n","        else:\n","            print(f\"ERROR: Could not load {version.upper()} evaluation data\")\n","\n","if generated_files:\n","    print(f\"\\n\" + \"=\" * 60)\n","    print(\"CASME II CONFUSION MATRIX GENERATION COMPLETED\")\n","    print(\"=\" * 60)\n","\n","    print(f\"\\nGenerated visualization files:\")\n","    for file_path in generated_files:\n","        filename = os.path.basename(file_path)\n","        print(f\"  {filename}\")\n","\n","    for version in ['v7', 'v8']:\n","        if version in results_summary:\n","            variant = 'AF' if version == 'v7' else 'KFS'\n","            print(f\"\\n{variant} ({version.upper()}) Performance Summary:\")\n","            metrics = results_summary[version]\n","            print(f\"  Accuracy:       {metrics['accuracy']:.4f}\")\n","            print(f\"  Macro F1:       {metrics['macro_f1']:.4f}\")\n","            print(f\"  Weighted F1:    {metrics['weighted_f1']:.4f}\")\n","            print(f\"  Balanced Acc:   {metrics['balanced_accuracy']:.4f}\")\n","\n","    print(f\"\\nFiles saved in: {output_dir}\")\n","    print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","else:\n","    print(f\"\\nERROR: No visualizations were generated\")\n","    print(\"Please check:\")\n","    print(\"1. Cell 3 evaluation results exist\")\n","    print(\"2. JSON file structure is correct\")\n","    print(\"3. No file permission issues\")\n","\n","print(\"\\nCell 4 completed - CASME II confusion matrix analysis generated\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"cellView":"form","id":"QCjKzvIdRhvS","executionInfo":{"status":"ok","timestamp":1761145865468,"user_tz":-420,"elapsed":3319,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"f3c4256d-928f-4578-f513-900f8eefed40"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II ViT Confusion Matrix Generation\n","============================================================\n","Found V7 evaluation file: casme2_vit_evaluation_results_v7.json\n","Found V7 wrong predictions: casme2_vit_wrong_predictions_v7.json\n","Found V8 evaluation file: casme2_vit_evaluation_results_v8.json\n","Found V8 wrong predictions: casme2_vit_wrong_predictions_v8.json\n","\n","Found 2 evaluation result(s)\n","\n","============================================================\n","Processing V7 Evaluation Results\n","============================================================\n","Successfully loaded: casme2_vit_evaluation_results_v7.json\n","Successfully loaded: casme2_vit_wrong_predictions_v7.json\n","Processing confusion matrix for AF (v7)\n","Dataset: Apex Frame with Face-Aware Preprocessing\n","Evaluation mode: frame_level\n","Confusion matrix shape: (7, 7)\n","Calculated metrics - Macro F1: 0.1241, Weighted F1: 0.2798, Balanced Acc: 0.5180, Accuracy: 0.3571\n","Confusion matrix saved to: confusion_matrix_CASME2_ViT_v7.png\n","\n","SUCCESS: V7 confusion matrix generated successfully\n","Output file: confusion_matrix_CASME2_ViT_v7.png\n","\n","Performance Metrics Summary:\n","  Accuracy:        0.3571\n","  Macro F1:        0.1241\n","  Weighted F1:     0.2798\n","  Balanced Acc:    0.5180\n","  Missing classes: ['fear']\n","\n","============================================================\n","CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\n","============================================================\n","Dataset: CASME_II\n","Variant: AF\n","Dataset version: v7\n","Preprocessing: Apex Frame with Face-Aware Preprocessing\n","Test samples: 28\n","Model: ViTCASME2Baseline\n","Evaluation date: 2025-10-22T15:10:57.947543\n","\n","Overall Performance:\n","  Accuracy:         0.3571\n","  Macro Precision:  0.1020\n","  Macro Recall:     0.1612\n","  Macro F1:         0.1241\n","  Macro AUC:        0.5185\n","\n","Per-Class Performance:\n","Class        F1       Precision  Recall   AUC      Support  In Test\n","-----------------------------------------------------------------\n","others       0.5833   0.5000     0.7000   0.7056   10       Yes\n","disgust      0.2857   0.2143     0.4286   0.5510   7        Yes\n","happiness    0.0000   0.0000     0.0000   0.6042   4        Yes\n","repression   0.0000   0.0000     0.0000   0.7467   3        Yes\n","surprise     0.0000   0.0000     0.0000   0.4667   3        Yes\n","sadness      0.0000   0.0000     0.0000   0.0370   1        Yes\n","fear         0.0000   0.0000     0.0000   0.0000   0        No\n","\n","Training vs Test Comparison:\n","  Training Val F1:  0.1691\n","  Test F1:          0.1241\n","  Performance Gap:  +0.0450\n","  Best Epoch:       0\n","\n","Class Availability Analysis:\n","  Available classes: 6/7\n","  Missing classes: ['fear']\n","\n","Error Analysis:\n","  Total errors: 18/28\n","  Error rate: 64.29%\n","\n","Top Confusion Patterns:\n","  disgust -> others: 4 cases\n","  happiness -> disgust: 4 cases\n","  others -> disgust: 3 cases\n","\n","Inference Performance:\n","  Total time: 0.40s\n","  Speed: 14.2 ms/sample\n","\n","============================================================\n","Processing V8 Evaluation Results\n","============================================================\n","Successfully loaded: casme2_vit_evaluation_results_v8.json\n","Successfully loaded: casme2_vit_wrong_predictions_v8.json\n","Processing confusion matrix for KFS (v8)\n","Dataset: Key Frame Sequence with Face-Aware Preprocessing\n","Evaluation mode: video_level_late_fusion\n","Confusion matrix shape: (7, 7)\n","Calculated metrics - Macro F1: 0.1296, Weighted F1: 0.2871, Balanced Acc: 0.5245, Accuracy: 0.3690\n","Confusion matrix saved to: confusion_matrix_CASME2_ViT_v8.png\n","\n","SUCCESS: V8 confusion matrix generated successfully\n","Output file: confusion_matrix_CASME2_ViT_v8.png\n","\n","Performance Metrics Summary:\n","  Accuracy:        0.3690\n","  Macro F1:        0.1296\n","  Weighted F1:     0.2871\n","  Balanced Acc:    0.5245\n","  Missing classes: ['fear']\n","\n","============================================================\n","CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\n","============================================================\n","Dataset: CASME_II\n","Variant: KFS\n","Dataset version: v8\n","Preprocessing: Key Frame Sequence with Face-Aware Preprocessing\n","Test samples: 84\n","Model: ViTCASME2Baseline\n","Evaluation date: 2025-10-22T15:11:02.057656\n","\n","Late Fusion Information:\n","  Total frames: 84\n","  Video predictions: 84\n","  Aggregation: probability_averaging\n","\n","Overall Performance:\n","  Accuracy:         0.3690\n","  Macro Precision:  0.1062\n","  Macro Recall:     0.1701\n","  Macro F1:         0.1296\n","  Macro AUC:        0.5390\n","\n","Per-Class Performance:\n","Class        F1       Precision  Recall   AUC      Support  In Test\n","-----------------------------------------------------------------\n","others       0.5634   0.4878     0.6667   0.7191   30       Yes\n","disgust      0.3438   0.2558     0.5238   0.5971   21       Yes\n","happiness    0.0000   0.0000     0.0000   0.6007   12       Yes\n","repression   0.0000   0.0000     0.0000   0.7570   9        Yes\n","surprise     0.0000   0.0000     0.0000   0.4696   9        Yes\n","sadness      0.0000   0.0000     0.0000   0.0905   3        Yes\n","fear         0.0000   0.0000     0.0000   0.0000   0        No\n","\n","Training vs Test Comparison:\n","  Training Val F1:  0.1691\n","  Test F1:          0.1296\n","  Performance Gap:  +0.0395\n","  Best Epoch:       0\n","\n","Class Availability Analysis:\n","  Available classes: 6/7\n","  Missing classes: ['fear']\n","\n","Error Analysis:\n","  Total errors: 53/84\n","  Error rate: 63.10%\n","\n","Top Confusion Patterns:\n","  others -> disgust: 10 cases\n","  disgust -> others: 10 cases\n","  happiness -> disgust: 10 cases\n","\n","Inference Performance:\n","  Total time: 0.68s\n","  Speed: 8.1 ms/sample\n","\n","============================================================\n","CASME II CONFUSION MATRIX GENERATION COMPLETED\n","============================================================\n","\n","Generated visualization files:\n","  confusion_matrix_CASME2_ViT_v7.png\n","  confusion_matrix_CASME2_ViT_v8.png\n","\n","AF (V7) Performance Summary:\n","  Accuracy:       0.3571\n","  Macro F1:       0.1241\n","  Weighted F1:    0.2798\n","  Balanced Acc:   0.5180\n","\n","KFS (V8) Performance Summary:\n","  Accuracy:       0.3690\n","  Macro F1:       0.1296\n","  Weighted F1:    0.2871\n","  Balanced Acc:   0.5245\n","\n","Files saved in: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/06_01_vit_casme2_kfs_prep/confusion_matrix_analysis\n","Analysis completed at: 2025-10-22 15:11:05\n","\n","Cell 4 completed - CASME II confusion matrix analysis generated\n"]}]}]}