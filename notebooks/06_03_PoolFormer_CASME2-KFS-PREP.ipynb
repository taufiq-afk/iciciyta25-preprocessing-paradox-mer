{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyOwiWrKRHQV7I2X9/rjX+Mx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3dd4d1afdb4a4638b0d7d092a49e154f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c3eb15f5a9e43ec8fbd58c3c5679694","IPY_MODEL_f6d99535715a45bebfbd7c32897798d7","IPY_MODEL_6a69f1a166d04058bce1d9f830cf4d92"],"layout":"IPY_MODEL_1a71e5a84f3a443687bb637c276d5311"}},"5c3eb15f5a9e43ec8fbd58c3c5679694":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fe8f64189954fc098d092849acce3fc","placeholder":"​","style":"IPY_MODEL_437aa8d627b447c0ab959c250c919be8","value":"preprocessor_config.json: 100%"}},"f6d99535715a45bebfbd7c32897798d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_45d80320c16a40de8c91347e2f6c8e9d","max":283,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58ca455bf97346b0869f01820f700422","value":283}},"6a69f1a166d04058bce1d9f830cf4d92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0438328de7a49ff9d33cca1c02cbb90","placeholder":"​","style":"IPY_MODEL_7c3e2d30c452428794c6982d660dcc64","value":" 283/283 [00:00&lt;00:00, 36.3kB/s]"}},"1a71e5a84f3a443687bb637c276d5311":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fe8f64189954fc098d092849acce3fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"437aa8d627b447c0ab959c250c919be8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45d80320c16a40de8c91347e2f6c8e9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58ca455bf97346b0869f01820f700422":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e0438328de7a49ff9d33cca1c02cbb90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c3e2d30c452428794c6982d660dcc64":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1367323c56334025a681608ed8782fe2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea7018db87e7452289477c29741cee56","IPY_MODEL_21a4a15c5601464e8771457b28002dee","IPY_MODEL_f3a6a6d9be1f4f61a8df74a61a95f4ef"],"layout":"IPY_MODEL_1ec6844731584b7fa400de186cd04a47"}},"ea7018db87e7452289477c29741cee56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ec8038061914795a1e53d656a2a3195","placeholder":"​","style":"IPY_MODEL_0e3920bd2de941de82c8ed0e5ead171e","value":"config.json: "}},"21a4a15c5601464e8771457b28002dee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fbf37b2ea424993be76b7aa6ba0a595","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad5ddc3c1305475aaff59a8365de8e08","value":1}},"f3a6a6d9be1f4f61a8df74a61a95f4ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52eb06b5999f40cc94afc61ba6e95f5f","placeholder":"​","style":"IPY_MODEL_c1cbb365b94a4e3a9b7071b723143089","value":" 69.9k/? [00:00&lt;00:00, 7.85MB/s]"}},"1ec6844731584b7fa400de186cd04a47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ec8038061914795a1e53d656a2a3195":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e3920bd2de941de82c8ed0e5ead171e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fbf37b2ea424993be76b7aa6ba0a595":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"ad5ddc3c1305475aaff59a8365de8e08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"52eb06b5999f40cc94afc61ba6e95f5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1cbb365b94a4e3a9b7071b723143089":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f71a7395dbe472aa85addb22a683b5d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_37453e5b8eeb40589d8f90bf462de49c","IPY_MODEL_1059f80ec5da4fd4bb4fa8ca387b83e0","IPY_MODEL_37146956892a402a939ce19a65019864"],"layout":"IPY_MODEL_3e2609b758b645a6a78ae42613effc3a"}},"37453e5b8eeb40589d8f90bf462de49c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abbf966c26604e7fb85ab9934f5115aa","placeholder":"​","style":"IPY_MODEL_b0841dbcd0a94cf8b79bd4ef7fd4e4b7","value":"model.safetensors: 100%"}},"1059f80ec5da4fd4bb4fa8ca387b83e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d984dcac29c4e9c97d4940c65e130ed","max":293950574,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f78686b00ab844dc9abb6aac0cd2ac9f","value":293950574}},"37146956892a402a939ce19a65019864":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6abf895d65e4288bf73a5511706a17c","placeholder":"​","style":"IPY_MODEL_bd0d19bc269a42d9a6c446635c89ac11","value":" 294M/294M [00:02&lt;00:00, 97.3MB/s]"}},"3e2609b758b645a6a78ae42613effc3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abbf966c26604e7fb85ab9934f5115aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0841dbcd0a94cf8b79bd4ef7fd4e4b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d984dcac29c4e9c97d4940c65e130ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f78686b00ab844dc9abb6aac0cd2ac9f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6abf895d65e4288bf73a5511706a17c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd0d19bc269a42d9a6c446635c89ac11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3dd4d1afdb4a4638b0d7d092a49e154f","5c3eb15f5a9e43ec8fbd58c3c5679694","f6d99535715a45bebfbd7c32897798d7","6a69f1a166d04058bce1d9f830cf4d92","1a71e5a84f3a443687bb637c276d5311","7fe8f64189954fc098d092849acce3fc","437aa8d627b447c0ab959c250c919be8","45d80320c16a40de8c91347e2f6c8e9d","58ca455bf97346b0869f01820f700422","e0438328de7a49ff9d33cca1c02cbb90","7c3e2d30c452428794c6982d660dcc64","1367323c56334025a681608ed8782fe2","ea7018db87e7452289477c29741cee56","21a4a15c5601464e8771457b28002dee","f3a6a6d9be1f4f61a8df74a61a95f4ef","1ec6844731584b7fa400de186cd04a47","9ec8038061914795a1e53d656a2a3195","0e3920bd2de941de82c8ed0e5ead171e","6fbf37b2ea424993be76b7aa6ba0a595","ad5ddc3c1305475aaff59a8365de8e08","52eb06b5999f40cc94afc61ba6e95f5f","c1cbb365b94a4e3a9b7071b723143089","6f71a7395dbe472aa85addb22a683b5d","37453e5b8eeb40589d8f90bf462de49c","1059f80ec5da4fd4bb4fa8ca387b83e0","37146956892a402a939ce19a65019864","3e2609b758b645a6a78ae42613effc3a","abbf966c26604e7fb85ab9934f5115aa","b0841dbcd0a94cf8b79bd4ef7fd4e4b7","7d984dcac29c4e9c97d4940c65e130ed","f78686b00ab844dc9abb6aac0cd2ac9f","e6abf895d65e4288bf73a5511706a17c","bd0d19bc269a42d9a6c446635c89ac11"]},"cellView":"form","id":"UH-PH-kiMd5f","executionInfo":{"status":"ok","timestamp":1761202076589,"user_tz":-420,"elapsed":59037,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"05186396-fcd7-428a-a43c-fff75531a538","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","CASME II KEY FRAME SEQUENCE POOLFORMER WITH FACE-AWARE PREPROCESSING\n","============================================================\n","\n","[1] Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully\n","\n","[2] Importing required libraries...\n","CASME II Key Frame Sequence PoolFormer - Face-Aware Preprocessing Infrastructure\n","============================================================\n","Loading CASME II v8 preprocessing metadata...\n","Dataset variant: KFS\n","Processing date: 2025-10-19T08:17:23.905019\n","Preprocessing method: face_bbox_expansion_all_directions\n","Total images processed: 765\n","Face detection rate: 100.00%\n","Target size: 224x224px\n","BBox expansion: 20px (all directions)\n","\n","Dataset split information:\n","  Train samples: 603\n","  Validation samples: 78\n","  Test samples: 84\n","Using PoolFormer-M48 for enhanced micro-expression recognition (73M parameters)\n","\n","==================================================\n","EXPERIMENT CONFIGURATION - KEY FRAME SEQUENCE FACE-AWARE\n","==================================================\n","Dataset: v8 Key Frame Sequence with Face-Aware Preprocessing\n","Frame strategy: onset, apex, offset (3 frames per video)\n","Training approach: Frame-level independent learning for late fusion\n","Loss Function: Focal Loss\n","  Gamma: 2.0\n","  Alpha Weights (per-class): [0.055, 0.069, 0.098, 0.107, 0.109, 0.219, 0.343]\n","  Alpha Sum Validation: 1.000\n","PoolFormer Model Variant: M48\n","  Model: sail/poolformer_m48\n","  Parameters: 73M\n","Input Resolution: 384x384px (PoolFormer native, upscaled from 224px)\n","Image Format: Grayscale converted to RGB (3-channel)\n","==================================================\n","\n","Device: cuda\n","GPU: NVIDIA L4 (23.8 GB)\n","GPU optimization enabled for NVIDIA L4\n","Moderate dataset configuration: Batch size 8 (optimal for 603 samples at 384px)\n","Iterations per epoch: 75 (~75 iterations per epoch)\n","RAM preload workers: 32 (parallel image loading)\n","\n","Loading v8 class distribution...\n","\n","v8 Train distribution: [237, 150, 75, 63, 60, 15, 3]\n","v8 Val distribution: [30, 18, 9, 9, 6, 3, 3]\n","v8 Test distribution: [30, 21, 12, 9, 9, 3, 0]\n","Applied Focal Loss alpha weights: [0.055 0.069 0.098 0.107 0.109 0.219 0.343]\n","Alpha weights sum: 1.000\n","\n","Setting up PoolFormer Image Processor for 384px input...\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/283 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dd4d1afdb4a4638b0d7d092a49e154f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["PoolFormer preprocessing configured:\n","  Input size: 384x384px\n","  Resize from: 224x224px (v8 native)\n","  Token mixing: Pooling-based attention\n","PoolFormer Image Processor configured for 384px with token mixing\n","\n","Dataset paths:\n","Train: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8/train\n","Validation: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8/val\n","Test: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8/test\n","\n","PoolFormer CASME II architecture validation...\n"]},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1367323c56334025a681608ed8782fe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/294M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f71a7395dbe472aa85addb22a683b5d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["PoolFormer feature dimension: 768\n","Classification head: 768 -> 256 -> 7\n","Dropout rate: 0.3 (regularization for moderate dataset)\n","Validation successful: Output shape torch.Size([1, 7])\n","PoolFormer M48 with 73M parameters\n","Token mixing with pooling operations for efficient feature extraction\n","\n","============================================================\n","CASME II KEY FRAME SEQUENCE POOLFORMER CONFIGURATION COMPLETE\n","============================================================\n","Loss Configuration:\n","  Function: Optimized Focal Loss\n","  Gamma: 2.0\n","  Per-class Alpha: [0.055, 0.069, 0.098, 0.107, 0.109, 0.219, 0.343]\n","  Alpha Sum: 1.000\n","\n","Model Configuration:\n","  Architecture: PoolFormer\n","  Variant: M48\n","  Model: sail/poolformer_m48\n","  Parameters: 73M\n","  Input Resolution: 384px (upscaled from 224px v8 native)\n","  Feature Dimension: 512\n","  Token Mixing: Pooling-based attention mechanism\n","  Classification Head: 512 -> 256 -> 7 (simplified)\n","\n","Dataset Configuration:\n","  Version: v8\n","  Classes: 7\n","  Frame strategy: key_frame_sequence\n","  Frame types: ['onset', 'apex', 'offset']\n","  Training approach: frame_level_independent\n","  Inference strategy: late_fusion_video_level\n","  Weight Optimization: Per-class Alpha\n","\n","Training Configuration:\n","  Train samples: 603 frames\n","  Validation samples: 78 frames\n","  Test samples: 84 frames\n","  Batch size: 8\n","  Learning rate: 2e-05\n","  Dropout rate: 0.3\n","\n","Next: Cell 2 - Dataset Loading and Training Pipeline\n"]}],"source":["# @title Cell 1: CASME II Key Frame Sequence PoolFormer Infrastructure Configuration\n","\n","# File: 06_03_PoolFormer_CASME2_KFS_Cell1_FIXED.py\n","# Location: experiments/06_03_PoolFormer_CASME2-KFS-PREP.ipynb\n","# Purpose: PoolFormer for CASME II micro-expression recognition with key frame sequence strategy and face-aware preprocessing\n","\n","from google.colab import drive\n","print(\"=\" * 60)\n","print(\"CASME II KEY FRAME SEQUENCE POOLFORMER WITH FACE-AWARE PREPROCESSING\")\n","print(\"=\" * 60)\n","print(\"\\n[1] Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Google Drive mounted successfully\")\n","\n","print(\"\\n[2] Importing required libraries...\")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import json\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import time\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Project paths configuration\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","DATASET_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/preprocessed_v8\"\n","CHECKPOINT_ROOT = f\"{PROJECT_ROOT}/models/06_03_poolformer_casme2_kfs_prep\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/06_03_poolformer_casme2_kfs_prep\"\n","\n","# Load CASME II v8 preprocessing metadata\n","PREPROCESSING_SUMMARY = f\"{DATASET_ROOT}/preprocessing_summary.json\"\n","\n","print(\"CASME II Key Frame Sequence PoolFormer - Face-Aware Preprocessing Infrastructure\")\n","print(\"=\" * 60)\n","\n","# Validate preprocessing metadata exists\n","if not os.path.exists(PREPROCESSING_SUMMARY):\n","    raise FileNotFoundError(f\"v8 preprocessing summary not found: {PREPROCESSING_SUMMARY}\")\n","\n","# Load v8 preprocessing metadata\n","print(\"Loading CASME II v8 preprocessing metadata...\")\n","with open(PREPROCESSING_SUMMARY, 'r') as f:\n","    preprocessing_info = json.load(f)\n","\n","print(f\"Dataset variant: {preprocessing_info['variant']}\")\n","print(f\"Processing date: {preprocessing_info['processing_date']}\")\n","print(f\"Preprocessing method: {preprocessing_info['preprocessing_method']}\")\n","print(f\"Total images processed: {preprocessing_info['total_processed']}\")\n","print(f\"Face detection rate: {preprocessing_info['face_detection_stats']['detection_rate']:.2%}\")\n","\n","# Extract preprocessing parameters\n","preproc_params = preprocessing_info['preprocessing_parameters']\n","print(f\"Target size: {preproc_params['target_size']}x{preproc_params['target_size']}px\")\n","print(f\"BBox expansion: {preproc_params['bbox_expansion']}px (all directions)\")\n","\n","# Display split information\n","print(f\"\\nDataset split information:\")\n","print(f\"  Train samples: {preprocessing_info['splits']['train']['total_images']}\")\n","print(f\"  Validation samples: {preprocessing_info['splits']['val']['total_images']}\")\n","print(f\"  Test samples: {preprocessing_info['splits']['test']['total_images']}\")\n","\n","# =====================================================\n","# EXPERIMENT CONFIGURATION - Key Frame Sequence with Face-Aware Preprocessing\n","# =====================================================\n","# This configuration supports 4 experiment scenarios:\n","# 1. PoolFormer-M36 + CrossEntropy Loss\n","# 2. PoolFormer-M36 + Focal Loss\n","# 3. PoolFormer-M48 + CrossEntropy Loss\n","# 4. PoolFormer-M48 + Focal Loss\n","#\n","# Toggle POOLFORMER_MODEL_VARIANT for model selection: 'm36' or 'm48'\n","# Toggle USE_FOCAL_LOSS for loss function: False (CrossEntropy) or True (Focal)\n","# =====================================================\n","\n","# FOCAL LOSS CONFIGURATION - Toggle for experimentation\n","USE_FOCAL_LOSS = True  # Default: CrossEntropy, Set True to enable Focal Loss\n","FOCAL_LOSS_GAMMA = 2.0  # Focal loss focusing parameter (if enabled)\n","\n","# OPTIMIZED CLASS WEIGHTS CONFIGURATION\n","# CASME II classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","# v8 Train distribution: [237, 150, 75, 63, 60, 15, 3] - inverse sqrt frequency approach\n","\n","# CrossEntropy Loss - Optimized inverse square root frequency weights\n","CROSSENTROPY_CLASS_WEIGHTS = [1.00, 1.26, 1.78, 1.94, 1.99, 3.98, 8.90]\n","\n","# Focal Loss - Normalized per-class alpha values (sum = 1.0)\n","FOCAL_LOSS_ALPHA_WEIGHTS = [0.055, 0.069, 0.098, 0.107, 0.109, 0.219, 0.343]\n","\n","# POOLFORMER MODEL CONFIGURATION - Support M36 and M48 variants\n","# PoolFormer-M36: 56M parameters, efficient token mixing with pooling\n","# PoolFormer-M48: 73M parameters, enhanced hierarchical feature extraction\n","POOLFORMER_MODEL_VARIANT = 'm48'  # Options: 'm36' or 'm48'\n","\n","# Dynamic PoolFormer model selection based on variant\n","if POOLFORMER_MODEL_VARIANT == 'm36':\n","    POOLFORMER_MODEL_NAME = 'sail/poolformer_m36'\n","    MODEL_PARAMS = '56M'\n","    print(\"Using PoolFormer-M36 for efficient micro-expression analysis (56M parameters)\")\n","elif POOLFORMER_MODEL_VARIANT == 'm48':\n","    POOLFORMER_MODEL_NAME = 'sail/poolformer_m48'\n","    MODEL_PARAMS = '73M'\n","    print(\"Using PoolFormer-M48 for enhanced micro-expression recognition (73M parameters)\")\n","else:\n","    raise ValueError(f\"Unsupported POOLFORMER_MODEL_VARIANT: {POOLFORMER_MODEL_VARIANT}\")\n","\n","# Display experiment configuration\n","print(\"\\n\" + \"=\" * 50)\n","print(\"EXPERIMENT CONFIGURATION - KEY FRAME SEQUENCE FACE-AWARE\")\n","print(\"=\" * 50)\n","print(f\"Dataset: v8 Key Frame Sequence with Face-Aware Preprocessing\")\n","print(f\"Frame strategy: onset, apex, offset (3 frames per video)\")\n","print(f\"Training approach: Frame-level independent learning for late fusion\")\n","print(f\"Loss Function: {'Focal Loss' if USE_FOCAL_LOSS else 'CrossEntropy Loss'}\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Alpha Weights (per-class): {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum Validation: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Class Weights (inverse sqrt freq): {CROSSENTROPY_CLASS_WEIGHTS}\")\n","print(f\"PoolFormer Model Variant: {POOLFORMER_MODEL_VARIANT.upper()}\")\n","print(f\"  Model: {POOLFORMER_MODEL_NAME}\")\n","print(f\"  Parameters: {MODEL_PARAMS}\")\n","print(f\"Input Resolution: 384x384px (PoolFormer native, upscaled from 224px)\")\n","print(f\"Image Format: Grayscale converted to RGB (3-channel)\")\n","print(\"=\" * 50)\n","\n","# Enhanced GPU configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n","gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n","\n","print(f\"\\nDevice: {device}\")\n","print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n","\n","# Optimized batch size for moderate dataset (603 train samples) with 384px input\n","# Using batch size 8 for better training stability with 603 samples at 384px resolution\n","BATCH_SIZE = 8\n","NUM_WORKERS = 4\n","\n","if 'A100' in gpu_name or 'L4' in gpu_name:\n","    torch.backends.cudnn.benchmark = True\n","    print(f\"GPU optimization enabled for {gpu_name}\")\n","\n","print(f\"Moderate dataset configuration: Batch size {BATCH_SIZE} (optimal for 603 samples at 384px)\")\n","print(f\"Iterations per epoch: {603 // BATCH_SIZE} (~75 iterations per epoch)\")\n","\n","# RAM preloading workers (separate from DataLoader workers)\n","RAM_PRELOAD_WORKERS = 32\n","print(f\"RAM preload workers: {RAM_PRELOAD_WORKERS} (parallel image loading)\")\n","\n","# CASME II class mapping and analysis\n","CASME2_CLASSES = ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CASME2_CLASSES)}\n","\n","# Extract class distribution from v8 preprocessing metadata\n","print(\"\\nLoading v8 class distribution...\")\n","train_dist = preprocessing_info['splits']['train']['emotion_distribution']\n","val_dist = preprocessing_info['splits']['val']['emotion_distribution']\n","test_dist = preprocessing_info['splits']['test']['emotion_distribution']\n","\n","# Convert to ordered list matching CASME2_CLASSES\n","def emotion_dist_to_list(emotion_dict, class_names):\n","    \"\"\"Convert emotion distribution dict to ordered list\"\"\"\n","    return [emotion_dict.get(cls, 0) for cls in class_names]\n","\n","train_dist_list = emotion_dist_to_list(train_dist, CASME2_CLASSES)\n","val_dist_list = emotion_dist_to_list(val_dist, CASME2_CLASSES)\n","test_dist_list = emotion_dist_to_list(test_dist, CASME2_CLASSES)\n","\n","print(f\"\\nv8 Train distribution: {train_dist_list}\")\n","print(f\"v8 Val distribution: {val_dist_list}\")\n","print(f\"v8 Test distribution: {test_dist_list}\")\n","\n","# Apply optimized class weights based on loss function selection\n","if USE_FOCAL_LOSS:\n","    class_weights = torch.tensor(FOCAL_LOSS_ALPHA_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied Focal Loss alpha weights: {class_weights.cpu().numpy()}\")\n","    print(f\"Alpha weights sum: {class_weights.sum().item():.3f}\")\n","else:\n","    class_weights = torch.tensor(CROSSENTROPY_CLASS_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied CrossEntropy class weights: {class_weights.cpu().numpy()}\")\n","\n","# CASME II PoolFormer Configuration for Key Frame Sequence with Face-Aware Preprocessing\n","# Optimized for moderate dataset (603 train samples) with class imbalance\n","CASME2_POOLFORMER_CONFIG = {\n","    # Architecture configuration - PoolFormer specific\n","    'poolformer_model': POOLFORMER_MODEL_NAME,\n","    'model_variant': POOLFORMER_MODEL_VARIANT,\n","    'model_params': MODEL_PARAMS,\n","    'input_size': 224,  # PoolFormer native resolution (upscaled from 224px)\n","    'num_classes': 7,\n","    'dropout_rate': 0.3,  # Increased for stronger regularization\n","    'expected_feature_dim': 512,  # CORRECTED: PoolFormer M36/M48 both output 512 features\n","\n","    # Training configuration - optimized for moderate dataset with KFS strategy\n","    'learning_rate': 2e-5,  # Proven optimal for 603 samples\n","    'weight_decay': 1e-5,\n","    'gradient_clip': 1.0,\n","    'num_epochs': 50,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","\n","    # Scheduler configuration - proven effective\n","    'scheduler_type': 'plateau',\n","    'scheduler_mode': 'max',\n","    'scheduler_factor': 0.5,\n","    'scheduler_patience': 3,  # Proven optimal for moderate dataset\n","    'scheduler_min_lr': 1e-7,\n","    'scheduler_monitor': 'validation F1',\n","\n","    # Loss function configuration\n","    'use_focal_loss': USE_FOCAL_LOSS,\n","    'focal_loss_gamma': FOCAL_LOSS_GAMMA,\n","    'focal_loss_alpha_weights': FOCAL_LOSS_ALPHA_WEIGHTS,\n","    'crossentropy_class_weights': CROSSENTROPY_CLASS_WEIGHTS,\n","\n","    # Dataset configuration - v8 KFS specific\n","    'dataset_version': 'v8',\n","    'frame_strategy': 'key_frame_sequence',\n","    'frame_types': ['onset', 'apex', 'offset'],\n","    'training_approach': 'frame_level_independent',\n","    'inference_strategy': 'late_fusion_video_level',\n","\n","    # Regularization configuration\n","    'label_smoothing': 0.0,\n","    'mixup_alpha': 0.0,\n","    'cutmix_alpha': 0.0\n","}\n","\n","# Optimized Focal Loss implementation\n","class OptimizedFocalLoss(nn.Module):\n","    \"\"\"\n","    Optimized Focal Loss with per-class alpha weights\n","    Handles class imbalance with normalized alpha weights (sum = 1.0)\n","    \"\"\"\n","    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n","        super(OptimizedFocalLoss, self).__init__()\n","        if alpha is not None:\n","            self.alpha = torch.tensor(alpha, dtype=torch.float32)\n","        else:\n","            self.alpha = None\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        if self.alpha is not None and self.alpha.device != inputs.device:\n","            self.alpha = self.alpha.to(inputs.device)\n","\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = (1 - pt) ** self.gamma * ce_loss\n","\n","        if self.alpha is not None:\n","            alpha_t = self.alpha[targets]\n","            focal_loss = alpha_t * focal_loss\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","\n","# PoolFormer CASME II Baseline Model - CORRECTED ARCHITECTURE\n","class PoolFormerCASME2Baseline(nn.Module):\n","    \"\"\"\n","    PoolFormer baseline for CASME II micro-expression recognition\n","    CORRECTED: Proper feature dimension handling and spatial pooling\n","    \"\"\"\n","\n","    def __init__(self, num_classes=7, dropout_rate=0.3):\n","        super(PoolFormerCASME2Baseline, self).__init__()\n","\n","        # Load pretrained PoolFormer backbone from HuggingFace\n","        from transformers import PoolFormerModel\n","\n","        self.poolformer = PoolFormerModel.from_pretrained(\n","            CASME2_POOLFORMER_CONFIG['poolformer_model']\n","        )\n","\n","        # Enable fine-tuning for better adaptation\n","        for param in self.poolformer.parameters():\n","            param.requires_grad = True\n","\n","        # Get actual feature dimension from backbone configuration\n","        self.poolformer_feature_dim = self.poolformer.config.hidden_sizes[-1]\n","\n","        print(f\"PoolFormer feature dimension: {self.poolformer_feature_dim}\")\n","\n","        # Simplified classification head for moderate dataset\n","        # Architecture: 512 -> 256 -> 7 (proven effective for small-medium datasets)\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.poolformer_feature_dim, 256),\n","            nn.LayerNorm(256),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","        )\n","\n","        self.classifier = nn.Linear(256, num_classes)\n","\n","        print(f\"Classification head: {self.poolformer_feature_dim} -> 256 -> {num_classes}\")\n","        print(f\"Dropout rate: {dropout_rate} (regularization for moderate dataset)\")\n","\n","    def forward(self, pixel_values):\n","        # Extract features from PoolFormer backbone\n","        poolformer_outputs = self.poolformer(pixel_values=pixel_values)\n","\n","        # PoolFormer output format: [batch_size, channels, height, width]\n","        poolformer_features = poolformer_outputs.last_hidden_state\n","\n","        # CORRECTED: Global average pooling across spatial dimensions\n","        # [batch_size, channels, height, width] -> [batch_size, channels]\n","        poolformer_features = poolformer_features.mean(dim=[2, 3])\n","\n","        # Classification pipeline\n","        processed_features = self.classifier_layers(poolformer_features)\n","        output = self.classifier(processed_features)\n","\n","        return output\n","\n","# Enhanced optimizer and scheduler factory\n","def create_optimizer_scheduler_casme2(model, config):\n","    \"\"\"Create optimizer and scheduler for CASME II PoolFormer training\"\"\"\n","\n","    # AdamW optimizer with proven configuration\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=config['learning_rate'],\n","        weight_decay=config['weight_decay'],\n","        betas=(0.9, 0.999)\n","    )\n","\n","    # ReduceLROnPlateau scheduler monitoring validation F1\n","    if config['scheduler_type'] == 'plateau':\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer,\n","            mode=config['scheduler_mode'],\n","            factor=config['scheduler_factor'],\n","            patience=config['scheduler_patience'],\n","            min_lr=config['scheduler_min_lr']\n","        )\n","        print(f\"Scheduler: ReduceLROnPlateau monitoring {config['scheduler_monitor']}\")\n","    else:\n","        scheduler = None\n","\n","    return optimizer, scheduler\n","\n","# PoolFormer Image Processor setup for 384px input\n","from transformers import PoolFormerImageProcessor\n","\n","print(\"\\nSetting up PoolFormer Image Processor for 384px input...\")\n","\n","poolformer_processor = PoolFormerImageProcessor.from_pretrained(\n","    CASME2_POOLFORMER_CONFIG['poolformer_model'],\n","    do_resize=True,\n","    size={'height': 384, 'width': 384},\n","    do_normalize=True,\n","    do_rescale=True,\n","    do_center_crop=False\n",")\n","\n","print(f\"PoolFormer preprocessing configured:\")\n","print(f\"  Input size: 384x384px\")\n","print(f\"  Resize from: 224x224px (v8 native)\")\n","print(f\"  Token mixing: Pooling-based attention\")\n","\n","# Transform functions for PoolFormer\n","def poolformer_transform_train(image):\n","    \"\"\"\n","    Training transform with PoolFormer Image Processor\n","    Handles grayscale to RGB conversion\n","    \"\"\"\n","    # Ensure RGB format (grayscale images will be converted via channel repetition)\n","    if image.mode != 'RGB':\n","        image = image.convert('RGB')\n","\n","    inputs = poolformer_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","def poolformer_transform_val(image):\n","    \"\"\"\n","    Validation transform with PoolFormer Image Processor\n","    Handles grayscale to RGB conversion\n","    \"\"\"\n","    # Ensure RGB format (grayscale images will be converted via channel repetition)\n","    if image.mode != 'RGB':\n","        image = image.convert('RGB')\n","\n","    inputs = poolformer_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","print(\"PoolFormer Image Processor configured for 384px with token mixing\")\n","\n","# CASME II Dataset for v8 KFS with flat directory structure\n","class CASME2DatasetKFS(Dataset):\n","    \"\"\"CASME II v8 Key Frame Sequence dataset with flexible filename parsing\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        # Load all images from flat directory structure\n","        print(f\"Loading {split} dataset from {split_path}...\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        if len(all_files) > 0:\n","            print(f\"Sample filename: {all_files[0]}\")\n","\n","        loaded_count = 0\n","        skipped_count = 0\n","\n","        for filename in sorted(all_files):\n","            # Extract emotion from filename - flexible approach\n","            # Try multiple patterns to handle different filename formats\n","            emotion_found = None\n","\n","            # Remove file extension first\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            # Pattern 1: Check if any emotion class appears in filename\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                loaded_count += 1\n","            else:\n","                skipped_count += 1\n","                if skipped_count <= 3:\n","                    print(f\"  Skipped (no emotion found): {filename}\")\n","\n","        print(f\"Loaded {len(self.images)} samples for {split} split\")\n","        if skipped_count > 0:\n","            print(f\"  Skipped {skipped_count} files (no recognizable emotion)\")\n","\n","        if len(self.images) == 0:\n","            print(f\"ERROR: No samples loaded! Check filename format and emotion labels.\")\n","            print(f\"Expected emotions in filenames: {CASME2_CLASSES}\")\n","\n","        self._print_distribution()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"  No samples to display distribution\")\n","            return\n","\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.images[idx]\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","# Create directories\n","os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/training_logs\", exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/evaluation_results\", exist_ok=True)\n","\n","print(f\"\\nDataset paths:\")\n","print(f\"Train: {DATASET_ROOT}/train\")\n","print(f\"Validation: {DATASET_ROOT}/val\")\n","print(f\"Test: {DATASET_ROOT}/test\")\n","\n","# Architecture validation\n","print(\"\\nPoolFormer CASME II architecture validation...\")\n","\n","try:\n","    test_model = PoolFormerCASME2Baseline(num_classes=7, dropout_rate=0.3).to(device)\n","    test_input = torch.randn(1, 3, 384, 384).to(device)\n","    test_output = test_model(test_input)\n","\n","    print(f\"Validation successful: Output shape {test_output.shape}\")\n","    print(f\"PoolFormer {POOLFORMER_MODEL_VARIANT.upper()} with {MODEL_PARAMS} parameters\")\n","    print(f\"Token mixing with pooling operations for efficient feature extraction\")\n","\n","    del test_model, test_input, test_output\n","    torch.cuda.empty_cache()\n","\n","except Exception as e:\n","    print(f\"Validation failed: {e}\")\n","    import traceback\n","    traceback.print_exc()\n","\n","# Loss function factory\n","def create_criterion_casme2(weights, use_focal_loss=False, alpha_weights=None, gamma=2.0):\n","    \"\"\"\n","    Factory function to create loss criterion based on configuration\n","\n","    Args:\n","        weights: Class weights for CrossEntropy\n","        use_focal_loss: Whether to use Focal Loss or CrossEntropy\n","        alpha_weights: Per-class alpha weights for Focal Loss\n","        gamma: Focal loss gamma parameter\n","\n","    Returns:\n","        Loss function\n","    \"\"\"\n","    if use_focal_loss:\n","        print(f\"Using Optimized Focal Loss with gamma={gamma}\")\n","        if alpha_weights:\n","            print(f\"Per-class alpha weights: {alpha_weights}\")\n","            print(f\"Alpha sum: {sum(alpha_weights):.3f}\")\n","        return OptimizedFocalLoss(alpha=alpha_weights, gamma=gamma)\n","    else:\n","        print(f\"Using CrossEntropy Loss with optimized class weights\")\n","        print(f\"Class weights: {weights.cpu().numpy()}\")\n","        return nn.CrossEntropyLoss(weight=weights)\n","\n","# Global configuration for training pipeline\n","GLOBAL_CONFIG_CASME2 = {\n","    'device': device,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'num_classes': 7,\n","    'class_weights': class_weights,\n","    'class_names': CASME2_CLASSES,\n","    'class_to_idx': CLASS_TO_IDX,\n","    'transform_train': poolformer_transform_train,\n","    'transform_val': poolformer_transform_val,\n","    'poolformer_config': CASME2_POOLFORMER_CONFIG,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'results_root': RESULTS_ROOT,\n","    'dataset_root': DATASET_ROOT,\n","    'optimizer_scheduler_factory': create_optimizer_scheduler_casme2,\n","    'criterion_factory': create_criterion_casme2\n","}\n","\n","# Configuration validation and summary\n","print(\"\\n\" + \"=\" * 60)\n","print(\"CASME II KEY FRAME SEQUENCE POOLFORMER CONFIGURATION COMPLETE\")\n","print(\"=\" * 60)\n","\n","print(f\"Loss Configuration:\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Function: Optimized Focal Loss\")\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Per-class Alpha: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Function: CrossEntropy with Optimized Weights\")\n","    print(f\"  Class Weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","\n","print(f\"\\nModel Configuration:\")\n","print(f\"  Architecture: PoolFormer\")\n","print(f\"  Variant: {POOLFORMER_MODEL_VARIANT.upper()}\")\n","print(f\"  Model: {POOLFORMER_MODEL_NAME}\")\n","print(f\"  Parameters: {MODEL_PARAMS}\")\n","print(f\"  Input Resolution: 384px (upscaled from 224px v8 native)\")\n","print(f\"  Feature Dimension: {CASME2_POOLFORMER_CONFIG['expected_feature_dim']}\")\n","print(f\"  Token Mixing: Pooling-based attention mechanism\")\n","print(f\"  Classification Head: {CASME2_POOLFORMER_CONFIG['expected_feature_dim']} -> 256 -> 7 (simplified)\")\n","\n","print(f\"\\nDataset Configuration:\")\n","print(f\"  Version: {CASME2_POOLFORMER_CONFIG['dataset_version']}\")\n","print(f\"  Classes: {len(CASME2_CLASSES)}\")\n","print(f\"  Frame strategy: {CASME2_POOLFORMER_CONFIG['frame_strategy']}\")\n","print(f\"  Frame types: {CASME2_POOLFORMER_CONFIG['frame_types']}\")\n","print(f\"  Training approach: {CASME2_POOLFORMER_CONFIG['training_approach']}\")\n","print(f\"  Inference strategy: {CASME2_POOLFORMER_CONFIG['inference_strategy']}\")\n","print(f\"  Weight Optimization: {'Per-class Alpha' if USE_FOCAL_LOSS else 'Inverse Sqrt Frequency'}\")\n","\n","print(f\"\\nTraining Configuration:\")\n","print(f\"  Train samples: {preprocessing_info['splits']['train']['total_images']} frames\")\n","print(f\"  Validation samples: {preprocessing_info['splits']['val']['total_images']} frames\")\n","print(f\"  Test samples: {preprocessing_info['splits']['test']['total_images']} frames\")\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","print(f\"  Learning rate: {CASME2_POOLFORMER_CONFIG['learning_rate']}\")\n","print(f\"  Dropout rate: {CASME2_POOLFORMER_CONFIG['dropout_rate']}\")\n","\n","print(\"\\nNext: Cell 2 - Dataset Loading and Training Pipeline\")"]},{"cell_type":"code","source":["# @title Cell 2: CASME II Key Frame Sequence PoolFormer Training Pipeline\n","\n","# File: 06_03_PoolFormer_CASME2_KFS_Cell2.py\n","# Location: experiments/06_03_PoolFormer_CASME2-KFS-PREP.ipynb\n","# Purpose: Enhanced training pipeline for CASME II Key Frame Sequence PoolFormer with hardened checkpoint system\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing as mp\n","import shutil\n","import tempfile\n","\n","print(\"CASME II Key Frame Sequence PoolFormer Training Pipeline\")\n","print(\"=\" * 70)\n","print(f\"Loss Function: {'Optimized Focal Loss' if CASME2_POOLFORMER_CONFIG['use_focal_loss'] else 'CrossEntropy Loss'}\")\n","if CASME2_POOLFORMER_CONFIG['use_focal_loss']:\n","    print(f\"Focal Loss Parameters:\")\n","    print(f\"  Gamma: {CASME2_POOLFORMER_CONFIG['focal_loss_gamma']}\")\n","    print(f\"  Per-class Alpha: {CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights']}\")\n","    print(f\"  Alpha Sum: {sum(CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","else:\n","    print(f\"CrossEntropy Parameters:\")\n","    print(f\"  Optimized Class Weights: {CASME2_POOLFORMER_CONFIG['crossentropy_class_weights']}\")\n","print(f\"Dataset version: {CASME2_POOLFORMER_CONFIG['dataset_version']}\")\n","print(f\"Frame strategy: {CASME2_POOLFORMER_CONFIG['frame_strategy']}\")\n","print(f\"Training approach: {CASME2_POOLFORMER_CONFIG['training_approach']}\")\n","print(f\"PoolFormer variant: {CASME2_POOLFORMER_CONFIG['model_variant'].upper()}\")\n","print(f\"Model parameters: {CASME2_POOLFORMER_CONFIG['model_params']}\")\n","print(f\"Training epochs: {CASME2_POOLFORMER_CONFIG['num_epochs']}\")\n","print(f\"Scheduler patience: {CASME2_POOLFORMER_CONFIG['scheduler_patience']}\")\n","\n","# Enhanced CASME II Dataset with optimized RAM caching\n","class CASME2DatasetTraining(Dataset):\n","    \"\"\"Enhanced CASME II dataset for training with RAM caching optimization\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","        self.cached_images = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading CASME II {split} dataset for training...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        if len(all_files) > 0:\n","            print(f\"Sample filename: {all_files[0]}\")\n","\n","        loaded_count = 0\n","        skipped_count = 0\n","\n","        for filename in sorted(all_files):\n","            # Extract emotion from filename - flexible approach\n","            emotion_found = None\n","\n","            # Remove file extension first\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            # Check if any emotion class appears in filename\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                loaded_count += 1\n","            else:\n","                skipped_count += 1\n","                if skipped_count <= 3:\n","                    print(f\"  Skipped (no emotion found): {filename}\")\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples\")\n","        if skipped_count > 0:\n","            print(f\"  Skipped {skipped_count} files (no recognizable emotion)\")\n","\n","        if len(self.images) == 0:\n","            print(f\"ERROR: No samples loaded! Check filename format and emotion labels.\")\n","            print(f\"Expected emotions in filenames: {CASME2_CLASSES}\")\n","\n","        self._print_distribution()\n","\n","        if self.use_ram_cache and len(self.images) > 0:\n","            self._preload_to_ram()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"  No samples to display distribution\")\n","            return\n","\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def _preload_to_ram(self):\n","        \"\"\"RAM preloading with parallel loading for training efficiency\"\"\"\n","        if len(self.images) == 0:\n","            print(f\"Skipping RAM preload: No images to load\")\n","            return\n","\n","        print(f\"Preloading {len(self.images)} {self.split} images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            \"\"\"Load single image with error handling\"\"\"\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (224, 224), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=f\"Loading {self.split} to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * 224 * 224 * 3 * 4 / 1e9\n","        print(f\"{self.split.upper()} RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (224, 224), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","# Enhanced metrics calculation with comprehensive error handling\n","def calculate_metrics_safe_robust(predictions, labels, class_names, average='macro'):\n","    \"\"\"\n","    Calculate metrics with enhanced error handling and validation\n","\n","    Args:\n","        predictions: numpy array or list of predicted class indices (NOT logits)\n","        labels: numpy array or list of true class indices\n","        class_names: list of class names\n","        average: averaging method for metrics\n","    \"\"\"\n","    try:\n","        # Convert to numpy arrays if needed\n","        if isinstance(predictions, torch.Tensor):\n","            predictions = predictions.cpu().numpy()\n","        else:\n","            predictions = np.array(predictions)\n","\n","        if isinstance(labels, torch.Tensor):\n","            labels = labels.cpu().numpy()\n","        else:\n","            labels = np.array(labels)\n","\n","        # Validate inputs\n","        if len(predictions) == 0 or len(labels) == 0:\n","            return {\n","                'accuracy': 0.0,\n","                'precision': 0.0,\n","                'recall': 0.0,\n","                'f1_score': 0.0\n","            }\n","\n","        if len(predictions) != len(labels):\n","            print(f\"WARNING: predictions length {len(predictions)} != labels length {len(labels)}\")\n","            min_len = min(len(predictions), len(labels))\n","            predictions = predictions[:min_len]\n","            labels = labels[:min_len]\n","\n","        # Calculate accuracy\n","        accuracy = accuracy_score(labels, predictions)\n","\n","        # Calculate precision, recall, f1\n","        unique_labels = np.unique(np.concatenate([labels, predictions]))\n","\n","        precision, recall, f1, support = precision_recall_fscore_support(\n","            labels, predictions,\n","            labels=unique_labels,\n","            average=average,\n","            zero_division=0\n","        )\n","\n","        return {\n","            'accuracy': float(accuracy),\n","            'precision': float(precision),\n","            'recall': float(recall),\n","            'f1_score': float(f1)\n","        }\n","\n","    except Exception as e:\n","        print(f\"ERROR in calculate_metrics_safe_robust: {e}\")\n","        return {\n","            'accuracy': 0.0,\n","            'precision': 0.0,\n","            'recall': 0.0,\n","            'f1_score': 0.0\n","        }\n","\n","# Enhanced training epoch function\n","def train_epoch(model, dataloader, criterion, optimizer, device, epoch, total_epochs):\n","    \"\"\"Enhanced training epoch with comprehensive metrics\"\"\"\n","    model.train()\n","\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_labels = []\n","\n","    progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch+1}/{total_epochs}\")\n","\n","    for batch_idx, (images, labels, filenames) in enumerate(progress_bar):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","\n","        if CASME2_POOLFORMER_CONFIG['gradient_clip'] > 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), CASME2_POOLFORMER_CONFIG['gradient_clip'])\n","\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","        _, predicted = torch.max(outputs.data, 1)\n","        all_predictions.extend(predicted.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","        if (batch_idx + 1) % 10 == 0:\n","            avg_loss = running_loss / (batch_idx + 1)\n","            progress_bar.set_postfix({'loss': f'{avg_loss:.4f}'})\n","\n","    epoch_loss = running_loss / len(dataloader)\n","    metrics = calculate_metrics_safe_robust(all_predictions, all_labels, CASME2_CLASSES)\n","\n","    return epoch_loss, metrics\n","\n","# Enhanced validation epoch function\n","def validate_epoch(model, dataloader, criterion, device, epoch, total_epochs):\n","    \"\"\"Enhanced validation epoch with comprehensive metrics\"\"\"\n","    model.eval()\n","\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_labels = []\n","    all_filenames = []\n","\n","    progress_bar = tqdm(dataloader, desc=f\"Validation Epoch {epoch+1}/{total_epochs}\")\n","\n","    with torch.no_grad():\n","        for images, labels, filenames in progress_bar:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs.data, 1)\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","            all_filenames.extend(filenames)\n","\n","    epoch_loss = running_loss / len(dataloader)\n","    metrics = calculate_metrics_safe_robust(all_predictions, all_labels, CASME2_CLASSES)\n","\n","    return epoch_loss, metrics, all_filenames\n","\n","# JSON serialization helper\n","def safe_json_serialize(obj):\n","    \"\"\"Convert objects to JSON-serializable format\"\"\"\n","    if isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    elif isinstance(obj, np.integer):\n","        return int(obj)\n","    elif isinstance(obj, np.floating):\n","        return float(obj)\n","    elif isinstance(obj, torch.Tensor):\n","        return obj.cpu().numpy().tolist()\n","    elif isinstance(obj, dict):\n","        return {k: safe_json_serialize(v) for k, v in obj.items()}\n","    elif isinstance(obj, list):\n","        return [safe_json_serialize(item) for item in obj]\n","    else:\n","        return obj\n","\n","# Enhanced checkpoint saving with atomic write\n","def save_checkpoint_robust(model, optimizer, scheduler, epoch, train_metrics, val_metrics,\n","                          checkpoint_dir, best_metrics, config):\n","    \"\"\"\n","    Enhanced checkpoint saving with atomic write and validation\n","\n","    Args:\n","        model: PyTorch model\n","        optimizer: Optimizer\n","        scheduler: Learning rate scheduler\n","        epoch: Current epoch\n","        train_metrics: Training metrics dict\n","        val_metrics: Validation metrics dict\n","        checkpoint_dir: Directory to save checkpoint\n","        best_metrics: Best metrics dict\n","        config: Model configuration dict\n","\n","    Returns:\n","        str: Path to saved checkpoint or None if failed\n","    \"\"\"\n","    try:\n","        os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","        checkpoint_filename = 'casme2_poolformer_kfs_best_f1.pth'\n","        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n","        temp_path = os.path.join(checkpoint_dir, f\"temp_{checkpoint_filename}\")\n","\n","        checkpoint = {\n","            'epoch': epoch + 1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","            'best_val_f1': best_metrics['f1'],\n","            'best_val_loss': best_metrics['loss'],\n","            'best_val_accuracy': best_metrics['accuracy'],\n","            'train_metrics': safe_json_serialize(train_metrics),\n","            'val_metrics': safe_json_serialize(val_metrics),\n","            'config': safe_json_serialize(config)\n","        }\n","\n","        torch.save(checkpoint, temp_path)\n","\n","        validation_checkpoint = torch.load(temp_path, map_location='cpu')\n","        if 'model_state_dict' not in validation_checkpoint:\n","            raise ValueError(\"Checkpoint validation failed: missing model_state_dict\")\n","\n","        if os.path.exists(checkpoint_path):\n","            backup_path = checkpoint_path + '.backup'\n","            shutil.copy2(checkpoint_path, backup_path)\n","\n","        shutil.move(temp_path, checkpoint_path)\n","\n","        if os.path.exists(checkpoint_path + '.backup'):\n","            os.remove(checkpoint_path + '.backup')\n","\n","        return checkpoint_path\n","\n","    except Exception as e:\n","        print(f\"ERROR saving checkpoint: {e}\")\n","        if os.path.exists(temp_path):\n","            os.remove(temp_path)\n","        return None\n","\n","# Create datasets\n","print(\"\\nCreating training and validation datasets...\")\n","train_dataset = CASME2DatasetTraining(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='train',\n","    transform=GLOBAL_CONFIG_CASME2['transform_train'],\n","    use_ram_cache=True\n",")\n","\n","val_dataset = CASME2DatasetTraining(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='val',\n","    transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","    use_ram_cache=True\n",")\n","\n","# Create data loaders\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=CASME2_POOLFORMER_CONFIG['batch_size'],\n","    shuffle=True,\n","    num_workers=CASME2_POOLFORMER_CONFIG['num_workers'],\n","    pin_memory=True,\n","    persistent_workers=True if CASME2_POOLFORMER_CONFIG['num_workers'] > 0 else False\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=CASME2_POOLFORMER_CONFIG['batch_size'],\n","    shuffle=False,\n","    num_workers=CASME2_POOLFORMER_CONFIG['num_workers'],\n","    pin_memory=True,\n","    persistent_workers=True if CASME2_POOLFORMER_CONFIG['num_workers'] > 0 else False\n",")\n","\n","print(f\"\\nDataLoader Configuration:\")\n","print(f\"  Train batches: {len(train_loader)}\")\n","print(f\"  Validation batches: {len(val_loader)}\")\n","print(f\"  Batch size: {CASME2_POOLFORMER_CONFIG['batch_size']}\")\n","print(f\"  Workers: {CASME2_POOLFORMER_CONFIG['num_workers']}\")\n","\n","# Initialize model\n","print(\"\\nInitializing PoolFormer model...\")\n","model = PoolFormerCASME2Baseline(\n","    num_classes=CASME2_POOLFORMER_CONFIG['num_classes'],\n","    dropout_rate=CASME2_POOLFORMER_CONFIG['dropout_rate']\n",").to(GLOBAL_CONFIG_CASME2['device'])\n","\n","print(f\"Model initialized: PoolFormerCASME2Baseline\")\n","print(f\"  Variant: {CASME2_POOLFORMER_CONFIG['model_variant'].upper()}\")\n","print(f\"  Parameters: {CASME2_POOLFORMER_CONFIG['model_params']}\")\n","print(f\"  Feature dim: {CASME2_POOLFORMER_CONFIG['expected_feature_dim']}\")\n","print(f\"  Dropout: {CASME2_POOLFORMER_CONFIG['dropout_rate']}\")\n","\n","# Create optimizer and scheduler\n","optimizer, scheduler = GLOBAL_CONFIG_CASME2['optimizer_scheduler_factory'](model, CASME2_POOLFORMER_CONFIG)\n","\n","print(f\"\\nOptimizer: AdamW\")\n","print(f\"  Learning rate: {CASME2_POOLFORMER_CONFIG['learning_rate']}\")\n","print(f\"  Weight decay: {CASME2_POOLFORMER_CONFIG['weight_decay']}\")\n","\n","# Create loss criterion\n","criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","    weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","    use_focal_loss=CASME2_POOLFORMER_CONFIG['use_focal_loss'],\n","    alpha_weights=CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights'],\n","    gamma=CASME2_POOLFORMER_CONFIG['focal_loss_gamma']\n",")\n","\n","# Training history tracking\n","training_history = {\n","    'train_loss': [],\n","    'val_loss': [],\n","    'train_f1': [],\n","    'val_f1': [],\n","    'train_acc': [],\n","    'val_acc': [],\n","    'learning_rate': [],\n","    'epoch_time': []\n","}\n","\n","# Best metrics tracking with multi-criteria logic\n","best_metrics = {\n","    'f1': 0.0,\n","    'loss': float('inf'),\n","    'accuracy': 0.0,\n","    'epoch': 0\n","}\n","\n","print(\"\\nStarting CASME II Key Frame Sequence PoolFormer training...\")\n","print(f\"Training configuration: {CASME2_POOLFORMER_CONFIG['num_epochs']} epochs\")\n","print(\"=\" * 70)\n","\n","# Main training loop with hardened checkpoint system\n","start_time = time.time()\n","\n","for epoch in range(CASME2_POOLFORMER_CONFIG['num_epochs']):\n","    epoch_start_time = time.time()\n","    print(f\"\\nEpoch {epoch+1}/{CASME2_POOLFORMER_CONFIG['num_epochs']}\")\n","\n","    # Training phase\n","    train_loss, train_metrics = train_epoch(\n","        model, train_loader, criterion, optimizer,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_POOLFORMER_CONFIG['num_epochs']\n","    )\n","\n","    # Validation phase\n","    val_loss, val_metrics, val_filenames = validate_epoch(\n","        model, val_loader, criterion,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_POOLFORMER_CONFIG['num_epochs']\n","    )\n","\n","    # Update scheduler\n","    if scheduler:\n","        scheduler.step(val_metrics['f1_score'])\n","\n","    # Record training history\n","    epoch_time = time.time() - epoch_start_time\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    training_history['train_loss'].append(float(train_loss))\n","    training_history['val_loss'].append(float(val_loss))\n","    training_history['train_f1'].append(float(train_metrics['f1_score']))\n","    training_history['val_f1'].append(float(val_metrics['f1_score']))\n","    training_history['train_acc'].append(float(train_metrics['accuracy']))\n","    training_history['val_acc'].append(float(val_metrics['accuracy']))\n","    training_history['learning_rate'].append(float(current_lr))\n","    training_history['epoch_time'].append(float(epoch_time))\n","\n","    # Print epoch summary\n","    print(f\"Train - Loss: {train_loss:.4f}, F1: {train_metrics['f1_score']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n","    print(f\"Val   - Loss: {val_loss:.4f}, F1: {val_metrics['f1_score']:.4f}, Acc: {val_metrics['accuracy']:.4f}\")\n","    print(f\"Time  - Epoch: {epoch_time:.1f}s, LR: {current_lr:.2e}\")\n","\n","    # Enhanced multi-criteria checkpoint saving logic\n","    save_model = False\n","    improvement_reason = \"\"\n","\n","    if val_metrics['f1_score'] > best_metrics['f1']:\n","        save_model = True\n","        improvement_reason = \"Higher F1\"\n","    elif val_metrics['f1_score'] == best_metrics['f1']:\n","        if val_loss < best_metrics['loss']:\n","            save_model = True\n","            improvement_reason = \"Same F1, Lower Loss\"\n","        elif val_loss == best_metrics['loss'] and val_metrics['accuracy'] > best_metrics['accuracy']:\n","            save_model = True\n","            improvement_reason = \"Same F1&Loss, Higher Accuracy\"\n","\n","    if save_model:\n","        best_metrics['f1'] = val_metrics['f1_score']\n","        best_metrics['loss'] = val_loss\n","        best_metrics['accuracy'] = val_metrics['accuracy']\n","        best_metrics['epoch'] = epoch + 1\n","\n","        best_model_path = save_checkpoint_robust(\n","            model, optimizer, scheduler, epoch,\n","            train_metrics, val_metrics, GLOBAL_CONFIG_CASME2['checkpoint_root'],\n","            best_metrics, CASME2_POOLFORMER_CONFIG\n","        )\n","\n","        if best_model_path:\n","            print(f\"New best model: {improvement_reason} - F1: {best_metrics['f1']:.4f}\")\n","        else:\n","            print(f\"Warning: Failed to save checkpoint despite improvement\")\n","\n","    # Progress tracking\n","    elapsed_time = time.time() - start_time\n","    estimated_total = (elapsed_time / (epoch + 1)) * CASME2_POOLFORMER_CONFIG['num_epochs']\n","    remaining_time = estimated_total - elapsed_time\n","    progress_pct = ((epoch + 1) / CASME2_POOLFORMER_CONFIG['num_epochs']) * 100\n","\n","    print(f\"Progress: {progress_pct:.1f}% | Best F1: {best_metrics['f1']:.4f} | ETA: {remaining_time/60:.1f}min\")\n","\n","# Training completion\n","total_time = time.time() - start_time\n","actual_epochs = CASME2_POOLFORMER_CONFIG['num_epochs']\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II KEY FRAME SEQUENCE POOLFORMER TRAINING COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Training time: {total_time/60:.1f} minutes\")\n","print(f\"Epochs completed: {actual_epochs}\")\n","print(f\"Best validation F1: {best_metrics['f1']:.4f} (epoch {best_metrics['epoch']})\")\n","print(f\"Final train F1: {training_history['train_f1'][-1]:.4f}\")\n","print(f\"Final validation F1: {training_history['val_f1'][-1]:.4f}\")\n","\n","# Enhanced training documentation export\n","results_dir = GLOBAL_CONFIG_CASME2['results_root']\n","os.makedirs(f\"{results_dir}/training_logs\", exist_ok=True)\n","\n","training_history_path = f\"{results_dir}/training_logs/casme2_poolformer_kfs_training_history.json\"\n","\n","print(\"\\nExporting enhanced training documentation...\")\n","\n","try:\n","    training_summary = {\n","        'experiment_type': 'CASME2_PoolFormer_KeyFrameSequence',\n","        'experiment_configuration': {\n","            'dataset_version': CASME2_POOLFORMER_CONFIG['dataset_version'],\n","            'frame_strategy': CASME2_POOLFORMER_CONFIG['frame_strategy'],\n","            'frame_types': CASME2_POOLFORMER_CONFIG['frame_types'],\n","            'training_approach': CASME2_POOLFORMER_CONFIG['training_approach'],\n","            'inference_strategy': CASME2_POOLFORMER_CONFIG['inference_strategy'],\n","            'loss_function': 'Optimized Focal Loss' if CASME2_POOLFORMER_CONFIG['use_focal_loss'] else 'CrossEntropy',\n","            'weight_approach': 'Per-class Alpha (sum=1.0)' if CASME2_POOLFORMER_CONFIG['use_focal_loss'] else 'Inverse Sqrt Frequency',\n","            'focal_loss_gamma': CASME2_POOLFORMER_CONFIG['focal_loss_gamma'],\n","            'focal_loss_alpha_weights': CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights'],\n","            'crossentropy_class_weights': CASME2_POOLFORMER_CONFIG['crossentropy_class_weights'],\n","            'poolformer_model': CASME2_POOLFORMER_CONFIG['poolformer_model'],\n","            'model_variant': CASME2_POOLFORMER_CONFIG['model_variant'],\n","            'model_params': CASME2_POOLFORMER_CONFIG['model_params']\n","        },\n","        'training_history': safe_json_serialize(training_history),\n","        'best_val_f1': float(best_metrics['f1']),\n","        'best_val_loss': float(best_metrics['loss']),\n","        'best_val_accuracy': float(best_metrics['accuracy']),\n","        'best_epoch': int(best_metrics['epoch']),\n","        'total_epochs': int(actual_epochs),\n","        'total_time_minutes': float(total_time / 60),\n","        'average_epoch_time_seconds': float(np.mean(training_history['epoch_time'])),\n","        'config': safe_json_serialize(CASME2_POOLFORMER_CONFIG),\n","        'final_train_f1': float(training_history['train_f1'][-1]),\n","        'final_val_f1': float(training_history['val_f1'][-1]),\n","        'model_checkpoint': 'casme2_poolformer_kfs_best_f1.pth',\n","        'dataset_info': {\n","            'name': 'CASME_II',\n","            'version': CASME2_POOLFORMER_CONFIG['dataset_version'],\n","            'frame_strategy': CASME2_POOLFORMER_CONFIG['frame_strategy'],\n","            'frame_types': CASME2_POOLFORMER_CONFIG['frame_types'],\n","            'train_samples': len(train_dataset),\n","            'val_samples': len(val_dataset),\n","            'num_classes': 7,\n","            'class_names': CASME2_CLASSES\n","        },\n","        'architecture_info': {\n","            'model_type': 'PoolFormerCASME2Baseline',\n","            'backbone': CASME2_POOLFORMER_CONFIG['poolformer_model'],\n","            'variant': CASME2_POOLFORMER_CONFIG['model_variant'],\n","            'parameters': CASME2_POOLFORMER_CONFIG['model_params'],\n","            'input_size': f\"{CASME2_POOLFORMER_CONFIG['input_size']}x{CASME2_POOLFORMER_CONFIG['input_size']}\",\n","            'expected_feature_dim': CASME2_POOLFORMER_CONFIG['expected_feature_dim'],\n","            'classification_head': f\"{CASME2_POOLFORMER_CONFIG['expected_feature_dim']}->256->7\",\n","            'token_mixing': 'pooling_based_attention'\n","        },\n","        'enhanced_features': {\n","            'hardened_checkpoint_system': True,\n","            'atomic_checkpoint_save': True,\n","            'checkpoint_validation': True,\n","            'model_output_validation': True,\n","            'enhanced_error_handling': True,\n","            'multi_criteria_checkpoint_logic': True,\n","            'memory_optimized_training': True,\n","            'ram_caching': True,\n","            'token_mixing_pooling': True\n","        }\n","    }\n","\n","    with open(training_history_path, 'w') as f:\n","        json.dump(training_summary, f, indent=2)\n","\n","    print(f\"Enhanced training documentation saved successfully: {training_history_path}\")\n","    print(f\"Experiment details: {training_summary['experiment_configuration']['loss_function']} loss\")\n","    if CASME2_POOLFORMER_CONFIG['use_focal_loss']:\n","        print(f\"  Gamma: {CASME2_POOLFORMER_CONFIG['focal_loss_gamma']}, Alpha Sum: {sum(CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","    print(f\"Model variant: {CASME2_POOLFORMER_CONFIG['model_variant'].upper()}\")\n","    print(f\"Model parameters: {CASME2_POOLFORMER_CONFIG['model_params']}\")\n","    print(f\"Dataset version: {CASME2_POOLFORMER_CONFIG['dataset_version']}\")\n","\n","except Exception as e:\n","    print(f\"Warning: Could not save training documentation: {e}\")\n","    print(\"Training completed successfully, but documentation export failed\")\n","\n","# Enhanced memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\nNext: Cell 3 - CASME II Key Frame Sequence PoolFormer Evaluation with Late Fusion\")\n","print(\"Enhanced training pipeline with hardened checkpoint system completed successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"uk6Rq18pPa3T","executionInfo":{"status":"ok","timestamp":1761203575295,"user_tz":-420,"elapsed":1498702,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"b7680a7c-d940-4654-d2ed-6736508e0fd9","collapsed":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II Key Frame Sequence PoolFormer Training Pipeline\n","======================================================================\n","Loss Function: Optimized Focal Loss\n","Focal Loss Parameters:\n","  Gamma: 2.0\n","  Per-class Alpha: [0.055, 0.069, 0.098, 0.107, 0.109, 0.219, 0.343]\n","  Alpha Sum: 1.000\n","Dataset version: v8\n","Frame strategy: key_frame_sequence\n","Training approach: frame_level_independent\n","PoolFormer variant: M48\n","Model parameters: 73M\n","Training epochs: 50\n","Scheduler patience: 3\n","\n","Creating training and validation datasets...\n","Loading CASME II train dataset for training...\n","Found 603 image files in directory\n","Sample filename: sub01_EP02_01f_onset_happiness.jpg\n","Loaded 603 CASME II train samples\n","  others: 237 samples (39.3%)\n","  disgust: 150 samples (24.9%)\n","  happiness: 75 samples (12.4%)\n","  repression: 63 samples (10.4%)\n","  surprise: 60 samples (10.0%)\n","  sadness: 15 samples (2.5%)\n","  fear: 3 samples (0.5%)\n","Preloading 603 train images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading train to RAM: 100%|██████████| 603/603 [00:12<00:00, 46.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TRAIN RAM caching completed: 603/603 images, ~0.36GB\n","Loading CASME II val dataset for training...\n","Found 78 image files in directory\n","Sample filename: sub01_EP03_02_apex_others.jpg\n","Loaded 78 CASME II val samples\n","  others: 30 samples (38.5%)\n","  disgust: 18 samples (23.1%)\n","  happiness: 9 samples (11.5%)\n","  repression: 9 samples (11.5%)\n","  surprise: 6 samples (7.7%)\n","  sadness: 3 samples (3.8%)\n","  fear: 3 samples (3.8%)\n","Preloading 78 val images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading val to RAM: 100%|██████████| 78/78 [00:03<00:00, 25.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["VAL RAM caching completed: 78/78 images, ~0.05GB\n","\n","DataLoader Configuration:\n","  Train batches: 76\n","  Validation batches: 10\n","  Batch size: 8\n","  Workers: 4\n","\n","Initializing PoolFormer model...\n","PoolFormer feature dimension: 768\n","Classification head: 768 -> 256 -> 7\n","Dropout rate: 0.3 (regularization for moderate dataset)\n","Model initialized: PoolFormerCASME2Baseline\n","  Variant: M48\n","  Parameters: 73M\n","  Feature dim: 512\n","  Dropout: 0.3\n","Scheduler: ReduceLROnPlateau monitoring validation F1\n","\n","Optimizer: AdamW\n","  Learning rate: 2e-05\n","  Weight decay: 1e-05\n","Using Optimized Focal Loss with gamma=2.0\n","Per-class alpha weights: [0.055, 0.069, 0.098, 0.107, 0.109, 0.219, 0.343]\n","Alpha sum: 1.000\n","\n","Starting CASME II Key Frame Sequence PoolFormer training...\n","Training configuration: 50 epochs\n","======================================================================\n","\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1/50: 100%|██████████| 76/76 [00:31<00:00,  2.41it/s, loss=0.1065]\n","Validation Epoch 1/50: 100%|██████████| 10/10 [00:02<00:00,  4.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.1030, F1: 0.2010, Acc: 0.3367\n","Val   - Loss: 0.1368, F1: 0.1257, Acc: 0.2436\n","Time  - Epoch: 33.8s, LR: 2.00e-05\n","New best model: Higher F1 - F1: 0.1257\n","Progress: 2.0% | Best F1: 0.1257 | ETA: 29.8min\n","\n","Epoch 2/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 2/50: 100%|██████████| 76/76 [00:27<00:00,  2.80it/s, loss=0.0734]\n","Validation Epoch 2/50: 100%|██████████| 10/10 [00:01<00:00,  7.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0712, F1: 0.3447, Acc: 0.5207\n","Val   - Loss: 0.1303, F1: 0.2383, Acc: 0.3718\n","Time  - Epoch: 28.5s, LR: 2.00e-05\n","New best model: Higher F1 - F1: 0.2383\n","Progress: 4.0% | Best F1: 0.2383 | ETA: 28.2min\n","\n","Epoch 3/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 3/50: 100%|██████████| 76/76 [00:27<00:00,  2.76it/s, loss=0.0447]\n","Validation Epoch 3/50: 100%|██████████| 10/10 [00:01<00:00,  7.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0459, F1: 0.5410, Acc: 0.6965\n","Val   - Loss: 0.1434, F1: 0.1907, Acc: 0.3462\n","Time  - Epoch: 29.0s, LR: 2.00e-05\n","Progress: 6.0% | Best F1: 0.2383 | ETA: 26.0min\n","\n","Epoch 4/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 4/50: 100%|██████████| 76/76 [00:27<00:00,  2.72it/s, loss=0.0243]\n","Validation Epoch 4/50: 100%|██████████| 10/10 [00:01<00:00,  7.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0239, F1: 0.7097, Acc: 0.8524\n","Val   - Loss: 0.1511, F1: 0.1627, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 2.00e-05\n","Progress: 8.0% | Best F1: 0.2383 | ETA: 24.7min\n","\n","Epoch 5/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 5/50: 100%|██████████| 76/76 [00:28<00:00,  2.67it/s, loss=0.0139]\n","Validation Epoch 5/50: 100%|██████████| 10/10 [00:01<00:00,  7.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0135, F1: 0.8795, Acc: 0.9221\n","Val   - Loss: 0.1588, F1: 0.1422, Acc: 0.3077\n","Time  - Epoch: 29.9s, LR: 2.00e-05\n","Progress: 10.0% | Best F1: 0.2383 | ETA: 23.8min\n","\n","Epoch 6/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 6/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0061]\n","Validation Epoch 6/50: 100%|██████████| 10/10 [00:01<00:00,  7.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0060, F1: 0.9803, Acc: 0.9818\n","Val   - Loss: 0.1735, F1: 0.1186, Acc: 0.2949\n","Time  - Epoch: 29.4s, LR: 1.00e-05\n","Progress: 12.0% | Best F1: 0.2383 | ETA: 23.0min\n","\n","Epoch 7/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 7/50: 100%|██████████| 76/76 [00:27<00:00,  2.72it/s, loss=0.0025]\n","Validation Epoch 7/50: 100%|██████████| 10/10 [00:01<00:00,  7.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0026, F1: 0.9951, Acc: 0.9950\n","Val   - Loss: 0.1717, F1: 0.1682, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 1.00e-05\n","Progress: 14.0% | Best F1: 0.2383 | ETA: 22.3min\n","\n","Epoch 8/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 8/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0019]\n","Validation Epoch 8/50: 100%|██████████| 10/10 [00:01<00:00,  7.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0019, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1769, F1: 0.1842, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 1.00e-05\n","Progress: 16.0% | Best F1: 0.2383 | ETA: 21.6min\n","\n","Epoch 9/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 9/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0014]\n","Validation Epoch 9/50: 100%|██████████| 10/10 [00:01<00:00,  7.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0014, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1824, F1: 0.1517, Acc: 0.2949\n","Time  - Epoch: 29.4s, LR: 1.00e-05\n","Progress: 18.0% | Best F1: 0.2383 | ETA: 21.0min\n","\n","Epoch 10/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 10/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0010]\n","Validation Epoch 10/50: 100%|██████████| 10/10 [00:01<00:00,  7.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0010, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1797, F1: 0.1506, Acc: 0.2949\n","Time  - Epoch: 29.4s, LR: 5.00e-06\n","Progress: 20.0% | Best F1: 0.2383 | ETA: 20.4min\n","\n","Epoch 11/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 11/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0010]\n","Validation Epoch 11/50: 100%|██████████| 10/10 [00:01<00:00,  7.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0010, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1816, F1: 0.1616, Acc: 0.3205\n","Time  - Epoch: 29.4s, LR: 5.00e-06\n","Progress: 22.0% | Best F1: 0.2383 | ETA: 19.8min\n","\n","Epoch 12/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 12/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0009]\n","Validation Epoch 12/50: 100%|██████████| 10/10 [00:01<00:00,  7.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1827, F1: 0.1618, Acc: 0.3205\n","Time  - Epoch: 29.4s, LR: 5.00e-06\n","Progress: 24.0% | Best F1: 0.2383 | ETA: 19.2min\n","\n","Epoch 13/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 13/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0007]\n","Validation Epoch 13/50: 100%|██████████| 10/10 [00:01<00:00,  7.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0007, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1836, F1: 0.1896, Acc: 0.3590\n","Time  - Epoch: 29.4s, LR: 5.00e-06\n","Progress: 26.0% | Best F1: 0.2383 | ETA: 18.7min\n","\n","Epoch 14/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 14/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0006]\n","Validation Epoch 14/50: 100%|██████████| 10/10 [00:01<00:00,  7.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0007, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1834, F1: 0.1843, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 2.50e-06\n","Progress: 28.0% | Best F1: 0.2383 | ETA: 18.1min\n","\n","Epoch 15/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 15/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0006]\n","Validation Epoch 15/50: 100%|██████████| 10/10 [00:01<00:00,  7.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1858, F1: 0.1843, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 2.50e-06\n","Progress: 30.0% | Best F1: 0.2383 | ETA: 17.6min\n","\n","Epoch 16/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 16/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0005]\n","Validation Epoch 16/50: 100%|██████████| 10/10 [00:01<00:00,  7.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1855, F1: 0.1618, Acc: 0.3205\n","Time  - Epoch: 29.4s, LR: 2.50e-06\n","Progress: 32.0% | Best F1: 0.2383 | ETA: 17.1min\n","\n","Epoch 17/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 17/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0006]\n","Validation Epoch 17/50: 100%|██████████| 10/10 [00:01<00:00,  7.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1857, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 2.50e-06\n","Progress: 34.0% | Best F1: 0.2383 | ETA: 16.5min\n","\n","Epoch 18/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 18/50: 100%|██████████| 76/76 [00:27<00:00,  2.71it/s, loss=0.0006]\n","Validation Epoch 18/50: 100%|██████████| 10/10 [00:01<00:00,  7.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1868, F1: 0.1896, Acc: 0.3590\n","Time  - Epoch: 29.4s, LR: 1.25e-06\n","Progress: 36.0% | Best F1: 0.2383 | ETA: 16.0min\n","\n","Epoch 19/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 19/50: 100%|██████████| 76/76 [00:27<00:00,  2.72it/s, loss=0.0006]\n","Validation Epoch 19/50: 100%|██████████| 10/10 [00:01<00:00,  7.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1862, F1: 0.1843, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 1.25e-06\n","Progress: 38.0% | Best F1: 0.2383 | ETA: 15.5min\n","\n","Epoch 20/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 20/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0005]\n","Validation Epoch 20/50: 100%|██████████| 10/10 [00:01<00:00,  7.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1871, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 1.25e-06\n","Progress: 40.0% | Best F1: 0.2383 | ETA: 15.0min\n","\n","Epoch 21/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 21/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 21/50: 100%|██████████| 10/10 [00:01<00:00,  7.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1872, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 1.25e-06\n","Progress: 42.0% | Best F1: 0.2383 | ETA: 14.5min\n","\n","Epoch 22/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 22/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0005]\n","Validation Epoch 22/50: 100%|██████████| 10/10 [00:01<00:00,  7.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1875, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 6.25e-07\n","Progress: 44.0% | Best F1: 0.2383 | ETA: 14.0min\n","\n","Epoch 23/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 23/50: 100%|██████████| 76/76 [00:27<00:00,  2.71it/s, loss=0.0005]\n","Validation Epoch 23/50: 100%|██████████| 10/10 [00:01<00:00,  7.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1878, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 6.25e-07\n","Progress: 46.0% | Best F1: 0.2383 | ETA: 13.5min\n","\n","Epoch 24/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 24/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 24/50: 100%|██████████| 10/10 [00:01<00:00,  7.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1884, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 6.25e-07\n","Progress: 48.0% | Best F1: 0.2383 | ETA: 12.9min\n","\n","Epoch 25/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 25/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0005]\n","Validation Epoch 25/50: 100%|██████████| 10/10 [00:01<00:00,  7.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1887, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 6.25e-07\n","Progress: 50.0% | Best F1: 0.2383 | ETA: 12.4min\n","\n","Epoch 26/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 26/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 26/50: 100%|██████████| 10/10 [00:01<00:00,  7.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1889, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 3.13e-07\n","Progress: 52.0% | Best F1: 0.2383 | ETA: 11.9min\n","\n","Epoch 27/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 27/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 27/50: 100%|██████████| 10/10 [00:01<00:00,  7.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1887, F1: 0.1618, Acc: 0.3205\n","Time  - Epoch: 29.4s, LR: 3.13e-07\n","Progress: 54.0% | Best F1: 0.2383 | ETA: 11.4min\n","\n","Epoch 28/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 28/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 28/50: 100%|██████████| 10/10 [00:01<00:00,  7.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1887, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 3.13e-07\n","Progress: 56.0% | Best F1: 0.2383 | ETA: 10.9min\n","\n","Epoch 29/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 29/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 29/50: 100%|██████████| 10/10 [00:01<00:00,  7.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1885, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 3.13e-07\n","Progress: 58.0% | Best F1: 0.2383 | ETA: 10.4min\n","\n","Epoch 30/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 30/50: 100%|██████████| 76/76 [00:27<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 30/50: 100%|██████████| 10/10 [00:01<00:00,  7.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1885, F1: 0.1618, Acc: 0.3205\n","Time  - Epoch: 29.4s, LR: 1.56e-07\n","Progress: 60.0% | Best F1: 0.2383 | ETA: 9.9min\n","\n","Epoch 31/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 31/50: 100%|██████████| 76/76 [00:27<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 31/50: 100%|██████████| 10/10 [00:01<00:00,  7.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1886, F1: 0.1563, Acc: 0.3077\n","Time  - Epoch: 29.4s, LR: 1.56e-07\n","Progress: 62.0% | Best F1: 0.2383 | ETA: 9.4min\n","\n","Epoch 32/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 32/50: 100%|██████████| 76/76 [00:28<00:00,  2.70it/s, loss=0.0004]\n","Validation Epoch 32/50: 100%|██████████| 10/10 [00:01<00:00,  7.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1887, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.5s, LR: 1.56e-07\n","Progress: 64.0% | Best F1: 0.2383 | ETA: 8.9min\n","\n","Epoch 33/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 33/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 33/50: 100%|██████████| 10/10 [00:01<00:00,  7.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1887, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 1.56e-07\n","Progress: 66.0% | Best F1: 0.2383 | ETA: 8.4min\n","\n","Epoch 34/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 34/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 34/50: 100%|██████████| 10/10 [00:01<00:00,  7.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1888, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 68.0% | Best F1: 0.2383 | ETA: 7.9min\n","\n","Epoch 35/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 35/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 35/50: 100%|██████████| 10/10 [00:01<00:00,  7.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1889, F1: 0.1845, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 70.0% | Best F1: 0.2383 | ETA: 7.4min\n","\n","Epoch 36/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 36/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0005]\n","Validation Epoch 36/50: 100%|██████████| 10/10 [00:01<00:00,  7.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1889, F1: 0.1845, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 72.0% | Best F1: 0.2383 | ETA: 6.9min\n","\n","Epoch 37/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 37/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0005]\n","Validation Epoch 37/50: 100%|██████████| 10/10 [00:01<00:00,  7.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1888, F1: 0.1845, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 74.0% | Best F1: 0.2383 | ETA: 6.4min\n","\n","Epoch 38/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 38/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 38/50: 100%|██████████| 10/10 [00:01<00:00,  7.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1887, F1: 0.1845, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 76.0% | Best F1: 0.2383 | ETA: 5.9min\n","\n","Epoch 39/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 39/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 39/50: 100%|██████████| 10/10 [00:01<00:00,  7.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1889, F1: 0.1845, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 78.0% | Best F1: 0.2383 | ETA: 5.4min\n","\n","Epoch 40/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 40/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 40/50: 100%|██████████| 10/10 [00:01<00:00,  7.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1892, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 80.0% | Best F1: 0.2383 | ETA: 4.9min\n","\n","Epoch 41/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 41/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 41/50: 100%|██████████| 10/10 [00:01<00:00,  7.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1893, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 82.0% | Best F1: 0.2383 | ETA: 4.5min\n","\n","Epoch 42/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 42/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 42/50: 100%|██████████| 10/10 [00:01<00:00,  7.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1893, F1: 0.1845, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 84.0% | Best F1: 0.2383 | ETA: 4.0min\n","\n","Epoch 43/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 43/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 43/50: 100%|██████████| 10/10 [00:01<00:00,  7.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1893, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 86.0% | Best F1: 0.2383 | ETA: 3.5min\n","\n","Epoch 44/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 44/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 44/50: 100%|██████████| 10/10 [00:01<00:00,  7.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1893, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 88.0% | Best F1: 0.2383 | ETA: 3.0min\n","\n","Epoch 45/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 45/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 45/50: 100%|██████████| 10/10 [00:01<00:00,  7.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1892, F1: 0.1845, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 90.0% | Best F1: 0.2383 | ETA: 2.5min\n","\n","Epoch 46/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 46/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 46/50: 100%|██████████| 10/10 [00:01<00:00,  7.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1892, F1: 0.1845, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 92.0% | Best F1: 0.2383 | ETA: 2.0min\n","\n","Epoch 47/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 47/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 47/50: 100%|██████████| 10/10 [00:01<00:00,  7.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1892, F1: 0.1672, Acc: 0.3333\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 94.0% | Best F1: 0.2383 | ETA: 1.5min\n","\n","Epoch 48/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 48/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 48/50: 100%|██████████| 10/10 [00:01<00:00,  7.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1893, F1: 0.1845, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 96.0% | Best F1: 0.2383 | ETA: 1.0min\n","\n","Epoch 49/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 49/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 49/50: 100%|██████████| 10/10 [00:01<00:00,  7.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1895, F1: 0.1845, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 98.0% | Best F1: 0.2383 | ETA: 0.5min\n","\n","Epoch 50/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 50/50: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s, loss=0.0004]\n","Validation Epoch 50/50: 100%|██████████| 10/10 [00:01<00:00,  7.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.1895, F1: 0.1845, Acc: 0.3462\n","Time  - Epoch: 29.4s, LR: 1.00e-07\n","Progress: 100.0% | Best F1: 0.2383 | ETA: 0.0min\n","\n","======================================================================\n","CASME II KEY FRAME SEQUENCE POOLFORMER TRAINING COMPLETED\n","======================================================================\n","Training time: 24.7 minutes\n","Epochs completed: 50\n","Best validation F1: 0.2383 (epoch 2)\n","Final train F1: 1.0000\n","Final validation F1: 0.1845\n","\n","Exporting enhanced training documentation...\n","Enhanced training documentation saved successfully: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/06_03_poolformer_casme2_kfs_prep/training_logs/casme2_poolformer_kfs_training_history.json\n","Experiment details: Optimized Focal Loss loss\n","  Gamma: 2.0, Alpha Sum: 1.000\n","Model variant: M48\n","Model parameters: 73M\n","Dataset version: v8\n","\n","Next: Cell 3 - CASME II Key Frame Sequence PoolFormer Evaluation with Late Fusion\n","Enhanced training pipeline with hardened checkpoint system completed successfully!\n"]}]},{"cell_type":"code","source":["# @title Cell 3: CASME II PoolFormer Evaluation with Dual Dataset Support\n","\n","# File: 06_03_PoolFormer_CASME2_KFS_Cell3.py\n","# Location: experiments/06_03_PoolFormer_CASME2-KFS-PREP.ipynb\n","# Purpose: Comprehensive evaluation framework with support for AF (v7) and KFS (v8) with late fusion\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from datetime import datetime\n","from collections import defaultdict\n","\n","from sklearn.metrics import (\n","    accuracy_score, precision_recall_fscore_support,\n","    classification_report, confusion_matrix,\n","    roc_curve, auc\n",")\n","from sklearn.preprocessing import label_binarize\n","from concurrent.futures import ThreadPoolExecutor\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =====================================================\n","# DUAL DATASET EVALUATION CONFIGURATION\n","# =====================================================\n","# Configure which test datasets to evaluate:\n","# 'v7' = Apex Frame preprocessing (28 samples, frame-level evaluation)\n","# 'v8' = Key Frame Sequence preprocessing (84 frames -> 28 videos with late fusion)\n","\n","EVALUATE_DATASETS = ['v7', 'v8']  # Can be ['v7'], ['v8'], or ['v7', 'v8']\n","\n","print(\"CASME II PoolFormer Evaluation Framework with Dual Dataset Support\")\n","print(\"=\" * 60)\n","print(f\"Datasets to evaluate: {EVALUATE_DATASETS}\")\n","print(\"=\" * 60)\n","\n","# =====================================================\n","# DATASET CONFIGURATION FUNCTION\n","# =====================================================\n","\n","def get_test_dataset_config(version, project_root):\n","    \"\"\"\n","    Get test dataset configuration based on version\n","\n","    Args:\n","        version: 'v7' (AF) or 'v8' (KFS)\n","        project_root: Project root path\n","\n","    Returns:\n","        dict: Configuration for selected test dataset\n","    \"\"\"\n","    if version == 'v7':\n","        config = {\n","            'version': 'v7',\n","            'variant': 'AF',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/preprocessed_v7\",\n","            'preprocessing_summary': 'preprocessing_summary.json',\n","            'description': 'Apex Frame with Face-Aware Preprocessing',\n","            'expected_samples': 28,\n","            'frame_strategy': 'apex_frame',\n","            'evaluation_mode': 'frame_level',\n","            'aggregation': None\n","        }\n","    elif version == 'v8':\n","        config = {\n","            'version': 'v8',\n","            'variant': 'KFS',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/preprocessed_v8\",\n","            'preprocessing_summary': 'preprocessing_summary.json',\n","            'description': 'Key Frame Sequence with Face-Aware Preprocessing',\n","            'expected_frames': 84,\n","            'expected_videos': 28,\n","            'frame_strategy': 'key_frame_sequence',\n","            'frame_types': ['onset', 'apex', 'offset'],\n","            'evaluation_mode': 'video_level',\n","            'aggregation': 'late_fusion'\n","        }\n","    else:\n","        raise ValueError(f\"Invalid version: {version}. Must be 'v7' or 'v8'\")\n","\n","    return config\n","\n","# =====================================================\n","# VIDEO ID EXTRACTION FOR KFS LATE FUSION\n","# =====================================================\n","\n","def extract_video_id_from_filename(filename):\n","    \"\"\"\n","    Extract video ID from KFS filename by removing frame type suffix\n","\n","    Expected format: sub01_EP02_01f_happiness_onset.jpg\n","    Video ID: sub01_EP02_01f_happiness\n","\n","    Args:\n","        filename: Image filename with frame type\n","\n","    Returns:\n","        str: Video ID without frame type\n","    \"\"\"\n","    # Remove file extension\n","    name_without_ext = filename.rsplit('.', 1)[0]\n","\n","    # Remove frame type suffix (onset, apex, offset)\n","    for frame_type in ['onset', 'apex', 'offset']:\n","        if name_without_ext.endswith(f'_{frame_type}'):\n","            video_id = name_without_ext.rsplit(f'_{frame_type}', 1)[0]\n","            return video_id\n","\n","    # If no frame type found, return as is\n","    return name_without_ext\n","\n","# =====================================================\n","# ENHANCED TEST DATASET CLASS\n","# =====================================================\n","\n","class CASME2DatasetEvaluation(Dataset):\n","    \"\"\"Enhanced CASME II test dataset with comprehensive evaluation support\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","        self.emotions = []\n","        self.video_ids = []\n","        self.cached_images = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading CASME II {split} dataset for evaluation...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        loaded_count = 0\n","\n","        for filename in sorted(all_files):\n","            emotion_found = None\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                video_id = extract_video_id_from_filename(filename)\n","\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                self.emotions.append(emotion_found)\n","                self.video_ids.append(video_id)\n","                loaded_count += 1\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples for evaluation\")\n","        self._print_evaluation_distribution()\n","\n","        if self.use_ram_cache and len(self.images) > 0:\n","            self._preload_to_ram_evaluation()\n","\n","    def _print_evaluation_distribution(self):\n","        \"\"\"Print comprehensive distribution for evaluation analysis\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"No test samples found!\")\n","            return\n","\n","        label_counts = {}\n","        unique_videos = set(self.video_ids)\n","\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        print(\"Test set class distribution:\")\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","        print(f\"Unique video IDs: {len(unique_videos)}\")\n","\n","        missing_classes = []\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i not in label_counts:\n","                missing_classes.append(class_name)\n","\n","        if missing_classes:\n","            print(f\"Missing classes in test set: {missing_classes}\")\n","\n","    def _preload_to_ram_evaluation(self):\n","        \"\"\"RAM preloading optimized for evaluation\"\"\"\n","        if len(self.images) == 0:\n","            return\n","\n","        print(f\"Preloading {len(self.images)} {self.split} images to RAM...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except:\n","                return idx, Image.new('RGB', (224, 224), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=f\"Loading {self.split} to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        print(f\"RAM caching completed: {valid_images}/{len(self.images)} images\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (224, 224), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","# =====================================================\n","# MODEL LOADING FUNCTION\n","# =====================================================\n","\n","def load_trained_model_casme2(checkpoint_path, device):\n","    \"\"\"Load trained PoolFormer model from checkpoint\"\"\"\n","    print(f\"Loading trained model from: {os.path.basename(checkpoint_path)}\")\n","\n","    if not os.path.exists(checkpoint_path):\n","        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n","\n","    checkpoint = torch.load(checkpoint_path, map_location=device)\n","\n","    model = PoolFormerCASME2Baseline(\n","        num_classes=CASME2_POOLFORMER_CONFIG['num_classes'],\n","        dropout_rate=CASME2_POOLFORMER_CONFIG['dropout_rate']\n","    ).to(device)\n","\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","\n","    training_info = {\n","        'best_epoch': checkpoint.get('epoch', 0),\n","        'best_val_f1': checkpoint.get('best_val_f1', 0.0),\n","        'best_val_loss': checkpoint.get('best_val_loss', 0.0),\n","        'best_val_accuracy': checkpoint.get('best_val_accuracy', 0.0)\n","    }\n","\n","    print(f\"Model loaded successfully\")\n","    print(f\"  Best epoch: {training_info['best_epoch']}\")\n","    print(f\"  Best val F1: {training_info['best_val_f1']:.4f}\")\n","    print(f\"  Best val loss: {training_info['best_val_loss']:.4f}\")\n","\n","    return model, training_info\n","\n","# =====================================================\n","# FRAME-LEVEL INFERENCE (v7 AF)\n","# =====================================================\n","\n","def run_frame_level_inference(model, dataloader, device):\n","    \"\"\"Run frame-level inference for AF evaluation\"\"\"\n","    print(\"Running frame-level inference...\")\n","\n","    model.eval()\n","\n","    all_predictions = []\n","    all_labels = []\n","    all_probs = []\n","    all_filenames = []\n","\n","    inference_start = time.time()\n","\n","    with torch.no_grad():\n","        for images, labels, filenames in tqdm(dataloader, desc=\"Frame-level inference\"):\n","            images = images.to(device)\n","\n","            outputs = model(images)\n","            probs = torch.softmax(outputs, dim=1)\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","            all_probs.extend(probs.cpu().numpy())\n","            all_filenames.extend(filenames)\n","\n","    inference_time = time.time() - inference_start\n","\n","    print(f\"Frame-level inference completed: {len(all_predictions)} predictions in {inference_time:.2f}s\")\n","\n","    return {\n","        'predictions': np.array(all_predictions),\n","        'labels': np.array(all_labels),\n","        'probabilities': np.array(all_probs),\n","        'filenames': all_filenames,\n","        'inference_time': inference_time,\n","        'evaluation_mode': 'frame_level'\n","    }\n","\n","# =====================================================\n","# VIDEO-LEVEL INFERENCE WITH LATE FUSION (v8 KFS)\n","# =====================================================\n","\n","def run_video_level_inference_late_fusion(model, dataloader, device):\n","    \"\"\"Run video-level inference with late fusion for KFS evaluation\"\"\"\n","    print(\"Running video-level inference with late fusion...\")\n","\n","    model.eval()\n","\n","    frame_predictions = []\n","    frame_labels = []\n","    frame_probs = []\n","    frame_filenames = []\n","\n","    inference_start = time.time()\n","\n","    with torch.no_grad():\n","        for images, labels, filenames in tqdm(dataloader, desc=\"Frame-level inference\"):\n","            images = images.to(device)\n","\n","            outputs = model(images)\n","            probs = torch.softmax(outputs, dim=1)\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            frame_predictions.extend(predicted.cpu().numpy())\n","            frame_labels.extend(labels.numpy())\n","            frame_probs.extend(probs.cpu().numpy())\n","            frame_filenames.extend(filenames)\n","\n","    print(f\"Frame-level predictions completed: {len(frame_predictions)} frames\")\n","\n","    # Group frames by video ID for late fusion\n","    video_data = defaultdict(lambda: {\n","        'frame_probs': [],\n","        'frame_predictions': [],\n","        'true_label': None,\n","        'filenames': []\n","    })\n","\n","    for idx, filename in enumerate(frame_filenames):\n","        video_id = extract_video_id_from_filename(filename)\n","        video_data[video_id]['frame_probs'].append(frame_probs[idx])\n","        video_data[video_id]['frame_predictions'].append(frame_predictions[idx])\n","        video_data[video_id]['filenames'].append(filename)\n","\n","        if video_data[video_id]['true_label'] is None:\n","            video_data[video_id]['true_label'] = frame_labels[idx]\n","\n","    print(f\"Grouped into {len(video_data)} unique videos\")\n","\n","    # Perform late fusion (average probabilities)\n","    video_predictions = []\n","    video_labels = []\n","    video_probs = []\n","    video_ids = []\n","\n","    for video_id, data in sorted(video_data.items()):\n","        # Average probabilities across all frames of the video\n","        avg_probs = np.mean(data['frame_probs'], axis=0)\n","\n","        # Final prediction from averaged probabilities\n","        video_pred = np.argmax(avg_probs)\n","\n","        video_predictions.append(video_pred)\n","        video_labels.append(data['true_label'])\n","        video_probs.append(avg_probs)\n","        video_ids.append(video_id)\n","\n","    inference_time = time.time() - inference_start\n","\n","    print(f\"Late fusion completed: {len(video_predictions)} video predictions\")\n","    print(f\"Inference time: {inference_time:.2f}s\")\n","\n","    return {\n","        'predictions': np.array(video_predictions),\n","        'labels': np.array(video_labels),\n","        'probabilities': np.array(video_probs),\n","        'filenames': video_ids,\n","        'inference_time': inference_time,\n","        'evaluation_mode': 'video_level',\n","        'frame_count': len(frame_predictions),\n","        'video_count': len(video_predictions),\n","        'aggregation_method': 'late_fusion_average_probabilities'\n","    }\n","\n","# =====================================================\n","# COMPREHENSIVE METRICS CALCULATION\n","# =====================================================\n","\n","def calculate_comprehensive_metrics(inference_results):\n","    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n","    predictions = inference_results['predictions']\n","    labels = inference_results['labels']\n","    probs = inference_results['probabilities']\n","    filenames = inference_results['filenames']\n","\n","    num_classes = len(CASME2_CLASSES)\n","\n","    # Overall metrics\n","    accuracy = accuracy_score(labels, predictions)\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions, labels=range(num_classes), average='macro', zero_division=0\n","    )\n","\n","    # Per-class metrics - ensure all classes are included even if missing\n","    precision_per_class, recall_per_class, f1_per_class, support_per_class = \\\n","        precision_recall_fscore_support(labels, predictions, labels=range(num_classes), average=None, zero_division=0)\n","\n","    # AUC calculation\n","    labels_binarized = label_binarize(labels, classes=range(num_classes))\n","\n","    auc_per_class = []\n","    for i in range(num_classes):\n","        if np.sum(labels_binarized[:, i]) > 0:\n","            fpr, tpr, _ = roc_curve(labels_binarized[:, i], probs[:, i])\n","            auc_score = auc(fpr, tpr)\n","            auc_per_class.append(auc_score)\n","        else:\n","            auc_per_class.append(0.0)\n","\n","    macro_auc = np.mean([auc_val for auc_val in auc_per_class if auc_val > 0])\n","\n","    # Identify classes present in test set\n","    unique_labels = set(labels)\n","    available_classes = [CASME2_CLASSES[i] for i in unique_labels]\n","    missing_classes = [CASME2_CLASSES[i] for i in range(num_classes) if i not in unique_labels]\n","\n","    # Per-class performance - iterate through all classes\n","    per_class_performance = {}\n","    for i, class_name in enumerate(CASME2_CLASSES):\n","        in_test_set = i in unique_labels\n","        per_class_performance[class_name] = {\n","            'precision': float(precision_per_class[i]) if in_test_set else 0.0,\n","            'recall': float(recall_per_class[i]) if in_test_set else 0.0,\n","            'f1_score': float(f1_per_class[i]) if in_test_set else 0.0,\n","            'support': int(support_per_class[i]) if i < len(support_per_class) else 0,\n","            'auc': float(auc_per_class[i]) if i < len(auc_per_class) and in_test_set else 0.0,\n","            'in_test_set': in_test_set\n","        }\n","\n","    # Confusion matrix - ensure all classes included\n","    cm = confusion_matrix(labels, predictions, labels=list(range(num_classes)))\n","\n","    # Inference performance\n","    total_samples = len(predictions)\n","    total_time = inference_results['inference_time']\n","    avg_time_ms = (total_time / total_samples) * 1000\n","\n","    results = {\n","        'overall_performance': {\n","            'accuracy': float(accuracy),\n","            'macro_precision': float(precision),\n","            'macro_recall': float(recall),\n","            'macro_f1': float(f1),\n","            'macro_auc': float(macro_auc)\n","        },\n","        'per_class_performance': per_class_performance,\n","        'confusion_matrix': cm.tolist(),\n","        'inference_performance': {\n","            'total_time_seconds': float(total_time),\n","            'total_samples': int(total_samples),\n","            'average_time_ms_per_sample': float(avg_time_ms)\n","        },\n","        'evaluation_metadata': {\n","            'dataset': 'CASME_II',\n","            'model_type': 'PoolFormerCASME2Baseline',\n","            'test_samples': int(total_samples),\n","            'class_names': CASME2_CLASSES,\n","            'available_classes': available_classes,\n","            'missing_classes': missing_classes,\n","            'evaluation_mode': inference_results['evaluation_mode'],\n","            'evaluation_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","        }\n","    }\n","\n","    # Add KFS-specific information if video-level evaluation\n","    if inference_results['evaluation_mode'] == 'video_level':\n","        results['kfs_late_fusion_info'] = {\n","            'total_frames': inference_results['frame_count'],\n","            'total_videos': inference_results['video_count'],\n","            'aggregation_method': inference_results['aggregation_method']\n","        }\n","\n","    return results\n","\n","# =====================================================\n","# WRONG PREDICTIONS ANALYSIS\n","# =====================================================\n","\n","def analyze_wrong_predictions(inference_results):\n","    \"\"\"Analyze wrong predictions for error analysis\"\"\"\n","    predictions = inference_results['predictions']\n","    labels = inference_results['labels']\n","    filenames = inference_results['filenames']\n","\n","    wrong_predictions = []\n","    wrong_by_class = defaultdict(int)\n","    confusion_patterns = defaultdict(int)\n","\n","    for i in range(len(predictions)):\n","        if predictions[i] != labels[i]:\n","            # Ensure valid class indices\n","            true_idx = int(labels[i])\n","            pred_idx = int(predictions[i])\n","\n","            if true_idx < 0 or true_idx >= len(CASME2_CLASSES):\n","                continue\n","            if pred_idx < 0 or pred_idx >= len(CASME2_CLASSES):\n","                continue\n","\n","            true_class = CASME2_CLASSES[true_idx]\n","            pred_class = CASME2_CLASSES[pred_idx]\n","\n","            wrong_predictions.append({\n","                'filename': filenames[i],\n","                'true_label': true_class,\n","                'predicted_label': pred_class,\n","                'true_idx': true_idx,\n","                'predicted_idx': pred_idx\n","            })\n","\n","            wrong_by_class[true_class] += 1\n","            confusion_patterns[f\"{true_class}->{pred_class}\"] += 1\n","\n","    error_summary = {}\n","    for class_name in CASME2_CLASSES:\n","        class_idx = CLASS_TO_IDX.get(class_name, -1)\n","        if class_idx < 0:\n","            continue\n","\n","        class_total = np.sum(labels == class_idx)\n","        class_errors = wrong_by_class.get(class_name, 0)\n","        error_rate = (class_errors / class_total * 100) if class_total > 0 else 0.0\n","\n","        error_summary[class_name] = {\n","            'total_samples': int(class_total),\n","            'errors': int(class_errors),\n","            'error_rate': float(error_rate)\n","        }\n","\n","    results = {\n","        'analysis_metadata': {\n","            'total_samples': len(predictions),\n","            'total_wrong_predictions': len(wrong_predictions),\n","            'overall_error_rate': (len(wrong_predictions) / len(predictions) * 100) if len(predictions) > 0 else 0.0\n","        },\n","        'wrong_predictions': wrong_predictions,\n","        'wrong_predictions_by_class': dict(wrong_by_class),\n","        'error_summary': error_summary,\n","        'confusion_patterns': dict(confusion_patterns)\n","    }\n","\n","    return results\n","\n","# =====================================================\n","# SAVE EVALUATION RESULTS\n","# =====================================================\n","\n","def save_evaluation_results(evaluation_results, wrong_predictions_results, results_dir, test_version):\n","    \"\"\"Save comprehensive evaluation results\"\"\"\n","    os.makedirs(results_dir, exist_ok=True)\n","\n","    results_file = f\"{results_dir}/casme2_poolformer_evaluation_results_{test_version}.json\"\n","    with open(results_file, 'w') as f:\n","        json.dump(evaluation_results, f, indent=2, default=str)\n","\n","    wrong_predictions_file = f\"{results_dir}/casme2_poolformer_wrong_predictions_{test_version}.json\"\n","    with open(wrong_predictions_file, 'w') as f:\n","        json.dump(wrong_predictions_results, f, indent=2, default=str)\n","\n","    print(f\"Evaluation results saved:\")\n","    print(f\"  Main results: {os.path.basename(results_file)}\")\n","    print(f\"  Wrong predictions: {os.path.basename(wrong_predictions_file)}\")\n","\n","    return results_file, wrong_predictions_file\n","\n","# =====================================================\n","# MAIN EVALUATION EXECUTION\n","# =====================================================\n","\n","all_evaluation_results = {}\n","\n","for dataset_version in EVALUATE_DATASETS:\n","    print(\"\\n\" + \"=\" * 70)\n","    print(f\"EVALUATING DATASET: {dataset_version.upper()}\")\n","    print(\"=\" * 70)\n","\n","    try:\n","        # Get dataset configuration\n","        test_config = get_test_dataset_config(dataset_version, PROJECT_ROOT)\n","\n","        print(f\"\\nTest Dataset Configuration:\")\n","        print(f\"  Version: {test_config['version']}\")\n","        print(f\"  Variant: {test_config['variant']}\")\n","        print(f\"  Description: {test_config['description']}\")\n","        print(f\"  Frame strategy: {test_config['frame_strategy']}\")\n","        print(f\"  Evaluation mode: {test_config['evaluation_mode']}\")\n","        if 'aggregation' in test_config and test_config['aggregation']:\n","            print(f\"  Aggregation: {test_config['aggregation']}\")\n","        print(f\"  Dataset path: {test_config['dataset_path']}\")\n","\n","        # Create test dataset\n","        print(f\"\\nCreating CASME II test dataset from {test_config['variant']}...\")\n","        test_dataset = CASME2DatasetEvaluation(\n","            dataset_root=test_config['dataset_path'],\n","            split='test',\n","            transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","            use_ram_cache=True\n","        )\n","\n","        if len(test_dataset) == 0:\n","            raise ValueError(f\"No test samples found for {dataset_version}!\")\n","\n","        test_loader = DataLoader(\n","            test_dataset,\n","            batch_size=CASME2_POOLFORMER_CONFIG['batch_size'],\n","            shuffle=False,\n","            num_workers=CASME2_POOLFORMER_CONFIG['num_workers'],\n","            pin_memory=True\n","        )\n","\n","        # Load trained model\n","        checkpoint_path = f\"{GLOBAL_CONFIG_CASME2['checkpoint_root']}/casme2_poolformer_kfs_best_f1.pth\"\n","        model, training_info = load_trained_model_casme2(checkpoint_path, GLOBAL_CONFIG_CASME2['device'])\n","\n","        # Run inference based on evaluation mode\n","        if test_config['evaluation_mode'] == 'frame_level':\n","            print(f\"\\nRunning frame-level evaluation for {test_config['variant']}...\")\n","            inference_results = run_frame_level_inference(model, test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","        elif test_config['evaluation_mode'] == 'video_level':\n","            print(f\"\\nRunning video-level evaluation with late fusion for {test_config['variant']}...\")\n","            inference_results = run_video_level_inference_late_fusion(model, test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","        else:\n","            raise ValueError(f\"Unknown evaluation mode: {test_config['evaluation_mode']}\")\n","\n","        # Calculate comprehensive metrics\n","        evaluation_results = calculate_comprehensive_metrics(inference_results)\n","\n","        # Analyze wrong predictions\n","        wrong_predictions_results = analyze_wrong_predictions(inference_results)\n","\n","        # Add training information\n","        evaluation_results['training_information'] = training_info\n","        evaluation_results['test_configuration'] = test_config\n","\n","        # Save results\n","        results_dir = f\"{GLOBAL_CONFIG_CASME2['results_root']}/evaluation_results\"\n","        save_evaluation_results(\n","            evaluation_results, wrong_predictions_results, results_dir, test_config['version']\n","        )\n","\n","        # Store for comparison\n","        all_evaluation_results[dataset_version] = {\n","            'evaluation': evaluation_results,\n","            'wrong_predictions': wrong_predictions_results,\n","            'config': test_config\n","        }\n","\n","        # Display results\n","        print(\"\\n\" + \"=\" * 60)\n","        print(f\"EVALUATION RESULTS - {test_config['variant']} ({dataset_version})\")\n","        print(\"=\" * 60)\n","\n","        overall = evaluation_results['overall_performance']\n","        print(f\"\\nOverall Performance:\")\n","        print(f\"  Accuracy:  {overall['accuracy']:.4f}\")\n","        print(f\"  Precision: {overall['macro_precision']:.4f}\")\n","        print(f\"  Recall:    {overall['macro_recall']:.4f}\")\n","        print(f\"  F1 Score:  {overall['macro_f1']:.4f}\")\n","        print(f\"  AUC:       {overall['macro_auc']:.4f}\")\n","\n","        if 'kfs_late_fusion_info' in evaluation_results:\n","            fusion_info = evaluation_results['kfs_late_fusion_info']\n","            print(f\"\\nLate Fusion Info:\")\n","            print(f\"  Total frames processed: {fusion_info['total_frames']}\")\n","            print(f\"  Video-level predictions: {fusion_info['total_videos']}\")\n","            print(f\"  Aggregation method: {fusion_info['aggregation_method']}\")\n","\n","        print(f\"\\nPer-Class Performance:\")\n","        for class_name, metrics in evaluation_results['per_class_performance'].items():\n","            in_test = \"Present\" if metrics['in_test_set'] else \"Missing\"\n","            print(f\"  {class_name} [{in_test}]: F1={metrics['f1_score']:.4f}, \"\n","                  f\"Support={metrics['support']}\")\n","\n","        wrong_meta = wrong_predictions_results['analysis_metadata']\n","        print(f\"\\nWrong Predictions Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']} / {wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        print(f\"\\nInference Performance:\")\n","        print(f\"  Total time: {inference_results['inference_time']:.2f}s\")\n","        print(f\"  Speed: {evaluation_results['inference_performance']['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","    except Exception as e:\n","        print(f\"Evaluation failed for {dataset_version}: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        continue\n","\n","# =====================================================\n","# COMPARATIVE ANALYSIS (if both datasets evaluated)\n","# =====================================================\n","\n","if len(all_evaluation_results) == 2 and 'v7' in all_evaluation_results and 'v8' in all_evaluation_results:\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"COMPARATIVE ANALYSIS: AF (v7) vs KFS (v8)\")\n","    print(\"=\" * 70)\n","\n","    v7_results = all_evaluation_results['v7']['evaluation']\n","    v8_results = all_evaluation_results['v8']['evaluation']\n","\n","    print(\"\\nOverall Performance Comparison:\")\n","    print(f\"{'Metric':<20} {'AF (v7)':<15} {'KFS (v8)':<15} {'Difference':<15}\")\n","    print(\"-\" * 65)\n","\n","    metrics_to_compare = ['accuracy', 'macro_precision', 'macro_recall', 'macro_f1', 'macro_auc']\n","\n","    for metric in metrics_to_compare:\n","        v7_val = v7_results['overall_performance'][metric]\n","        v8_val = v8_results['overall_performance'][metric]\n","        diff = v8_val - v7_val\n","\n","        print(f\"{metric:<20} {v7_val:<15.4f} {v8_val:<15.4f} {diff:+.4f}\")\n","\n","    print(f\"\\nEvaluation Modes:\")\n","    print(f\"  AF (v7): {v7_results['evaluation_metadata']['evaluation_mode']}\")\n","    print(f\"  KFS (v8): {v8_results['evaluation_metadata']['evaluation_mode']}\")\n","\n","    if 'kfs_late_fusion_info' in v8_results:\n","        print(f\"\\nKFS Late Fusion Strategy:\")\n","        print(f\"  Frames used: {v8_results['kfs_late_fusion_info']['total_frames']}\")\n","        print(f\"  Video predictions: {v8_results['kfs_late_fusion_info']['total_videos']}\")\n","        print(f\"  Aggregation: {v8_results['kfs_late_fusion_info']['aggregation_method']}\")\n","\n","# Memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II POOLFORMER EVALUATION COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Evaluated datasets: {EVALUATE_DATASETS}\")\n","print(\"Next: Cell 4 - Generate confusion matrices and visualization\")"],"metadata":{"cellView":"form","id":"Rm7JpLsEQLkP","executionInfo":{"status":"ok","timestamp":1761203590475,"user_tz":-420,"elapsed":15118,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"outputId":"1c82f33e-f542-4316-b125-2a5a29f7de3a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II PoolFormer Evaluation Framework with Dual Dataset Support\n","============================================================\n","Datasets to evaluate: ['v7', 'v8']\n","============================================================\n","\n","======================================================================\n","EVALUATING DATASET: V7\n","======================================================================\n","\n","Test Dataset Configuration:\n","  Version: v7\n","  Variant: AF\n","  Description: Apex Frame with Face-Aware Preprocessing\n","  Frame strategy: apex_frame\n","  Evaluation mode: frame_level\n","  Dataset path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7\n","\n","Creating CASME II test dataset from AF...\n","Loading CASME II test dataset for evaluation...\n","Found 28 image files in directory\n","Loaded 28 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 10 samples (35.7%)\n","  disgust: 7 samples (25.0%)\n","  happiness: 4 samples (14.3%)\n","  repression: 3 samples (10.7%)\n","  surprise: 3 samples (10.7%)\n","  sadness: 1 samples (3.6%)\n","Unique video IDs: 28\n","Missing classes in test set: ['fear']\n","Preloading 28 test images to RAM...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test to RAM: 100%|██████████| 28/28 [00:01<00:00, 15.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["RAM caching completed: 28/28 images\n","Loading trained model from: casme2_poolformer_kfs_best_f1.pth\n","PoolFormer feature dimension: 768\n","Classification head: 768 -> 256 -> 7\n","Dropout rate: 0.3 (regularization for moderate dataset)\n","Model loaded successfully\n","  Best epoch: 2\n","  Best val F1: 0.2383\n","  Best val loss: 0.1303\n","\n","Running frame-level evaluation for AF...\n","Running frame-level inference...\n"]},{"output_type":"stream","name":"stderr","text":["Frame-level inference: 100%|██████████| 4/4 [00:01<00:00,  2.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Frame-level inference completed: 28 predictions in 1.46s\n","Evaluation results saved:\n","  Main results: casme2_poolformer_evaluation_results_v7.json\n","  Wrong predictions: casme2_poolformer_wrong_predictions_v7.json\n","\n","============================================================\n","EVALUATION RESULTS - AF (v7)\n","============================================================\n","\n","Overall Performance:\n","  Accuracy:  0.4643\n","  Precision: 0.4136\n","  Recall:    0.3255\n","  F1 Score:  0.3241\n","  AUC:       0.5910\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.5714, Support=10\n","  disgust [Present]: F1=0.5000, Support=7\n","  happiness [Present]: F1=0.3333, Support=4\n","  repression [Present]: F1=0.3636, Support=3\n","  surprise [Present]: F1=0.5000, Support=3\n","  sadness [Present]: F1=0.0000, Support=1\n","  fear [Missing]: F1=0.0000, Support=0\n","\n","Wrong Predictions Analysis:\n","  Total errors: 15 / 28\n","  Error rate: 53.57%\n","\n","Inference Performance:\n","  Total time: 1.46s\n","  Speed: 52.1 ms/sample\n","\n","======================================================================\n","EVALUATING DATASET: V8\n","======================================================================\n","\n","Test Dataset Configuration:\n","  Version: v8\n","  Variant: KFS\n","  Description: Key Frame Sequence with Face-Aware Preprocessing\n","  Frame strategy: key_frame_sequence\n","  Evaluation mode: video_level\n","  Aggregation: late_fusion\n","  Dataset path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8\n","\n","Creating CASME II test dataset from KFS...\n","Loading CASME II test dataset for evaluation...\n","Found 84 image files in directory\n","Loaded 84 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 30 samples (35.7%)\n","  disgust: 21 samples (25.0%)\n","  happiness: 12 samples (14.3%)\n","  repression: 9 samples (10.7%)\n","  surprise: 9 samples (10.7%)\n","  sadness: 3 samples (3.6%)\n","Unique video IDs: 84\n","Missing classes in test set: ['fear']\n","Preloading 84 test images to RAM...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test to RAM: 100%|██████████| 84/84 [00:02<00:00, 29.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["RAM caching completed: 84/84 images\n","Loading trained model from: casme2_poolformer_kfs_best_f1.pth\n","PoolFormer feature dimension: 768\n","Classification head: 768 -> 256 -> 7\n","Dropout rate: 0.3 (regularization for moderate dataset)\n","Model loaded successfully\n","  Best epoch: 2\n","  Best val F1: 0.2383\n","  Best val loss: 0.1303\n","\n","Running video-level evaluation with late fusion for KFS...\n","Running video-level inference with late fusion...\n"]},{"output_type":"stream","name":"stderr","text":["Frame-level inference: 100%|██████████| 11/11 [00:01<00:00,  6.54it/s]"]},{"output_type":"stream","name":"stdout","text":["Frame-level predictions completed: 84 frames\n","Grouped into 84 unique videos\n","Late fusion completed: 84 video predictions\n","Inference time: 1.69s\n","Evaluation results saved:\n","  Main results: casme2_poolformer_evaluation_results_v8.json\n","  Wrong predictions: casme2_poolformer_wrong_predictions_v8.json\n","\n","============================================================\n","EVALUATION RESULTS - KFS (v8)\n","============================================================\n","\n","Overall Performance:\n","  Accuracy:  0.4405\n","  Precision: 0.3636\n","  Recall:    0.3049\n","  F1 Score:  0.3054\n","  AUC:       0.5848\n","\n","Late Fusion Info:\n","  Total frames processed: 84\n","  Video-level predictions: 84\n","  Aggregation method: late_fusion_average_probabilities\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.5667, Support=30\n","  disgust [Present]: F1=0.5000, Support=21\n","  happiness [Present]: F1=0.3158, Support=12\n","  repression [Present]: F1=0.2941, Support=9\n","  surprise [Present]: F1=0.4615, Support=9\n","  sadness [Present]: F1=0.0000, Support=3\n","  fear [Missing]: F1=0.0000, Support=0\n","\n","Wrong Predictions Analysis:\n","  Total errors: 47 / 84\n","  Error rate: 55.95%\n","\n","Inference Performance:\n","  Total time: 1.69s\n","  Speed: 20.1 ms/sample\n","\n","======================================================================\n","COMPARATIVE ANALYSIS: AF (v7) vs KFS (v8)\n","======================================================================\n","\n","Overall Performance Comparison:\n","Metric               AF (v7)         KFS (v8)        Difference     \n","-----------------------------------------------------------------\n","accuracy             0.4643          0.4405          -0.0238\n","macro_precision      0.4136          0.3636          -0.0500\n","macro_recall         0.3255          0.3049          -0.0206\n","macro_f1             0.3241          0.3054          -0.0186\n","macro_auc            0.5910          0.5848          -0.0061\n","\n","Evaluation Modes:\n","  AF (v7): frame_level\n","  KFS (v8): video_level\n","\n","KFS Late Fusion Strategy:\n","  Frames used: 84\n","  Video predictions: 84\n","  Aggregation: late_fusion_average_probabilities\n","\n","======================================================================\n","CASME II POOLFORMER EVALUATION COMPLETED\n","======================================================================\n","Evaluated datasets: ['v7', 'v8']\n","Next: Cell 4 - Generate confusion matrices and visualization\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title Cell 4: CASME II PoolFormer Confusion Matrix Generation\n","\n","# File: 06_03_PoolFormer_CASME2_KFS_Cell4.py\n","# Location: experiments/06_03_PoolFormer_CASME2-KFS-PREP.ipynb\n","# Purpose: Generate professional confusion matrix visualization for AF and KFS evaluations\n","\n","import json\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import seaborn as sns\n","from datetime import datetime\n","\n","print(\"CASME II PoolFormer Confusion Matrix Generation\")\n","print(\"=\" * 60)\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/06_03_poolformer_casme2_kfs_prep\"\n","\n","def find_evaluation_json_files(results_path):\n","    \"\"\"Find evaluation JSON files with version detection\"\"\"\n","    json_files = {}\n","    eval_dir = f\"{results_path}/evaluation_results\"\n","\n","    if os.path.exists(eval_dir):\n","        for version in ['v7', 'v8']:\n","            eval_pattern = f\"{eval_dir}/casme2_poolformer_evaluation_results_{version}.json\"\n","            eval_files = glob.glob(eval_pattern)\n","\n","            if eval_files:\n","                json_files[f'main_{version}'] = eval_files[0]\n","                print(f\"Found {version.upper()} evaluation file: {os.path.basename(eval_files[0])}\")\n","\n","            wrong_pattern = f\"{eval_dir}/casme2_poolformer_wrong_predictions_{version}.json\"\n","            wrong_files = glob.glob(wrong_pattern)\n","\n","            if wrong_files:\n","                json_files[f'wrong_{version}'] = wrong_files[0]\n","                print(f\"Found {version.upper()} wrong predictions: {os.path.basename(wrong_files[0])}\")\n","\n","        if not json_files:\n","            print(f\"WARNING: No evaluation results found in {eval_dir}\")\n","            print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","    else:\n","        print(f\"ERROR: Evaluation directory not found: {eval_dir}\")\n","\n","    return json_files\n","\n","def load_evaluation_results(json_path):\n","    \"\"\"Load and parse evaluation results JSON\"\"\"\n","    try:\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"Successfully loaded: {os.path.basename(json_path)}\")\n","        return data\n","    except Exception as e:\n","        print(f\"ERROR loading {json_path}: {str(e)}\")\n","        return None\n","\n","def calculate_weighted_f1(per_class_performance):\n","    \"\"\"Calculate weighted F1 score\"\"\"\n","    total_support = sum([class_data['support'] for class_data in per_class_performance.values()\n","                        if class_data['support'] > 0])\n","\n","    if total_support == 0:\n","        return 0.0\n","\n","    weighted_f1 = 0.0\n","\n","    for class_name, class_data in per_class_performance.items():\n","        if class_data['support'] > 0:\n","            weight = class_data['support'] / total_support\n","            weighted_f1 += class_data['f1_score'] * weight\n","\n","    return weighted_f1\n","\n","def calculate_balanced_accuracy(confusion_matrix):\n","    \"\"\"Calculate balanced accuracy handling classes with zero support\"\"\"\n","    cm = np.array(confusion_matrix)\n","    n_classes = cm.shape[0]\n","\n","    per_class_balanced_acc = []\n","    classes_with_samples = []\n","\n","    for i in range(n_classes):\n","        if cm[i, :].sum() > 0:\n","            classes_with_samples.append(i)\n","\n","    for i in classes_with_samples:\n","        tp = cm[i, i]\n","        fn = cm[i, :].sum() - tp\n","        fp = cm[:, i].sum() - tp\n","        tn = cm.sum() - tp - fn - fp\n","\n","        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n","\n","        class_balanced_acc = (sensitivity + specificity) / 2\n","        per_class_balanced_acc.append(class_balanced_acc)\n","\n","    balanced_acc = np.mean(per_class_balanced_acc) if per_class_balanced_acc else 0.0\n","\n","    return balanced_acc\n","\n","def determine_text_color(color_value, threshold=0.5):\n","    \"\"\"Determine optimal text color based on background intensity\"\"\"\n","    return 'white' if color_value > threshold else 'black'\n","\n","def create_confusion_matrix_plot(data, output_path, test_version):\n","    \"\"\"Create professional confusion matrix visualization for CASME II micro-expression recognition\"\"\"\n","\n","    meta = data['evaluation_metadata']\n","    class_names = meta['class_names']\n","    cm = np.array(data['confusion_matrix'], dtype=int)\n","    overall = data['overall_performance']\n","    per_class = data['per_class_performance']\n","\n","    test_config = data.get('test_configuration', {})\n","    variant = test_config.get('variant', test_version.upper())\n","    description = test_config.get('description', f'{test_version} preprocessing')\n","    eval_mode = meta.get('evaluation_mode', 'frame_level')\n","\n","    print(f\"Processing confusion matrix for {variant} ({test_version})\")\n","    print(f\"Dataset: {description}\")\n","    print(f\"Evaluation mode: {eval_mode}\")\n","    print(f\"Confusion matrix shape: {cm.shape}\")\n","\n","    macro_f1 = overall.get('macro_f1', 0.0)\n","    accuracy = overall.get('accuracy', 0.0)\n","    weighted_f1 = calculate_weighted_f1(per_class)\n","    balanced_acc = calculate_balanced_accuracy(cm)\n","\n","    print(f\"Calculated metrics - Macro F1: {macro_f1:.4f}, Weighted F1: {weighted_f1:.4f}, \"\n","          f\"Balanced Acc: {balanced_acc:.4f}, Accuracy: {accuracy:.4f}\")\n","\n","    row_sums = cm.sum(axis=1, keepdims=True)\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        cm_pct = np.divide(cm, row_sums, where=(row_sums!=0))\n","        cm_pct = np.nan_to_num(cm_pct)\n","\n","    fig, ax = plt.subplots(figsize=(12, 10))\n","\n","    cmap = 'Blues'\n","    im = ax.imshow(cm_pct, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=0.8)\n","\n","    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","    cbar.set_label('True Class Percentage', rotation=270, labelpad=15, fontsize=11)\n","\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            count = cm[i, j]\n","\n","            if row_sums[i, 0] > 0:\n","                percentage = cm_pct[i, j] * 100\n","                text = f\"{count}\\n{percentage:.1f}%\"\n","            else:\n","                text = f\"{count}\\n(N/A)\"\n","\n","            cell_value = cm_pct[i, j]\n","            text_color = determine_text_color(cell_value, threshold=0.4)\n","\n","            ax.text(j, i, text, ha=\"center\", va=\"center\",\n","                   color=text_color, fontsize=9, fontweight='bold')\n","\n","    ax.set_xticks(np.arange(len(class_names)))\n","    ax.set_yticks(np.arange(len(class_names)))\n","    ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=10)\n","    ax.set_yticklabels(class_names, fontsize=10)\n","    ax.set_xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n","    ax.set_ylabel(\"True Label\", fontsize=12, fontweight='bold')\n","\n","    preprocessing_note = f\"Preprocessing: {description}\\n\"\n","    preprocessing_note += f\"Dataset: {test_version}\\n\"\n","    preprocessing_note += f\"Evaluation: {eval_mode.replace('_', ' ').title()}\"\n","\n","    if 'kfs_late_fusion_info' in data:\n","        fusion_info = data['kfs_late_fusion_info']\n","        preprocessing_note += f\"\\nFrames: {fusion_info['total_frames']}, Videos: {fusion_info['total_videos']}\"\n","\n","    missing_classes = meta.get('missing_classes', [])\n","    if missing_classes:\n","        preprocessing_note += f\"\\nMissing: {', '.join(missing_classes)}\"\n","\n","    ax.text(0.02, 0.98, preprocessing_note, transform=ax.transAxes, fontsize=8,\n","            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n","\n","    title = f\"CASME II {variant} Micro-Expression Recognition - PoolFormer\\n\"\n","    title += f\"Acc: {accuracy:.4f}  |  Macro F1: {macro_f1:.4f}  |  Weighted F1: {weighted_f1:.4f}  |  Balanced Acc: {balanced_acc:.4f}\"\n","    ax.set_title(title, fontsize=12, pad=25, fontweight='bold')\n","\n","    plt.tight_layout()\n","    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n","    plt.close(fig)\n","\n","    print(f\"Confusion matrix saved to: {os.path.basename(output_path)}\")\n","\n","    return {\n","        'accuracy': accuracy,\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        'balanced_accuracy': balanced_acc,\n","        'missing_classes': missing_classes\n","    }\n","\n","def generate_performance_summary(evaluation_data, wrong_predictions_data=None):\n","    \"\"\"Generate comprehensive performance summary\"\"\"\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\")\n","    print(\"=\" * 60)\n","\n","    overall = evaluation_data['overall_performance']\n","    meta = evaluation_data['evaluation_metadata']\n","    test_config = evaluation_data.get('test_configuration', {})\n","\n","    variant = test_config.get('variant', 'N/A')\n","\n","    print(f\"Dataset: {meta['dataset']}\")\n","    print(f\"Variant: {variant}\")\n","    print(f\"Dataset version: {test_config.get('version', 'N/A')}\")\n","    print(f\"Preprocessing: {test_config.get('description', 'N/A')}\")\n","    print(f\"Test samples: {meta['test_samples']}\")\n","    print(f\"Model: {meta['model_type']}\")\n","    print(f\"Evaluation date: {meta['evaluation_timestamp']}\")\n","\n","    if 'kfs_late_fusion_info' in evaluation_data:\n","        fusion_info = evaluation_data['kfs_late_fusion_info']\n","        print(f\"\\nLate Fusion Information:\")\n","        print(f\"  Total frames: {fusion_info['total_frames']}\")\n","        print(f\"  Video predictions: {fusion_info['total_videos']}\")\n","        print(f\"  Aggregation: {fusion_info['aggregation_method']}\")\n","\n","    print(f\"\\nOverall Performance:\")\n","    print(f\"  Accuracy:         {overall['accuracy']:.4f}\")\n","    print(f\"  Macro Precision:  {overall['macro_precision']:.4f}\")\n","    print(f\"  Macro Recall:     {overall['macro_recall']:.4f}\")\n","    print(f\"  Macro F1:         {overall['macro_f1']:.4f}\")\n","    print(f\"  Macro AUC:        {overall['macro_auc']:.4f}\")\n","\n","    print(f\"\\nPer-Class Performance:\")\n","    per_class = evaluation_data['per_class_performance']\n","\n","    print(f\"{'Class':<12} {'F1':<8} {'Precision':<10} {'Recall':<8} {'AUC':<8} {'Support':<8} {'In Test'}\")\n","    print(\"-\" * 65)\n","\n","    for class_name, metrics in per_class.items():\n","        in_test = \"Yes\" if metrics['in_test_set'] else \"No\"\n","        print(f\"{class_name:<12} {metrics['f1_score']:<8.4f} {metrics['precision']:<10.4f} \"\n","              f\"{metrics['recall']:<8.4f} {metrics['auc']:<8.4f} {metrics['support']:<8} {in_test}\")\n","\n","    if 'training_information' in evaluation_data:\n","        training = evaluation_data['training_information']\n","        print(f\"\\nTraining vs Test Comparison:\")\n","        print(f\"  Training Val F1:  {training['best_val_f1']:.4f}\")\n","        print(f\"  Test F1:          {overall['macro_f1']:.4f}\")\n","        print(f\"  Performance Gap:  {training['best_val_f1'] - overall['macro_f1']:+.4f}\")\n","        print(f\"  Best Epoch:       {training['best_epoch']}\")\n","\n","    missing_classes = meta.get('missing_classes', [])\n","    available_classes = meta.get('available_classes', [])\n","\n","    print(f\"\\nClass Availability Analysis:\")\n","    print(f\"  Available classes: {len(available_classes)}/7\")\n","    print(f\"  Missing classes: {missing_classes if missing_classes else 'None'}\")\n","\n","    if wrong_predictions_data:\n","        wrong_meta = wrong_predictions_data['analysis_metadata']\n","        print(f\"\\nError Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']}/{wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        patterns = wrong_predictions_data.get('confusion_patterns', {})\n","        if patterns:\n","            print(f\"\\nTop Confusion Patterns:\")\n","            sorted_patterns = sorted(patterns.items(), key=lambda x: x[1], reverse=True)[:3]\n","            for pattern, count in sorted_patterns:\n","                print(f\"  {pattern}: {count} cases\")\n","\n","    print(f\"\\nInference Performance:\")\n","    inference = evaluation_data['inference_performance']\n","    print(f\"  Total time: {inference['total_time_seconds']:.2f}s\")\n","    print(f\"  Speed: {inference['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","json_files = find_evaluation_json_files(RESULTS_ROOT)\n","\n","if not json_files:\n","    print(f\"ERROR: No evaluation JSON files found in {RESULTS_ROOT}\")\n","    print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","else:\n","    print(f\"\\nFound {len([k for k in json_files.keys() if k.startswith('main_')])} evaluation result(s)\")\n","\n","output_dir = f\"{RESULTS_ROOT}/confusion_matrix_analysis\"\n","Path(output_dir).mkdir(parents=True, exist_ok=True)\n","\n","results_summary = {}\n","generated_files = []\n","\n","for version in ['v7', 'v8']:\n","    main_key = f'main_{version}'\n","    wrong_key = f'wrong_{version}'\n","\n","    if main_key in json_files:\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Processing {version.upper()} Evaluation Results\")\n","        print(f\"{'='*60}\")\n","\n","        eval_data = load_evaluation_results(json_files[main_key])\n","\n","        wrong_data = None\n","        if wrong_key in json_files:\n","            wrong_data = load_evaluation_results(json_files[wrong_key])\n","\n","        if eval_data is not None:\n","            try:\n","                cm_output_path = os.path.join(output_dir, f\"confusion_matrix_CASME2_PoolFormer_{version}.png\")\n","                metrics = create_confusion_matrix_plot(eval_data, cm_output_path, version)\n","                generated_files.append(cm_output_path)\n","\n","                results_summary[version] = metrics\n","\n","                print(f\"\\nSUCCESS: {version.upper()} confusion matrix generated successfully\")\n","                print(f\"Output file: {os.path.basename(cm_output_path)}\")\n","\n","                print(f\"\\nPerformance Metrics Summary:\")\n","                print(f\"  Accuracy:        {metrics['accuracy']:.4f}\")\n","                print(f\"  Macro F1:        {metrics['macro_f1']:.4f}\")\n","                print(f\"  Weighted F1:     {metrics['weighted_f1']:.4f}\")\n","                print(f\"  Balanced Acc:    {metrics['balanced_accuracy']:.4f}\")\n","\n","                if metrics['missing_classes']:\n","                    print(f\"  Missing classes: {metrics['missing_classes']}\")\n","\n","            except Exception as e:\n","                print(f\"ERROR: Failed to generate {version.upper()} confusion matrix: {str(e)}\")\n","                import traceback\n","                traceback.print_exc()\n","\n","            generate_performance_summary(eval_data, wrong_data)\n","        else:\n","            print(f\"ERROR: Could not load {version.upper()} evaluation data\")\n","\n","if generated_files:\n","    print(f\"\\n\" + \"=\" * 60)\n","    print(\"CASME II CONFUSION MATRIX GENERATION COMPLETED\")\n","    print(\"=\" * 60)\n","\n","    print(f\"\\nGenerated visualization files:\")\n","    for file_path in generated_files:\n","        filename = os.path.basename(file_path)\n","        print(f\"  {filename}\")\n","\n","    for version in ['v7', 'v8']:\n","        if version in results_summary:\n","            variant = 'AF' if version == 'v7' else 'KFS'\n","            print(f\"\\n{variant} ({version.upper()}) Performance Summary:\")\n","            metrics = results_summary[version]\n","            print(f\"  Accuracy:       {metrics['accuracy']:.4f}\")\n","            print(f\"  Macro F1:       {metrics['macro_f1']:.4f}\")\n","            print(f\"  Weighted F1:    {metrics['weighted_f1']:.4f}\")\n","            print(f\"  Balanced Acc:   {metrics['balanced_accuracy']:.4f}\")\n","\n","    print(f\"\\nFiles saved in: {output_dir}\")\n","    print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","else:\n","    print(f\"\\nERROR: No visualizations were generated\")\n","    print(\"Please check:\")\n","    print(\"1. Cell 3 evaluation results exist\")\n","    print(\"2. JSON file structure is correct\")\n","    print(\"3. No file permission issues\")\n","\n","print(\"\\nCell 4 completed - CASME II confusion matrix analysis generated\")"],"metadata":{"cellView":"form","id":"N57BNknaQ6gD","executionInfo":{"status":"ok","timestamp":1761203593956,"user_tz":-420,"elapsed":3451,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"outputId":"d811eb59-fb26-4e19-dea9-58f1a6a1fdc4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II PoolFormer Confusion Matrix Generation\n","============================================================\n","Found V7 evaluation file: casme2_poolformer_evaluation_results_v7.json\n","Found V7 wrong predictions: casme2_poolformer_wrong_predictions_v7.json\n","Found V8 evaluation file: casme2_poolformer_evaluation_results_v8.json\n","Found V8 wrong predictions: casme2_poolformer_wrong_predictions_v8.json\n","\n","Found 2 evaluation result(s)\n","\n","============================================================\n","Processing V7 Evaluation Results\n","============================================================\n","Successfully loaded: casme2_poolformer_evaluation_results_v7.json\n","Successfully loaded: casme2_poolformer_wrong_predictions_v7.json\n","Processing confusion matrix for AF (v7)\n","Dataset: Apex Frame with Face-Aware Preprocessing\n","Evaluation mode: frame_level\n","Confusion matrix shape: (7, 7)\n","Calculated metrics - Macro F1: 0.3241, Weighted F1: 0.4692, Balanced Acc: 0.6322, Accuracy: 0.4643\n","Confusion matrix saved to: confusion_matrix_CASME2_PoolFormer_v7.png\n","\n","SUCCESS: V7 confusion matrix generated successfully\n","Output file: confusion_matrix_CASME2_PoolFormer_v7.png\n","\n","Performance Metrics Summary:\n","  Accuracy:        0.4643\n","  Macro F1:        0.3241\n","  Weighted F1:     0.4692\n","  Balanced Acc:    0.6322\n","  Missing classes: ['fear']\n","\n","============================================================\n","CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\n","============================================================\n","Dataset: CASME_II\n","Variant: AF\n","Dataset version: v7\n","Preprocessing: Apex Frame with Face-Aware Preprocessing\n","Test samples: 28\n","Model: PoolFormerCASME2Baseline\n","Evaluation date: 2025-10-23 07:13:03\n","\n","Overall Performance:\n","  Accuracy:         0.4643\n","  Macro Precision:  0.4136\n","  Macro Recall:     0.3255\n","  Macro F1:         0.3241\n","  Macro AUC:        0.5910\n","\n","Per-Class Performance:\n","Class        F1       Precision  Recall   AUC      Support  In Test\n","-----------------------------------------------------------------\n","others       0.5714   0.5455     0.6000   0.7278   10       Yes\n","disgust      0.5000   0.6000     0.4286   0.6735   7        Yes\n","happiness    0.3333   0.5000     0.2500   0.5625   4        Yes\n","repression   0.3636   0.2500     0.6667   0.7467   3        Yes\n","surprise     0.5000   1.0000     0.3333   0.6133   3        Yes\n","sadness      0.0000   0.0000     0.0000   0.2222   1        Yes\n","fear         0.0000   0.0000     0.0000   0.0000   0        No\n","\n","Training vs Test Comparison:\n","  Training Val F1:  0.2383\n","  Test F1:          0.3241\n","  Performance Gap:  -0.0858\n","  Best Epoch:       2\n","\n","Class Availability Analysis:\n","  Available classes: 6/7\n","  Missing classes: ['fear']\n","\n","Error Analysis:\n","  Total errors: 15/28\n","  Error rate: 53.57%\n","\n","Top Confusion Patterns:\n","  disgust->others: 3 cases\n","  happiness->repression: 2 cases\n","  surprise->others: 1 cases\n","\n","Inference Performance:\n","  Total time: 1.46s\n","  Speed: 52.1 ms/sample\n","\n","============================================================\n","Processing V8 Evaluation Results\n","============================================================\n","Successfully loaded: casme2_poolformer_evaluation_results_v8.json\n","Successfully loaded: casme2_poolformer_wrong_predictions_v8.json\n","Processing confusion matrix for KFS (v8)\n","Dataset: Key Frame Sequence with Face-Aware Preprocessing\n","Evaluation mode: video_level\n","Confusion matrix shape: (7, 7)\n","Calculated metrics - Macro F1: 0.3054, Weighted F1: 0.4535, Balanced Acc: 0.6188, Accuracy: 0.4405\n","Confusion matrix saved to: confusion_matrix_CASME2_PoolFormer_v8.png\n","\n","SUCCESS: V8 confusion matrix generated successfully\n","Output file: confusion_matrix_CASME2_PoolFormer_v8.png\n","\n","Performance Metrics Summary:\n","  Accuracy:        0.4405\n","  Macro F1:        0.3054\n","  Weighted F1:     0.4535\n","  Balanced Acc:    0.6188\n","  Missing classes: ['fear']\n","\n","============================================================\n","CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\n","============================================================\n","Dataset: CASME_II\n","Variant: KFS\n","Dataset version: v8\n","Preprocessing: Key Frame Sequence with Face-Aware Preprocessing\n","Test samples: 84\n","Model: PoolFormerCASME2Baseline\n","Evaluation date: 2025-10-23 07:13:10\n","\n","Late Fusion Information:\n","  Total frames: 84\n","  Video predictions: 84\n","  Aggregation: late_fusion_average_probabilities\n","\n","Overall Performance:\n","  Accuracy:         0.4405\n","  Macro Precision:  0.3636\n","  Macro Recall:     0.3049\n","  Macro F1:         0.3054\n","  Macro AUC:        0.5848\n","\n","Per-Class Performance:\n","Class        F1       Precision  Recall   AUC      Support  In Test\n","-----------------------------------------------------------------\n","others       0.5667   0.5667     0.5667   0.7426   30       Yes\n","disgust      0.5000   0.6000     0.4286   0.6720   21       Yes\n","happiness    0.3158   0.4286     0.2500   0.5671   12       Yes\n","repression   0.2941   0.2000     0.5556   0.7348   9        Yes\n","surprise     0.4615   0.7500     0.3333   0.5704   9        Yes\n","sadness      0.0000   0.0000     0.0000   0.2222   3        Yes\n","fear         0.0000   0.0000     0.0000   0.0000   0        No\n","\n","Training vs Test Comparison:\n","  Training Val F1:  0.2383\n","  Test F1:          0.3054\n","  Performance Gap:  -0.0672\n","  Best Epoch:       2\n","\n","Class Availability Analysis:\n","  Available classes: 6/7\n","  Missing classes: ['fear']\n","\n","Error Analysis:\n","  Total errors: 47/84\n","  Error rate: 55.95%\n","\n","Top Confusion Patterns:\n","  disgust->others: 9 cases\n","  happiness->repression: 6 cases\n","  surprise->repression: 5 cases\n","\n","Inference Performance:\n","  Total time: 1.69s\n","  Speed: 20.1 ms/sample\n","\n","============================================================\n","CASME II CONFUSION MATRIX GENERATION COMPLETED\n","============================================================\n","\n","Generated visualization files:\n","  confusion_matrix_CASME2_PoolFormer_v7.png\n","  confusion_matrix_CASME2_PoolFormer_v8.png\n","\n","AF (V7) Performance Summary:\n","  Accuracy:       0.4643\n","  Macro F1:       0.3241\n","  Weighted F1:    0.4692\n","  Balanced Acc:   0.6322\n","\n","KFS (V8) Performance Summary:\n","  Accuracy:       0.4405\n","  Macro F1:       0.3054\n","  Weighted F1:    0.4535\n","  Balanced Acc:   0.6188\n","\n","Files saved in: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/06_03_poolformer_casme2_kfs_prep/confusion_matrix_analysis\n","Analysis completed at: 2025-10-23 07:13:14\n","\n","Cell 4 completed - CASME II confusion matrix analysis generated\n"]}]}]}