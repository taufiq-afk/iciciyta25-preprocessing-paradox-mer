{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyNChp32Pjaqu16yQZLqs4XC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d522ea04d48a470ea6008aa422c21523":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9803e2f59dd2423db67d27afd5fdff51","IPY_MODEL_3a5298957b4048f19f8e221ac6ba18c2","IPY_MODEL_eff9cd56d9644c8a8a48745f9347a5dc"],"layout":"IPY_MODEL_4f1a978568e34196821567baa6bccdcd"}},"9803e2f59dd2423db67d27afd5fdff51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab7b4038d1e4487f8468035d9aa311a6","placeholder":"​","style":"IPY_MODEL_704994b519174ba78ba95a0bd559f6be","value":"preprocessor_config.json: 100%"}},"3a5298957b4048f19f8e221ac6ba18c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_74b38659f95a4265b34642eeca916c88","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af6ce223d2dc4343bc307f3ec54b5e2f","value":160}},"eff9cd56d9644c8a8a48745f9347a5dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad340f29f9154105857659df41b7805c","placeholder":"​","style":"IPY_MODEL_362672a625fd49c582a51c7cc385d73f","value":" 160/160 [00:00&lt;00:00, 19.7kB/s]"}},"4f1a978568e34196821567baa6bccdcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab7b4038d1e4487f8468035d9aa311a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"704994b519174ba78ba95a0bd559f6be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74b38659f95a4265b34642eeca916c88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af6ce223d2dc4343bc307f3ec54b5e2f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ad340f29f9154105857659df41b7805c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"362672a625fd49c582a51c7cc385d73f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8707b93a0cd44aad95604ba1c4692547":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_461b9a0c5aba4487b60652a30bcefcc9","IPY_MODEL_edce3507fddf4449b621a0656633c61b","IPY_MODEL_07cacf30f56740d4bcb394eba36a0a18"],"layout":"IPY_MODEL_c4100e91016e4125a2935e16c27b4832"}},"461b9a0c5aba4487b60652a30bcefcc9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfee84e3d2d44fe0ae0bb4f9b3bb2e90","placeholder":"​","style":"IPY_MODEL_32e22b41bc6d4b7d9d181d001778eed7","value":"config.json: 100%"}},"edce3507fddf4449b621a0656633c61b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_699bb8f6935a47e7b96c85b43258e35a","max":502,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ff54ca56b0e45638d93ad10039dbb87","value":502}},"07cacf30f56740d4bcb394eba36a0a18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7f17eff5b4c463ab026b1194fd875d7","placeholder":"​","style":"IPY_MODEL_c0c4317f328f40bcb1efdb853028aae9","value":" 502/502 [00:00&lt;00:00, 68.0kB/s]"}},"c4100e91016e4125a2935e16c27b4832":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfee84e3d2d44fe0ae0bb4f9b3bb2e90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32e22b41bc6d4b7d9d181d001778eed7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"699bb8f6935a47e7b96c85b43258e35a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ff54ca56b0e45638d93ad10039dbb87":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7f17eff5b4c463ab026b1194fd875d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0c4317f328f40bcb1efdb853028aae9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43984d742e224cddaccc6a4bd4a584b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_59de12c77639455c8bbf1464fd12c958","IPY_MODEL_6763ae3d43344d7a92131c856f9d0422","IPY_MODEL_54f9bc2d1410430da3074fb4e2f888f7"],"layout":"IPY_MODEL_9e1be8432e394734b7d5e47a11b7e3fd"}},"59de12c77639455c8bbf1464fd12c958":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93a300bce72a43b3b8eca2be46902cc8","placeholder":"​","style":"IPY_MODEL_3d6f70e5943c442a9c80a9318ee75b94","value":"model.safetensors: 100%"}},"6763ae3d43344d7a92131c856f9d0422":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_84739320ca8f49f193e6ed6a03e1d7c2","max":345579424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fdc11ab27a2e4a409c9d3338ecfe9b80","value":345579424}},"54f9bc2d1410430da3074fb4e2f888f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64cd8746fd504e7596e16d526820ca56","placeholder":"​","style":"IPY_MODEL_f815a0f2bb2441049ba771bf775b0159","value":" 346M/346M [00:02&lt;00:00, 34.4MB/s]"}},"9e1be8432e394734b7d5e47a11b7e3fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93a300bce72a43b3b8eca2be46902cc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d6f70e5943c442a9c80a9318ee75b94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84739320ca8f49f193e6ed6a03e1d7c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdc11ab27a2e4a409c9d3338ecfe9b80":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64cd8746fd504e7596e16d526820ca56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f815a0f2bb2441049ba771bf775b0159":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d522ea04d48a470ea6008aa422c21523","9803e2f59dd2423db67d27afd5fdff51","3a5298957b4048f19f8e221ac6ba18c2","eff9cd56d9644c8a8a48745f9347a5dc","4f1a978568e34196821567baa6bccdcd","ab7b4038d1e4487f8468035d9aa311a6","704994b519174ba78ba95a0bd559f6be","74b38659f95a4265b34642eeca916c88","af6ce223d2dc4343bc307f3ec54b5e2f","ad340f29f9154105857659df41b7805c","362672a625fd49c582a51c7cc385d73f","8707b93a0cd44aad95604ba1c4692547","461b9a0c5aba4487b60652a30bcefcc9","edce3507fddf4449b621a0656633c61b","07cacf30f56740d4bcb394eba36a0a18","c4100e91016e4125a2935e16c27b4832","dfee84e3d2d44fe0ae0bb4f9b3bb2e90","32e22b41bc6d4b7d9d181d001778eed7","699bb8f6935a47e7b96c85b43258e35a","9ff54ca56b0e45638d93ad10039dbb87","b7f17eff5b4c463ab026b1194fd875d7","c0c4317f328f40bcb1efdb853028aae9","43984d742e224cddaccc6a4bd4a584b7","59de12c77639455c8bbf1464fd12c958","6763ae3d43344d7a92131c856f9d0422","54f9bc2d1410430da3074fb4e2f888f7","9e1be8432e394734b7d5e47a11b7e3fd","93a300bce72a43b3b8eca2be46902cc8","3d6f70e5943c442a9c80a9318ee75b94","84739320ca8f49f193e6ed6a03e1d7c2","fdc11ab27a2e4a409c9d3338ecfe9b80","64cd8746fd504e7596e16d526820ca56","f815a0f2bb2441049ba771bf775b0159"]},"collapsed":true,"cellView":"form","id":"Muw2sX4SiLlw","executionInfo":{"status":"ok","timestamp":1761211972122,"user_tz":-420,"elapsed":57862,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"b9145ab0-a369-483b-dfda-d796b351b3fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","CASME II MULTI-FRAME SEQUENCE ViT WITH FACE-AWARE PREPROCESSING\n","============================================================\n","\n","[1] Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully\n","\n","[2] Importing required libraries...\n","CASME II Multi-Frame Sequence ViT - Face-Aware Preprocessing Infrastructure\n","============================================================\n","Loading CASME II v9 preprocessing metadata...\n","Dataset variant: MFS\n","Processing date: 2025-10-19T08:20:12.098301\n","Preprocessing method: face_bbox_expansion_all_directions\n","Total images processed: 2774\n","Face detection rate: 100.00%\n","Target size: 224x224px\n","BBox expansion: 20px (all directions)\n","\n","Dataset split information:\n","  Train samples: 2613\n","  Validation samples: 78\n","  Test samples: 83\n","Using ViT-Base Patch16 for fine-grained micro-expression analysis (86M parameters)\n","\n","==================================================\n","EXPERIMENT CONFIGURATION - MULTI-FRAME SEQUENCE FACE-AWARE\n","==================================================\n","Dataset: v9 Multi-Frame Sequence with Face-Aware Preprocessing\n","Frame strategy: Multiple frames per video (dense sampling)\n","Training approach: Frame-level independent learning\n","Loss Function: Focal Loss\n","  Gamma: 2.0\n","  Alpha Weights (per-class): [0.048, 0.06, 0.085, 0.093, 0.095, 0.191, 0.427]\n","  Alpha Sum Validation: 0.999\n","ViT Model Variant: PATCH16\n","  Model: google/vit-base-patch16-224-in21k\n","  Patch Size: 16px\n","Input Resolution: 384x384px (upscaled from 224px with position interpolation)\n","Image Format: Grayscale converted to RGB (3-channel)\n","==================================================\n","\n","Device: cuda\n","GPU: NVIDIA L4 (23.8 GB)\n","GPU optimization enabled for NVIDIA L4\n","Large dataset configuration: Batch size 16 (optimal for 2613 samples at 384px)\n","Iterations per epoch: 163 (~82 iterations per epoch)\n","RAM preload workers: 32 (parallel image loading)\n","\n","Loading v9 class distribution...\n","\n","v9 Train distribution: [1027, 650, 325, 273, 260, 65, 13]\n","v9 Val distribution: [30, 18, 9, 9, 6, 3, 3]\n","v9 Test distribution: [30, 21, 12, 8, 9, 3, 0]\n","Applied Focal Loss alpha weights: [0.048 0.06  0.085 0.093 0.095 0.191 0.427]\n","Alpha weights sum: 0.999\n","\n","Setting up ViT Image Processor for 384px input...\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d522ea04d48a470ea6008aa422c21523"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ViT preprocessing configured:\n","  Input size: 384x384px\n","  Resize from: 224x224px (v9 native)\n","  Position encoding interpolation: Enabled\n","ViT Image Processor configured for 384px with position interpolation\n","\n","Dataset paths:\n","Train: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/train\n","Validation: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/val\n","Test: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/test\n","\n","ViT CASME II architecture validation...\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8707b93a0cd44aad95604ba1c4692547"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43984d742e224cddaccc6a4bd4a584b7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["ViT feature dimension: 768\n","Classification head: 768 -> 512 -> 128 -> 7\n","Dropout rate: 0.2 (balanced for large dataset)\n","Validation successful: Output shape torch.Size([1, 7])\n","Expected tokens for 384px with patch16: 576 tokens\n","ViT PATCH16 architecture validated\n","\n","============================================================\n","CASME II MULTI-FRAME SEQUENCE ViT CONFIGURATION COMPLETE\n","============================================================\n","Loss Configuration:\n","  Function: Optimized Focal Loss\n","  Gamma: 2.0\n","  Per-class Alpha: [0.048, 0.06, 0.085, 0.093, 0.095, 0.191, 0.427]\n","  Alpha Sum: 0.999\n","\n","Model Configuration:\n","  Architecture: ViT-Base\n","  Variant: PATCH16\n","  Model: google/vit-base-patch16-224-in21k\n","  Patch Size: 16px\n","  Input Resolution: 384px (upscaled from 224px v9 native)\n","  Feature Dimension: 768\n","  Position Interpolation: Enabled\n","  Classification Head: 768 -> 512 -> 128 -> 7 (enhanced)\n","\n","Dataset Configuration:\n","  Version: v9\n","  Classes: 7\n","  Frame strategy: multi_frame_sequence\n","  Training approach: frame_level_independent\n","  Inference strategy: frame_level_evaluation\n","  Weight Optimization: Per-class Alpha\n","\n","Training Configuration:\n","  Train samples: 2613 frames\n","  Validation samples: 78 frames\n","  Test samples: 83 frames\n","  Batch size: 16\n","  Learning rate: 2e-05\n","  Dropout rate: 0.3\n","  Expected tokens (patch16): 576\n","\n","Next: Cell 2 - Dataset Loading and Multi-Frame Training Pipeline\n"]}],"source":["# @title Cell 1: CASME II Multi-Frame Sequence ViT Infrastructure Configuration\n","\n","# File: 07_01_ViT_CASME2_MFS_Cell1.py\n","# Location: experiments/07_01_ViT_CASME2-MFS-PREP.ipynb\n","# Purpose: ViT-Base for CASME II micro-expression recognition with multi-frame sequence strategy and face-aware preprocessing\n","\n","from google.colab import drive\n","print(\"=\" * 60)\n","print(\"CASME II MULTI-FRAME SEQUENCE ViT WITH FACE-AWARE PREPROCESSING\")\n","print(\"=\" * 60)\n","print(\"\\n[1] Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Google Drive mounted successfully\")\n","\n","print(\"\\n[2] Importing required libraries...\")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import json\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import time\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Project paths configuration\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","DATASET_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/preprocessed_v9\"\n","CHECKPOINT_ROOT = f\"{PROJECT_ROOT}/models/07_01_vit_casme2_mfs_prep\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/07_01_vit_casme2_mfs_prep\"\n","\n","# Load CASME II v9 preprocessing metadata\n","PREPROCESSING_SUMMARY = f\"{DATASET_ROOT}/preprocessing_summary.json\"\n","\n","print(\"CASME II Multi-Frame Sequence ViT - Face-Aware Preprocessing Infrastructure\")\n","print(\"=\" * 60)\n","\n","# Validate preprocessing metadata exists\n","if not os.path.exists(PREPROCESSING_SUMMARY):\n","    raise FileNotFoundError(f\"v9 preprocessing summary not found: {PREPROCESSING_SUMMARY}\")\n","\n","# Load v9 preprocessing metadata\n","print(\"Loading CASME II v9 preprocessing metadata...\")\n","with open(PREPROCESSING_SUMMARY, 'r') as f:\n","    preprocessing_info = json.load(f)\n","\n","print(f\"Dataset variant: {preprocessing_info['variant']}\")\n","print(f\"Processing date: {preprocessing_info['processing_date']}\")\n","print(f\"Preprocessing method: {preprocessing_info['preprocessing_method']}\")\n","print(f\"Total images processed: {preprocessing_info['total_processed']}\")\n","print(f\"Face detection rate: {preprocessing_info['face_detection_stats']['detection_rate']:.2%}\")\n","\n","# Extract preprocessing parameters\n","preproc_params = preprocessing_info['preprocessing_parameters']\n","print(f\"Target size: {preproc_params['target_size']}x{preproc_params['target_size']}px\")\n","print(f\"BBox expansion: {preproc_params['bbox_expansion']}px (all directions)\")\n","\n","# Display split information\n","print(f\"\\nDataset split information:\")\n","print(f\"  Train samples: {preprocessing_info['splits']['train']['total_images']}\")\n","print(f\"  Validation samples: {preprocessing_info['splits']['val']['total_images']}\")\n","print(f\"  Test samples: {preprocessing_info['splits']['test']['total_images']}\")\n","\n","# =====================================================\n","# EXPERIMENT CONFIGURATION - Multi-Frame Sequence with Face-Aware Preprocessing\n","# =====================================================\n","# This configuration supports 4 experiment scenarios:\n","# 1. ViT-Base Patch16 + CrossEntropy Loss\n","# 2. ViT-Base Patch16 + Focal Loss\n","# 3. ViT-Base Patch32 + CrossEntropy Loss\n","# 4. ViT-Base Patch32 + Focal Loss\n","#\n","# Toggle VIT_MODEL_VARIANT for model selection: 'patch16' or 'patch32'\n","# Toggle USE_FOCAL_LOSS for loss function: False (CrossEntropy) or True (Focal)\n","# =====================================================\n","\n","# FOCAL LOSS CONFIGURATION - Toggle for experimentation\n","USE_FOCAL_LOSS = True  # Default: CrossEntropy, Set True to enable Focal Loss\n","FOCAL_LOSS_GAMMA = 2.0  # Focal loss focusing parameter (if enabled)\n","\n","# OPTIMIZED CLASS WEIGHTS CONFIGURATION\n","# CASME II classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","# v9 Train distribution: [1027, 650, 325, 273, 260, 65, 13] - inverse sqrt frequency approach\n","\n","# CrossEntropy Loss - Optimized inverse square root frequency weights\n","CROSSENTROPY_CLASS_WEIGHTS = [1.00, 1.26, 1.78, 1.94, 1.99, 3.98, 8.90]\n","\n","# Focal Loss - Normalized per-class alpha values (sum = 1.0)\n","FOCAL_LOSS_ALPHA_WEIGHTS = [0.048, 0.060, 0.085, 0.093, 0.095, 0.191, 0.427]\n","\n","# VIT MODEL CONFIGURATION - Support Patch16 and Patch32 variants\n","# ViT-Base Patch16: 86M parameters, fine-grained attention with 14x14 patches at 224px\n","# ViT-Base Patch32: 88M parameters, efficient attention with 7x7 patches at 224px\n","VIT_MODEL_VARIANT = 'patch16'  # Options: 'patch16' or 'patch32'\n","\n","# Dynamic ViT model selection based on patch size\n","if VIT_MODEL_VARIANT == 'patch16':\n","    VIT_MODEL_NAME = 'google/vit-base-patch16-224-in21k'\n","    PATCH_SIZE = 16\n","    print(\"Using ViT-Base Patch16 for fine-grained micro-expression analysis (86M parameters)\")\n","elif VIT_MODEL_VARIANT == 'patch32':\n","    VIT_MODEL_NAME = 'google/vit-base-patch32-224-in21k'\n","    PATCH_SIZE = 32\n","    print(\"Using ViT-Base Patch32 for efficient micro-expression recognition (88M parameters)\")\n","else:\n","    raise ValueError(f\"Unsupported VIT_MODEL_VARIANT: {VIT_MODEL_VARIANT}\")\n","\n","# Display experiment configuration\n","print(\"\\n\" + \"=\" * 50)\n","print(\"EXPERIMENT CONFIGURATION - MULTI-FRAME SEQUENCE FACE-AWARE\")\n","print(\"=\" * 50)\n","print(f\"Dataset: v9 Multi-Frame Sequence with Face-Aware Preprocessing\")\n","print(f\"Frame strategy: Multiple frames per video (dense sampling)\")\n","print(f\"Training approach: Frame-level independent learning\")\n","print(f\"Loss Function: {'Focal Loss' if USE_FOCAL_LOSS else 'CrossEntropy Loss'}\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Alpha Weights (per-class): {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum Validation: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Class Weights (inverse sqrt freq): {CROSSENTROPY_CLASS_WEIGHTS}\")\n","print(f\"ViT Model Variant: {VIT_MODEL_VARIANT.upper()}\")\n","print(f\"  Model: {VIT_MODEL_NAME}\")\n","print(f\"  Patch Size: {PATCH_SIZE}px\")\n","print(f\"Input Resolution: 384x384px (upscaled from 224px with position interpolation)\")\n","print(f\"Image Format: Grayscale converted to RGB (3-channel)\")\n","print(\"=\" * 50)\n","\n","# Enhanced GPU configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n","gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n","\n","print(f\"\\nDevice: {device}\")\n","print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n","\n","# Fixed batch size configuration for large dataset (2613 train samples)\n","BATCH_SIZE = 16  # Optimized for large dataset at 384px resolution\n","NUM_WORKERS = 4\n","\n","if 'A100' in gpu_name or 'L4' in gpu_name:\n","    torch.backends.cudnn.benchmark = True\n","    print(f\"GPU optimization enabled for {gpu_name}\")\n","\n","print(f\"Large dataset configuration: Batch size {BATCH_SIZE} (optimal for 2613 samples at 384px)\")\n","print(f\"Iterations per epoch: {2613 // BATCH_SIZE} (~82 iterations per epoch)\")\n","\n","# RAM preloading workers (separate from DataLoader workers)\n","RAM_PRELOAD_WORKERS = 32\n","print(f\"RAM preload workers: {RAM_PRELOAD_WORKERS} (parallel image loading)\")\n","\n","# CASME II class mapping and analysis\n","CASME2_CLASSES = ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CASME2_CLASSES)}\n","\n","# Extract class distribution from v9 preprocessing metadata\n","print(\"\\nLoading v9 class distribution...\")\n","train_dist = preprocessing_info['splits']['train']['emotion_distribution']\n","val_dist = preprocessing_info['splits']['val']['emotion_distribution']\n","test_dist = preprocessing_info['splits']['test']['emotion_distribution']\n","\n","# Convert to ordered list matching CASME2_CLASSES\n","def emotion_dist_to_list(emotion_dict, class_names):\n","    \"\"\"Convert emotion distribution dict to ordered list\"\"\"\n","    return [emotion_dict.get(cls, 0) for cls in class_names]\n","\n","train_dist_list = emotion_dist_to_list(train_dist, CASME2_CLASSES)\n","val_dist_list = emotion_dist_to_list(val_dist, CASME2_CLASSES)\n","test_dist_list = emotion_dist_to_list(test_dist, CASME2_CLASSES)\n","\n","print(f\"\\nv9 Train distribution: {train_dist_list}\")\n","print(f\"v9 Val distribution: {val_dist_list}\")\n","print(f\"v9 Test distribution: {test_dist_list}\")\n","\n","# Apply optimized class weights based on loss function selection\n","if USE_FOCAL_LOSS:\n","    class_weights = torch.tensor(FOCAL_LOSS_ALPHA_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied Focal Loss alpha weights: {class_weights.cpu().numpy()}\")\n","    print(f\"Alpha weights sum: {class_weights.sum().item():.3f}\")\n","else:\n","    class_weights = torch.tensor(CROSSENTROPY_CLASS_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied CrossEntropy class weights: {class_weights.cpu().numpy()}\")\n","\n","# CASME II ViT Configuration for Multi-Frame Sequence with Face-Aware Preprocessing\n","# Optimized for large dataset (2613 train samples) with multi-frame strategy\n","CASME2_VIT_CONFIG = {\n","    # Architecture configuration - ViT specific\n","    'vit_model': VIT_MODEL_NAME,\n","    'model_variant': VIT_MODEL_VARIANT,\n","    'patch_size': PATCH_SIZE,\n","    'input_size': 224,  # Upscaled from 224px with position interpolation\n","    'num_classes': 7,\n","    'dropout_rate': 0.3,  # Balanced regularization for large dataset\n","    'expected_feature_dim': 768,  # ViT-Base hidden dimension\n","    'interpolate_pos_encoding': True,  # Enable for 384px input\n","\n","    # Training configuration - proven optimal from KFS-PREP\n","    'learning_rate': 2e-5,  # Proven optimal for transformer fine-tuning\n","    'weight_decay': 1e-5,\n","    'gradient_clip': 1.0,\n","    'num_epochs': 50,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","\n","    # Scheduler configuration - stable for large dataset\n","    'scheduler_type': 'plateau',\n","    'scheduler_mode': 'max',\n","    'scheduler_factor': 0.5,\n","    'scheduler_patience': 3,  # Stable patience for large dataset\n","    'scheduler_min_lr': 1e-7,\n","    'scheduler_monitor': 'validation F1',\n","\n","    # Loss function configuration\n","    'use_focal_loss': USE_FOCAL_LOSS,\n","    'focal_loss_gamma': FOCAL_LOSS_GAMMA,\n","    'focal_loss_alpha_weights': FOCAL_LOSS_ALPHA_WEIGHTS,\n","    'crossentropy_class_weights': CROSSENTROPY_CLASS_WEIGHTS,\n","\n","    # Dataset configuration - v9 MFS specific\n","    'dataset_version': 'v9',\n","    'frame_strategy': 'multi_frame_sequence',\n","    'frame_types': ['multiple_frames_per_video'],\n","    'training_approach': 'frame_level_independent',\n","    'inference_strategy': 'frame_level_evaluation',\n","\n","    # Regularization configuration\n","    'label_smoothing': 0.0,\n","    'mixup_alpha': 0.0,\n","    'cutmix_alpha': 0.0\n","}\n","\n","# Optimized Focal Loss implementation\n","class OptimizedFocalLoss(nn.Module):\n","    \"\"\"\n","    Optimized Focal Loss with per-class alpha weights\n","    Handles class imbalance with normalized alpha weights (sum = 1.0)\n","    \"\"\"\n","    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n","        super(OptimizedFocalLoss, self).__init__()\n","        if alpha is not None:\n","            self.alpha = torch.tensor(alpha, dtype=torch.float32)\n","        else:\n","            self.alpha = None\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        if self.alpha is not None and self.alpha.device != inputs.device:\n","            self.alpha = self.alpha.to(inputs.device)\n","\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = (1 - pt) ** self.gamma * ce_loss\n","\n","        if self.alpha is not None:\n","            alpha_t = self.alpha[targets]\n","            focal_loss = alpha_t * focal_loss\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","\n","# ViT CASME II Baseline Model - Enhanced Architecture for MFS\n","class ViTCASME2Baseline(nn.Module):\n","    \"\"\"\n","    ViT baseline for CASME II micro-expression recognition\n","    Enhanced architecture: 768 -> 512 -> 128 -> 7 (deeper for large dataset)\n","    \"\"\"\n","\n","    def __init__(self, num_classes=7, dropout_rate=0.2):\n","        super(ViTCASME2Baseline, self).__init__()\n","\n","        # Load pretrained ViT backbone from HuggingFace\n","        from transformers import ViTModel\n","\n","        self.vit = ViTModel.from_pretrained(\n","            CASME2_VIT_CONFIG['vit_model'],\n","            add_pooling_layer=False\n","        )\n","\n","        # Enable fine-tuning for micro-expression domain adaptation\n","        for param in self.vit.parameters():\n","            param.requires_grad = True\n","\n","        # Get ViT feature dimension from configuration\n","        self.vit_feature_dim = self.vit.config.hidden_size\n","\n","        print(f\"ViT feature dimension: {self.vit_feature_dim}\")\n","\n","        # Enhanced classification head for large dataset\n","        # Architecture: 768 -> 512 -> 128 -> 7 (proven effective for MFS)\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.vit_feature_dim, 512),\n","            nn.LayerNorm(512),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","\n","            nn.Linear(512, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","        print(f\"Classification head: {self.vit_feature_dim} -> 512 -> 128 -> {num_classes}\")\n","        print(f\"Dropout rate: {dropout_rate} (balanced for large dataset)\")\n","\n","    def forward(self, pixel_values):\n","        # ViT forward pass with position embedding interpolation for 384px\n","        vit_outputs = self.vit(\n","            pixel_values=pixel_values,\n","            interpolate_pos_encoding=CASME2_VIT_CONFIG['interpolate_pos_encoding']\n","        )\n","\n","        # Extract CLS token features\n","        # CLS token is at index 0 of last_hidden_state\n","        vit_features = vit_outputs.last_hidden_state[:, 0]\n","\n","        # Classification pipeline\n","        processed_features = self.classifier_layers(vit_features)\n","        output = self.classifier(processed_features)\n","\n","        return output\n","\n","# Enhanced optimizer and scheduler factory\n","def create_optimizer_scheduler_casme2(model, config):\n","    \"\"\"Create optimizer and scheduler for CASME II ViT training\"\"\"\n","\n","    # AdamW optimizer with proven configuration\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=config['learning_rate'],\n","        weight_decay=config['weight_decay'],\n","        betas=(0.9, 0.999)\n","    )\n","\n","    # ReduceLROnPlateau scheduler monitoring validation F1\n","    if config['scheduler_type'] == 'plateau':\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer,\n","            mode=config['scheduler_mode'],\n","            factor=config['scheduler_factor'],\n","            patience=config['scheduler_patience'],\n","            min_lr=config['scheduler_min_lr']\n","        )\n","        print(f\"Scheduler: ReduceLROnPlateau monitoring {config['scheduler_monitor']}\")\n","    else:\n","        scheduler = None\n","\n","    return optimizer, scheduler\n","\n","# ViT Image Processor setup for 384px input\n","from transformers import ViTImageProcessor\n","\n","print(\"\\nSetting up ViT Image Processor for 384px input...\")\n","\n","vit_processor = ViTImageProcessor.from_pretrained(\n","    CASME2_VIT_CONFIG['vit_model'],\n","    do_resize=True,\n","    size={'height': 384, 'width': 384},\n","    do_normalize=True,\n","    do_rescale=True,\n","    do_center_crop=False\n",")\n","\n","print(f\"ViT preprocessing configured:\")\n","print(f\"  Input size: 384x384px\")\n","print(f\"  Resize from: 224x224px (v9 native)\")\n","print(f\"  Position encoding interpolation: Enabled\")\n","\n","# Transform functions for ViT\n","def vit_transform_train(image):\n","    \"\"\"\n","    Training transform with ViT Image Processor\n","    Handles grayscale to RGB conversion\n","    \"\"\"\n","    # Ensure RGB format (grayscale images will be converted via channel repetition)\n","    if image.mode != 'RGB':\n","        image = image.convert('RGB')\n","\n","    inputs = vit_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","def vit_transform_val(image):\n","    \"\"\"\n","    Validation transform with ViT Image Processor\n","    Handles grayscale to RGB conversion\n","    \"\"\"\n","    # Ensure RGB format (grayscale images will be converted via channel repetition)\n","    if image.mode != 'RGB':\n","        image = image.convert('RGB')\n","\n","    inputs = vit_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","print(\"ViT Image Processor configured for 384px with position interpolation\")\n","\n","# CASME II Dataset for v9 MFS with flat directory structure\n","class CASME2DatasetMFS(Dataset):\n","    \"\"\"CASME II v9 Multi-Frame Sequence dataset with flexible filename parsing\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        # Load all images from flat directory structure\n","        print(f\"Loading {split} dataset from {split_path}...\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        if len(all_files) > 0:\n","            print(f\"Sample filename: {all_files[0]}\")\n","\n","        loaded_count = 0\n","        skipped_count = 0\n","\n","        for filename in sorted(all_files):\n","            # Extract emotion from filename - flexible approach\n","            # Try multiple patterns to handle different filename formats\n","            emotion_found = None\n","\n","            # Remove file extension first\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            # Pattern 1: Check if any emotion class appears in filename\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                loaded_count += 1\n","            else:\n","                skipped_count += 1\n","                if skipped_count <= 3:\n","                    print(f\"  Skipped (no emotion found): {filename}\")\n","\n","        print(f\"Loaded {len(self.images)} samples for {split} split\")\n","        if skipped_count > 0:\n","            print(f\"  Skipped {skipped_count} files (no recognizable emotion)\")\n","\n","        if len(self.images) == 0:\n","            print(f\"ERROR: No samples loaded! Check filename format and emotion labels.\")\n","            print(f\"Expected emotions in filenames: {CASME2_CLASSES}\")\n","\n","        self._print_distribution()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"  No samples to display distribution\")\n","            return\n","\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.images[idx]\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","# Create directories\n","os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/training_logs\", exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/evaluation_results\", exist_ok=True)\n","\n","print(f\"\\nDataset paths:\")\n","print(f\"Train: {DATASET_ROOT}/train\")\n","print(f\"Validation: {DATASET_ROOT}/val\")\n","print(f\"Test: {DATASET_ROOT}/test\")\n","\n","# Architecture validation\n","print(\"\\nViT CASME II architecture validation...\")\n","\n","try:\n","    test_model = ViTCASME2Baseline(num_classes=7, dropout_rate=0.2).to(device)\n","    test_input = torch.randn(1, 3, 384, 384).to(device)\n","    test_output = test_model(test_input)\n","\n","    # Dynamic token calculation based on configured patch size\n","    expected_tokens = (384 // CASME2_VIT_CONFIG['patch_size']) ** 2\n","\n","    print(f\"Validation successful: Output shape {test_output.shape}\")\n","    print(f\"Expected tokens for 384px with patch{CASME2_VIT_CONFIG['patch_size']}: {expected_tokens} tokens\")\n","    print(f\"ViT {VIT_MODEL_VARIANT.upper()} architecture validated\")\n","\n","    del test_model, test_input, test_output\n","    torch.cuda.empty_cache()\n","\n","except Exception as e:\n","    print(f\"Validation failed: {e}\")\n","    import traceback\n","    traceback.print_exc()\n","\n","# Loss function factory\n","def create_criterion_casme2(weights, use_focal_loss=False, alpha_weights=None, gamma=2.0):\n","    \"\"\"\n","    Factory function to create loss criterion based on configuration\n","\n","    Args:\n","        weights: Class weights for CrossEntropy\n","        use_focal_loss: Whether to use Focal Loss or CrossEntropy\n","        alpha_weights: Per-class alpha weights for Focal Loss\n","        gamma: Focal loss gamma parameter\n","\n","    Returns:\n","        Loss function\n","    \"\"\"\n","    if use_focal_loss:\n","        print(f\"Using Optimized Focal Loss with gamma={gamma}\")\n","        if alpha_weights:\n","            print(f\"Per-class alpha weights: {alpha_weights}\")\n","            print(f\"Alpha sum: {sum(alpha_weights):.3f}\")\n","        return OptimizedFocalLoss(alpha=alpha_weights, gamma=gamma)\n","    else:\n","        print(f\"Using CrossEntropy Loss with optimized class weights\")\n","        print(f\"Class weights: {weights.cpu().numpy()}\")\n","        return nn.CrossEntropyLoss(weight=weights)\n","\n","# Global configuration for training pipeline\n","GLOBAL_CONFIG_CASME2 = {\n","    'device': device,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'num_classes': 7,\n","    'class_weights': class_weights,\n","    'class_names': CASME2_CLASSES,\n","    'class_to_idx': CLASS_TO_IDX,\n","    'transform_train': vit_transform_train,\n","    'transform_val': vit_transform_val,\n","    'vit_config': CASME2_VIT_CONFIG,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'results_root': RESULTS_ROOT,\n","    'dataset_root': DATASET_ROOT,\n","    'optimizer_scheduler_factory': create_optimizer_scheduler_casme2,\n","    'criterion_factory': create_criterion_casme2\n","}\n","\n","# Configuration validation and summary\n","print(\"\\n\" + \"=\" * 60)\n","print(\"CASME II MULTI-FRAME SEQUENCE ViT CONFIGURATION COMPLETE\")\n","print(\"=\" * 60)\n","\n","print(f\"Loss Configuration:\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Function: Optimized Focal Loss\")\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Per-class Alpha: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Function: CrossEntropy with Optimized Weights\")\n","    print(f\"  Class Weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","\n","print(f\"\\nModel Configuration:\")\n","print(f\"  Architecture: ViT-Base\")\n","print(f\"  Variant: {VIT_MODEL_VARIANT.upper()}\")\n","print(f\"  Model: {VIT_MODEL_NAME}\")\n","print(f\"  Patch Size: {PATCH_SIZE}px\")\n","print(f\"  Input Resolution: 384px (upscaled from 224px v9 native)\")\n","print(f\"  Feature Dimension: {CASME2_VIT_CONFIG['expected_feature_dim']}\")\n","print(f\"  Position Interpolation: Enabled\")\n","print(f\"  Classification Head: {CASME2_VIT_CONFIG['expected_feature_dim']} -> 512 -> 128 -> 7 (enhanced)\")\n","\n","print(f\"\\nDataset Configuration:\")\n","print(f\"  Version: {CASME2_VIT_CONFIG['dataset_version']}\")\n","print(f\"  Classes: {len(CASME2_CLASSES)}\")\n","print(f\"  Frame strategy: {CASME2_VIT_CONFIG['frame_strategy']}\")\n","print(f\"  Training approach: {CASME2_VIT_CONFIG['training_approach']}\")\n","print(f\"  Inference strategy: {CASME2_VIT_CONFIG['inference_strategy']}\")\n","print(f\"  Weight Optimization: {'Per-class Alpha' if USE_FOCAL_LOSS else 'Inverse Sqrt Frequency'}\")\n","\n","print(f\"\\nTraining Configuration:\")\n","print(f\"  Train samples: {preprocessing_info['splits']['train']['total_images']} frames\")\n","print(f\"  Validation samples: {preprocessing_info['splits']['val']['total_images']} frames\")\n","print(f\"  Test samples: {preprocessing_info['splits']['test']['total_images']} frames\")\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","print(f\"  Learning rate: {CASME2_VIT_CONFIG['learning_rate']}\")\n","print(f\"  Dropout rate: {CASME2_VIT_CONFIG['dropout_rate']}\")\n","print(f\"  Expected tokens (patch{PATCH_SIZE}): {(384 // PATCH_SIZE) ** 2}\")\n","\n","print(\"\\nNext: Cell 2 - Dataset Loading and Multi-Frame Training Pipeline\")"]},{"cell_type":"code","source":["# @title Cell 2: CASME II Multi-Frame Sequence ViT Training Pipeline\n","\n","# File: 07_01_ViT_CASME2_MFS_Cell2.py\n","# Location: experiments/07_01_ViT_CASME2-MFS-PREP.ipynb\n","# Purpose: Enhanced training pipeline for CASME II Multi-Frame Sequence ViT with optimized RAM caching\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing as mp\n","import shutil\n","import tempfile\n","\n","print(\"CASME II Multi-Frame Sequence ViT Training Pipeline\")\n","print(\"=\" * 70)\n","print(f\"Loss Function: {'Optimized Focal Loss' if CASME2_VIT_CONFIG['use_focal_loss'] else 'CrossEntropy Loss'}\")\n","if CASME2_VIT_CONFIG['use_focal_loss']:\n","    print(f\"Focal Loss Parameters:\")\n","    print(f\"  Gamma: {CASME2_VIT_CONFIG['focal_loss_gamma']}\")\n","    print(f\"  Per-class Alpha: {CASME2_VIT_CONFIG['focal_loss_alpha_weights']}\")\n","    print(f\"  Alpha Sum: {sum(CASME2_VIT_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","else:\n","    print(f\"CrossEntropy Parameters:\")\n","    print(f\"  Optimized Class Weights: {CASME2_VIT_CONFIG['crossentropy_class_weights']}\")\n","print(f\"Dataset version: {CASME2_VIT_CONFIG['dataset_version']}\")\n","print(f\"Frame strategy: {CASME2_VIT_CONFIG['frame_strategy']}\")\n","print(f\"Training approach: {CASME2_VIT_CONFIG['training_approach']}\")\n","print(f\"ViT variant: {CASME2_VIT_CONFIG['model_variant'].upper()}\")\n","print(f\"Patch size: {CASME2_VIT_CONFIG['patch_size']}px\")\n","print(f\"Training epochs: {CASME2_VIT_CONFIG['num_epochs']}\")\n","print(f\"Batch size: {CASME2_VIT_CONFIG['batch_size']}\")\n","print(f\"Scheduler patience: {CASME2_VIT_CONFIG['scheduler_patience']}\")\n","\n","# Enhanced CASME II Dataset with optimized RAM caching for large dataset\n","class CASME2DatasetTraining(Dataset):\n","    \"\"\"Enhanced CASME II dataset for training with RAM caching optimization for large dataset\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","        self.cached_images = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading CASME II {split} dataset for training...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        if len(all_files) > 0:\n","            print(f\"Sample filename: {all_files[0]}\")\n","\n","        loaded_count = 0\n","        skipped_count = 0\n","\n","        for filename in sorted(all_files):\n","            # Extract emotion from filename - flexible approach\n","            emotion_found = None\n","\n","            # Remove file extension first\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            # Check if any emotion class appears in filename\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                loaded_count += 1\n","            else:\n","                skipped_count += 1\n","                if skipped_count <= 3:\n","                    print(f\"  Skipped (no emotion found): {filename}\")\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples\")\n","        if skipped_count > 0:\n","            print(f\"  Skipped {skipped_count} files (no recognizable emotion)\")\n","\n","        if len(self.images) == 0:\n","            print(f\"ERROR: No samples loaded! Check filename format and emotion labels.\")\n","            print(f\"Expected emotions in filenames: {CASME2_CLASSES}\")\n","\n","        self._print_distribution()\n","\n","        if self.use_ram_cache and len(self.images) > 0:\n","            self._preload_to_ram()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"  No samples to display distribution\")\n","            return\n","\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def _preload_to_ram(self):\n","        \"\"\"RAM preloading with parallel loading for training efficiency\"\"\"\n","        if len(self.images) == 0:\n","            print(f\"Skipping RAM preload: No images to load\")\n","            return\n","\n","        print(f\"Preloading {len(self.images)} {self.split} images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            \"\"\"Load single image with error handling\"\"\"\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (224, 224), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=f\"Loading {self.split} to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * 224 * 224 * 3 * 4 / 1e9\n","        print(f\"{self.split.upper()} RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (224, 224), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","# Enhanced metrics calculation with comprehensive error handling\n","def calculate_metrics_safe_robust(predictions, labels, class_names, average='macro'):\n","    \"\"\"\n","    Calculate metrics with enhanced error handling and validation\n","\n","    Args:\n","        predictions: Predicted labels\n","        labels: True labels\n","        class_names: List of class names\n","        average: Averaging method for metrics\n","\n","    Returns:\n","        dict: Computed metrics\n","    \"\"\"\n","    try:\n","        # Ensure arrays are numpy arrays\n","        predictions = np.array(predictions)\n","        labels = np.array(labels)\n","\n","        # Validate input\n","        if len(predictions) == 0 or len(labels) == 0:\n","            return {\n","                'accuracy': 0.0,\n","                'precision': 0.0,\n","                'recall': 0.0,\n","                'f1_score': 0.0\n","            }\n","\n","        # Calculate accuracy\n","        accuracy = accuracy_score(labels, predictions)\n","\n","        # Calculate precision, recall, F1 with zero_division handling\n","        precision, recall, f1, support = precision_recall_fscore_support(\n","            labels, predictions,\n","            average=average,\n","            zero_division=0\n","        )\n","\n","        return {\n","            'accuracy': float(accuracy),\n","            'precision': float(precision),\n","            'recall': float(recall),\n","            'f1_score': float(f1)\n","        }\n","\n","    except Exception as e:\n","        print(f\"Warning: Metrics calculation failed: {e}\")\n","        return {\n","            'accuracy': 0.0,\n","            'precision': 0.0,\n","            'recall': 0.0,\n","            'f1_score': 0.0\n","        }\n","\n","# Enhanced checkpoint saving with atomic operations\n","def save_checkpoint_robust(model, optimizer, scheduler, epoch, train_metrics, val_metrics,\n","                          checkpoint_root, best_metrics, config):\n","    \"\"\"\n","    Save checkpoint with enhanced robustness and atomic operations\n","\n","    Args:\n","        model: Model to save\n","        optimizer: Optimizer state\n","        scheduler: Scheduler state\n","        epoch: Current epoch\n","        train_metrics: Training metrics\n","        val_metrics: Validation metrics\n","        checkpoint_root: Root directory for checkpoints\n","        best_metrics: Best metrics tracker\n","        config: Configuration dict\n","\n","    Returns:\n","        str: Path to saved checkpoint or None if failed\n","    \"\"\"\n","    try:\n","        os.makedirs(checkpoint_root, exist_ok=True)\n","\n","        checkpoint_data = {\n","            'epoch': epoch + 1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","            'train_metrics': train_metrics,\n","            'val_metrics': val_metrics,\n","            'best_metrics': best_metrics,\n","            'config': config\n","        }\n","\n","        # Atomic save using temporary file\n","        final_path = os.path.join(checkpoint_root, 'casme2_vit_mfs_best_f1.pth')\n","\n","        with tempfile.NamedTemporaryFile(mode='wb', delete=False, dir=checkpoint_root) as tmp_file:\n","            tmp_path = tmp_file.name\n","            torch.save(checkpoint_data, tmp_file)\n","\n","        # Atomic rename\n","        shutil.move(tmp_path, final_path)\n","\n","        return final_path\n","\n","    except Exception as e:\n","        print(f\"ERROR: Checkpoint save failed: {e}\")\n","        return None\n","\n","# JSON serialization helper\n","def safe_json_serialize(obj):\n","    \"\"\"Safely convert objects to JSON-serializable format\"\"\"\n","    if isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    elif isinstance(obj, np.integer):\n","        return int(obj)\n","    elif isinstance(obj, np.floating):\n","        return float(obj)\n","    elif isinstance(obj, dict):\n","        return {key: safe_json_serialize(value) for key, value in obj.items()}\n","    elif isinstance(obj, list):\n","        return [safe_json_serialize(item) for item in obj]\n","    elif isinstance(obj, torch.Tensor):\n","        return obj.detach().cpu().numpy().tolist()\n","    else:\n","        return obj\n","\n","# Enhanced training epoch function\n","def train_epoch(model, train_loader, criterion, optimizer, device, epoch, total_epochs):\n","    \"\"\"Training epoch with enhanced progress tracking\"\"\"\n","    model.train()\n","\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_labels = []\n","\n","    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{total_epochs} [Train]\")\n","\n","    for batch_idx, (images, labels, filenames) in enumerate(pbar):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","\n","        if CASME2_VIT_CONFIG['gradient_clip'] > 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), CASME2_VIT_CONFIG['gradient_clip'])\n","\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        all_predictions.extend(predicted.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","        pbar.set_postfix({\n","            'loss': f\"{loss.item():.4f}\",\n","            'avg_loss': f\"{running_loss/(batch_idx+1):.4f}\"\n","        })\n","\n","    epoch_loss = running_loss / len(train_loader)\n","    metrics = calculate_metrics_safe_robust(all_predictions, all_labels, CASME2_CLASSES)\n","\n","    return epoch_loss, metrics, None\n","\n","# Enhanced validation epoch function\n","def validate_epoch(model, val_loader, criterion, device, epoch, total_epochs):\n","    \"\"\"Validation epoch with enhanced metrics\"\"\"\n","    model.eval()\n","\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_labels = []\n","    all_filenames = []\n","\n","    with torch.no_grad():\n","        pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{total_epochs} [Val]\")\n","\n","        for batch_idx, (images, labels, filenames) in enumerate(pbar):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","            all_filenames.extend(filenames)\n","\n","            pbar.set_postfix({\n","                'loss': f\"{loss.item():.4f}\",\n","                'avg_loss': f\"{running_loss/(batch_idx+1):.4f}\"\n","            })\n","\n","    epoch_loss = running_loss / len(val_loader)\n","    metrics = calculate_metrics_safe_robust(all_predictions, all_labels, CASME2_CLASSES)\n","\n","    return epoch_loss, metrics, all_filenames\n","\n","# Create datasets with RAM caching\n","print(\"\\nCreating CASME II datasets with RAM caching optimization...\")\n","\n","train_dataset = CASME2DatasetTraining(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='train',\n","    transform=GLOBAL_CONFIG_CASME2['transform_train'],\n","    use_ram_cache=True\n",")\n","\n","val_dataset = CASME2DatasetTraining(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='val',\n","    transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","    use_ram_cache=True\n",")\n","\n","print(f\"\\nDataset loading complete:\")\n","print(f\"  Train samples: {len(train_dataset)}\")\n","print(f\"  Validation samples: {len(val_dataset)}\")\n","\n","# Create data loaders\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=GLOBAL_CONFIG_CASME2['batch_size'],\n","    shuffle=True,\n","    num_workers=GLOBAL_CONFIG_CASME2['num_workers'],\n","    pin_memory=True\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=GLOBAL_CONFIG_CASME2['batch_size'],\n","    shuffle=False,\n","    num_workers=GLOBAL_CONFIG_CASME2['num_workers'],\n","    pin_memory=True\n",")\n","\n","print(f\"Data loaders created:\")\n","print(f\"  Train batches: {len(train_loader)}\")\n","print(f\"  Validation batches: {len(val_loader)}\")\n","\n","# Initialize model\n","print(\"\\nInitializing ViT CASME II model...\")\n","model = ViTCASME2Baseline(\n","    num_classes=CASME2_VIT_CONFIG['num_classes'],\n","    dropout_rate=CASME2_VIT_CONFIG['dropout_rate']\n",").to(GLOBAL_CONFIG_CASME2['device'])\n","\n","print(f\"Model initialized on {GLOBAL_CONFIG_CASME2['device']}\")\n","\n","# Create optimizer and scheduler\n","optimizer, scheduler = GLOBAL_CONFIG_CASME2['optimizer_scheduler_factory'](\n","    model, CASME2_VIT_CONFIG\n",")\n","\n","# Create loss criterion\n","criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","    weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","    use_focal_loss=CASME2_VIT_CONFIG['use_focal_loss'],\n","    alpha_weights=CASME2_VIT_CONFIG['focal_loss_alpha_weights'],\n","    gamma=CASME2_VIT_CONFIG['focal_loss_gamma']\n",")\n","\n","print(\"\\nTraining components initialized successfully\")\n","\n","# Training loop with enhanced tracking\n","print(\"\\n\" + \"=\" * 70)\n","print(\"Starting CASME II Multi-Frame Sequence ViT training...\")\n","print(\"=\" * 70)\n","\n","training_history = {\n","    'train_loss': [],\n","    'val_loss': [],\n","    'train_f1': [],\n","    'val_f1': [],\n","    'train_acc': [],\n","    'val_acc': [],\n","    'learning_rate': [],\n","    'epoch_time': []\n","}\n","\n","best_metrics = {\n","    'f1': 0.0,\n","    'loss': float('inf'),\n","    'accuracy': 0.0,\n","    'epoch': 0\n","}\n","\n","start_time = time.time()\n","\n","for epoch in range(CASME2_VIT_CONFIG['num_epochs']):\n","    epoch_start_time = time.time()\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"Epoch {epoch+1}/{CASME2_VIT_CONFIG['num_epochs']}\")\n","    print(f\"{'='*70}\")\n","\n","    # Training phase\n","    train_loss, train_metrics, train_filenames = train_epoch(\n","        model, train_loader, criterion, optimizer,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_VIT_CONFIG['num_epochs']\n","    )\n","\n","    # Validation phase\n","    val_loss, val_metrics, val_filenames = validate_epoch(\n","        model, val_loader, criterion,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_VIT_CONFIG['num_epochs']\n","    )\n","\n","    # Update scheduler\n","    if scheduler:\n","        scheduler.step(val_metrics['f1_score'])\n","\n","    # Record training history\n","    epoch_time = time.time() - epoch_start_time\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    training_history['train_loss'].append(float(train_loss))\n","    training_history['val_loss'].append(float(val_loss))\n","    training_history['train_f1'].append(float(train_metrics['f1_score']))\n","    training_history['val_f1'].append(float(val_metrics['f1_score']))\n","    training_history['train_acc'].append(float(train_metrics['accuracy']))\n","    training_history['val_acc'].append(float(val_metrics['accuracy']))\n","    training_history['learning_rate'].append(float(current_lr))\n","    training_history['epoch_time'].append(float(epoch_time))\n","\n","    # Print epoch summary\n","    print(f\"Train - Loss: {train_loss:.4f}, F1: {train_metrics['f1_score']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n","    print(f\"Val   - Loss: {val_loss:.4f}, F1: {val_metrics['f1_score']:.4f}, Acc: {val_metrics['accuracy']:.4f}\")\n","    print(f\"Time  - Epoch: {epoch_time:.1f}s, LR: {current_lr:.2e}\")\n","\n","    # Enhanced multi-criteria checkpoint saving logic\n","    save_model = False\n","    improvement_reason = \"\"\n","\n","    if val_metrics['f1_score'] > best_metrics['f1']:\n","        save_model = True\n","        improvement_reason = \"Higher F1\"\n","    elif val_metrics['f1_score'] == best_metrics['f1']:\n","        if val_loss < best_metrics['loss']:\n","            save_model = True\n","            improvement_reason = \"Same F1, Lower Loss\"\n","        elif val_loss == best_metrics['loss'] and val_metrics['accuracy'] > best_metrics['accuracy']:\n","            save_model = True\n","            improvement_reason = \"Same F1&Loss, Higher Accuracy\"\n","\n","    if save_model:\n","        best_metrics['f1'] = val_metrics['f1_score']\n","        best_metrics['loss'] = val_loss\n","        best_metrics['accuracy'] = val_metrics['accuracy']\n","        best_metrics['epoch'] = epoch + 1\n","\n","        best_model_path = save_checkpoint_robust(\n","            model, optimizer, scheduler, epoch,\n","            train_metrics, val_metrics, GLOBAL_CONFIG_CASME2['checkpoint_root'],\n","            best_metrics, CASME2_VIT_CONFIG\n","        )\n","\n","        if best_model_path:\n","            print(f\"New best model: {improvement_reason} - F1: {best_metrics['f1']:.4f}\")\n","        else:\n","            print(f\"Warning: Failed to save checkpoint despite improvement\")\n","\n","    # Progress tracking\n","    elapsed_time = time.time() - start_time\n","    estimated_total = (elapsed_time / (epoch + 1)) * CASME2_VIT_CONFIG['num_epochs']\n","    remaining_time = estimated_total - elapsed_time\n","    progress_pct = ((epoch + 1) / CASME2_VIT_CONFIG['num_epochs']) * 100\n","\n","    print(f\"Progress: {progress_pct:.1f}% | Best F1: {best_metrics['f1']:.4f} | ETA: {remaining_time/60:.1f}min\")\n","\n","# Training completion\n","total_time = time.time() - start_time\n","actual_epochs = CASME2_VIT_CONFIG['num_epochs']\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II MULTI-FRAME SEQUENCE ViT TRAINING COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Training time: {total_time/60:.1f} minutes\")\n","print(f\"Epochs completed: {actual_epochs}\")\n","print(f\"Best validation F1: {best_metrics['f1']:.4f} (epoch {best_metrics['epoch']})\")\n","print(f\"Final train F1: {training_history['train_f1'][-1]:.4f}\")\n","print(f\"Final validation F1: {training_history['val_f1'][-1]:.4f}\")\n","\n","# Enhanced training documentation export\n","results_dir = GLOBAL_CONFIG_CASME2['results_root']\n","os.makedirs(f\"{results_dir}/training_logs\", exist_ok=True)\n","\n","training_history_path = f\"{results_dir}/training_logs/casme2_vit_mfs_training_history.json\"\n","\n","print(\"\\nExporting enhanced training documentation...\")\n","\n","try:\n","    training_summary = {\n","        'experiment_type': 'CASME2_ViT_MultiFrameSequence',\n","        'experiment_configuration': {\n","            'dataset_version': CASME2_VIT_CONFIG['dataset_version'],\n","            'frame_strategy': CASME2_VIT_CONFIG['frame_strategy'],\n","            'training_approach': CASME2_VIT_CONFIG['training_approach'],\n","            'inference_strategy': CASME2_VIT_CONFIG['inference_strategy'],\n","            'loss_function': 'Optimized Focal Loss' if CASME2_VIT_CONFIG['use_focal_loss'] else 'CrossEntropy',\n","            'weight_approach': 'Per-class Alpha (sum=1.0)' if CASME2_VIT_CONFIG['use_focal_loss'] else 'Inverse Sqrt Frequency',\n","            'focal_loss_gamma': CASME2_VIT_CONFIG['focal_loss_gamma'],\n","            'focal_loss_alpha_weights': CASME2_VIT_CONFIG['focal_loss_alpha_weights'],\n","            'crossentropy_class_weights': CASME2_VIT_CONFIG['crossentropy_class_weights'],\n","            'vit_model': CASME2_VIT_CONFIG['vit_model'],\n","            'model_variant': CASME2_VIT_CONFIG['model_variant'],\n","            'patch_size': CASME2_VIT_CONFIG['patch_size']\n","        },\n","        'training_history': safe_json_serialize(training_history),\n","        'best_val_f1': float(best_metrics['f1']),\n","        'best_val_loss': float(best_metrics['loss']),\n","        'best_val_accuracy': float(best_metrics['accuracy']),\n","        'best_epoch': int(best_metrics['epoch']),\n","        'total_epochs': int(actual_epochs),\n","        'total_time_minutes': float(total_time / 60),\n","        'average_epoch_time_seconds': float(np.mean(training_history['epoch_time'])),\n","        'config': safe_json_serialize(CASME2_VIT_CONFIG),\n","        'final_train_f1': float(training_history['train_f1'][-1]),\n","        'final_val_f1': float(training_history['val_f1'][-1]),\n","        'model_checkpoint': 'casme2_vit_mfs_best_f1.pth',\n","        'dataset_info': {\n","            'name': 'CASME_II',\n","            'version': CASME2_VIT_CONFIG['dataset_version'],\n","            'frame_strategy': CASME2_VIT_CONFIG['frame_strategy'],\n","            'train_samples': len(train_dataset),\n","            'val_samples': len(val_dataset),\n","            'num_classes': 7,\n","            'class_names': CASME2_CLASSES\n","        },\n","        'architecture_info': {\n","            'model_type': 'ViTCASME2Baseline',\n","            'backbone': CASME2_VIT_CONFIG['vit_model'],\n","            'variant': CASME2_VIT_CONFIG['model_variant'],\n","            'patch_size': CASME2_VIT_CONFIG['patch_size'],\n","            'input_size': f\"{CASME2_VIT_CONFIG['input_size']}x{CASME2_VIT_CONFIG['input_size']}\",\n","            'expected_feature_dim': CASME2_VIT_CONFIG['expected_feature_dim'],\n","            'classification_head': f\"{CASME2_VIT_CONFIG['expected_feature_dim']}->512->128->7\",\n","            'position_interpolation': CASME2_VIT_CONFIG['interpolate_pos_encoding']\n","        },\n","        'enhanced_features': {\n","            'hardened_checkpoint_system': True,\n","            'atomic_checkpoint_save': True,\n","            'checkpoint_validation': True,\n","            'model_output_validation': True,\n","            'enhanced_error_handling': True,\n","            'multi_criteria_checkpoint_logic': True,\n","            'memory_optimized_training': True,\n","            'ram_caching': True,\n","            'position_encoding_interpolation': True\n","        }\n","    }\n","\n","    with open(training_history_path, 'w') as f:\n","        json.dump(training_summary, f, indent=2)\n","\n","    print(f\"Enhanced training documentation saved successfully: {training_history_path}\")\n","    print(f\"Experiment details: {training_summary['experiment_configuration']['loss_function']} loss\")\n","    if CASME2_VIT_CONFIG['use_focal_loss']:\n","        print(f\"  Gamma: {CASME2_VIT_CONFIG['focal_loss_gamma']}, Alpha Sum: {sum(CASME2_VIT_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","    print(f\"Model variant: {CASME2_VIT_CONFIG['model_variant'].upper()}\")\n","    print(f\"Patch size: {CASME2_VIT_CONFIG['patch_size']}px\")\n","    print(f\"Dataset version: {CASME2_VIT_CONFIG['dataset_version']}\")\n","\n","except Exception as e:\n","    print(f\"Warning: Could not save training documentation: {e}\")\n","    print(\"Training completed successfully, but documentation export failed\")\n","\n","# Enhanced memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\nNext: Cell 3 - CASME II Multi-Frame Sequence ViT Evaluation\")\n","print(\"Enhanced training pipeline completed successfully!\")"],"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"VP0QCCXgn4n0","executionInfo":{"status":"ok","timestamp":1761218231345,"user_tz":-420,"elapsed":232751,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"c48c01f6-bb5c-4e88-8af3-8286fc14cdf4"},"execution_count":2,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["CASME II Multi-Frame Sequence ViT Training Pipeline\n","======================================================================\n","Loss Function: Optimized Focal Loss\n","Focal Loss Parameters:\n","  Gamma: 2.0\n","  Per-class Alpha: [0.048, 0.06, 0.085, 0.093, 0.095, 0.191, 0.427]\n","  Alpha Sum: 0.999\n","Dataset version: v9\n","Frame strategy: multi_frame_sequence\n","Training approach: frame_level_independent\n","ViT variant: PATCH16\n","Patch size: 16px\n","Training epochs: 50\n","Batch size: 16\n","Scheduler patience: 3\n","\n","Creating CASME II datasets with RAM caching optimization...\n","Loading CASME II train dataset for training...\n","Found 2613 image files in directory\n","Sample filename: sub13_EP01_01_apex_p+1_others.jpg\n","Loaded 2613 CASME II train samples\n","  others: 1027 samples (39.3%)\n","  disgust: 650 samples (24.9%)\n","  happiness: 325 samples (12.4%)\n","  repression: 273 samples (10.4%)\n","  surprise: 260 samples (10.0%)\n","  sadness: 65 samples (2.5%)\n","  fear: 13 samples (0.5%)\n","Preloading 2613 train images to RAM with 32 workers...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Loading train to RAM: 100%|██████████| 2613/2613 [00:54<00:00, 48.20it/s] \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["TRAIN RAM caching completed: 2613/2613 images, ~1.57GB\n","Loading CASME II val dataset for training...\n","Found 78 image files in directory\n","Sample filename: sub01_EP03_02_onset_others.jpg\n","Loaded 78 CASME II val samples\n","  others: 30 samples (38.5%)\n","  disgust: 18 samples (23.1%)\n","  happiness: 9 samples (11.5%)\n","  repression: 9 samples (11.5%)\n","  surprise: 6 samples (7.7%)\n","  sadness: 3 samples (3.8%)\n","  fear: 3 samples (3.8%)\n","Preloading 78 val images to RAM with 32 workers...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Loading val to RAM: 100%|██████████| 78/78 [00:03<00:00, 25.06it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["VAL RAM caching completed: 78/78 images, ~0.05GB\n","\n","Dataset loading complete:\n","  Train samples: 2613\n","  Validation samples: 78\n","Data loaders created:\n","  Train batches: 164\n","  Validation batches: 5\n","\n","Initializing ViT CASME II model...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["ViT feature dimension: 768\n","Classification head: 768 -> 512 -> 128 -> 7\n","Dropout rate: 0.3 (balanced for large dataset)\n","Model initialized on cuda\n","Scheduler: ReduceLROnPlateau monitoring validation F1\n","Using Optimized Focal Loss with gamma=2.0\n","Per-class alpha weights: [0.048, 0.06, 0.085, 0.093, 0.095, 0.191, 0.427]\n","Alpha sum: 0.999\n","\n","Training components initialized successfully\n","\n","======================================================================\n","Starting CASME II Multi-Frame Sequence ViT training...\n","======================================================================\n","\n","======================================================================\n","Epoch 1/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 1/50 [Train]: 100%|██████████| 164/164 [01:59<00:00,  1.37it/s, loss=0.0611, avg_loss=0.0646]\n","Epoch 1/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  2.96it/s, loss=0.0712, avg_loss=0.1407]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0646, F1: 0.3965, Acc: 0.4868\n","Val   - Loss: 0.1407, F1: 0.1669, Acc: 0.3077\n","Time  - Epoch: 121.1s, LR: 2.00e-05\n","New best model: Higher F1 - F1: 0.1669\n","Progress: 2.0% | Best F1: 0.1669 | ETA: 103.0min\n","\n","======================================================================\n","Epoch 2/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 2/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0119, avg_loss=0.0279]\n","Epoch 2/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.07it/s, loss=0.0918, avg_loss=0.1680]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0279, F1: 0.7178, Acc: 0.7631\n","Val   - Loss: 0.1680, F1: 0.1313, Acc: 0.2949\n","Time  - Epoch: 123.7s, LR: 2.00e-05\n","Progress: 4.0% | Best F1: 0.1669 | ETA: 99.9min\n","\n","======================================================================\n","Epoch 3/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 3/50 [Train]: 100%|██████████| 164/164 [02:01<00:00,  1.34it/s, loss=0.0075, avg_loss=0.0116]\n","Epoch 3/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s, loss=0.1247, avg_loss=0.1896]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0116, F1: 0.9197, Acc: 0.9116\n","Val   - Loss: 0.1896, F1: 0.1722, Acc: 0.3333\n","Time  - Epoch: 123.6s, LR: 2.00e-05\n","New best model: Higher F1 - F1: 0.1722\n","Progress: 6.0% | Best F1: 0.1722 | ETA: 98.1min\n","\n","======================================================================\n","Epoch 4/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 4/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0020, avg_loss=0.0049]\n","Epoch 4/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.12it/s, loss=0.1197, avg_loss=0.1956]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0049, F1: 0.9788, Acc: 0.9728\n","Val   - Loss: 0.1956, F1: 0.1564, Acc: 0.3205\n","Time  - Epoch: 123.7s, LR: 2.00e-05\n","Progress: 8.0% | Best F1: 0.1722 | ETA: 95.7min\n","\n","======================================================================\n","Epoch 5/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 5/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0008, avg_loss=0.0022]\n","Epoch 5/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.10it/s, loss=0.1318, avg_loss=0.2062]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0022, F1: 0.9925, Acc: 0.9908\n","Val   - Loss: 0.2062, F1: 0.1803, Acc: 0.3462\n","Time  - Epoch: 123.8s, LR: 2.00e-05\n","New best model: Higher F1 - F1: 0.1803\n","Progress: 10.0% | Best F1: 0.1803 | ETA: 93.9min\n","\n","======================================================================\n","Epoch 6/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 6/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0008, avg_loss=0.0022]\n","Epoch 6/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.14it/s, loss=0.0990, avg_loss=0.1991]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0022, F1: 0.9748, Acc: 0.9881\n","Val   - Loss: 0.1991, F1: 0.1728, Acc: 0.3462\n","Time  - Epoch: 123.8s, LR: 2.00e-05\n","Progress: 12.0% | Best F1: 0.1803 | ETA: 91.6min\n","\n","======================================================================\n","Epoch 7/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 7/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0012, avg_loss=0.0018]\n","Epoch 7/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.18it/s, loss=0.1282, avg_loss=0.2187]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0018, F1: 0.9922, Acc: 0.9908\n","Val   - Loss: 0.2187, F1: 0.1218, Acc: 0.3205\n","Time  - Epoch: 123.8s, LR: 2.00e-05\n","Progress: 14.0% | Best F1: 0.1803 | ETA: 89.4min\n","\n","======================================================================\n","Epoch 8/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 8/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0194, avg_loss=0.0016]\n","Epoch 8/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.13it/s, loss=0.1357, avg_loss=0.2187]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0016, F1: 0.9931, Acc: 0.9916\n","Val   - Loss: 0.2187, F1: 0.1453, Acc: 0.3846\n","Time  - Epoch: 123.8s, LR: 2.00e-05\n","Progress: 16.0% | Best F1: 0.1803 | ETA: 87.2min\n","\n","======================================================================\n","Epoch 9/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 9/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0018, avg_loss=0.0036]\n","Epoch 9/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.16it/s, loss=0.1371, avg_loss=0.2124]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0036, F1: 0.9755, Acc: 0.9721\n","Val   - Loss: 0.2124, F1: 0.1745, Acc: 0.3462\n","Time  - Epoch: 123.8s, LR: 1.00e-05\n","Progress: 18.0% | Best F1: 0.1803 | ETA: 85.1min\n","\n","======================================================================\n","Epoch 10/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 10/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0003, avg_loss=0.0009]\n","Epoch 10/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.09it/s, loss=0.1161, avg_loss=0.2190]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0009, F1: 0.9981, Acc: 0.9969\n","Val   - Loss: 0.2190, F1: 0.1809, Acc: 0.3333\n","Time  - Epoch: 123.8s, LR: 1.00e-05\n","New best model: Higher F1 - F1: 0.1809\n","Progress: 20.0% | Best F1: 0.1809 | ETA: 83.1min\n","\n","======================================================================\n","Epoch 11/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 11/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0002, avg_loss=0.0006]\n","Epoch 11/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.14it/s, loss=0.1200, avg_loss=0.2138]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0006, F1: 0.9995, Acc: 0.9992\n","Val   - Loss: 0.2138, F1: 0.2273, Acc: 0.3974\n","Time  - Epoch: 123.7s, LR: 1.00e-05\n","New best model: Higher F1 - F1: 0.2273\n","Progress: 22.0% | Best F1: 0.2273 | ETA: 81.3min\n","\n","======================================================================\n","Epoch 12/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 12/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0004, avg_loss=0.0004]\n","Epoch 12/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.14it/s, loss=0.1242, avg_loss=0.2182]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2182, F1: 0.2210, Acc: 0.3846\n","Time  - Epoch: 123.9s, LR: 1.00e-05\n","Progress: 24.0% | Best F1: 0.2273 | ETA: 79.1min\n","\n","======================================================================\n","Epoch 13/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 13/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0003, avg_loss=0.0004]\n","Epoch 13/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s, loss=0.1224, avg_loss=0.2197]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2197, F1: 0.1913, Acc: 0.3718\n","Time  - Epoch: 123.8s, LR: 1.00e-05\n","Progress: 26.0% | Best F1: 0.2273 | ETA: 77.0min\n","\n","======================================================================\n","Epoch 14/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 14/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0005, avg_loss=0.0004]\n","Epoch 14/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.14it/s, loss=0.1263, avg_loss=0.2219]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2219, F1: 0.1851, Acc: 0.3590\n","Time  - Epoch: 123.6s, LR: 1.00e-05\n","Progress: 28.0% | Best F1: 0.2273 | ETA: 74.9min\n","\n","======================================================================\n","Epoch 15/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 15/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0003]\n","Epoch 15/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.09it/s, loss=0.1369, avg_loss=0.2270]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2270, F1: 0.1660, Acc: 0.3462\n","Time  - Epoch: 123.7s, LR: 5.00e-06\n","Progress: 30.0% | Best F1: 0.2273 | ETA: 72.8min\n","\n","======================================================================\n","Epoch 16/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 16/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0003]\n","Epoch 16/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s, loss=0.1357, avg_loss=0.2269]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2269, F1: 0.1800, Acc: 0.3462\n","Time  - Epoch: 123.8s, LR: 5.00e-06\n","Progress: 32.0% | Best F1: 0.2273 | ETA: 70.6min\n","\n","======================================================================\n","Epoch 17/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 17/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0003]\n","Epoch 17/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.10it/s, loss=0.1362, avg_loss=0.2276]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2276, F1: 0.1800, Acc: 0.3462\n","Time  - Epoch: 123.8s, LR: 5.00e-06\n","Progress: 34.0% | Best F1: 0.2273 | ETA: 68.5min\n","\n","======================================================================\n","Epoch 18/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 18/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0003]\n","Epoch 18/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s, loss=0.1352, avg_loss=0.2280]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2280, F1: 0.1800, Acc: 0.3462\n","Time  - Epoch: 123.7s, LR: 5.00e-06\n","Progress: 36.0% | Best F1: 0.2273 | ETA: 66.4min\n","\n","======================================================================\n","Epoch 19/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 19/50 [Train]: 100%|██████████| 164/164 [02:01<00:00,  1.34it/s, loss=0.0000, avg_loss=0.0003]\n","Epoch 19/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s, loss=0.1358, avg_loss=0.2289]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2289, F1: 0.1862, Acc: 0.3590\n","Time  - Epoch: 123.6s, LR: 2.50e-06\n","Progress: 38.0% | Best F1: 0.2273 | ETA: 64.3min\n","\n","======================================================================\n","Epoch 20/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 20/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0002, avg_loss=0.0003]\n","Epoch 20/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.15it/s, loss=0.1386, avg_loss=0.2300]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2300, F1: 0.1871, Acc: 0.3590\n","Time  - Epoch: 123.8s, LR: 2.50e-06\n","Progress: 40.0% | Best F1: 0.2273 | ETA: 62.2min\n","\n","======================================================================\n","Epoch 21/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 21/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0002, avg_loss=0.0003]\n","Epoch 21/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.12it/s, loss=0.1397, avg_loss=0.2307]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2307, F1: 0.1933, Acc: 0.3718\n","Time  - Epoch: 123.7s, LR: 2.50e-06\n","Progress: 42.0% | Best F1: 0.2273 | ETA: 60.1min\n","\n","======================================================================\n","Epoch 22/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 22/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0002, avg_loss=0.0003]\n","Epoch 22/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.09it/s, loss=0.1357, avg_loss=0.2296]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2296, F1: 0.1983, Acc: 0.3846\n","Time  - Epoch: 123.8s, LR: 2.50e-06\n","Progress: 44.0% | Best F1: 0.2273 | ETA: 58.1min\n","\n","======================================================================\n","Epoch 23/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 23/50 [Train]: 100%|██████████| 164/164 [02:01<00:00,  1.35it/s, loss=0.0007, avg_loss=0.0003]\n","Epoch 23/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s, loss=0.1415, avg_loss=0.2313]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2313, F1: 0.1933, Acc: 0.3718\n","Time  - Epoch: 123.5s, LR: 1.25e-06\n","Progress: 46.0% | Best F1: 0.2273 | ETA: 56.0min\n","\n","======================================================================\n","Epoch 24/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 24/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 24/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s, loss=0.1428, avg_loss=0.2318]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2318, F1: 0.1933, Acc: 0.3718\n","Time  - Epoch: 123.8s, LR: 1.25e-06\n","Progress: 48.0% | Best F1: 0.2273 | ETA: 53.9min\n","\n","======================================================================\n","Epoch 25/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 25/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0002, avg_loss=0.0002]\n","Epoch 25/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s, loss=0.1443, avg_loss=0.2327]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2327, F1: 0.1933, Acc: 0.3718\n","Time  - Epoch: 123.7s, LR: 1.25e-06\n","Progress: 50.0% | Best F1: 0.2273 | ETA: 51.8min\n","\n","======================================================================\n","Epoch 26/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 26/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 26/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.14it/s, loss=0.1434, avg_loss=0.2331]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2331, F1: 0.1933, Acc: 0.3718\n","Time  - Epoch: 123.8s, LR: 1.25e-06\n","Progress: 52.0% | Best F1: 0.2273 | ETA: 49.7min\n","\n","======================================================================\n","Epoch 27/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 27/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0002, avg_loss=0.0002]\n","Epoch 27/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.18it/s, loss=0.1432, avg_loss=0.2335]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2335, F1: 0.1933, Acc: 0.3718\n","Time  - Epoch: 123.8s, LR: 6.25e-07\n","Progress: 54.0% | Best F1: 0.2273 | ETA: 47.6min\n","\n","======================================================================\n","Epoch 28/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 28/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0002, avg_loss=0.0002]\n","Epoch 28/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.16it/s, loss=0.1416, avg_loss=0.2331]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2331, F1: 0.1933, Acc: 0.3718\n","Time  - Epoch: 123.7s, LR: 6.25e-07\n","Progress: 56.0% | Best F1: 0.2273 | ETA: 45.6min\n","\n","======================================================================\n","Epoch 29/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 29/50 [Train]: 100%|██████████| 164/164 [02:01<00:00,  1.34it/s, loss=0.0000, avg_loss=0.0002]\n","Epoch 29/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.10it/s, loss=0.1391, avg_loss=0.2322]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2322, F1: 0.1990, Acc: 0.3846\n","Time  - Epoch: 123.6s, LR: 6.25e-07\n","Progress: 58.0% | Best F1: 0.2273 | ETA: 43.5min\n","\n","======================================================================\n","Epoch 30/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 30/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0002, avg_loss=0.0002]\n","Epoch 30/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.13it/s, loss=0.1398, avg_loss=0.2325]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2325, F1: 0.1939, Acc: 0.3718\n","Time  - Epoch: 123.8s, LR: 6.25e-07\n","Progress: 60.0% | Best F1: 0.2273 | ETA: 41.4min\n","\n","======================================================================\n","Epoch 31/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 31/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0000, avg_loss=0.0002]\n","Epoch 31/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.15it/s, loss=0.1424, avg_loss=0.2334]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2334, F1: 0.1939, Acc: 0.3718\n","Time  - Epoch: 123.7s, LR: 3.13e-07\n","Progress: 62.0% | Best F1: 0.2273 | ETA: 39.3min\n","\n","======================================================================\n","Epoch 32/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 32/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 32/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.15it/s, loss=0.1434, avg_loss=0.2337]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2337, F1: 0.1933, Acc: 0.3718\n","Time  - Epoch: 123.7s, LR: 3.13e-07\n","Progress: 64.0% | Best F1: 0.2273 | ETA: 37.3min\n","\n","======================================================================\n","Epoch 33/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 33/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0002, avg_loss=0.0002]\n","Epoch 33/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.14it/s, loss=0.1446, avg_loss=0.2342]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2342, F1: 0.1933, Acc: 0.3718\n","Time  - Epoch: 123.7s, LR: 3.13e-07\n","Progress: 66.0% | Best F1: 0.2273 | ETA: 35.2min\n","\n","======================================================================\n","Epoch 34/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 34/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 34/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s, loss=0.1455, avg_loss=0.2346]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2346, F1: 0.1722, Acc: 0.3590\n","Time  - Epoch: 123.7s, LR: 3.13e-07\n","Progress: 68.0% | Best F1: 0.2273 | ETA: 33.1min\n","\n","======================================================================\n","Epoch 35/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 35/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 35/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.06it/s, loss=0.1457, avg_loss=0.2347]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2347, F1: 0.1722, Acc: 0.3590\n","Time  - Epoch: 123.6s, LR: 1.56e-07\n","Progress: 70.0% | Best F1: 0.2273 | ETA: 31.0min\n","\n","======================================================================\n","Epoch 36/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 36/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 36/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.08it/s, loss=0.1459, avg_loss=0.2349]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2349, F1: 0.1722, Acc: 0.3590\n","Time  - Epoch: 123.8s, LR: 1.56e-07\n","Progress: 72.0% | Best F1: 0.2273 | ETA: 29.0min\n","\n","======================================================================\n","Epoch 37/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 37/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0000, avg_loss=0.0002]\n","Epoch 37/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.18it/s, loss=0.1462, avg_loss=0.2350]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2350, F1: 0.1722, Acc: 0.3590\n","Time  - Epoch: 123.7s, LR: 1.56e-07\n","Progress: 74.0% | Best F1: 0.2273 | ETA: 26.9min\n","\n","======================================================================\n","Epoch 38/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 38/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0002, avg_loss=0.0002]\n","Epoch 38/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.08it/s, loss=0.1471, avg_loss=0.2354]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2354, F1: 0.1722, Acc: 0.3590\n","Time  - Epoch: 123.9s, LR: 1.56e-07\n","Progress: 76.0% | Best F1: 0.2273 | ETA: 24.8min\n","\n","======================================================================\n","Epoch 39/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 39/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 39/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.13it/s, loss=0.1455, avg_loss=0.2349]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2349, F1: 0.1722, Acc: 0.3590\n","Time  - Epoch: 123.8s, LR: 1.00e-07\n","Progress: 78.0% | Best F1: 0.2273 | ETA: 22.8min\n","\n","======================================================================\n","Epoch 40/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 40/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 40/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.14it/s, loss=0.1458, avg_loss=0.2352]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2352, F1: 0.1722, Acc: 0.3590\n","Time  - Epoch: 123.7s, LR: 1.00e-07\n","Progress: 80.0% | Best F1: 0.2273 | ETA: 20.7min\n","\n","======================================================================\n","Epoch 41/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 41/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0005, avg_loss=0.0002]\n","Epoch 41/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.10it/s, loss=0.1456, avg_loss=0.2351]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2351, F1: 0.1722, Acc: 0.3590\n","Time  - Epoch: 123.7s, LR: 1.00e-07\n","Progress: 82.0% | Best F1: 0.2273 | ETA: 18.6min\n","\n","======================================================================\n","Epoch 42/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 42/50 [Train]: 100%|██████████| 164/164 [02:01<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 42/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.12it/s, loss=0.1457, avg_loss=0.2352]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2352, F1: 0.1722, Acc: 0.3590\n","Time  - Epoch: 123.6s, LR: 1.00e-07\n","Progress: 84.0% | Best F1: 0.2273 | ETA: 16.5min\n","\n","======================================================================\n","Epoch 43/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 43/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 43/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s, loss=0.1453, avg_loss=0.2352]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2352, F1: 0.1722, Acc: 0.3590\n","Time  - Epoch: 123.7s, LR: 1.00e-07\n","Progress: 86.0% | Best F1: 0.2273 | ETA: 14.5min\n","\n","======================================================================\n","Epoch 44/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 44/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 44/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.18it/s, loss=0.1442, avg_loss=0.2348]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2348, F1: 0.1939, Acc: 0.3718\n","Time  - Epoch: 123.7s, LR: 1.00e-07\n","Progress: 88.0% | Best F1: 0.2273 | ETA: 12.4min\n","\n","======================================================================\n","Epoch 45/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 45/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0006, avg_loss=0.0002]\n","Epoch 45/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.13it/s, loss=0.1441, avg_loss=0.2348]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2348, F1: 0.1939, Acc: 0.3718\n","Time  - Epoch: 123.7s, LR: 1.00e-07\n","Progress: 90.0% | Best F1: 0.2273 | ETA: 10.3min\n","\n","======================================================================\n","Epoch 46/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 46/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 46/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.15it/s, loss=0.1439, avg_loss=0.2348]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2348, F1: 0.1939, Acc: 0.3718\n","Time  - Epoch: 123.6s, LR: 1.00e-07\n","Progress: 92.0% | Best F1: 0.2273 | ETA: 8.3min\n","\n","======================================================================\n","Epoch 47/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 47/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 47/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.08it/s, loss=0.1438, avg_loss=0.2349]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2349, F1: 0.1939, Acc: 0.3718\n","Time  - Epoch: 123.7s, LR: 1.00e-07\n","Progress: 94.0% | Best F1: 0.2273 | ETA: 6.2min\n","\n","======================================================================\n","Epoch 48/50\n","======================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 48/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0001, avg_loss=0.0002]\n","Epoch 48/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.14it/s, loss=0.1432, avg_loss=0.2348]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2348, F1: 0.1722, Acc: 0.3590\n","Time  - Epoch: 123.7s, LR: 1.00e-07\n","Progress: 96.0% | Best F1: 0.2273 | ETA: 4.1min\n","\n","======================================================================\n","Epoch 49/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 49/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0002, avg_loss=0.0002]\n","Epoch 49/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.10it/s, loss=0.1431, avg_loss=0.2349]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2349, F1: 0.1939, Acc: 0.3718\n","Time  - Epoch: 123.8s, LR: 1.00e-07\n","Progress: 98.0% | Best F1: 0.2273 | ETA: 2.1min\n","\n","======================================================================\n","Epoch 50/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 50/50 [Train]: 100%|██████████| 164/164 [02:02<00:00,  1.34it/s, loss=0.0002, avg_loss=0.0002]\n","Epoch 50/50 [Val]: 100%|██████████| 5/5 [00:01<00:00,  3.13it/s, loss=0.1451, avg_loss=0.2354]"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2354, F1: 0.1722, Acc: 0.3590\n","Time  - Epoch: 123.7s, LR: 1.00e-07\n","Progress: 100.0% | Best F1: 0.2273 | ETA: 0.0min\n","\n","======================================================================\n","CASME II MULTI-FRAME SEQUENCE ViT TRAINING COMPLETED\n","======================================================================\n","Training time: 103.3 minutes\n","Epochs completed: 50\n","Best validation F1: 0.2273 (epoch 11)\n","Final train F1: 1.0000\n","Final validation F1: 0.1722\n","\n","Exporting enhanced training documentation...\n","Enhanced training documentation saved successfully: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/07_01_vit_casme2_mfs_prep/training_logs/casme2_vit_mfs_training_history.json\n","Experiment details: Optimized Focal Loss loss\n","  Gamma: 2.0, Alpha Sum: 0.999\n","Model variant: PATCH16\n","Patch size: 16px\n","Dataset version: v9\n","\n","Next: Cell 3 - CASME II Multi-Frame Sequence ViT Evaluation\n","Enhanced training pipeline completed successfully!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title Cell 3: CASME II ViT Evaluation with Dual Dataset Support\n","\n","# File: 07_01_ViT_CASME2_MFS_Cell3.py\n","# Location: experiments/07_01_ViT_CASME2-MFS-PREP.ipynb\n","# Purpose: Comprehensive evaluation framework with support for AF (v7) and KFS (v8) test datasets\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from datetime import datetime\n","from collections import defaultdict\n","\n","from sklearn.metrics import (\n","    accuracy_score, precision_recall_fscore_support,\n","    classification_report, confusion_matrix,\n","    roc_curve, auc\n",")\n","from sklearn.preprocessing import label_binarize\n","from concurrent.futures import ThreadPoolExecutor\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =====================================================\n","# DUAL DATASET EVALUATION CONFIGURATION\n","# =====================================================\n","# Configure which test datasets to evaluate:\n","# 'v7' = Apex Frame preprocessing (28 samples, frame-level evaluation)\n","# 'v8' = Key Frame Sequence preprocessing (84 frames -> 28 videos with late fusion)\n","\n","EVALUATE_DATASETS = ['v7', 'v8']  # Can be ['v7'], ['v8'], or ['v7', 'v8']\n","\n","print(\"CASME II ViT Evaluation Framework with Dual Dataset Support\")\n","print(\"=\" * 60)\n","print(f\"Datasets to evaluate: {EVALUATE_DATASETS}\")\n","print(\"=\" * 60)\n","\n","# =====================================================\n","# DATASET CONFIGURATION FUNCTION\n","# =====================================================\n","\n","def get_test_dataset_config(version, project_root):\n","    \"\"\"\n","    Get test dataset configuration based on version\n","\n","    Args:\n","        version: 'v7' (AF) or 'v8' (KFS)\n","        project_root: Project root path\n","\n","    Returns:\n","        dict: Configuration for selected test dataset\n","    \"\"\"\n","    if version == 'v7':\n","        config = {\n","            'version': 'v7',\n","            'variant': 'AF',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/preprocessed_v7\",\n","            'preprocessing_summary': 'preprocessing_summary.json',\n","            'description': 'Apex Frame with Face-Aware Preprocessing',\n","            'expected_samples': 28,\n","            'frame_strategy': 'apex_frame',\n","            'evaluation_mode': 'frame_level',\n","            'aggregation': None\n","        }\n","    elif version == 'v8':\n","        config = {\n","            'version': 'v8',\n","            'variant': 'KFS',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/preprocessed_v8\",\n","            'preprocessing_summary': 'preprocessing_summary.json',\n","            'description': 'Key Frame Sequence with Face-Aware Preprocessing',\n","            'expected_frames': 84,\n","            'expected_videos': 28,\n","            'frame_strategy': 'key_frame_sequence',\n","            'frame_types': ['onset', 'apex', 'offset'],\n","            'evaluation_mode': 'video_level',\n","            'aggregation': 'late_fusion'\n","        }\n","    else:\n","        raise ValueError(f\"Invalid version: {version}. Must be 'v7' or 'v8'\")\n","\n","    return config\n","\n","# =====================================================\n","# VIDEO ID EXTRACTION FOR KFS LATE FUSION\n","# =====================================================\n","\n","def extract_video_id_from_filename(filename):\n","    \"\"\"\n","    Extract video ID from KFS filename by removing frame type suffix\n","\n","    Expected format: sub01_EP02_01f_happiness_onset.jpg\n","    Video ID: sub01_EP02_01f_happiness\n","\n","    Args:\n","        filename: Image filename with frame type\n","\n","    Returns:\n","        str: Video ID without frame type\n","    \"\"\"\n","    # Remove file extension\n","    name_without_ext = filename.rsplit('.', 1)[0]\n","\n","    # Remove frame type suffix (onset, apex, offset)\n","    for frame_type in ['onset', 'apex', 'offset']:\n","        if name_without_ext.endswith(f'_{frame_type}'):\n","            video_id = name_without_ext.rsplit(f'_{frame_type}', 1)[0]\n","            return video_id\n","\n","    # If no frame type found, return as is\n","    return name_without_ext\n","\n","# =====================================================\n","# ENHANCED TEST DATASET CLASS\n","# =====================================================\n","\n","class CASME2DatasetEvaluation(Dataset):\n","    \"\"\"Enhanced CASME II test dataset with comprehensive evaluation support\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","        self.emotions = []\n","        self.video_ids = []\n","        self.cached_images = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading CASME II {split} dataset for evaluation...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        loaded_count = 0\n","\n","        for filename in sorted(all_files):\n","            emotion_found = None\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                video_id = extract_video_id_from_filename(filename)\n","\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                self.emotions.append(emotion_found)\n","                self.video_ids.append(video_id)\n","                loaded_count += 1\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples for evaluation\")\n","        self._print_evaluation_distribution()\n","\n","        if self.use_ram_cache and len(self.images) > 0:\n","            self._preload_to_ram_evaluation()\n","\n","    def _print_evaluation_distribution(self):\n","        \"\"\"Print comprehensive distribution for evaluation analysis\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"No test samples found!\")\n","            return\n","\n","        label_counts = {}\n","        unique_videos = set(self.video_ids)\n","\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        print(\"Test set class distribution:\")\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","        print(f\"Unique video IDs: {len(unique_videos)}\")\n","\n","        missing_classes = []\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i not in label_counts:\n","                missing_classes.append(class_name)\n","\n","        if missing_classes:\n","            print(f\"Missing classes in test set: {missing_classes}\")\n","\n","    def _preload_to_ram_evaluation(self):\n","        \"\"\"RAM preloading optimized for evaluation\"\"\"\n","        if len(self.images) == 0:\n","            return\n","\n","        print(f\"Preloading {len(self.images)} test images to RAM...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","\n","        def load_single_image(idx, img_path):\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except:\n","                return idx, Image.new('RGB', (224, 224), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=\"Loading test to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","\n","        print(f\"RAM caching completed: {len(self.cached_images)} test images\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (224, 224), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx], self.video_ids[idx]\n","\n","# =====================================================\n","# MODEL LOADING FUNCTION\n","# =====================================================\n","\n","def load_trained_model_casme2(checkpoint_path, device):\n","    \"\"\"Load trained ViT model from checkpoint\"\"\"\n","    print(f\"Loading trained model from: {checkpoint_path}\")\n","\n","    if not os.path.exists(checkpoint_path):\n","        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n","\n","    checkpoint = torch.load(checkpoint_path, map_location=device)\n","\n","    model = ViTCASME2Baseline(\n","        num_classes=7,\n","        dropout_rate=checkpoint['config']['dropout_rate']\n","    ).to(device)\n","\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","\n","    print(f\"Model loaded successfully from epoch {checkpoint['epoch']}\")\n","\n","    training_info = {\n","        'best_epoch': checkpoint['epoch'],\n","        'best_val_f1': checkpoint['val_metrics']['f1_score'],\n","        'best_val_accuracy': checkpoint['val_metrics']['accuracy'],\n","        'config': checkpoint['config']\n","    }\n","\n","    return model, training_info\n","\n","# =====================================================\n","# FRAME-LEVEL INFERENCE (AF v7)\n","# =====================================================\n","\n","def run_frame_level_inference(model, test_loader, device):\n","    \"\"\"Run frame-level inference for AF evaluation\"\"\"\n","    print(\"Running frame-level inference...\")\n","\n","    model.eval()\n","    all_predictions = []\n","    all_labels = []\n","    all_filenames = []\n","    all_probabilities = []\n","\n","    start_time = time.time()\n","\n","    with torch.no_grad():\n","        for images, labels, filenames, video_ids in tqdm(test_loader, desc=\"Frame-level inference\"):\n","            images = images.to(device)\n","\n","            outputs = model(images)\n","            probabilities = F.softmax(outputs, dim=1)\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","            all_filenames.extend(filenames)\n","            all_probabilities.extend(probabilities.cpu().numpy())\n","\n","    inference_time = time.time() - start_time\n","\n","    return {\n","        'predictions': all_predictions,\n","        'labels': all_labels,\n","        'filenames': all_filenames,\n","        'probabilities': all_probabilities,\n","        'inference_time': inference_time,\n","        'evaluation_mode': 'frame_level'\n","    }\n","\n","# =====================================================\n","# VIDEO-LEVEL INFERENCE WITH LATE FUSION (KFS v8)\n","# =====================================================\n","\n","def run_video_level_inference_late_fusion(model, test_loader, device):\n","    \"\"\"Run video-level inference with late fusion for KFS evaluation\"\"\"\n","    print(\"Running video-level inference with late fusion...\")\n","\n","    model.eval()\n","\n","    # Collect frame-level predictions\n","    frame_predictions = []\n","    frame_labels = []\n","    frame_filenames = []\n","    frame_video_ids = []\n","    frame_probabilities = []\n","\n","    start_time = time.time()\n","\n","    with torch.no_grad():\n","        for images, labels, filenames, video_ids in tqdm(test_loader, desc=\"Frame inference\"):\n","            images = images.to(device)\n","\n","            outputs = model(images)\n","            probabilities = F.softmax(outputs, dim=1)\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            frame_predictions.extend(predicted.cpu().numpy())\n","            frame_labels.extend(labels.numpy())\n","            frame_filenames.extend(filenames)\n","            frame_video_ids.extend(video_ids)\n","            frame_probabilities.extend(probabilities.cpu().numpy())\n","\n","    # Aggregate to video-level using late fusion\n","    print(\"Aggregating frame predictions to video level...\")\n","\n","    video_data = {}\n","    for pred, label, filename, video_id, prob in zip(\n","        frame_predictions, frame_labels, frame_filenames, frame_video_ids, frame_probabilities\n","    ):\n","        if video_id not in video_data:\n","            video_data[video_id] = {\n","                'predictions': [],\n","                'probabilities': [],\n","                'label': label,\n","                'filenames': []\n","            }\n","\n","        video_data[video_id]['predictions'].append(pred)\n","        video_data[video_id]['probabilities'].append(prob)\n","        video_data[video_id]['filenames'].append(filename)\n","\n","    # Late fusion: average probabilities across frames\n","    video_predictions = []\n","    video_labels = []\n","    video_ids_list = []\n","\n","    for video_id, data in video_data.items():\n","        avg_prob = np.mean(data['probabilities'], axis=0)\n","        final_pred = np.argmax(avg_prob)\n","\n","        video_predictions.append(final_pred)\n","        video_labels.append(data['label'])\n","        video_ids_list.append(video_id)\n","\n","    inference_time = time.time() - start_time\n","\n","    return {\n","        'predictions': video_predictions,\n","        'labels': video_labels,\n","        'video_ids': video_ids_list,\n","        'inference_time': inference_time,\n","        'evaluation_mode': 'video_level',\n","        'kfs_late_fusion_info': {\n","            'total_frames': len(frame_predictions),\n","            'total_videos': len(video_predictions),\n","            'aggregation_method': 'average_probability'\n","        }\n","    }\n","\n","# =====================================================\n","# COMPREHENSIVE METRICS CALCULATION\n","# =====================================================\n","\n","def calculate_comprehensive_metrics(inference_results):\n","    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n","\n","    predictions = np.array(inference_results['predictions'])\n","    labels = np.array(inference_results['labels'])\n","\n","    # Overall metrics\n","    accuracy = accuracy_score(labels, predictions)\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions, average='macro', zero_division=0\n","    )\n","\n","    # Per-class metrics\n","    per_class_precision, per_class_recall, per_class_f1, per_class_support = \\\n","        precision_recall_fscore_support(labels, predictions, average=None, zero_division=0)\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(labels, predictions)\n","\n","    # AUC calculation (if probabilities available)\n","    try:\n","        if 'probabilities' in inference_results:\n","            labels_bin = label_binarize(labels, classes=range(len(CASME2_CLASSES)))\n","            auc_scores = []\n","\n","            for i in range(len(CASME2_CLASSES)):\n","                if labels_bin[:, i].sum() > 0:\n","                    fpr, tpr, _ = roc_curve(labels_bin[:, i],\n","                                           np.array(inference_results['probabilities'])[:, i])\n","                    auc_scores.append(auc(fpr, tpr))\n","                else:\n","                    auc_scores.append(0.0)\n","\n","            macro_auc = np.mean([score for score in auc_scores if score > 0])\n","        else:\n","            auc_scores = [0.0] * len(CASME2_CLASSES)\n","            macro_auc = 0.0\n","    except:\n","        auc_scores = [0.0] * len(CASME2_CLASSES)\n","        macro_auc = 0.0\n","\n","    # Check which classes are in test set\n","    unique_labels = set(labels)\n","    available_classes = [CASME2_CLASSES[i] for i in unique_labels]\n","    missing_classes = [cls for i, cls in enumerate(CASME2_CLASSES) if i not in unique_labels]\n","\n","    # Build per-class performance dict\n","    per_class_performance = {}\n","    for i, class_name in enumerate(CASME2_CLASSES):\n","        per_class_performance[class_name] = {\n","            'precision': float(per_class_precision[i]) if i < len(per_class_precision) else 0.0,\n","            'recall': float(per_class_recall[i]) if i < len(per_class_recall) else 0.0,\n","            'f1_score': float(per_class_f1[i]) if i < len(per_class_f1) else 0.0,\n","            'support': int(per_class_support[i]) if i < len(per_class_support) else 0,\n","            'auc': float(auc_scores[i]) if i < len(auc_scores) else 0.0,\n","            'in_test_set': i in unique_labels\n","        }\n","\n","    # Inference performance\n","    inference_performance = {\n","        'total_time_seconds': inference_results['inference_time'],\n","        'average_time_ms_per_sample': (inference_results['inference_time'] / len(predictions)) * 1000\n","    }\n","\n","    results = {\n","        'evaluation_metadata': {\n","            'dataset': 'CASME_II',\n","            'model_type': 'ViTCASME2Baseline',\n","            'evaluation_timestamp': datetime.now().isoformat(),\n","            'evaluation_mode': inference_results['evaluation_mode'],\n","            'test_samples': len(predictions),\n","            'class_names': CASME2_CLASSES,\n","            'available_classes': available_classes,\n","            'missing_classes': missing_classes\n","        },\n","        'overall_performance': {\n","            'accuracy': float(accuracy),\n","            'macro_precision': float(precision),\n","            'macro_recall': float(recall),\n","            'macro_f1': float(f1),\n","            'macro_auc': float(macro_auc)\n","        },\n","        'per_class_performance': per_class_performance,\n","        'confusion_matrix': cm.tolist(),\n","        'inference_performance': inference_performance\n","    }\n","\n","    # Add KFS late fusion info if available\n","    if 'kfs_late_fusion_info' in inference_results:\n","        results['kfs_late_fusion_info'] = inference_results['kfs_late_fusion_info']\n","\n","    return results\n","\n","# =====================================================\n","# WRONG PREDICTIONS ANALYSIS\n","# =====================================================\n","\n","def analyze_wrong_predictions(inference_results):\n","    \"\"\"Analyze wrong predictions for error patterns\"\"\"\n","\n","    predictions = np.array(inference_results['predictions'])\n","    labels = np.array(inference_results['labels'])\n","\n","    if 'filenames' in inference_results:\n","        filenames = inference_results['filenames']\n","    elif 'video_ids' in inference_results:\n","        filenames = inference_results['video_ids']\n","    else:\n","        filenames = [f\"sample_{i}\" for i in range(len(predictions))]\n","\n","    wrong_predictions = []\n","    wrong_by_class = defaultdict(int)\n","    confusion_patterns = defaultdict(int)\n","\n","    for i, (pred, true_label) in enumerate(zip(predictions, labels)):\n","        if pred != true_label:\n","            true_class = CASME2_CLASSES[true_label]\n","            pred_class = CASME2_CLASSES[pred]\n","\n","            wrong_predictions.append({\n","                'filename': filenames[i],\n","                'true_label': int(true_label),\n","                'true_class': true_class,\n","                'predicted_label': int(pred),\n","                'predicted_class': pred_class\n","            })\n","\n","            wrong_by_class[true_class] += 1\n","            confusion_patterns[f\"{true_class}->{pred_class}\"] += 1\n","\n","    error_summary = {}\n","    for class_name in CASME2_CLASSES:\n","        class_idx = CLASS_TO_IDX[class_name]\n","        class_mask = labels == class_idx\n","        class_total = class_mask.sum()\n","\n","        if class_total > 0:\n","            class_wrong = wrong_by_class.get(class_name, 0)\n","            error_rate = (class_wrong / class_total) * 100\n","        else:\n","            class_wrong = 0\n","            error_rate = 0.0\n","\n","        error_summary[class_name] = {\n","            'total_samples': int(class_total),\n","            'wrong_predictions': int(class_wrong),\n","            'error_rate_percent': float(error_rate)\n","        }\n","\n","    results = {\n","        'analysis_metadata': {\n","            'total_samples': len(predictions),\n","            'total_wrong_predictions': len(wrong_predictions),\n","            'overall_error_rate': (len(wrong_predictions) / len(predictions) * 100) if len(predictions) > 0 else 0.0\n","        },\n","        'wrong_predictions': wrong_predictions,\n","        'wrong_predictions_by_class': dict(wrong_by_class),\n","        'error_summary': error_summary,\n","        'confusion_patterns': dict(confusion_patterns)\n","    }\n","\n","    return results\n","\n","# =====================================================\n","# SAVE EVALUATION RESULTS\n","# =====================================================\n","\n","def save_evaluation_results(evaluation_results, wrong_predictions_results, results_dir, test_version):\n","    \"\"\"Save comprehensive evaluation results\"\"\"\n","    os.makedirs(results_dir, exist_ok=True)\n","\n","    results_file = f\"{results_dir}/casme2_vit_evaluation_results_{test_version}.json\"\n","    with open(results_file, 'w') as f:\n","        json.dump(evaluation_results, f, indent=2, default=str)\n","\n","    wrong_predictions_file = f\"{results_dir}/casme2_vit_wrong_predictions_{test_version}.json\"\n","    with open(wrong_predictions_file, 'w') as f:\n","        json.dump(wrong_predictions_results, f, indent=2, default=str)\n","\n","    print(f\"Evaluation results saved:\")\n","    print(f\"  Main results: {os.path.basename(results_file)}\")\n","    print(f\"  Wrong predictions: {os.path.basename(wrong_predictions_file)}\")\n","\n","    return results_file, wrong_predictions_file\n","\n","# =====================================================\n","# MAIN EVALUATION EXECUTION\n","# =====================================================\n","\n","all_evaluation_results = {}\n","\n","for dataset_version in EVALUATE_DATASETS:\n","    print(\"\\n\" + \"=\" * 70)\n","    print(f\"EVALUATING DATASET: {dataset_version.upper()}\")\n","    print(\"=\" * 70)\n","\n","    try:\n","        # Get dataset configuration\n","        test_config = get_test_dataset_config(dataset_version, PROJECT_ROOT)\n","\n","        print(f\"\\nTest Dataset Configuration:\")\n","        print(f\"  Version: {test_config['version']}\")\n","        print(f\"  Variant: {test_config['variant']}\")\n","        print(f\"  Description: {test_config['description']}\")\n","        print(f\"  Frame strategy: {test_config['frame_strategy']}\")\n","        print(f\"  Evaluation mode: {test_config['evaluation_mode']}\")\n","        if 'aggregation' in test_config and test_config['aggregation']:\n","            print(f\"  Aggregation: {test_config['aggregation']}\")\n","        print(f\"  Dataset path: {test_config['dataset_path']}\")\n","\n","        # Create test dataset\n","        print(f\"\\nCreating CASME II test dataset from {test_config['variant']}...\")\n","        test_dataset = CASME2DatasetEvaluation(\n","            dataset_root=test_config['dataset_path'],\n","            split='test',\n","            transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","            use_ram_cache=True\n","        )\n","\n","        if len(test_dataset) == 0:\n","            raise ValueError(f\"No test samples found for {dataset_version}!\")\n","\n","        test_loader = DataLoader(\n","            test_dataset,\n","            batch_size=CASME2_VIT_CONFIG['batch_size'],\n","            shuffle=False,\n","            num_workers=CASME2_VIT_CONFIG['num_workers'],\n","            pin_memory=True\n","        )\n","\n","        # Load trained model\n","        checkpoint_path = f\"{GLOBAL_CONFIG_CASME2['checkpoint_root']}/casme2_vit_mfs_best_f1.pth\"\n","        model, training_info = load_trained_model_casme2(checkpoint_path, GLOBAL_CONFIG_CASME2['device'])\n","\n","        # Run inference based on evaluation mode\n","        if test_config['evaluation_mode'] == 'frame_level':\n","            print(f\"\\nRunning frame-level evaluation for {test_config['variant']}...\")\n","            inference_results = run_frame_level_inference(model, test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","        elif test_config['evaluation_mode'] == 'video_level':\n","            print(f\"\\nRunning video-level evaluation with late fusion for {test_config['variant']}...\")\n","            inference_results = run_video_level_inference_late_fusion(model, test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","        else:\n","            raise ValueError(f\"Unknown evaluation mode: {test_config['evaluation_mode']}\")\n","\n","        # Calculate comprehensive metrics\n","        evaluation_results = calculate_comprehensive_metrics(inference_results)\n","\n","        # Analyze wrong predictions\n","        wrong_predictions_results = analyze_wrong_predictions(inference_results)\n","\n","        # Add training information\n","        evaluation_results['training_information'] = training_info\n","        evaluation_results['test_configuration'] = test_config\n","\n","        # Save results\n","        results_dir = f\"{GLOBAL_CONFIG_CASME2['results_root']}/evaluation_results\"\n","        save_evaluation_results(\n","            evaluation_results, wrong_predictions_results, results_dir, test_config['version']\n","        )\n","\n","        # Store for comparison\n","        all_evaluation_results[dataset_version] = {\n","            'evaluation': evaluation_results,\n","            'wrong_predictions': wrong_predictions_results,\n","            'config': test_config\n","        }\n","\n","        # Display results\n","        print(\"\\n\" + \"=\" * 60)\n","        print(f\"EVALUATION RESULTS - {test_config['variant']} ({dataset_version})\")\n","        print(\"=\" * 60)\n","\n","        overall = evaluation_results['overall_performance']\n","        print(f\"\\nOverall Performance:\")\n","        print(f\"  Accuracy:  {overall['accuracy']:.4f}\")\n","        print(f\"  Precision: {overall['macro_precision']:.4f}\")\n","        print(f\"  Recall:    {overall['macro_recall']:.4f}\")\n","        print(f\"  F1 Score:  {overall['macro_f1']:.4f}\")\n","        print(f\"  AUC:       {overall['macro_auc']:.4f}\")\n","\n","        if 'kfs_late_fusion_info' in evaluation_results:\n","            fusion_info = evaluation_results['kfs_late_fusion_info']\n","            print(f\"\\nLate Fusion Info:\")\n","            print(f\"  Total frames processed: {fusion_info['total_frames']}\")\n","            print(f\"  Video-level predictions: {fusion_info['total_videos']}\")\n","            print(f\"  Aggregation method: {fusion_info['aggregation_method']}\")\n","\n","        print(f\"\\nPer-Class Performance:\")\n","        for class_name, metrics in evaluation_results['per_class_performance'].items():\n","            in_test = \"Present\" if metrics['in_test_set'] else \"Missing\"\n","            print(f\"  {class_name} [{in_test}]: F1={metrics['f1_score']:.4f}, \"\n","                  f\"Support={metrics['support']}\")\n","\n","        wrong_meta = wrong_predictions_results['analysis_metadata']\n","        print(f\"\\nWrong Predictions Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']} / {wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        print(f\"\\nInference Performance:\")\n","        print(f\"  Total time: {inference_results['inference_time']:.2f}s\")\n","        print(f\"  Speed: {evaluation_results['inference_performance']['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","    except Exception as e:\n","        print(f\"Evaluation failed for {dataset_version}: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        continue\n","\n","# =====================================================\n","# COMPARATIVE ANALYSIS (if both datasets evaluated)\n","# =====================================================\n","\n","if len(all_evaluation_results) == 2 and 'v7' in all_evaluation_results and 'v8' in all_evaluation_results:\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"COMPARATIVE ANALYSIS: AF (v7) vs KFS (v8)\")\n","    print(\"=\" * 70)\n","\n","    v7_results = all_evaluation_results['v7']['evaluation']\n","    v8_results = all_evaluation_results['v8']['evaluation']\n","\n","    print(\"\\nOverall Performance Comparison:\")\n","    print(f\"{'Metric':<20} {'AF (v7)':<15} {'KFS (v8)':<15} {'Difference':<15}\")\n","    print(\"-\" * 65)\n","\n","    metrics_to_compare = ['accuracy', 'macro_precision', 'macro_recall', 'macro_f1', 'macro_auc']\n","\n","    for metric in metrics_to_compare:\n","        v7_val = v7_results['overall_performance'][metric]\n","        v8_val = v8_results['overall_performance'][metric]\n","        diff = v8_val - v7_val\n","\n","        print(f\"{metric:<20} {v7_val:<15.4f} {v8_val:<15.4f} {diff:+.4f}\")\n","\n","    print(f\"\\nEvaluation Modes:\")\n","    print(f\"  AF (v7): {v7_results['evaluation_metadata']['evaluation_mode']}\")\n","    print(f\"  KFS (v8): {v8_results['evaluation_metadata']['evaluation_mode']}\")\n","\n","    if 'kfs_late_fusion_info' in v8_results:\n","        print(f\"\\nKFS Late Fusion Strategy:\")\n","        print(f\"  Frames used: {v8_results['kfs_late_fusion_info']['total_frames']}\")\n","        print(f\"  Video predictions: {v8_results['kfs_late_fusion_info']['total_videos']}\")\n","        print(f\"  Aggregation: {v8_results['kfs_late_fusion_info']['aggregation_method']}\")\n","\n","# Memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II ViT EVALUATION COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Evaluated datasets: {EVALUATE_DATASETS}\")\n","print(\"Next: Cell 4 - Generate confusion matrices and visualization\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","collapsed":true,"id":"_RgwHC0GoTM_","executionInfo":{"status":"ok","timestamp":1761218245058,"user_tz":-420,"elapsed":13679,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"525b1e46-5976-472a-b8c6-0d294d2e3b8c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II ViT Evaluation Framework with Dual Dataset Support\n","============================================================\n","Datasets to evaluate: ['v7', 'v8']\n","============================================================\n","\n","======================================================================\n","EVALUATING DATASET: V7\n","======================================================================\n","\n","Test Dataset Configuration:\n","  Version: v7\n","  Variant: AF\n","  Description: Apex Frame with Face-Aware Preprocessing\n","  Frame strategy: apex_frame\n","  Evaluation mode: frame_level\n","  Dataset path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7\n","\n","Creating CASME II test dataset from AF...\n","Loading CASME II test dataset for evaluation...\n","Found 28 image files in directory\n","Loaded 28 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 10 samples (35.7%)\n","  disgust: 7 samples (25.0%)\n","  happiness: 4 samples (14.3%)\n","  repression: 3 samples (10.7%)\n","  surprise: 3 samples (10.7%)\n","  sadness: 1 samples (3.6%)\n","Unique video IDs: 28\n","Missing classes in test set: ['fear']\n","Preloading 28 test images to RAM...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test to RAM: 100%|██████████| 28/28 [00:01<00:00, 15.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["RAM caching completed: 28 test images\n","Loading trained model from: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/07_01_vit_casme2_mfs_prep/casme2_vit_mfs_best_f1.pth\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["ViT feature dimension: 768\n","Classification head: 768 -> 512 -> 128 -> 7\n","Dropout rate: 0.3 (balanced for large dataset)\n","Model loaded successfully from epoch 11\n","\n","Running frame-level evaluation for AF...\n","Running frame-level inference...\n"]},{"output_type":"stream","name":"stderr","text":["Frame-level inference: 100%|██████████| 2/2 [00:00<00:00,  2.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Evaluation results saved:\n","  Main results: casme2_vit_evaluation_results_v7.json\n","  Wrong predictions: casme2_vit_wrong_predictions_v7.json\n","\n","============================================================\n","EVALUATION RESULTS - AF (v7)\n","============================================================\n","\n","Overall Performance:\n","  Accuracy:  0.5000\n","  Precision: 0.4097\n","  Recall:    0.3329\n","  F1 Score:  0.3393\n","  AUC:       0.7100\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.5833, Support=10\n","  disgust [Present]: F1=0.6667, Support=7\n","  happiness [Present]: F1=0.2857, Support=4\n","  repression [Present]: F1=0.0000, Support=3\n","  surprise [Present]: F1=0.5000, Support=3\n","  sadness [Present]: F1=0.0000, Support=1\n","  fear [Missing]: F1=0.0000, Support=0\n","\n","Wrong Predictions Analysis:\n","  Total errors: 14 / 28\n","  Error rate: 50.00%\n","\n","Inference Performance:\n","  Total time: 0.86s\n","  Speed: 30.8 ms/sample\n","\n","======================================================================\n","EVALUATING DATASET: V8\n","======================================================================\n","\n","Test Dataset Configuration:\n","  Version: v8\n","  Variant: KFS\n","  Description: Key Frame Sequence with Face-Aware Preprocessing\n","  Frame strategy: key_frame_sequence\n","  Evaluation mode: video_level\n","  Aggregation: late_fusion\n","  Dataset path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8\n","\n","Creating CASME II test dataset from KFS...\n","Loading CASME II test dataset for evaluation...\n","Found 84 image files in directory\n","Loaded 84 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 30 samples (35.7%)\n","  disgust: 21 samples (25.0%)\n","  happiness: 12 samples (14.3%)\n","  repression: 9 samples (10.7%)\n","  surprise: 9 samples (10.7%)\n","  sadness: 3 samples (3.6%)\n","Unique video IDs: 84\n","Missing classes in test set: ['fear']\n","Preloading 84 test images to RAM...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test to RAM: 100%|██████████| 84/84 [00:02<00:00, 28.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["RAM caching completed: 84 test images\n","Loading trained model from: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/07_01_vit_casme2_mfs_prep/casme2_vit_mfs_best_f1.pth\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["ViT feature dimension: 768\n","Classification head: 768 -> 512 -> 128 -> 7\n","Dropout rate: 0.3 (balanced for large dataset)\n","Model loaded successfully from epoch 11\n","\n","Running video-level evaluation with late fusion for KFS...\n","Running video-level inference with late fusion...\n"]},{"output_type":"stream","name":"stderr","text":["Frame inference: 100%|██████████| 6/6 [00:01<00:00,  3.43it/s]"]},{"output_type":"stream","name":"stdout","text":["Aggregating frame predictions to video level...\n","Evaluation results saved:\n","  Main results: casme2_vit_evaluation_results_v8.json\n","  Wrong predictions: casme2_vit_wrong_predictions_v8.json\n","\n","============================================================\n","EVALUATION RESULTS - KFS (v8)\n","============================================================\n","\n","Overall Performance:\n","  Accuracy:  0.5000\n","  Precision: 0.3736\n","  Recall:    0.3459\n","  F1 Score:  0.3476\n","  AUC:       0.0000\n","\n","Late Fusion Info:\n","  Total frames processed: 84\n","  Video-level predictions: 84\n","  Aggregation method: average_probability\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.5714, Support=30\n","  disgust [Present]: F1=0.6522, Support=21\n","  happiness [Present]: F1=0.3000, Support=12\n","  repression [Present]: F1=0.1333, Support=9\n","  surprise [Present]: F1=0.4286, Support=9\n","  sadness [Present]: F1=0.0000, Support=3\n","  fear [Missing]: F1=0.0000, Support=0\n","\n","Wrong Predictions Analysis:\n","  Total errors: 42 / 84\n","  Error rate: 50.00%\n","\n","Inference Performance:\n","  Total time: 1.76s\n","  Speed: 20.9 ms/sample\n","\n","======================================================================\n","COMPARATIVE ANALYSIS: AF (v7) vs KFS (v8)\n","======================================================================\n","\n","Overall Performance Comparison:\n","Metric               AF (v7)         KFS (v8)        Difference     \n","-----------------------------------------------------------------\n","accuracy             0.5000          0.5000          +0.0000\n","macro_precision      0.4097          0.3736          -0.0361\n","macro_recall         0.3329          0.3459          +0.0130\n","macro_f1             0.3393          0.3476          +0.0083\n","macro_auc            0.7100          0.0000          -0.7100\n","\n","Evaluation Modes:\n","  AF (v7): frame_level\n","  KFS (v8): video_level\n","\n","KFS Late Fusion Strategy:\n","  Frames used: 84\n","  Video predictions: 84\n","  Aggregation: average_probability\n","\n","======================================================================\n","CASME II ViT EVALUATION COMPLETED\n","======================================================================\n","Evaluated datasets: ['v7', 'v8']\n","Next: Cell 4 - Generate confusion matrices and visualization\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title Cell 4: CASME II ViT Confusion Matrix Generation\n","\n","# File: 07_01_ViT_CASME2_MFS_Cell4.py\n","# Location: experiments/07_01_ViT_CASME2-MFS-PREP.ipynb\n","# Purpose: Generate professional confusion matrix visualization for AF and KFS evaluations\n","\n","import json\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import seaborn as sns\n","from datetime import datetime\n","\n","print(\"CASME II ViT Confusion Matrix Generation\")\n","print(\"=\" * 60)\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/07_01_vit_casme2_mfs_prep\"\n","\n","def find_evaluation_json_files(results_path):\n","    \"\"\"Find evaluation JSON files with version detection\"\"\"\n","    json_files = {}\n","    eval_dir = f\"{results_path}/evaluation_results\"\n","\n","    if os.path.exists(eval_dir):\n","        for version in ['v7', 'v8']:\n","            eval_pattern = f\"{eval_dir}/casme2_vit_evaluation_results_{version}.json\"\n","            eval_files = glob.glob(eval_pattern)\n","\n","            if eval_files:\n","                json_files[f'main_{version}'] = eval_files[0]\n","                print(f\"Found {version.upper()} evaluation file: {os.path.basename(eval_files[0])}\")\n","\n","            wrong_pattern = f\"{eval_dir}/casme2_vit_wrong_predictions_{version}.json\"\n","            wrong_files = glob.glob(wrong_pattern)\n","\n","            if wrong_files:\n","                json_files[f'wrong_{version}'] = wrong_files[0]\n","                print(f\"Found {version.upper()} wrong predictions: {os.path.basename(wrong_files[0])}\")\n","\n","        if not json_files:\n","            print(f\"WARNING: No evaluation results found in {eval_dir}\")\n","            print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","    else:\n","        print(f\"ERROR: Evaluation directory not found: {eval_dir}\")\n","\n","    return json_files\n","\n","def load_evaluation_results(json_path):\n","    \"\"\"Load and parse evaluation results JSON\"\"\"\n","    try:\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"Successfully loaded: {os.path.basename(json_path)}\")\n","        return data\n","    except Exception as e:\n","        print(f\"ERROR loading {json_path}: {str(e)}\")\n","        return None\n","\n","def calculate_weighted_f1(per_class_performance):\n","    \"\"\"Calculate weighted F1 score\"\"\"\n","    total_support = sum([class_data['support'] for class_data in per_class_performance.values()\n","                        if class_data['support'] > 0])\n","\n","    if total_support == 0:\n","        return 0.0\n","\n","    weighted_f1 = 0.0\n","\n","    for class_name, class_data in per_class_performance.items():\n","        if class_data['support'] > 0:\n","            weight = class_data['support'] / total_support\n","            weighted_f1 += class_data['f1_score'] * weight\n","\n","    return weighted_f1\n","\n","def calculate_balanced_accuracy(confusion_matrix):\n","    \"\"\"Calculate balanced accuracy handling classes with zero support\"\"\"\n","    cm = np.array(confusion_matrix)\n","    n_classes = cm.shape[0]\n","\n","    per_class_balanced_acc = []\n","    classes_with_samples = []\n","\n","    for i in range(n_classes):\n","        if cm[i, :].sum() > 0:\n","            classes_with_samples.append(i)\n","\n","    for i in classes_with_samples:\n","        tp = cm[i, i]\n","        fn = cm[i, :].sum() - tp\n","        fp = cm[:, i].sum() - tp\n","        tn = cm.sum() - tp - fn - fp\n","\n","        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n","\n","        class_balanced_acc = (sensitivity + specificity) / 2\n","        per_class_balanced_acc.append(class_balanced_acc)\n","\n","    balanced_acc = np.mean(per_class_balanced_acc) if per_class_balanced_acc else 0.0\n","\n","    return balanced_acc\n","\n","def determine_text_color(color_value, threshold=0.5):\n","    \"\"\"Determine optimal text color based on background intensity\"\"\"\n","    return 'white' if color_value > threshold else 'black'\n","\n","def create_confusion_matrix_plot(data, output_path, test_version):\n","    \"\"\"Create professional confusion matrix visualization for CASME II micro-expression recognition\"\"\"\n","\n","    meta = data['evaluation_metadata']\n","    class_names = meta['class_names']\n","    cm = np.array(data['confusion_matrix'], dtype=int)\n","    overall = data['overall_performance']\n","    per_class = data['per_class_performance']\n","\n","    test_config = data.get('test_configuration', {})\n","    variant = test_config.get('variant', test_version.upper())\n","    description = test_config.get('description', f'{test_version} preprocessing')\n","    eval_mode = meta.get('evaluation_mode', 'frame_level')\n","\n","    print(f\"Processing confusion matrix for {variant} ({test_version})\")\n","    print(f\"Dataset: {description}\")\n","    print(f\"Evaluation mode: {eval_mode}\")\n","    print(f\"Confusion matrix shape: {cm.shape}\")\n","\n","    macro_f1 = overall.get('macro_f1', 0.0)\n","    accuracy = overall.get('accuracy', 0.0)\n","    weighted_f1 = calculate_weighted_f1(per_class)\n","    balanced_acc = calculate_balanced_accuracy(cm)\n","\n","    print(f\"Calculated metrics - Macro F1: {macro_f1:.4f}, Weighted F1: {weighted_f1:.4f}, \"\n","          f\"Balanced Acc: {balanced_acc:.4f}, Accuracy: {accuracy:.4f}\")\n","\n","    row_sums = cm.sum(axis=1, keepdims=True)\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        cm_pct = np.divide(cm, row_sums, where=(row_sums!=0))\n","        cm_pct = np.nan_to_num(cm_pct)\n","\n","    fig, ax = plt.subplots(figsize=(12, 10))\n","\n","    cmap = 'Blues'\n","    im = ax.imshow(cm_pct, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=0.8)\n","\n","    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","    cbar.set_label('True Class Percentage', rotation=270, labelpad=15, fontsize=11)\n","\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            count = cm[i, j]\n","\n","            if row_sums[i, 0] > 0:\n","                percentage = cm_pct[i, j] * 100\n","                text = f\"{count}\\n{percentage:.1f}%\"\n","            else:\n","                text = f\"{count}\\n(N/A)\"\n","\n","            cell_value = cm_pct[i, j]\n","            text_color = determine_text_color(cell_value, threshold=0.4)\n","\n","            ax.text(j, i, text, ha=\"center\", va=\"center\",\n","                   color=text_color, fontsize=9, fontweight='bold')\n","\n","    ax.set_xticks(np.arange(len(class_names)))\n","    ax.set_yticks(np.arange(len(class_names)))\n","    ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=10)\n","    ax.set_yticklabels(class_names, fontsize=10)\n","    ax.set_xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n","    ax.set_ylabel(\"True Label\", fontsize=12, fontweight='bold')\n","\n","    preprocessing_note = f\"Preprocessing: {description}\\n\"\n","    preprocessing_note += f\"Dataset: {test_version}\\n\"\n","    preprocessing_note += f\"Evaluation: {eval_mode.replace('_', ' ').title()}\"\n","\n","    if 'kfs_late_fusion_info' in data:\n","        fusion_info = data['kfs_late_fusion_info']\n","        preprocessing_note += f\"\\nFrames: {fusion_info['total_frames']}, Videos: {fusion_info['total_videos']}\"\n","\n","    missing_classes = meta.get('missing_classes', [])\n","    if missing_classes:\n","        preprocessing_note += f\"\\nMissing: {', '.join(missing_classes)}\"\n","\n","    ax.text(0.02, 0.98, preprocessing_note, transform=ax.transAxes, fontsize=8,\n","            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n","\n","    title = f\"CASME II {variant} Micro-Expression Recognition - ViT\\n\"\n","    title += f\"Acc: {accuracy:.4f}  |  Macro F1: {macro_f1:.4f}  |  Weighted F1: {weighted_f1:.4f}  |  Balanced Acc: {balanced_acc:.4f}\"\n","    ax.set_title(title, fontsize=12, pad=25, fontweight='bold')\n","\n","    plt.tight_layout()\n","    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n","    plt.close(fig)\n","\n","    print(f\"Confusion matrix saved to: {os.path.basename(output_path)}\")\n","\n","    return {\n","        'accuracy': accuracy,\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        'balanced_accuracy': balanced_acc,\n","        'missing_classes': missing_classes\n","    }\n","\n","def generate_performance_summary(evaluation_data, wrong_predictions_data=None):\n","    \"\"\"Generate comprehensive performance summary\"\"\"\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\")\n","    print(\"=\" * 60)\n","\n","    overall = evaluation_data['overall_performance']\n","    meta = evaluation_data['evaluation_metadata']\n","    test_config = evaluation_data.get('test_configuration', {})\n","\n","    variant = test_config.get('variant', 'N/A')\n","\n","    print(f\"Dataset: {meta['dataset']}\")\n","    print(f\"Variant: {variant}\")\n","    print(f\"Dataset version: {test_config.get('version', 'N/A')}\")\n","    print(f\"Preprocessing: {test_config.get('description', 'N/A')}\")\n","    print(f\"Test samples: {meta['test_samples']}\")\n","    print(f\"Model: {meta['model_type']}\")\n","    print(f\"Evaluation date: {meta['evaluation_timestamp']}\")\n","\n","    if 'kfs_late_fusion_info' in evaluation_data:\n","        fusion_info = evaluation_data['kfs_late_fusion_info']\n","        print(f\"\\nLate Fusion Information:\")\n","        print(f\"  Total frames: {fusion_info['total_frames']}\")\n","        print(f\"  Video predictions: {fusion_info['total_videos']}\")\n","        print(f\"  Aggregation: {fusion_info['aggregation_method']}\")\n","\n","    print(f\"\\nOverall Performance:\")\n","    print(f\"  Accuracy:         {overall['accuracy']:.4f}\")\n","    print(f\"  Macro Precision:  {overall['macro_precision']:.4f}\")\n","    print(f\"  Macro Recall:     {overall['macro_recall']:.4f}\")\n","    print(f\"  Macro F1:         {overall['macro_f1']:.4f}\")\n","    print(f\"  Macro AUC:        {overall['macro_auc']:.4f}\")\n","\n","    print(f\"\\nPer-Class Performance:\")\n","    per_class = evaluation_data['per_class_performance']\n","\n","    print(f\"{'Class':<12} {'F1':<8} {'Precision':<10} {'Recall':<8} {'AUC':<8} {'Support':<8} {'In Test'}\")\n","    print(\"-\" * 65)\n","\n","    for class_name, metrics in per_class.items():\n","        in_test = \"Yes\" if metrics['in_test_set'] else \"No\"\n","        print(f\"{class_name:<12} {metrics['f1_score']:<8.4f} {metrics['precision']:<10.4f} \"\n","              f\"{metrics['recall']:<8.4f} {metrics['auc']:<8.4f} {metrics['support']:<8} {in_test}\")\n","\n","    if 'training_information' in evaluation_data:\n","        training = evaluation_data['training_information']\n","        print(f\"\\nTraining vs Test Comparison:\")\n","        print(f\"  Training Val F1:  {training['best_val_f1']:.4f}\")\n","        print(f\"  Test F1:          {overall['macro_f1']:.4f}\")\n","        print(f\"  Performance Gap:  {training['best_val_f1'] - overall['macro_f1']:+.4f}\")\n","        print(f\"  Best Epoch:       {training['best_epoch']}\")\n","\n","    missing_classes = meta.get('missing_classes', [])\n","    available_classes = meta.get('available_classes', [])\n","\n","    print(f\"\\nClass Availability Analysis:\")\n","    print(f\"  Available classes: {len(available_classes)}/7\")\n","    print(f\"  Missing classes: {missing_classes if missing_classes else 'None'}\")\n","\n","    if wrong_predictions_data:\n","        wrong_meta = wrong_predictions_data['analysis_metadata']\n","        print(f\"\\nError Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']}/{wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        patterns = wrong_predictions_data.get('confusion_patterns', {})\n","        if patterns:\n","            print(f\"\\nTop Confusion Patterns:\")\n","            sorted_patterns = sorted(patterns.items(), key=lambda x: x[1], reverse=True)[:3]\n","            for pattern, count in sorted_patterns:\n","                print(f\"  {pattern}: {count} cases\")\n","\n","    print(f\"\\nInference Performance:\")\n","    inference = evaluation_data['inference_performance']\n","    print(f\"  Total time: {inference['total_time_seconds']:.2f}s\")\n","    print(f\"  Speed: {inference['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","json_files = find_evaluation_json_files(RESULTS_ROOT)\n","\n","if not json_files:\n","    print(f\"ERROR: No evaluation JSON files found in {RESULTS_ROOT}\")\n","    print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","else:\n","    print(f\"\\nFound {len([k for k in json_files.keys() if k.startswith('main_')])} evaluation result(s)\")\n","\n","output_dir = f\"{RESULTS_ROOT}/confusion_matrix_analysis\"\n","Path(output_dir).mkdir(parents=True, exist_ok=True)\n","\n","results_summary = {}\n","generated_files = []\n","\n","for version in ['v7', 'v8']:\n","    main_key = f'main_{version}'\n","    wrong_key = f'wrong_{version}'\n","\n","    if main_key in json_files:\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Processing {version.upper()} Evaluation Results\")\n","        print(f\"{'='*60}\")\n","\n","        eval_data = load_evaluation_results(json_files[main_key])\n","\n","        wrong_data = None\n","        if wrong_key in json_files:\n","            wrong_data = load_evaluation_results(json_files[wrong_key])\n","\n","        if eval_data is not None:\n","            try:\n","                cm_output_path = os.path.join(output_dir, f\"confusion_matrix_CASME2_ViT_{version}.png\")\n","                metrics = create_confusion_matrix_plot(eval_data, cm_output_path, version)\n","                generated_files.append(cm_output_path)\n","\n","                results_summary[version] = metrics\n","\n","                print(f\"\\nSUCCESS: {version.upper()} confusion matrix generated successfully\")\n","                print(f\"Output file: {os.path.basename(cm_output_path)}\")\n","\n","                print(f\"\\nPerformance Metrics Summary:\")\n","                print(f\"  Accuracy:        {metrics['accuracy']:.4f}\")\n","                print(f\"  Macro F1:        {metrics['macro_f1']:.4f}\")\n","                print(f\"  Weighted F1:     {metrics['weighted_f1']:.4f}\")\n","                print(f\"  Balanced Acc:    {metrics['balanced_accuracy']:.4f}\")\n","\n","                if metrics['missing_classes']:\n","                    print(f\"  Missing classes: {metrics['missing_classes']}\")\n","\n","            except Exception as e:\n","                print(f\"ERROR: Failed to generate {version.upper()} confusion matrix: {str(e)}\")\n","                import traceback\n","                traceback.print_exc()\n","\n","            generate_performance_summary(eval_data, wrong_data)\n","        else:\n","            print(f\"ERROR: Could not load {version.upper()} evaluation data\")\n","\n","if generated_files:\n","    print(f\"\\n\" + \"=\" * 60)\n","    print(\"CASME II CONFUSION MATRIX GENERATION COMPLETED\")\n","    print(\"=\" * 60)\n","\n","    print(f\"\\nGenerated visualization files:\")\n","    for file_path in generated_files:\n","        filename = os.path.basename(file_path)\n","        print(f\"  {filename}\")\n","\n","    for version in ['v7', 'v8']:\n","        if version in results_summary:\n","            variant = 'AF' if version == 'v7' else 'KFS'\n","            print(f\"\\n{variant} ({version.upper()}) Performance Summary:\")\n","            metrics = results_summary[version]\n","            print(f\"  Accuracy:       {metrics['accuracy']:.4f}\")\n","            print(f\"  Macro F1:       {metrics['macro_f1']:.4f}\")\n","            print(f\"  Weighted F1:    {metrics['weighted_f1']:.4f}\")\n","            print(f\"  Balanced Acc:   {metrics['balanced_accuracy']:.4f}\")\n","\n","    print(f\"\\nFiles saved in: {output_dir}\")\n","    print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","else:\n","    print(f\"\\nERROR: No visualizations were generated\")\n","    print(\"Please check:\")\n","    print(\"1. Cell 3 evaluation results exist\")\n","    print(\"2. JSON file structure is correct\")\n","    print(\"3. No file permission issues\")\n","\n","print(\"\\nCell 4 completed - CASME II confusion matrix analysis generated\")"],"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"hVkfExoTqjDw","executionInfo":{"status":"ok","timestamp":1761218248483,"user_tz":-420,"elapsed":3383,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"655cb0be-ae8f-4c7c-bebc-f67bfcdc1c0c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II ViT Confusion Matrix Generation\n","============================================================\n","Found V7 evaluation file: casme2_vit_evaluation_results_v7.json\n","Found V7 wrong predictions: casme2_vit_wrong_predictions_v7.json\n","Found V8 evaluation file: casme2_vit_evaluation_results_v8.json\n","Found V8 wrong predictions: casme2_vit_wrong_predictions_v8.json\n","\n","Found 2 evaluation result(s)\n","\n","============================================================\n","Processing V7 Evaluation Results\n","============================================================\n","Successfully loaded: casme2_vit_evaluation_results_v7.json\n","Successfully loaded: casme2_vit_wrong_predictions_v7.json\n","Processing confusion matrix for AF (v7)\n","Dataset: Apex Frame with Face-Aware Preprocessing\n","Evaluation mode: frame_level\n","Confusion matrix shape: (6, 6)\n","Calculated metrics - Macro F1: 0.3393, Weighted F1: 0.4694, Balanced Acc: 0.6085, Accuracy: 0.5000\n","Confusion matrix saved to: confusion_matrix_CASME2_ViT_v7.png\n","\n","SUCCESS: V7 confusion matrix generated successfully\n","Output file: confusion_matrix_CASME2_ViT_v7.png\n","\n","Performance Metrics Summary:\n","  Accuracy:        0.5000\n","  Macro F1:        0.3393\n","  Weighted F1:     0.4694\n","  Balanced Acc:    0.6085\n","  Missing classes: ['fear']\n","\n","============================================================\n","CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\n","============================================================\n","Dataset: CASME_II\n","Variant: AF\n","Dataset version: v7\n","Preprocessing: Apex Frame with Face-Aware Preprocessing\n","Test samples: 28\n","Model: ViTCASME2Baseline\n","Evaluation date: 2025-10-23T11:17:17.038201\n","\n","Overall Performance:\n","  Accuracy:         0.5000\n","  Macro Precision:  0.4097\n","  Macro Recall:     0.3329\n","  Macro F1:         0.3393\n","  Macro AUC:        0.7100\n","\n","Per-Class Performance:\n","Class        F1       Precision  Recall   AUC      Support  In Test\n","-----------------------------------------------------------------\n","others       0.5833   0.5000     0.7000   0.6833   10       Yes\n","disgust      0.6667   0.6250     0.7143   0.7143   7        Yes\n","happiness    0.2857   0.3333     0.2500   0.6875   4        Yes\n","repression   0.0000   0.0000     0.0000   0.4267   3        Yes\n","surprise     0.5000   1.0000     0.3333   0.9333   3        Yes\n","sadness      0.0000   0.0000     0.0000   0.8148   1        Yes\n","fear         0.0000   0.0000     0.0000   0.0000   0        No\n","\n","Training vs Test Comparison:\n","  Training Val F1:  0.2273\n","  Test F1:          0.3393\n","  Performance Gap:  -0.1119\n","  Best Epoch:       11\n","\n","Class Availability Analysis:\n","  Available classes: 6/7\n","  Missing classes: ['fear']\n","\n","Error Analysis:\n","  Total errors: 14/28\n","  Error rate: 50.00%\n","\n","Top Confusion Patterns:\n","  surprise->others: 2 cases\n","  others->disgust: 2 cases\n","  repression->others: 2 cases\n","\n","Inference Performance:\n","  Total time: 0.86s\n","  Speed: 30.8 ms/sample\n","\n","============================================================\n","Processing V8 Evaluation Results\n","============================================================\n","Successfully loaded: casme2_vit_evaluation_results_v8.json\n","Successfully loaded: casme2_vit_wrong_predictions_v8.json\n","Processing confusion matrix for KFS (v8)\n","Dataset: Key Frame Sequence with Face-Aware Preprocessing\n","Evaluation mode: video_level\n","Confusion matrix shape: (6, 6)\n","Calculated metrics - Macro F1: 0.3476, Weighted F1: 0.4702, Balanced Acc: 0.6153, Accuracy: 0.5000\n","Confusion matrix saved to: confusion_matrix_CASME2_ViT_v8.png\n","\n","SUCCESS: V8 confusion matrix generated successfully\n","Output file: confusion_matrix_CASME2_ViT_v8.png\n","\n","Performance Metrics Summary:\n","  Accuracy:        0.5000\n","  Macro F1:        0.3476\n","  Weighted F1:     0.4702\n","  Balanced Acc:    0.6153\n","  Missing classes: ['fear']\n","\n","============================================================\n","CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\n","============================================================\n","Dataset: CASME_II\n","Variant: KFS\n","Dataset version: v8\n","Preprocessing: Key Frame Sequence with Face-Aware Preprocessing\n","Test samples: 84\n","Model: ViTCASME2Baseline\n","Evaluation date: 2025-10-23T11:17:25.030026\n","\n","Late Fusion Information:\n","  Total frames: 84\n","  Video predictions: 84\n","  Aggregation: average_probability\n","\n","Overall Performance:\n","  Accuracy:         0.5000\n","  Macro Precision:  0.3736\n","  Macro Recall:     0.3459\n","  Macro F1:         0.3476\n","  Macro AUC:        0.0000\n","\n","Per-Class Performance:\n","Class        F1       Precision  Recall   AUC      Support  In Test\n","-----------------------------------------------------------------\n","others       0.5714   0.5000     0.6667   0.0000   30       Yes\n","disgust      0.6522   0.6000     0.7143   0.0000   21       Yes\n","happiness    0.3000   0.3750     0.2500   0.0000   12       Yes\n","repression   0.1333   0.1667     0.1111   0.0000   9        Yes\n","surprise     0.4286   0.6000     0.3333   0.0000   9        Yes\n","sadness      0.0000   0.0000     0.0000   0.0000   3        Yes\n","fear         0.0000   0.0000     0.0000   0.0000   0        No\n","\n","Training vs Test Comparison:\n","  Training Val F1:  0.2273\n","  Test F1:          0.3476\n","  Performance Gap:  -0.1202\n","  Best Epoch:       11\n","\n","Class Availability Analysis:\n","  Available classes: 6/7\n","  Missing classes: ['fear']\n","\n","Error Analysis:\n","  Total errors: 42/84\n","  Error rate: 50.00%\n","\n","Top Confusion Patterns:\n","  others->disgust: 6 cases\n","  surprise->others: 5 cases\n","  repression->others: 5 cases\n","\n","Inference Performance:\n","  Total time: 1.76s\n","  Speed: 20.9 ms/sample\n","\n","============================================================\n","CASME II CONFUSION MATRIX GENERATION COMPLETED\n","============================================================\n","\n","Generated visualization files:\n","  confusion_matrix_CASME2_ViT_v7.png\n","  confusion_matrix_CASME2_ViT_v8.png\n","\n","AF (V7) Performance Summary:\n","  Accuracy:       0.5000\n","  Macro F1:       0.3393\n","  Weighted F1:    0.4694\n","  Balanced Acc:   0.6085\n","\n","KFS (V8) Performance Summary:\n","  Accuracy:       0.5000\n","  Macro F1:       0.3476\n","  Weighted F1:    0.4702\n","  Balanced Acc:   0.6153\n","\n","Files saved in: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/07_01_vit_casme2_mfs_prep/confusion_matrix_analysis\n","Analysis completed at: 2025-10-23 11:17:28\n","\n","Cell 4 completed - CASME II confusion matrix analysis generated\n"]}]}]}