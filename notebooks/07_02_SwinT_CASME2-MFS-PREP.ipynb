{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2bd492a1dca8427bba1be6edeb27f14f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45945a65fc7f4879846ccff500046858","IPY_MODEL_6fbafbb99c86478daf85c5b78b003ab4","IPY_MODEL_36313d5fdfec4593bf369f18814a368e"],"layout":"IPY_MODEL_94102b9e47a341239d0611c5761c15aa"}},"45945a65fc7f4879846ccff500046858":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34be61edd7084b629f16b9d039deccaf","placeholder":"​","style":"IPY_MODEL_3770c4eb60674552b62e4ddca0eceed9","value":"preprocessor_config.json: 100%"}},"6fbafbb99c86478daf85c5b78b003ab4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d37636945f634db1b6c27b65a13da443","max":255,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81b6fef35c1c4d48a94f044cfe452d37","value":255}},"36313d5fdfec4593bf369f18814a368e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9446c54995fe4c8f98c407cf4b3b3b09","placeholder":"​","style":"IPY_MODEL_61b6be345da241a6ad6d3967c81f2b13","value":" 255/255 [00:00&lt;00:00, 28.8kB/s]"}},"94102b9e47a341239d0611c5761c15aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34be61edd7084b629f16b9d039deccaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3770c4eb60674552b62e4ddca0eceed9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d37636945f634db1b6c27b65a13da443":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81b6fef35c1c4d48a94f044cfe452d37":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9446c54995fe4c8f98c407cf4b3b3b09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61b6be345da241a6ad6d3967c81f2b13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40b178c890744947932fd93063a24436":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_360dd545172f460780d2540f42e61e28","IPY_MODEL_1105c2c93b15418eaf51e33dca7760b2","IPY_MODEL_0ad0aae54eb14040b142d037548355eb"],"layout":"IPY_MODEL_ecfbb69d9b4b48a793346328627ab67b"}},"360dd545172f460780d2540f42e61e28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d4c504eb5694b14a46b0ad11633db5e","placeholder":"​","style":"IPY_MODEL_c742b3a99eee42479a56c7f52950889a","value":"config.json: "}},"1105c2c93b15418eaf51e33dca7760b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_30ab292be8ca49d7a25d4d1962cca46e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_effc69a99de74a478efcb1ce69b0c1a7","value":1}},"0ad0aae54eb14040b142d037548355eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30ebe6e18cab416da2a1345d2c745a41","placeholder":"​","style":"IPY_MODEL_03885182afae457491376a572e5104d3","value":" 71.8k/? [00:00&lt;00:00, 7.94MB/s]"}},"ecfbb69d9b4b48a793346328627ab67b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d4c504eb5694b14a46b0ad11633db5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c742b3a99eee42479a56c7f52950889a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30ab292be8ca49d7a25d4d1962cca46e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"effc69a99de74a478efcb1ce69b0c1a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30ebe6e18cab416da2a1345d2c745a41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03885182afae457491376a572e5104d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7419cfdef26247a485bf422b66257832":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e369d5e75e64a65bb782069473403d0","IPY_MODEL_91bd5fc8e60e430ca924e6846a14ac28","IPY_MODEL_8adf92a0e53d4e128ba21b577b55bac0"],"layout":"IPY_MODEL_8577b5a7ce334ae88d0f9f34cf9a0aad"}},"6e369d5e75e64a65bb782069473403d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5896d3e66b894aebad435d6fbe8f207d","placeholder":"​","style":"IPY_MODEL_bc86abd3f79d4273a147b0cac2ec9ec5","value":"model.safetensors: 100%"}},"91bd5fc8e60e430ca924e6846a14ac28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_02cfb59e02904720b6c84ceb3b9ce9a8","max":351590690,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5d7fc4657696494fa261e72eaa09735b","value":351590690}},"8adf92a0e53d4e128ba21b577b55bac0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e926d42e68bb44f1a1522cdb26c2de7e","placeholder":"​","style":"IPY_MODEL_252319770087413cad7999ab34c97a31","value":" 352M/352M [00:02&lt;00:00, 248MB/s]"}},"8577b5a7ce334ae88d0f9f34cf9a0aad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5896d3e66b894aebad435d6fbe8f207d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc86abd3f79d4273a147b0cac2ec9ec5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02cfb59e02904720b6c84ceb3b9ce9a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d7fc4657696494fa261e72eaa09735b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e926d42e68bb44f1a1522cdb26c2de7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"252319770087413cad7999ab34c97a31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"IpSOjfdxvVfi","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["2bd492a1dca8427bba1be6edeb27f14f","45945a65fc7f4879846ccff500046858","6fbafbb99c86478daf85c5b78b003ab4","36313d5fdfec4593bf369f18814a368e","94102b9e47a341239d0611c5761c15aa","34be61edd7084b629f16b9d039deccaf","3770c4eb60674552b62e4ddca0eceed9","d37636945f634db1b6c27b65a13da443","81b6fef35c1c4d48a94f044cfe452d37","9446c54995fe4c8f98c407cf4b3b3b09","61b6be345da241a6ad6d3967c81f2b13","40b178c890744947932fd93063a24436","360dd545172f460780d2540f42e61e28","1105c2c93b15418eaf51e33dca7760b2","0ad0aae54eb14040b142d037548355eb","ecfbb69d9b4b48a793346328627ab67b","6d4c504eb5694b14a46b0ad11633db5e","c742b3a99eee42479a56c7f52950889a","30ab292be8ca49d7a25d4d1962cca46e","effc69a99de74a478efcb1ce69b0c1a7","30ebe6e18cab416da2a1345d2c745a41","03885182afae457491376a572e5104d3","7419cfdef26247a485bf422b66257832","6e369d5e75e64a65bb782069473403d0","91bd5fc8e60e430ca924e6846a14ac28","8adf92a0e53d4e128ba21b577b55bac0","8577b5a7ce334ae88d0f9f34cf9a0aad","5896d3e66b894aebad435d6fbe8f207d","bc86abd3f79d4273a147b0cac2ec9ec5","02cfb59e02904720b6c84ceb3b9ce9a8","5d7fc4657696494fa261e72eaa09735b","e926d42e68bb44f1a1522cdb26c2de7e","252319770087413cad7999ab34c97a31"]},"collapsed":true,"outputId":"604172f9-4adb-4be6-8d3b-f8d2e2f93b3b","cellView":"form","executionInfo":{"status":"ok","timestamp":1761280013884,"user_tz":-420,"elapsed":69894,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","CASME II MULTI-FRAME SEQUENCE SWIN TRANSFORMER WITH FACE-AWARE PREPROCESSING\n","============================================================\n","\n","[1] Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully\n","\n","[2] Importing required libraries...\n","CASME II Multi-Frame Sequence Swin Transformer - Face-Aware Preprocessing Infrastructure\n","============================================================\n","Loading CASME II v9 preprocessing metadata...\n","Dataset variant: MFS\n","Processing date: 2025-10-19T08:20:12.098301\n","Preprocessing method: face_bbox_expansion_all_directions\n","Total images processed: 2774\n","Face detection rate: 100.00%\n","Target size: 224x224px\n","BBox expansion: 20px (all directions)\n","\n","Dataset split information:\n","  Train samples: 2613\n","  Validation samples: 78\n","  Test samples: 83\n","Using Swin-Base for enhanced hierarchical micro-expression recognition (88M parameters)\n","\n","==================================================\n","EXPERIMENT CONFIGURATION - MULTI-FRAME SEQUENCE FACE-AWARE\n","==================================================\n","Dataset: v9 Multi-Frame Sequence with Face-Aware Preprocessing\n","Frame strategy: Multiple frames per video (dense sampling)\n","Training approach: Frame-level independent learning\n","Loss Function: Focal Loss\n","  Gamma: 2.0\n","  Alpha Weights (per-class): [0.048, 0.06, 0.085, 0.093, 0.095, 0.191, 0.427]\n","  Alpha Sum Validation: 0.999\n","Swin Model Variant: BASE\n","  Model: microsoft/swin-base-patch4-window7-224\n","  Parameters: 88M\n","  Window Size: 7x7\n","  Patch Size: 4x4\n","  Hidden Dimension: 1024\n","Input Resolution: 224x224px\n","Image Format: Grayscale converted to RGB (3-channel)\n","==================================================\n","\n","Device: cuda\n","GPU: NVIDIA L4 (23.8 GB)\n","GPU optimization enabled for NVIDIA L4\n","Large dataset configuration: Batch size 16 (optimal for 2613 samples at 224px)\n","Iterations per epoch: 163 (~82 iterations per epoch)\n","RAM preload workers: 32 (parallel image loading)\n","\n","Loading v9 class distribution...\n","\n","v9 Train distribution: [1027, 650, 325, 273, 260, 65, 13]\n","v9 Val distribution: [30, 18, 9, 9, 6, 3, 3]\n","v9 Test distribution: [30, 21, 12, 8, 9, 3, 0]\n","Applied Focal Loss alpha weights: [0.048 0.06  0.085 0.093 0.095 0.191 0.427]\n","Alpha weights sum: 0.999\n","\n","Setting up Swin Transformer Image Processor for 224px input...\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bd492a1dca8427bba1be6edeb27f14f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"]},{"output_type":"stream","name":"stdout","text":["Swin Transformer preprocessing configured:\n","  Input size: 224x224px\n","  Resize from: 224x224px (v9 native)\n","  Window-based attention: 7x7 windows\n","  Hierarchical processing: Multi-stage with shifted windows\n","Swin Transformer Image Processor configured for 224px with hierarchical processing\n","\n","Dataset paths:\n","Train: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/train\n","Validation: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/val\n","Test: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/test\n","\n","Swin Transformer CASME II architecture validation...\n"]},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40b178c890744947932fd93063a24436"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/352M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7419cfdef26247a485bf422b66257832"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Swin feature dimension (final stage): 1024\n","Base embed_dim: 128, Stages: 4\n","Classification head: 1024 -> GAP -> 512 -> 128 -> 7\n","Dropout rate: 0.2 (balanced for large dataset)\n","Validation successful: Output shape torch.Size([1, 7])\n","Initial patches (Stage 1): 3136\n","Window size: 7x7\n","Hierarchical processing: Multi-stage feature extraction with shifted windows\n","Swin BASE architecture validated\n","\n","============================================================\n","CASME II MULTI-FRAME SEQUENCE SWIN TRANSFORMER CONFIGURATION COMPLETE\n","============================================================\n","Loss Configuration:\n","  Function: Optimized Focal Loss\n","  Gamma: 2.0\n","  Per-class Alpha: [0.048, 0.06, 0.085, 0.093, 0.095, 0.191, 0.427]\n","  Alpha Sum: 0.999\n","\n","Model Configuration:\n","  Architecture: Swin Transformer\n","  Variant: BASE\n","  Model: microsoft/swin-base-patch4-window7-224\n","  Parameters: 88M\n","  Window Size: 7x7\n","  Patch Size: 4x4\n","  Input Resolution: 224px\n","  Hidden Dimension: 1024\n","  Hierarchical Stages: Multi-stage with shifted windows\n","  Classification Head: 1024 -> GAP -> 512 -> 128 -> 7 (hierarchical)\n","\n","Dataset Configuration:\n","  Version: v9\n","  Classes: 7\n","  Frame strategy: multi_frame_sequence\n","  Training approach: frame_level_independent\n","  Inference strategy: frame_level_evaluation\n","  Weight Optimization: Per-class Alpha\n","\n","Training Configuration:\n","  Train samples: 2613 frames\n","  Validation samples: 78 frames\n","  Test samples: 83 frames\n","  Batch size: 16\n","  Learning rate: 2e-05\n","  Dropout rate: 0.3\n","  Initial patches: 3136\n","\n","Next: Cell 2 - Dataset Loading and Multi-Frame Swin Transformer Training Pipeline\n"]}],"source":["# @title Cell 1: CASME II Multi-Frame Sequence Swin Transformer Infrastructure Configuration\n","\n","# File: 07_02_SwinT_CASME2_MFS_Cell1.py\n","# Location: experiments/07_02_SwinT_CASME2-MFS-PREP.ipynb\n","# Purpose: Swin Transformer for CASME II micro-expression recognition with multi-frame sequence strategy and face-aware preprocessing\n","\n","from google.colab import drive\n","print(\"=\" * 60)\n","print(\"CASME II MULTI-FRAME SEQUENCE SWIN TRANSFORMER WITH FACE-AWARE PREPROCESSING\")\n","print(\"=\" * 60)\n","print(\"\\n[1] Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Google Drive mounted successfully\")\n","\n","print(\"\\n[2] Importing required libraries...\")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import json\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import time\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Project paths configuration\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","DATASET_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/preprocessed_v9\"\n","CHECKPOINT_ROOT = f\"{PROJECT_ROOT}/models/07_02_swint_casme2_mfs_prep\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/07_02_swint_casme2_mfs_prep\"\n","\n","# Load CASME II v9 preprocessing metadata\n","PREPROCESSING_SUMMARY = f\"{DATASET_ROOT}/preprocessing_summary.json\"\n","\n","print(\"CASME II Multi-Frame Sequence Swin Transformer - Face-Aware Preprocessing Infrastructure\")\n","print(\"=\" * 60)\n","\n","# Validate preprocessing metadata exists\n","if not os.path.exists(PREPROCESSING_SUMMARY):\n","    raise FileNotFoundError(f\"v9 preprocessing summary not found: {PREPROCESSING_SUMMARY}\")\n","\n","# Load v9 preprocessing metadata\n","print(\"Loading CASME II v9 preprocessing metadata...\")\n","with open(PREPROCESSING_SUMMARY, 'r') as f:\n","    preprocessing_info = json.load(f)\n","\n","print(f\"Dataset variant: {preprocessing_info['variant']}\")\n","print(f\"Processing date: {preprocessing_info['processing_date']}\")\n","print(f\"Preprocessing method: {preprocessing_info['preprocessing_method']}\")\n","print(f\"Total images processed: {preprocessing_info['total_processed']}\")\n","print(f\"Face detection rate: {preprocessing_info['face_detection_stats']['detection_rate']:.2%}\")\n","\n","# Extract preprocessing parameters\n","preproc_params = preprocessing_info['preprocessing_parameters']\n","print(f\"Target size: {preproc_params['target_size']}x{preproc_params['target_size']}px\")\n","print(f\"BBox expansion: {preproc_params['bbox_expansion']}px (all directions)\")\n","\n","# Display split information\n","print(f\"\\nDataset split information:\")\n","print(f\"  Train samples: {preprocessing_info['splits']['train']['total_images']}\")\n","print(f\"  Validation samples: {preprocessing_info['splits']['val']['total_images']}\")\n","print(f\"  Test samples: {preprocessing_info['splits']['test']['total_images']}\")\n","\n","# =====================================================\n","# EXPERIMENT CONFIGURATION - Multi-Frame Sequence with Face-Aware Preprocessing\n","# =====================================================\n","# This configuration supports 4 experiment scenarios:\n","# 1. Swin-Tiny + CrossEntropy Loss\n","# 2. Swin-Tiny + Focal Loss\n","# 3. Swin-Base + CrossEntropy Loss\n","# 4. Swin-Base + Focal Loss\n","#\n","# Toggle SWIN_MODEL_VARIANT for model selection: 'tiny' or 'base'\n","# Toggle USE_FOCAL_LOSS for loss function: False (CrossEntropy) or True (Focal)\n","# =====================================================\n","\n","# FOCAL LOSS CONFIGURATION - Toggle for experimentation\n","USE_FOCAL_LOSS = True  # Default: CrossEntropy, Set True to enable Focal Loss\n","FOCAL_LOSS_GAMMA = 2.0  # Focal loss focusing parameter (if enabled)\n","\n","# OPTIMIZED CLASS WEIGHTS CONFIGURATION\n","# CASME II classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","# v9 Train distribution: [1027, 650, 325, 273, 260, 65, 13] - inverse sqrt frequency approach\n","\n","# CrossEntropy Loss - Optimized inverse square root frequency weights\n","CROSSENTROPY_CLASS_WEIGHTS = [1.00, 1.26, 1.78, 1.94, 1.99, 3.98, 8.90]\n","\n","# Focal Loss - Normalized per-class alpha values (sum = 1.0)\n","FOCAL_LOSS_ALPHA_WEIGHTS = [0.048, 0.060, 0.085, 0.093, 0.095, 0.191, 0.427]\n","\n","# SWIN TRANSFORMER MODEL CONFIGURATION - Support Tiny and Base variants\n","# Swin-Tiny: 28M parameters, window-based attention with 768 hidden dim\n","# Swin-Base: 88M parameters, enhanced hierarchical features with 1024 hidden dim\n","SWIN_MODEL_VARIANT = 'base'  # Options: 'tiny' or 'base'\n","\n","# Dynamic Swin model selection based on variant\n","if SWIN_MODEL_VARIANT == 'tiny':\n","    SWIN_MODEL_NAME = 'microsoft/swin-tiny-patch4-window7-224'\n","    EXPECTED_HIDDEN_DIM = 768\n","    WINDOW_SIZE = 7\n","    PATCH_SIZE = 4\n","    MODEL_PARAMS = '28M'\n","    print(\"Using Swin-Tiny for efficient hierarchical micro-expression analysis (28M parameters)\")\n","elif SWIN_MODEL_VARIANT == 'base':\n","    SWIN_MODEL_NAME = 'microsoft/swin-base-patch4-window7-224'\n","    EXPECTED_HIDDEN_DIM = 1024\n","    WINDOW_SIZE = 7\n","    PATCH_SIZE = 4\n","    MODEL_PARAMS = '88M'\n","    print(\"Using Swin-Base for enhanced hierarchical micro-expression recognition (88M parameters)\")\n","else:\n","    raise ValueError(f\"Unsupported SWIN_MODEL_VARIANT: {SWIN_MODEL_VARIANT}\")\n","\n","# Display experiment configuration\n","print(\"\\n\" + \"=\" * 50)\n","print(\"EXPERIMENT CONFIGURATION - MULTI-FRAME SEQUENCE FACE-AWARE\")\n","print(\"=\" * 50)\n","print(f\"Dataset: v9 Multi-Frame Sequence with Face-Aware Preprocessing\")\n","print(f\"Frame strategy: Multiple frames per video (dense sampling)\")\n","print(f\"Training approach: Frame-level independent learning\")\n","print(f\"Loss Function: {'Focal Loss' if USE_FOCAL_LOSS else 'CrossEntropy Loss'}\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Alpha Weights (per-class): {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum Validation: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Class Weights (inverse sqrt freq): {CROSSENTROPY_CLASS_WEIGHTS}\")\n","print(f\"Swin Model Variant: {SWIN_MODEL_VARIANT.upper()}\")\n","print(f\"  Model: {SWIN_MODEL_NAME}\")\n","print(f\"  Parameters: {MODEL_PARAMS}\")\n","print(f\"  Window Size: {WINDOW_SIZE}x{WINDOW_SIZE}\")\n","print(f\"  Patch Size: {PATCH_SIZE}x{PATCH_SIZE}\")\n","print(f\"  Hidden Dimension: {EXPECTED_HIDDEN_DIM}\")\n","print(f\"Input Resolution: 224x224px\")\n","print(f\"Image Format: Grayscale converted to RGB (3-channel)\")\n","print(\"=\" * 50)\n","\n","# Enhanced GPU configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n","gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n","\n","print(f\"\\nDevice: {device}\")\n","print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n","\n","# Fixed batch size configuration for large dataset (2613 train samples)\n","BATCH_SIZE = 16  # Optimized for large dataset at 224px resolution\n","NUM_WORKERS = 4\n","\n","if 'A100' in gpu_name or 'L4' in gpu_name:\n","    torch.backends.cudnn.benchmark = True\n","    print(f\"GPU optimization enabled for {gpu_name}\")\n","\n","print(f\"Large dataset configuration: Batch size {BATCH_SIZE} (optimal for 2613 samples at 224px)\")\n","print(f\"Iterations per epoch: {2613 // BATCH_SIZE} (~82 iterations per epoch)\")\n","\n","# RAM preloading workers (separate from DataLoader workers)\n","RAM_PRELOAD_WORKERS = 32\n","print(f\"RAM preload workers: {RAM_PRELOAD_WORKERS} (parallel image loading)\")\n","\n","# CASME II class mapping and analysis\n","CASME2_CLASSES = ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CASME2_CLASSES)}\n","\n","# Extract class distribution from v9 preprocessing metadata\n","print(\"\\nLoading v9 class distribution...\")\n","train_dist = preprocessing_info['splits']['train']['emotion_distribution']\n","val_dist = preprocessing_info['splits']['val']['emotion_distribution']\n","test_dist = preprocessing_info['splits']['test']['emotion_distribution']\n","\n","# Convert to ordered list matching CASME2_CLASSES\n","def emotion_dist_to_list(emotion_dict, class_names):\n","    \"\"\"Convert emotion distribution dict to ordered list\"\"\"\n","    return [emotion_dict.get(cls, 0) for cls in class_names]\n","\n","train_dist_list = emotion_dist_to_list(train_dist, CASME2_CLASSES)\n","val_dist_list = emotion_dist_to_list(val_dist, CASME2_CLASSES)\n","test_dist_list = emotion_dist_to_list(test_dist, CASME2_CLASSES)\n","\n","print(f\"\\nv9 Train distribution: {train_dist_list}\")\n","print(f\"v9 Val distribution: {val_dist_list}\")\n","print(f\"v9 Test distribution: {test_dist_list}\")\n","\n","# Apply optimized class weights based on loss function selection\n","if USE_FOCAL_LOSS:\n","    class_weights = torch.tensor(FOCAL_LOSS_ALPHA_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied Focal Loss alpha weights: {class_weights.cpu().numpy()}\")\n","    print(f\"Alpha weights sum: {class_weights.sum().item():.3f}\")\n","else:\n","    class_weights = torch.tensor(CROSSENTROPY_CLASS_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied CrossEntropy class weights: {class_weights.cpu().numpy()}\")\n","\n","# CASME II Swin Transformer Configuration for Multi-Frame Sequence with Face-Aware Preprocessing\n","# Optimized for large dataset (2613 train samples) with multi-frame strategy\n","CASME2_SWIN_CONFIG = {\n","    # Architecture configuration - Swin Transformer specific\n","    'swin_model': SWIN_MODEL_NAME,\n","    'model_variant': SWIN_MODEL_VARIANT,\n","    'model_params': MODEL_PARAMS,\n","    'window_size': WINDOW_SIZE,\n","    'patch_size': PATCH_SIZE,\n","    'input_size': 224,\n","    'num_classes': 7,\n","    'dropout_rate': 0.3,  # Balanced regularization for large dataset\n","    'expected_hidden_dim': EXPECTED_HIDDEN_DIM,\n","    'interpolate_pos_encoding': True,  # Enable for 224px input\n","\n","    # Training configuration - proven optimal from KFS-PREP\n","    'learning_rate': 2e-5,  # Proven optimal for transformer fine-tuning\n","    'weight_decay': 1e-5,\n","    'gradient_clip': 1.0,\n","    'num_epochs': 50,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","\n","    # Scheduler configuration - stable for large dataset\n","    'scheduler_type': 'plateau',\n","    'scheduler_mode': 'max',\n","    'scheduler_factor': 0.5,\n","    'scheduler_patience': 3,  # Stable patience for large dataset\n","    'scheduler_min_lr': 1e-7,\n","    'scheduler_monitor': 'validation F1',\n","\n","    # Loss function configuration\n","    'use_focal_loss': USE_FOCAL_LOSS,\n","    'focal_loss_gamma': FOCAL_LOSS_GAMMA,\n","    'focal_loss_alpha_weights': FOCAL_LOSS_ALPHA_WEIGHTS,\n","    'crossentropy_class_weights': CROSSENTROPY_CLASS_WEIGHTS,\n","\n","    # Dataset configuration - v9 MFS specific\n","    'dataset_version': 'v9',\n","    'frame_strategy': 'multi_frame_sequence',\n","    'frame_types': ['multiple_frames_per_video'],\n","    'training_approach': 'frame_level_independent',\n","    'inference_strategy': 'frame_level_evaluation',\n","\n","    # Regularization configuration\n","    'label_smoothing': 0.0,\n","    'mixup_alpha': 0.0,\n","    'cutmix_alpha': 0.0\n","}\n","\n","# Optimized Focal Loss implementation\n","class OptimizedFocalLoss(nn.Module):\n","    \"\"\"\n","    Optimized Focal Loss with per-class alpha weights\n","    Handles class imbalance with normalized alpha weights (sum = 1.0)\n","    \"\"\"\n","    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n","        super(OptimizedFocalLoss, self).__init__()\n","        if alpha is not None:\n","            self.alpha = torch.tensor(alpha, dtype=torch.float32)\n","        else:\n","            self.alpha = None\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        if self.alpha is not None and self.alpha.device != inputs.device:\n","            self.alpha = self.alpha.to(inputs.device)\n","\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = (1 - pt) ** self.gamma * ce_loss\n","\n","        if self.alpha is not None:\n","            alpha_t = self.alpha[targets]\n","            focal_loss = alpha_t * focal_loss\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","\n","# Swin Transformer CASME II Baseline Model - Enhanced Hierarchical Architecture\n","class SwinCASME2Baseline(nn.Module):\n","    \"\"\"\n","    Swin Transformer baseline for CASME II micro-expression recognition\n","    Hierarchical architecture with shifted window attention\n","    \"\"\"\n","\n","    def __init__(self, num_classes=7, dropout_rate=0.2):\n","        super(SwinCASME2Baseline, self).__init__()\n","\n","        # Load pretrained Swin Transformer backbone from HuggingFace\n","        from transformers import SwinModel\n","\n","        self.swin = SwinModel.from_pretrained(\n","            CASME2_SWIN_CONFIG['swin_model'],\n","            add_pooling_layer=False\n","        )\n","\n","        # Enable fine-tuning for micro-expression domain adaptation\n","        for param in self.swin.parameters():\n","            param.requires_grad = True\n","\n","        # Get Swin feature dimensions from hierarchical structure\n","        # Swin uses embed_dim with hierarchical scaling: embed_dim * (2^(num_stages-1))\n","        base_embed_dim = self.swin.config.embed_dim\n","        num_stages = len(self.swin.config.depths)\n","        self.swin_feature_dim = base_embed_dim * (2 ** (num_stages - 1))\n","\n","        print(f\"Swin feature dimension (final stage): {self.swin_feature_dim}\")\n","        print(f\"Base embed_dim: {base_embed_dim}, Stages: {num_stages}\")\n","\n","        # Verify expected dimensions\n","        if self.swin_feature_dim != CASME2_SWIN_CONFIG['expected_hidden_dim']:\n","            print(f\"Warning: Expected {CASME2_SWIN_CONFIG['expected_hidden_dim']}, got {self.swin_feature_dim}\")\n","\n","        # Global Average Pooling for hierarchical features\n","        self.global_pool = nn.AdaptiveAvgPool1d(1)\n","\n","        # Enhanced classification head for large dataset\n","        # Architecture: feature_dim -> GAP -> 512 -> 128 -> 7 (proven effective for MFS)\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.swin_feature_dim, 512),\n","            nn.LayerNorm(512),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","\n","            nn.Linear(512, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","        print(f\"Classification head: {self.swin_feature_dim} -> GAP -> 512 -> 128 -> {num_classes}\")\n","        print(f\"Dropout rate: {dropout_rate} (balanced for large dataset)\")\n","\n","    def forward(self, pixel_values):\n","        # Swin forward pass with hierarchical feature extraction and shifted windows\n","        swin_outputs = self.swin(\n","            pixel_values=pixel_values,\n","            interpolate_pos_encoding=CASME2_SWIN_CONFIG['interpolate_pos_encoding']\n","        )\n","\n","        # Extract hierarchical features from last layer\n","        # Swin output: [batch_size, sequence_length, hidden_size]\n","        hierarchical_features = swin_outputs.last_hidden_state\n","\n","        # Global Average Pooling across spatial dimensions\n","        # [batch_size, hidden_size, sequence_length] -> [batch_size, hidden_size, 1] -> [batch_size, hidden_size]\n","        pooled_features = self.global_pool(hierarchical_features.transpose(1, 2)).squeeze(-1)\n","\n","        # Classification pipeline\n","        processed_features = self.classifier_layers(pooled_features)\n","        output = self.classifier(processed_features)\n","\n","        return output\n","\n","# Enhanced optimizer and scheduler factory\n","def create_optimizer_scheduler_casme2(model, config):\n","    \"\"\"Create optimizer and scheduler for CASME II Swin Transformer training\"\"\"\n","\n","    # AdamW optimizer with proven configuration\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=config['learning_rate'],\n","        weight_decay=config['weight_decay'],\n","        betas=(0.9, 0.999)\n","    )\n","\n","    # ReduceLROnPlateau scheduler monitoring validation F1\n","    if config['scheduler_type'] == 'plateau':\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer,\n","            mode=config['scheduler_mode'],\n","            factor=config['scheduler_factor'],\n","            patience=config['scheduler_patience'],\n","            min_lr=config['scheduler_min_lr']\n","        )\n","        print(f\"Scheduler: ReduceLROnPlateau monitoring {config['scheduler_monitor']}\")\n","    else:\n","        scheduler = None\n","\n","    return optimizer, scheduler\n","\n","# Swin Transformer Image Processor setup for 224px input\n","from transformers import AutoImageProcessor\n","\n","print(\"\\nSetting up Swin Transformer Image Processor for 224px input...\")\n","\n","swin_processor = AutoImageProcessor.from_pretrained(\n","    CASME2_SWIN_CONFIG['swin_model'],\n","    do_resize=True,\n","    size={'height': 224, 'width': 224},\n","    do_normalize=True,\n","    do_rescale=True,\n","    do_center_crop=False\n",")\n","\n","print(f\"Swin Transformer preprocessing configured:\")\n","print(f\"  Input size: 224x224px\")\n","print(f\"  Resize from: 224x224px (v9 native)\")\n","print(f\"  Window-based attention: {WINDOW_SIZE}x{WINDOW_SIZE} windows\")\n","print(f\"  Hierarchical processing: Multi-stage with shifted windows\")\n","\n","# Transform functions for Swin Transformer\n","def swint_transform_train(image):\n","    \"\"\"\n","    Training transform with Swin Transformer Image Processor\n","    Handles grayscale to RGB conversion\n","    \"\"\"\n","    # Ensure RGB format (grayscale images will be converted via channel repetition)\n","    if image.mode != 'RGB':\n","        image = image.convert('RGB')\n","\n","    inputs = swin_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","def swint_transform_val(image):\n","    \"\"\"\n","    Validation transform with Swin Transformer Image Processor\n","    Handles grayscale to RGB conversion\n","    \"\"\"\n","    # Ensure RGB format (grayscale images will be converted via channel repetition)\n","    if image.mode != 'RGB':\n","        image = image.convert('RGB')\n","\n","    inputs = swin_processor(image, return_tensors=\"pt\")\n","    return inputs['pixel_values'].squeeze(0)\n","\n","print(\"Swin Transformer Image Processor configured for 224px with hierarchical processing\")\n","\n","# CASME II Dataset for v9 MFS with flat directory structure\n","class CASME2DatasetMFS(Dataset):\n","    \"\"\"CASME II v9 Multi-Frame Sequence dataset with flexible filename parsing\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        # Load all images from flat directory structure\n","        print(f\"Loading {split} dataset from {split_path}...\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        if len(all_files) > 0:\n","            print(f\"Sample filename: {all_files[0]}\")\n","\n","        loaded_count = 0\n","        skipped_count = 0\n","\n","        for filename in sorted(all_files):\n","            # Extract emotion from filename - flexible approach\n","            # Try multiple patterns to handle different filename formats\n","            emotion_found = None\n","\n","            # Remove file extension first\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            # Pattern 1: Check if any emotion class appears in filename\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                loaded_count += 1\n","            else:\n","                skipped_count += 1\n","                if skipped_count <= 3:\n","                    print(f\"  Skipped (no emotion found): {filename}\")\n","\n","        print(f\"Loaded {len(self.images)} samples for {split} split\")\n","        if skipped_count > 0:\n","            print(f\"  Skipped {skipped_count} files (no recognizable emotion)\")\n","\n","        if len(self.images) == 0:\n","            print(f\"ERROR: No samples loaded! Check filename format and emotion labels.\")\n","            print(f\"Expected emotions in filenames: {CASME2_CLASSES}\")\n","\n","        self._print_distribution()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"  No samples to display distribution\")\n","            return\n","\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.images[idx]\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","# Create directories\n","os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/training_logs\", exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/evaluation_results\", exist_ok=True)\n","\n","print(f\"\\nDataset paths:\")\n","print(f\"Train: {DATASET_ROOT}/train\")\n","print(f\"Validation: {DATASET_ROOT}/val\")\n","print(f\"Test: {DATASET_ROOT}/test\")\n","\n","# Architecture validation\n","print(\"\\nSwin Transformer CASME II architecture validation...\")\n","\n","try:\n","    test_model = SwinCASME2Baseline(num_classes=7, dropout_rate=0.2).to(device)\n","    test_input = torch.randn(1, 3, 224, 224).to(device)\n","    test_output = test_model(test_input)\n","\n","    # Calculate expected hierarchical patches\n","    # Swin uses multi-stage downsampling with patch merging\n","    stage1_patches = (224 // CASME2_SWIN_CONFIG['patch_size']) ** 2\n","\n","    print(f\"Validation successful: Output shape {test_output.shape}\")\n","    print(f\"Initial patches (Stage 1): {stage1_patches}\")\n","    print(f\"Window size: {CASME2_SWIN_CONFIG['window_size']}x{CASME2_SWIN_CONFIG['window_size']}\")\n","    print(f\"Hierarchical processing: Multi-stage feature extraction with shifted windows\")\n","    print(f\"Swin {SWIN_MODEL_VARIANT.upper()} architecture validated\")\n","\n","    del test_model, test_input, test_output\n","    torch.cuda.empty_cache()\n","\n","except Exception as e:\n","    print(f\"Validation failed: {e}\")\n","    import traceback\n","    traceback.print_exc()\n","\n","# Loss function factory\n","def create_criterion_casme2(weights, use_focal_loss=False, alpha_weights=None, gamma=2.0):\n","    \"\"\"\n","    Factory function to create loss criterion based on configuration\n","\n","    Args:\n","        weights: Class weights for CrossEntropy\n","        use_focal_loss: Whether to use Focal Loss or CrossEntropy\n","        alpha_weights: Per-class alpha weights for Focal Loss\n","        gamma: Focal loss gamma parameter\n","\n","    Returns:\n","        Loss function\n","    \"\"\"\n","    if use_focal_loss:\n","        print(f\"Using Optimized Focal Loss with gamma={gamma}\")\n","        if alpha_weights:\n","            print(f\"Per-class alpha weights: {alpha_weights}\")\n","            print(f\"Alpha sum: {sum(alpha_weights):.3f}\")\n","        return OptimizedFocalLoss(alpha=alpha_weights, gamma=gamma)\n","    else:\n","        print(f\"Using CrossEntropy Loss with optimized class weights\")\n","        print(f\"Class weights: {weights.cpu().numpy()}\")\n","        return nn.CrossEntropyLoss(weight=weights)\n","\n","# Global configuration for training pipeline\n","GLOBAL_CONFIG_CASME2 = {\n","    'device': device,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'num_classes': 7,\n","    'class_weights': class_weights,\n","    'class_names': CASME2_CLASSES,\n","    'class_to_idx': CLASS_TO_IDX,\n","    'transform_train': swint_transform_train,\n","    'transform_val': swint_transform_val,\n","    'swin_config': CASME2_SWIN_CONFIG,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'results_root': RESULTS_ROOT,\n","    'dataset_root': DATASET_ROOT,\n","    'optimizer_scheduler_factory': create_optimizer_scheduler_casme2,\n","    'criterion_factory': create_criterion_casme2\n","}\n","\n","# Configuration validation and summary\n","print(\"\\n\" + \"=\" * 60)\n","print(\"CASME II MULTI-FRAME SEQUENCE SWIN TRANSFORMER CONFIGURATION COMPLETE\")\n","print(\"=\" * 60)\n","\n","print(f\"Loss Configuration:\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Function: Optimized Focal Loss\")\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Per-class Alpha: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Function: CrossEntropy with Optimized Weights\")\n","    print(f\"  Class Weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","\n","print(f\"\\nModel Configuration:\")\n","print(f\"  Architecture: Swin Transformer\")\n","print(f\"  Variant: {SWIN_MODEL_VARIANT.upper()}\")\n","print(f\"  Model: {SWIN_MODEL_NAME}\")\n","print(f\"  Parameters: {MODEL_PARAMS}\")\n","print(f\"  Window Size: {WINDOW_SIZE}x{WINDOW_SIZE}\")\n","print(f\"  Patch Size: {PATCH_SIZE}x{PATCH_SIZE}\")\n","print(f\"  Input Resolution: 224px\")\n","print(f\"  Hidden Dimension: {EXPECTED_HIDDEN_DIM}\")\n","print(f\"  Hierarchical Stages: Multi-stage with shifted windows\")\n","print(f\"  Classification Head: {EXPECTED_HIDDEN_DIM} -> GAP -> 512 -> 128 -> 7 (hierarchical)\")\n","\n","print(f\"\\nDataset Configuration:\")\n","print(f\"  Version: {CASME2_SWIN_CONFIG['dataset_version']}\")\n","print(f\"  Classes: {len(CASME2_CLASSES)}\")\n","print(f\"  Frame strategy: {CASME2_SWIN_CONFIG['frame_strategy']}\")\n","print(f\"  Training approach: {CASME2_SWIN_CONFIG['training_approach']}\")\n","print(f\"  Inference strategy: {CASME2_SWIN_CONFIG['inference_strategy']}\")\n","print(f\"  Weight Optimization: {'Per-class Alpha' if USE_FOCAL_LOSS else 'Inverse Sqrt Frequency'}\")\n","\n","print(f\"\\nTraining Configuration:\")\n","print(f\"  Train samples: {preprocessing_info['splits']['train']['total_images']} frames\")\n","print(f\"  Validation samples: {preprocessing_info['splits']['val']['total_images']} frames\")\n","print(f\"  Test samples: {preprocessing_info['splits']['test']['total_images']} frames\")\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","print(f\"  Learning rate: {CASME2_SWIN_CONFIG['learning_rate']}\")\n","print(f\"  Dropout rate: {CASME2_SWIN_CONFIG['dropout_rate']}\")\n","print(f\"  Initial patches: {(224 // PATCH_SIZE) ** 2}\")\n","\n","print(\"\\nNext: Cell 2 - Dataset Loading and Multi-Frame Swin Transformer Training Pipeline\")"]},{"cell_type":"code","source":["# @title Cell 2: CASME II Multi-Frame Sequence Swin Transformer Training Pipeline\n","\n","# File: 07_02_SwinT_CASME2_MFS_Cell2.py\n","# Location: experiments/07_02_SwinT_CASME2-MFS-PREP.ipynb\n","# Purpose: Enhanced training pipeline for CASME II Multi-Frame Sequence Swin Transformer with optimized RAM caching\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing as mp\n","import shutil\n","import tempfile\n","\n","print(\"CASME II Multi-Frame Sequence Swin Transformer Training Pipeline\")\n","print(\"=\" * 70)\n","print(f\"Loss Function: {'Optimized Focal Loss' if CASME2_SWIN_CONFIG['use_focal_loss'] else 'CrossEntropy Loss'}\")\n","if CASME2_SWIN_CONFIG['use_focal_loss']:\n","    print(f\"Focal Loss Parameters:\")\n","    print(f\"  Gamma: {CASME2_SWIN_CONFIG['focal_loss_gamma']}\")\n","    print(f\"  Per-class Alpha: {CASME2_SWIN_CONFIG['focal_loss_alpha_weights']}\")\n","    print(f\"  Alpha Sum: {sum(CASME2_SWIN_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","else:\n","    print(f\"CrossEntropy Parameters:\")\n","    print(f\"  Optimized Class Weights: {CASME2_SWIN_CONFIG['crossentropy_class_weights']}\")\n","print(f\"Dataset version: {CASME2_SWIN_CONFIG['dataset_version']}\")\n","print(f\"Frame strategy: {CASME2_SWIN_CONFIG['frame_strategy']}\")\n","print(f\"Training approach: {CASME2_SWIN_CONFIG['training_approach']}\")\n","print(f\"Swin variant: {SWIN_MODEL_VARIANT.upper()}\")\n","print(f\"Window size: {WINDOW_SIZE}x{WINDOW_SIZE}\")\n","print(f\"Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\n","print(f\"Training epochs: {CASME2_SWIN_CONFIG['num_epochs']}\")\n","print(f\"Batch size: {CASME2_SWIN_CONFIG['batch_size']}\")\n","print(f\"Scheduler patience: {CASME2_SWIN_CONFIG['scheduler_patience']}\")\n","\n","# Enhanced CASME II Dataset with optimized RAM caching for large dataset\n","class CASME2DatasetTraining(Dataset):\n","    \"\"\"Enhanced CASME II dataset for training with RAM caching optimization for large dataset\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","        self.cached_images = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading CASME II {split} dataset for training...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        if len(all_files) > 0:\n","            print(f\"Sample filename: {all_files[0]}\")\n","\n","        loaded_count = 0\n","        skipped_count = 0\n","\n","        for filename in sorted(all_files):\n","            # Extract emotion from filename - flexible approach\n","            emotion_found = None\n","\n","            # Remove file extension first\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            # Check if any emotion class appears in filename\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                loaded_count += 1\n","            else:\n","                skipped_count += 1\n","                if skipped_count <= 3:\n","                    print(f\"  Skipped (no emotion found): {filename}\")\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples\")\n","        if skipped_count > 0:\n","            print(f\"  Skipped {skipped_count} files (no recognizable emotion)\")\n","\n","        if len(self.images) == 0:\n","            print(f\"ERROR: No samples loaded! Check filename format and emotion labels.\")\n","            print(f\"Expected emotions in filenames: {CASME2_CLASSES}\")\n","\n","        self._print_distribution()\n","\n","        if self.use_ram_cache and len(self.images) > 0:\n","            self._preload_to_ram()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"  No samples to display distribution\")\n","            return\n","\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def _preload_to_ram(self):\n","        \"\"\"RAM preloading with parallel loading for training efficiency\"\"\"\n","        if len(self.images) == 0:\n","            print(f\"Skipping RAM preload: No images to load\")\n","            return\n","\n","        print(f\"Preloading {len(self.images)} {self.split} images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            \"\"\"Load single image with error handling\"\"\"\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (224, 224), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=f\"Loading {self.split} to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * 224 * 224 * 3 * 4 / 1e9\n","        print(f\"{self.split.upper()} RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (224, 224), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","# Enhanced metrics calculation with comprehensive error handling\n","def calculate_metrics_safe_robust(predictions, labels, class_names, average='macro'):\n","    \"\"\"\n","    Calculate metrics with enhanced error handling and validation\n","\n","    Args:\n","        predictions: Predicted labels\n","        labels: True labels\n","        class_names: List of class names\n","        average: Averaging method for metrics\n","\n","    Returns:\n","        dict: Computed metrics\n","    \"\"\"\n","    try:\n","        predictions = np.array(predictions)\n","        labels = np.array(labels)\n","\n","        if len(predictions) == 0 or len(labels) == 0:\n","            return {\n","                'accuracy': 0.0,\n","                'precision': 0.0,\n","                'recall': 0.0,\n","                'f1_score': 0.0\n","            }\n","\n","        if len(predictions) != len(labels):\n","            print(f\"Warning: Prediction and label length mismatch: {len(predictions)} vs {len(labels)}\")\n","            min_len = min(len(predictions), len(labels))\n","            predictions = predictions[:min_len]\n","            labels = labels[:min_len]\n","\n","        accuracy = accuracy_score(labels, predictions)\n","\n","        unique_labels = np.unique(np.concatenate([labels, predictions]))\n","        labels_present = [i for i in range(len(class_names)) if i in unique_labels]\n","\n","        precision, recall, f1, _ = precision_recall_fscore_support(\n","            labels, predictions,\n","            labels=labels_present,\n","            average=average,\n","            zero_division=0\n","        )\n","\n","        return {\n","            'accuracy': float(accuracy),\n","            'precision': float(precision),\n","            'recall': float(recall),\n","            'f1_score': float(f1)\n","        }\n","\n","    except Exception as e:\n","        print(f\"Error in metrics calculation: {e}\")\n","        return {\n","            'accuracy': 0.0,\n","            'precision': 0.0,\n","            'recall': 0.0,\n","            'f1_score': 0.0\n","        }\n","\n","# Enhanced training epoch with comprehensive validation\n","def train_epoch(model, dataloader, criterion, optimizer, device, epoch, total_epochs):\n","    \"\"\"Enhanced training epoch with comprehensive validation and progress tracking\"\"\"\n","    model.train()\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_labels = []\n","    all_filenames = []\n","    batch_count = 0\n","\n","    pbar = tqdm(dataloader, desc=f\"Train Epoch {epoch+1}/{total_epochs}\")\n","\n","    for images, labels, filenames in pbar:\n","        try:\n","            images = images.to(device, non_blocking=True)\n","            labels = labels.to(device, non_blocking=True)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","\n","            if outputs is None or torch.isnan(outputs).any() or torch.isinf(outputs).any():\n","                print(f\"Warning: Invalid model outputs detected at batch {batch_count}\")\n","                continue\n","\n","            loss = criterion(outputs, labels)\n","\n","            if torch.isnan(loss) or torch.isinf(loss):\n","                print(f\"Warning: Invalid loss detected at batch {batch_count}\")\n","                continue\n","\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            batch_count += 1\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","            all_filenames.extend(filenames)\n","\n","            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n","\n","        except Exception as e:\n","            print(f\"Error in training batch {batch_count}: {e}\")\n","            continue\n","\n","    avg_loss = running_loss / max(batch_count, 1)\n","\n","    metrics = calculate_metrics_safe_robust(\n","        all_predictions, all_labels, CASME2_CLASSES, average='macro'\n","    )\n","\n","    return avg_loss, metrics, all_filenames\n","\n","# Enhanced validation epoch\n","def validate_epoch(model, dataloader, criterion, device, epoch, total_epochs):\n","    \"\"\"Enhanced validation epoch with comprehensive metrics\"\"\"\n","    model.eval()\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_labels = []\n","    all_filenames = []\n","    batch_count = 0\n","\n","    pbar = tqdm(dataloader, desc=f\"Val Epoch {epoch+1}/{total_epochs}\")\n","\n","    with torch.no_grad():\n","        for images, labels, filenames in pbar:\n","            try:\n","                images = images.to(device, non_blocking=True)\n","                labels = labels.to(device, non_blocking=True)\n","\n","                outputs = model(images)\n","\n","                if outputs is None or torch.isnan(outputs).any() or torch.isinf(outputs).any():\n","                    print(f\"Warning: Invalid validation outputs at batch {batch_count}\")\n","                    continue\n","\n","                loss = criterion(outputs, labels)\n","\n","                if torch.isnan(loss) or torch.isinf(loss):\n","                    print(f\"Warning: Invalid validation loss at batch {batch_count}\")\n","                    continue\n","\n","                running_loss += loss.item()\n","                batch_count += 1\n","\n","                _, predicted = torch.max(outputs, 1)\n","\n","                all_predictions.extend(predicted.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","                all_filenames.extend(filenames)\n","\n","                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n","\n","            except Exception as e:\n","                print(f\"Error in validation batch {batch_count}: {e}\")\n","                continue\n","\n","    avg_loss = running_loss / max(batch_count, 1)\n","\n","    metrics = calculate_metrics_safe_robust(\n","        all_predictions, all_labels, CASME2_CLASSES, average='macro'\n","    )\n","\n","    return avg_loss, metrics, all_filenames\n","\n","# Enhanced atomic checkpoint saving\n","def save_checkpoint_robust(model, optimizer, scheduler, epoch, train_metrics, val_metrics,\n","                          checkpoint_root, best_metrics, config):\n","    \"\"\"Enhanced atomic checkpoint saving with validation\"\"\"\n","    try:\n","        checkpoint_data = {\n","            'epoch': epoch + 1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","            'train_metrics': train_metrics,\n","            'val_metrics': val_metrics,\n","            'best_metrics': best_metrics,\n","            'config': config\n","        }\n","\n","        checkpoint_path = os.path.join(checkpoint_root, 'casme2_swint_mfs_best_f1.pth')\n","\n","        with tempfile.NamedTemporaryFile(mode='wb', delete=False, dir=checkpoint_root, suffix='.tmp') as tmp_file:\n","            torch.save(checkpoint_data, tmp_file.name)\n","            tmp_path = tmp_file.name\n","\n","        if os.path.exists(checkpoint_path):\n","            backup_path = checkpoint_path + '.backup'\n","            shutil.copy2(checkpoint_path, backup_path)\n","\n","        shutil.move(tmp_path, checkpoint_path)\n","\n","        if os.path.exists(checkpoint_path + '.backup'):\n","            os.remove(checkpoint_path + '.backup')\n","\n","        checkpoint_size = os.path.getsize(checkpoint_path) / (1024 * 1024)\n","\n","        if checkpoint_size < 10:\n","            print(f\"Warning: Checkpoint size unusually small ({checkpoint_size:.1f}MB)\")\n","            return None\n","\n","        return checkpoint_path\n","\n","    except Exception as e:\n","        print(f\"ERROR: Checkpoint save failed: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","        if 'tmp_path' in locals() and os.path.exists(tmp_path):\n","            os.remove(tmp_path)\n","\n","        return None\n","\n","# JSON serialization helper\n","def safe_json_serialize(obj):\n","    \"\"\"Convert numpy types to Python native types for JSON serialization\"\"\"\n","    if isinstance(obj, dict):\n","        return {key: safe_json_serialize(value) for key, value in obj.items()}\n","    elif isinstance(obj, list):\n","        return [safe_json_serialize(item) for item in obj]\n","    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n","        return int(obj)\n","    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n","        return float(obj)\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    elif isinstance(obj, torch.Tensor):\n","        return obj.cpu().numpy().tolist()\n","    else:\n","        return obj\n","\n","# Dataset loading with RAM caching\n","print(\"\\n\" + \"=\" * 70)\n","print(\"LOADING CASME II DATASETS WITH RAM CACHING\")\n","print(\"=\" * 70)\n","\n","train_dataset = CASME2DatasetTraining(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='train',\n","    transform=GLOBAL_CONFIG_CASME2['transform_train'],\n","    use_ram_cache=True\n",")\n","\n","val_dataset = CASME2DatasetTraining(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='val',\n","    transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","    use_ram_cache=True\n",")\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=GLOBAL_CONFIG_CASME2['batch_size'],\n","    shuffle=True,\n","    num_workers=GLOBAL_CONFIG_CASME2['num_workers'],\n","    pin_memory=True,\n","    persistent_workers=True if GLOBAL_CONFIG_CASME2['num_workers'] > 0 else False\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=GLOBAL_CONFIG_CASME2['batch_size'],\n","    shuffle=False,\n","    num_workers=GLOBAL_CONFIG_CASME2['num_workers'],\n","    pin_memory=True,\n","    persistent_workers=True if GLOBAL_CONFIG_CASME2['num_workers'] > 0 else False\n",")\n","\n","print(f\"\\nDataset loading completed:\")\n","print(f\"Train samples: {len(train_dataset)}, batches: {len(train_loader)}\")\n","print(f\"Validation samples: {len(val_dataset)}, batches: {len(val_loader)}\")\n","\n","# Model initialization\n","print(\"\\n\" + \"=\" * 70)\n","print(\"INITIALIZING SWIN TRANSFORMER MODEL\")\n","print(\"=\" * 70)\n","\n","model = SwinCASME2Baseline(\n","    num_classes=GLOBAL_CONFIG_CASME2['num_classes'],\n","    dropout_rate=CASME2_SWIN_CONFIG['dropout_rate']\n",").to(GLOBAL_CONFIG_CASME2['device'])\n","\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f\"Model: SwinCASME2Baseline\")\n","print(f\"Variant: {SWIN_MODEL_VARIANT.upper()}\")\n","print(f\"Total parameters: {total_params:,}\")\n","print(f\"Trainable parameters: {trainable_params:,}\")\n","print(f\"Hidden dimension: {EXPECTED_HIDDEN_DIM}\")\n","\n","# Optimizer and criterion setup\n","optimizer, scheduler = GLOBAL_CONFIG_CASME2['optimizer_scheduler_factory'](\n","    model, CASME2_SWIN_CONFIG\n",")\n","\n","criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","    weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","    use_focal_loss=CASME2_SWIN_CONFIG['use_focal_loss'],\n","    alpha_weights=CASME2_SWIN_CONFIG['focal_loss_alpha_weights'],\n","    gamma=CASME2_SWIN_CONFIG['focal_loss_gamma']\n",")\n","\n","print(f\"Optimizer: {optimizer.__class__.__name__}\")\n","print(f\"Learning rate: {CASME2_SWIN_CONFIG['learning_rate']}\")\n","print(f\"Weight decay: {CASME2_SWIN_CONFIG['weight_decay']}\")\n","print(f\"Scheduler: ReduceLROnPlateau (patience={CASME2_SWIN_CONFIG['scheduler_patience']})\")\n","\n","# Training initialization\n","print(\"\\n\" + \"=\" * 70)\n","print(\"STARTING TRAINING\")\n","print(\"=\" * 70)\n","\n","training_history = {\n","    'train_loss': [],\n","    'val_loss': [],\n","    'train_f1': [],\n","    'val_f1': [],\n","    'train_acc': [],\n","    'val_acc': [],\n","    'learning_rate': [],\n","    'epoch_time': []\n","}\n","\n","best_metrics = {\n","    'f1': 0.0,\n","    'loss': float('inf'),\n","    'accuracy': 0.0,\n","    'epoch': 0\n","}\n","\n","start_time = time.time()\n","\n","for epoch in range(CASME2_SWIN_CONFIG['num_epochs']):\n","    epoch_start_time = time.time()\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"Epoch {epoch+1}/{CASME2_SWIN_CONFIG['num_epochs']}\")\n","    print(f\"{'='*70}\")\n","\n","    # Training phase\n","    train_loss, train_metrics, train_filenames = train_epoch(\n","        model, train_loader, criterion, optimizer,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_SWIN_CONFIG['num_epochs']\n","    )\n","\n","    # Validation phase\n","    val_loss, val_metrics, val_filenames = validate_epoch(\n","        model, val_loader, criterion,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_SWIN_CONFIG['num_epochs']\n","    )\n","\n","    # Update scheduler\n","    if scheduler:\n","        scheduler.step(val_metrics['f1_score'])\n","\n","    # Record training history\n","    epoch_time = time.time() - epoch_start_time\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    training_history['train_loss'].append(float(train_loss))\n","    training_history['val_loss'].append(float(val_loss))\n","    training_history['train_f1'].append(float(train_metrics['f1_score']))\n","    training_history['val_f1'].append(float(val_metrics['f1_score']))\n","    training_history['train_acc'].append(float(train_metrics['accuracy']))\n","    training_history['val_acc'].append(float(val_metrics['accuracy']))\n","    training_history['learning_rate'].append(float(current_lr))\n","    training_history['epoch_time'].append(float(epoch_time))\n","\n","    # Print epoch summary\n","    print(f\"Train - Loss: {train_loss:.4f}, F1: {train_metrics['f1_score']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n","    print(f\"Val   - Loss: {val_loss:.4f}, F1: {val_metrics['f1_score']:.4f}, Acc: {val_metrics['accuracy']:.4f}\")\n","    print(f\"Time  - Epoch: {epoch_time:.1f}s, LR: {current_lr:.2e}\")\n","\n","    # Enhanced multi-criteria checkpoint saving logic\n","    save_model = False\n","    improvement_reason = \"\"\n","\n","    if val_metrics['f1_score'] > best_metrics['f1']:\n","        save_model = True\n","        improvement_reason = \"Higher F1\"\n","    elif val_metrics['f1_score'] == best_metrics['f1']:\n","        if val_loss < best_metrics['loss']:\n","            save_model = True\n","            improvement_reason = \"Same F1, Lower Loss\"\n","        elif val_loss == best_metrics['loss'] and val_metrics['accuracy'] > best_metrics['accuracy']:\n","            save_model = True\n","            improvement_reason = \"Same F1&Loss, Higher Accuracy\"\n","\n","    if save_model:\n","        best_metrics['f1'] = val_metrics['f1_score']\n","        best_metrics['loss'] = val_loss\n","        best_metrics['accuracy'] = val_metrics['accuracy']\n","        best_metrics['epoch'] = epoch + 1\n","\n","        best_model_path = save_checkpoint_robust(\n","            model, optimizer, scheduler, epoch,\n","            train_metrics, val_metrics, GLOBAL_CONFIG_CASME2['checkpoint_root'],\n","            best_metrics, CASME2_SWIN_CONFIG\n","        )\n","\n","        if best_model_path:\n","            print(f\"New best model: {improvement_reason} - F1: {best_metrics['f1']:.4f}\")\n","        else:\n","            print(f\"Warning: Failed to save checkpoint despite improvement\")\n","\n","    # Progress tracking\n","    elapsed_time = time.time() - start_time\n","    estimated_total = (elapsed_time / (epoch + 1)) * CASME2_SWIN_CONFIG['num_epochs']\n","    remaining_time = estimated_total - elapsed_time\n","    progress_pct = ((epoch + 1) / CASME2_SWIN_CONFIG['num_epochs']) * 100\n","\n","    print(f\"Progress: {progress_pct:.1f}% | Best F1: {best_metrics['f1']:.4f} | ETA: {remaining_time/60:.1f}min\")\n","\n","# Training completion\n","total_time = time.time() - start_time\n","actual_epochs = CASME2_SWIN_CONFIG['num_epochs']\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II MULTI-FRAME SEQUENCE SWIN TRANSFORMER TRAINING COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Training time: {total_time/60:.1f} minutes\")\n","print(f\"Epochs completed: {actual_epochs}\")\n","print(f\"Best validation F1: {best_metrics['f1']:.4f} (epoch {best_metrics['epoch']})\")\n","print(f\"Final train F1: {training_history['train_f1'][-1]:.4f}\")\n","print(f\"Final validation F1: {training_history['val_f1'][-1]:.4f}\")\n","\n","# Enhanced training documentation export\n","results_dir = GLOBAL_CONFIG_CASME2['results_root']\n","os.makedirs(f\"{results_dir}/training_logs\", exist_ok=True)\n","\n","training_history_path = f\"{results_dir}/training_logs/casme2_swint_mfs_training_history.json\"\n","\n","print(\"\\nExporting enhanced training documentation...\")\n","\n","try:\n","    training_summary = {\n","        'experiment_type': 'CASME2_SwinT_MultiFrameSequence',\n","        'experiment_configuration': {\n","            'dataset_version': CASME2_SWIN_CONFIG['dataset_version'],\n","            'frame_strategy': CASME2_SWIN_CONFIG['frame_strategy'],\n","            'training_approach': CASME2_SWIN_CONFIG['training_approach'],\n","            'inference_strategy': CASME2_SWIN_CONFIG['inference_strategy'],\n","            'loss_function': 'Optimized Focal Loss' if CASME2_SWIN_CONFIG['use_focal_loss'] else 'CrossEntropy',\n","            'weight_approach': 'Per-class Alpha (sum=1.0)' if CASME2_SWIN_CONFIG['use_focal_loss'] else 'Inverse Sqrt Frequency',\n","            'focal_loss_gamma': CASME2_SWIN_CONFIG['focal_loss_gamma'],\n","            'focal_loss_alpha_weights': CASME2_SWIN_CONFIG['focal_loss_alpha_weights'],\n","            'crossentropy_class_weights': CASME2_SWIN_CONFIG['crossentropy_class_weights'],\n","            'swin_model': SWIN_MODEL_NAME,\n","            'model_variant': SWIN_MODEL_VARIANT,\n","            'window_size': WINDOW_SIZE,\n","            'patch_size': PATCH_SIZE\n","        },\n","        'training_history': safe_json_serialize(training_history),\n","        'best_val_f1': float(best_metrics['f1']),\n","        'best_val_loss': float(best_metrics['loss']),\n","        'best_val_accuracy': float(best_metrics['accuracy']),\n","        'best_epoch': int(best_metrics['epoch']),\n","        'total_epochs': int(actual_epochs),\n","        'total_time_minutes': float(total_time / 60),\n","        'average_epoch_time_seconds': float(np.mean(training_history['epoch_time'])),\n","        'config': safe_json_serialize(CASME2_SWIN_CONFIG),\n","        'final_train_f1': float(training_history['train_f1'][-1]),\n","        'final_val_f1': float(training_history['val_f1'][-1]),\n","        'model_checkpoint': 'casme2_swint_mfs_best_f1.pth',\n","        'dataset_info': {\n","            'name': 'CASME_II',\n","            'version': CASME2_SWIN_CONFIG['dataset_version'],\n","            'frame_strategy': CASME2_SWIN_CONFIG['frame_strategy'],\n","            'train_samples': len(train_dataset),\n","            'val_samples': len(val_dataset),\n","            'num_classes': 7,\n","            'class_names': CASME2_CLASSES\n","        },\n","        'architecture_info': {\n","            'model_type': 'SwinCASME2Baseline',\n","            'backbone': SWIN_MODEL_NAME,\n","            'variant': SWIN_MODEL_VARIANT,\n","            'window_size': WINDOW_SIZE,\n","            'patch_size': PATCH_SIZE,\n","            'input_size': f\"{CASME2_SWIN_CONFIG['input_size']}x{CASME2_SWIN_CONFIG['input_size']}\",\n","            'expected_hidden_dim': EXPECTED_HIDDEN_DIM,\n","            'classification_head': f\"{EXPECTED_HIDDEN_DIM}->GAP->512->128->7\",\n","            'hierarchical_attention': 'shifted_window'\n","        },\n","        'enhanced_features': {\n","            'hardened_checkpoint_system': True,\n","            'atomic_checkpoint_save': True,\n","            'checkpoint_validation': True,\n","            'model_output_validation': True,\n","            'enhanced_error_handling': True,\n","            'multi_criteria_checkpoint_logic': True,\n","            'memory_optimized_training': True,\n","            'ram_caching': True,\n","            'hierarchical_feature_extraction': True\n","        }\n","    }\n","\n","    with open(training_history_path, 'w') as f:\n","        json.dump(training_summary, f, indent=2)\n","\n","    print(f\"Enhanced training documentation saved successfully: {training_history_path}\")\n","    print(f\"Experiment details: {training_summary['experiment_configuration']['loss_function']} loss\")\n","    if CASME2_SWIN_CONFIG['use_focal_loss']:\n","        print(f\"  Gamma: {CASME2_SWIN_CONFIG['focal_loss_gamma']}, Alpha Sum: {sum(CASME2_SWIN_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","    print(f\"Model variant: {SWIN_MODEL_VARIANT.upper()}\")\n","    print(f\"Window size: {WINDOW_SIZE}x{WINDOW_SIZE}, Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\n","    print(f\"Dataset version: {CASME2_SWIN_CONFIG['dataset_version']}\")\n","\n","except Exception as e:\n","    print(f\"Warning: Could not save training documentation: {e}\")\n","    print(\"Training completed successfully, but documentation export failed\")\n","\n","# Enhanced memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\nNext: Cell 3 - CASME II Multi-Frame Sequence Swin Transformer Evaluation\")\n","print(\"Enhanced training pipeline completed successfully!\")"],"metadata":{"id":"LIPmKr6vx8pu","cellView":"form","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761282710570,"user_tz":-420,"elapsed":2696644,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"f73864c8-70ed-4c94-f1d2-d1518a4ee785"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II Multi-Frame Sequence Swin Transformer Training Pipeline\n","======================================================================\n","Loss Function: Optimized Focal Loss\n","Focal Loss Parameters:\n","  Gamma: 2.0\n","  Per-class Alpha: [0.048, 0.06, 0.085, 0.093, 0.095, 0.191, 0.427]\n","  Alpha Sum: 0.999\n","Dataset version: v9\n","Frame strategy: multi_frame_sequence\n","Training approach: frame_level_independent\n","Swin variant: BASE\n","Window size: 7x7\n","Patch size: 4x4\n","Training epochs: 50\n","Batch size: 16\n","Scheduler patience: 3\n","\n","======================================================================\n","LOADING CASME II DATASETS WITH RAM CACHING\n","======================================================================\n","Loading CASME II train dataset for training...\n","Found 2613 image files in directory\n","Sample filename: sub13_EP01_01_apex_p+1_others.jpg\n","Loaded 2613 CASME II train samples\n","  others: 1027 samples (39.3%)\n","  disgust: 650 samples (24.9%)\n","  happiness: 325 samples (12.4%)\n","  repression: 273 samples (10.4%)\n","  surprise: 260 samples (10.0%)\n","  sadness: 65 samples (2.5%)\n","  fear: 13 samples (0.5%)\n","Preloading 2613 train images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading train to RAM: 100%|██████████| 2613/2613 [00:54<00:00, 48.33it/s] \n"]},{"output_type":"stream","name":"stdout","text":["TRAIN RAM caching completed: 2613/2613 images, ~1.57GB\n","Loading CASME II val dataset for training...\n","Found 78 image files in directory\n","Sample filename: sub01_EP03_02_onset_others.jpg\n","Loaded 78 CASME II val samples\n","  others: 30 samples (38.5%)\n","  disgust: 18 samples (23.1%)\n","  happiness: 9 samples (11.5%)\n","  repression: 9 samples (11.5%)\n","  surprise: 6 samples (7.7%)\n","  sadness: 3 samples (3.8%)\n","  fear: 3 samples (3.8%)\n","Preloading 78 val images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading val to RAM: 100%|██████████| 78/78 [00:03<00:00, 20.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["VAL RAM caching completed: 78/78 images, ~0.05GB\n","\n","Dataset loading completed:\n","Train samples: 2613, batches: 164\n","Validation samples: 78, batches: 5\n","\n","======================================================================\n","INITIALIZING SWIN TRANSFORMER MODEL\n","======================================================================\n","Swin feature dimension (final stage): 1024\n","Base embed_dim: 128, Stages: 4\n","Classification head: 1024 -> GAP -> 512 -> 128 -> 7\n","Dropout rate: 0.3 (balanced for large dataset)\n","Model: SwinCASME2Baseline\n","Variant: BASE\n","Total parameters: 87,335,871\n","Trainable parameters: 87,335,871\n","Hidden dimension: 1024\n","Scheduler: ReduceLROnPlateau monitoring validation F1\n","Using Optimized Focal Loss with gamma=2.0\n","Per-class alpha weights: [0.048, 0.06, 0.085, 0.093, 0.095, 0.191, 0.427]\n","Alpha sum: 0.999\n","Optimizer: AdamW\n","Learning rate: 2e-05\n","Weight decay: 1e-05\n","Scheduler: ReduceLROnPlateau (patience=3)\n","\n","======================================================================\n","STARTING TRAINING\n","======================================================================\n","\n","======================================================================\n","Epoch 1/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 1/50: 100%|██████████| 164/164 [00:50<00:00,  3.25it/s, loss=0.0113]\n","Val Epoch 1/50: 100%|██████████| 5/5 [00:00<00:00,  7.35it/s, loss=0.0952]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0615, F1: 0.4235, Acc: 0.5587\n","Val   - Loss: 0.1520, F1: 0.3059, Acc: 0.4231\n","Time  - Epoch: 51.1s, LR: 2.00e-05\n","New best model: Higher F1 - F1: 0.3059\n","Progress: 2.0% | Best F1: 0.3059 | ETA: 43.6min\n","\n","======================================================================\n","Epoch 2/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 2/50: 100%|██████████| 164/164 [00:52<00:00,  3.15it/s, loss=0.0090]\n","Val Epoch 2/50: 100%|██████████| 5/5 [00:00<00:00,  9.44it/s, loss=0.1113]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0181, F1: 0.8797, Acc: 0.8833\n","Val   - Loss: 0.1801, F1: 0.1288, Acc: 0.3333\n","Time  - Epoch: 52.6s, LR: 2.00e-05\n","Progress: 4.0% | Best F1: 0.3059 | ETA: 42.4min\n","\n","======================================================================\n","Epoch 3/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 3/50: 100%|██████████| 164/164 [00:51<00:00,  3.15it/s, loss=0.0023]\n","Val Epoch 3/50: 100%|██████████| 5/5 [00:00<00:00,  9.75it/s, loss=0.1056]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0067, F1: 0.9652, Acc: 0.9656\n","Val   - Loss: 0.1921, F1: 0.1364, Acc: 0.3462\n","Time  - Epoch: 52.5s, LR: 2.00e-05\n","Progress: 6.0% | Best F1: 0.3059 | ETA: 41.4min\n","\n","======================================================================\n","Epoch 4/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 4/50: 100%|██████████| 164/164 [00:51<00:00,  3.16it/s, loss=0.0013]\n","Val Epoch 4/50: 100%|██████████| 5/5 [00:00<00:00,  9.40it/s, loss=0.1130]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0027, F1: 0.9904, Acc: 0.9900\n","Val   - Loss: 0.2076, F1: 0.1301, Acc: 0.3333\n","Time  - Epoch: 52.5s, LR: 2.00e-05\n","Progress: 8.0% | Best F1: 0.3059 | ETA: 40.5min\n","\n","======================================================================\n","Epoch 5/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 5/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0015]\n","Val Epoch 5/50: 100%|██████████| 5/5 [00:00<00:00,  9.50it/s, loss=0.1603]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0023, F1: 0.9895, Acc: 0.9885\n","Val   - Loss: 0.2118, F1: 0.1526, Acc: 0.3333\n","Time  - Epoch: 52.8s, LR: 1.00e-05\n","Progress: 10.0% | Best F1: 0.3059 | ETA: 39.6min\n","\n","======================================================================\n","Epoch 6/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 6/50: 100%|██████████| 164/164 [00:52<00:00,  3.15it/s, loss=0.0004]\n","Val Epoch 6/50: 100%|██████████| 5/5 [00:00<00:00,  9.54it/s, loss=0.1522]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0014, F1: 0.9928, Acc: 0.9943\n","Val   - Loss: 0.2208, F1: 0.1298, Acc: 0.3077\n","Time  - Epoch: 52.7s, LR: 1.00e-05\n","Progress: 12.0% | Best F1: 0.3059 | ETA: 38.7min\n","\n","======================================================================\n","Epoch 7/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 7/50: 100%|██████████| 164/164 [00:52<00:00,  3.15it/s, loss=0.0005]\n","Val Epoch 7/50: 100%|██████████| 5/5 [00:00<00:00,  9.55it/s, loss=0.1515]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0010, F1: 0.9981, Acc: 0.9973\n","Val   - Loss: 0.2196, F1: 0.1392, Acc: 0.3462\n","Time  - Epoch: 52.7s, LR: 1.00e-05\n","Progress: 14.0% | Best F1: 0.3059 | ETA: 37.8min\n","\n","======================================================================\n","Epoch 8/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 8/50: 100%|██████████| 164/164 [00:52<00:00,  3.15it/s, loss=0.0007]\n","Val Epoch 8/50: 100%|██████████| 5/5 [00:00<00:00,  9.36it/s, loss=0.1608]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0010, F1: 0.9985, Acc: 0.9985\n","Val   - Loss: 0.2247, F1: 0.1345, Acc: 0.3462\n","Time  - Epoch: 52.6s, LR: 1.00e-05\n","Progress: 16.0% | Best F1: 0.3059 | ETA: 36.9min\n","\n","======================================================================\n","Epoch 9/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 9/50: 100%|██████████| 164/164 [00:52<00:00,  3.15it/s, loss=0.0009]\n","Val Epoch 9/50: 100%|██████████| 5/5 [00:00<00:00,  9.36it/s, loss=0.1658]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0010, F1: 0.9979, Acc: 0.9977\n","Val   - Loss: 0.2317, F1: 0.1255, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 5.00e-06\n","Progress: 18.0% | Best F1: 0.3059 | ETA: 36.0min\n","\n","======================================================================\n","Epoch 10/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 10/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0008]\n","Val Epoch 10/50: 100%|██████████| 5/5 [00:00<00:00,  9.56it/s, loss=0.1693]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0007, F1: 0.9990, Acc: 0.9989\n","Val   - Loss: 0.2334, F1: 0.1208, Acc: 0.3077\n","Time  - Epoch: 52.8s, LR: 5.00e-06\n","Progress: 20.0% | Best F1: 0.3059 | ETA: 35.1min\n","\n","======================================================================\n","Epoch 11/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 11/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0001]\n","Val Epoch 11/50: 100%|██████████| 5/5 [00:00<00:00,  9.40it/s, loss=0.1726]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2364, F1: 0.1298, Acc: 0.3333\n","Time  - Epoch: 52.8s, LR: 5.00e-06\n","Progress: 22.0% | Best F1: 0.3059 | ETA: 34.3min\n","\n","======================================================================\n","Epoch 12/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 12/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0015]\n","Val Epoch 12/50: 100%|██████████| 5/5 [00:00<00:00,  9.41it/s, loss=0.1745]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 0.9997, Acc: 0.9996\n","Val   - Loss: 0.2416, F1: 0.1311, Acc: 0.3333\n","Time  - Epoch: 52.8s, LR: 5.00e-06\n","Progress: 24.0% | Best F1: 0.3059 | ETA: 33.4min\n","\n","======================================================================\n","Epoch 13/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 13/50: 100%|██████████| 164/164 [00:52<00:00,  3.15it/s, loss=0.0004]\n","Val Epoch 13/50: 100%|██████████| 5/5 [00:00<00:00,  9.41it/s, loss=0.1738]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2400, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 2.50e-06\n","Progress: 26.0% | Best F1: 0.3059 | ETA: 32.5min\n","\n","======================================================================\n","Epoch 14/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 14/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0004]\n","Val Epoch 14/50: 100%|██████████| 5/5 [00:00<00:00,  9.58it/s, loss=0.1592]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 0.9998, Acc: 0.9996\n","Val   - Loss: 0.2355, F1: 0.1333, Acc: 0.3462\n","Time  - Epoch: 52.7s, LR: 2.50e-06\n","Progress: 28.0% | Best F1: 0.3059 | ETA: 31.6min\n","\n","======================================================================\n","Epoch 15/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 15/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0003]\n","Val Epoch 15/50: 100%|██████████| 5/5 [00:00<00:00,  9.38it/s, loss=0.1727]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2403, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 2.50e-06\n","Progress: 30.0% | Best F1: 0.3059 | ETA: 30.8min\n","\n","======================================================================\n","Epoch 16/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 16/50: 100%|██████████| 164/164 [00:52<00:00,  3.13it/s, loss=0.0002]\n","Val Epoch 16/50: 100%|██████████| 5/5 [00:00<00:00,  9.33it/s, loss=0.1757]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2412, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 53.0s, LR: 2.50e-06\n","Progress: 32.0% | Best F1: 0.3059 | ETA: 29.9min\n","\n","======================================================================\n","Epoch 17/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 17/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0006]\n","Val Epoch 17/50: 100%|██████████| 5/5 [00:00<00:00,  9.18it/s, loss=0.1735]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2415, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.25e-06\n","Progress: 34.0% | Best F1: 0.3059 | ETA: 29.0min\n","\n","======================================================================\n","Epoch 18/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 18/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0002]\n","Val Epoch 18/50: 100%|██████████| 5/5 [00:00<00:00,  9.48it/s, loss=0.1735]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 0.9997, Acc: 0.9996\n","Val   - Loss: 0.2406, F1: 0.1284, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 1.25e-06\n","Progress: 36.0% | Best F1: 0.3059 | ETA: 28.1min\n","\n","======================================================================\n","Epoch 19/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 19/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0005]\n","Val Epoch 19/50: 100%|██████████| 5/5 [00:00<00:00,  9.53it/s, loss=0.1728]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2419, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 1.25e-06\n","Progress: 38.0% | Best F1: 0.3059 | ETA: 27.3min\n","\n","======================================================================\n","Epoch 20/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 20/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0003]\n","Val Epoch 20/50: 100%|██████████| 5/5 [00:00<00:00,  9.41it/s, loss=0.1746]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2421, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.25e-06\n","Progress: 40.0% | Best F1: 0.3059 | ETA: 26.4min\n","\n","======================================================================\n","Epoch 21/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 21/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0004]\n","Val Epoch 21/50: 100%|██████████| 5/5 [00:00<00:00,  9.48it/s, loss=0.1778]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2454, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 6.25e-07\n","Progress: 42.0% | Best F1: 0.3059 | ETA: 25.5min\n","\n","======================================================================\n","Epoch 22/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 22/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0006]\n","Val Epoch 22/50: 100%|██████████| 5/5 [00:00<00:00,  9.46it/s, loss=0.1767]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2452, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 6.25e-07\n","Progress: 44.0% | Best F1: 0.3059 | ETA: 24.6min\n","\n","======================================================================\n","Epoch 23/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 23/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0002]\n","Val Epoch 23/50: 100%|██████████| 5/5 [00:00<00:00,  9.70it/s, loss=0.1755]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2436, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 6.25e-07\n","Progress: 46.0% | Best F1: 0.3059 | ETA: 23.7min\n","\n","======================================================================\n","Epoch 24/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 24/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0008]\n","Val Epoch 24/50: 100%|██████████| 5/5 [00:00<00:00,  9.43it/s, loss=0.1765]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2450, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 6.25e-07\n","Progress: 48.0% | Best F1: 0.3059 | ETA: 22.9min\n","\n","======================================================================\n","Epoch 25/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 25/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0004]\n","Val Epoch 25/50: 100%|██████████| 5/5 [00:00<00:00,  9.65it/s, loss=0.1754]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2444, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 3.13e-07\n","Progress: 50.0% | Best F1: 0.3059 | ETA: 22.0min\n","\n","======================================================================\n","Epoch 26/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 26/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0001]\n","Val Epoch 26/50: 100%|██████████| 5/5 [00:00<00:00,  9.41it/s, loss=0.1748]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2449, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 3.13e-07\n","Progress: 52.0% | Best F1: 0.3059 | ETA: 21.1min\n","\n","======================================================================\n","Epoch 27/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 27/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0002]\n","Val Epoch 27/50: 100%|██████████| 5/5 [00:00<00:00,  9.48it/s, loss=0.1743]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2452, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 3.13e-07\n","Progress: 54.0% | Best F1: 0.3059 | ETA: 20.2min\n","\n","======================================================================\n","Epoch 28/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 28/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0004]\n","Val Epoch 28/50: 100%|██████████| 5/5 [00:00<00:00,  9.52it/s, loss=0.1741]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2451, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 3.13e-07\n","Progress: 56.0% | Best F1: 0.3059 | ETA: 19.3min\n","\n","======================================================================\n","Epoch 29/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 29/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0001]\n","Val Epoch 29/50: 100%|██████████| 5/5 [00:00<00:00,  9.48it/s, loss=0.1746]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2457, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 1.56e-07\n","Progress: 58.0% | Best F1: 0.3059 | ETA: 18.5min\n","\n","======================================================================\n","Epoch 30/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 30/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0001]\n","Val Epoch 30/50: 100%|██████████| 5/5 [00:00<00:00,  9.39it/s, loss=0.1779]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2466, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.56e-07\n","Progress: 60.0% | Best F1: 0.3059 | ETA: 17.6min\n","\n","======================================================================\n","Epoch 31/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 31/50: 100%|██████████| 164/164 [00:52<00:00,  3.13it/s, loss=0.0003]\n","Val Epoch 31/50: 100%|██████████| 5/5 [00:00<00:00,  9.63it/s, loss=0.1783]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2474, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 53.0s, LR: 1.56e-07\n","Progress: 62.0% | Best F1: 0.3059 | ETA: 16.7min\n","\n","======================================================================\n","Epoch 32/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 32/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0003]\n","Val Epoch 32/50: 100%|██████████| 5/5 [00:00<00:00,  9.41it/s, loss=0.1788]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2475, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.56e-07\n","Progress: 64.0% | Best F1: 0.3059 | ETA: 15.8min\n","\n","======================================================================\n","Epoch 33/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 33/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0002]\n","Val Epoch 33/50: 100%|██████████| 5/5 [00:00<00:00,  9.39it/s, loss=0.1822]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 0.9998, Acc: 0.9996\n","Val   - Loss: 0.2480, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.00e-07\n","Progress: 66.0% | Best F1: 0.3059 | ETA: 14.9min\n","\n","======================================================================\n","Epoch 34/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 34/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0005]\n","Val Epoch 34/50: 100%|██████████| 5/5 [00:00<00:00,  9.47it/s, loss=0.1825]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2484, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 1.00e-07\n","Progress: 68.0% | Best F1: 0.3059 | ETA: 14.1min\n","\n","======================================================================\n","Epoch 35/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 35/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0003]\n","Val Epoch 35/50: 100%|██████████| 5/5 [00:00<00:00,  9.33it/s, loss=0.1793]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2469, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 1.00e-07\n","Progress: 70.0% | Best F1: 0.3059 | ETA: 13.2min\n","\n","======================================================================\n","Epoch 36/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 36/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0001]\n","Val Epoch 36/50: 100%|██████████| 5/5 [00:00<00:00,  9.48it/s, loss=0.1797]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2473, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.00e-07\n","Progress: 72.0% | Best F1: 0.3059 | ETA: 12.3min\n","\n","======================================================================\n","Epoch 37/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 37/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0003]\n","Val Epoch 37/50: 100%|██████████| 5/5 [00:00<00:00,  9.45it/s, loss=0.1813]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2479, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.00e-07\n","Progress: 74.0% | Best F1: 0.3059 | ETA: 11.4min\n","\n","======================================================================\n","Epoch 38/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 38/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0007]\n","Val Epoch 38/50: 100%|██████████| 5/5 [00:00<00:00,  9.41it/s, loss=0.1809]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2478, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.00e-07\n","Progress: 76.0% | Best F1: 0.3059 | ETA: 10.6min\n","\n","======================================================================\n","Epoch 39/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 39/50: 100%|██████████| 164/164 [00:52<00:00,  3.15it/s, loss=0.0004]\n","Val Epoch 39/50: 100%|██████████| 5/5 [00:00<00:00,  9.71it/s, loss=0.1808]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 0.9997, Acc: 0.9996\n","Val   - Loss: 0.2478, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.6s, LR: 1.00e-07\n","Progress: 78.0% | Best F1: 0.3059 | ETA: 9.7min\n","\n","======================================================================\n","Epoch 40/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 40/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0013]\n","Val Epoch 40/50: 100%|██████████| 5/5 [00:00<00:00,  9.40it/s, loss=0.1802]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2477, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.00e-07\n","Progress: 80.0% | Best F1: 0.3059 | ETA: 8.8min\n","\n","======================================================================\n","Epoch 41/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 41/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0004]\n","Val Epoch 41/50: 100%|██████████| 5/5 [00:00<00:00,  9.47it/s, loss=0.1793]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2472, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 1.00e-07\n","Progress: 82.0% | Best F1: 0.3059 | ETA: 7.9min\n","\n","======================================================================\n","Epoch 42/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 42/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0001]\n","Val Epoch 42/50: 100%|██████████| 5/5 [00:00<00:00,  9.41it/s, loss=0.1790]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2478, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.00e-07\n","Progress: 84.0% | Best F1: 0.3059 | ETA: 7.0min\n","\n","======================================================================\n","Epoch 43/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 43/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0003]\n","Val Epoch 43/50: 100%|██████████| 5/5 [00:00<00:00,  9.38it/s, loss=0.1809]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2488, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.00e-07\n","Progress: 86.0% | Best F1: 0.3059 | ETA: 6.2min\n","\n","======================================================================\n","Epoch 44/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 44/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0002]\n","Val Epoch 44/50: 100%|██████████| 5/5 [00:00<00:00,  9.33it/s, loss=0.1810]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2486, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 1.00e-07\n","Progress: 88.0% | Best F1: 0.3059 | ETA: 5.3min\n","\n","======================================================================\n","Epoch 45/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 45/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0122]\n","Val Epoch 45/50: 100%|██████████| 5/5 [00:00<00:00,  9.42it/s, loss=0.1778]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 0.9995, Acc: 0.9992\n","Val   - Loss: 0.2473, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.00e-07\n","Progress: 90.0% | Best F1: 0.3059 | ETA: 4.4min\n","\n","======================================================================\n","Epoch 46/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 46/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0001]\n","Val Epoch 46/50: 100%|██████████| 5/5 [00:00<00:00,  9.43it/s, loss=0.1774]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2471, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 1.00e-07\n","Progress: 92.0% | Best F1: 0.3059 | ETA: 3.5min\n","\n","======================================================================\n","Epoch 47/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 47/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0001]\n","Val Epoch 47/50: 100%|██████████| 5/5 [00:00<00:00,  9.40it/s, loss=0.1767]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2476, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.00e-07\n","Progress: 94.0% | Best F1: 0.3059 | ETA: 2.6min\n","\n","======================================================================\n","Epoch 48/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 48/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0002]\n","Val Epoch 48/50: 100%|██████████| 5/5 [00:00<00:00,  9.67it/s, loss=0.1769]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2477, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.8s, LR: 1.00e-07\n","Progress: 96.0% | Best F1: 0.3059 | ETA: 1.8min\n","\n","======================================================================\n","Epoch 49/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 49/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0005]\n","Val Epoch 49/50: 100%|██████████| 5/5 [00:00<00:00,  9.34it/s, loss=0.1768]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2475, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 1.00e-07\n","Progress: 98.0% | Best F1: 0.3059 | ETA: 0.9min\n","\n","======================================================================\n","Epoch 50/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 50/50: 100%|██████████| 164/164 [00:52<00:00,  3.14it/s, loss=0.0005]\n","Val Epoch 50/50: 100%|██████████| 5/5 [00:00<00:00,  9.36it/s, loss=0.1775]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2481, F1: 0.1263, Acc: 0.3205\n","Time  - Epoch: 52.7s, LR: 1.00e-07\n","Progress: 100.0% | Best F1: 0.3059 | ETA: 0.0min\n","\n","======================================================================\n","CASME II MULTI-FRAME SEQUENCE SWIN TRANSFORMER TRAINING COMPLETED\n","======================================================================\n","Training time: 44.0 minutes\n","Epochs completed: 50\n","Best validation F1: 0.3059 (epoch 1)\n","Final train F1: 1.0000\n","Final validation F1: 0.1263\n","\n","Exporting enhanced training documentation...\n","Enhanced training documentation saved successfully: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/07_02_swint_casme2_mfs_prep/training_logs/casme2_swint_mfs_training_history.json\n","Experiment details: Optimized Focal Loss loss\n","  Gamma: 2.0, Alpha Sum: 0.999\n","Model variant: BASE\n","Window size: 7x7, Patch size: 4x4\n","Dataset version: v9\n","\n","Next: Cell 3 - CASME II Multi-Frame Sequence Swin Transformer Evaluation\n","Enhanced training pipeline completed successfully!\n"]}]},{"cell_type":"code","source":["# @title Cell 3: CASME II Swin Transformer Evaluation with Dual Dataset Support\n","\n","# File: 07_02_SwinT_CASME2_MFS_Cell3.py\n","# Location: experiments/07_02_SwinT_CASME2-MFS-PREP.ipynb\n","# Purpose: Comprehensive evaluation framework with support for AF (v7) and KFS (v8) test datasets\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from datetime import datetime\n","from collections import defaultdict\n","\n","from sklearn.metrics import (\n","    accuracy_score, precision_recall_fscore_support,\n","    classification_report, confusion_matrix,\n","    roc_curve, auc\n",")\n","from sklearn.preprocessing import label_binarize\n","from concurrent.futures import ThreadPoolExecutor\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =====================================================\n","# DUAL DATASET EVALUATION CONFIGURATION\n","# =====================================================\n","# Configure which test datasets to evaluate:\n","# 'v7' = Apex Frame preprocessing (28 samples, frame-level evaluation)\n","# 'v8' = Key Frame Sequence preprocessing (84 frames -> 28 videos with late fusion)\n","\n","EVALUATE_DATASETS = ['v7', 'v8']  # Can be ['v7'], ['v8'], or ['v7', 'v8']\n","\n","print(\"CASME II Swin Transformer Evaluation Framework with Dual Dataset Support\")\n","print(\"=\" * 60)\n","print(f\"Datasets to evaluate: {EVALUATE_DATASETS}\")\n","print(\"=\" * 60)\n","\n","# =====================================================\n","# DATASET CONFIGURATION FUNCTION\n","# =====================================================\n","\n","def get_test_dataset_config(version, project_root):\n","    \"\"\"\n","    Get test dataset configuration based on version\n","\n","    Args:\n","        version: 'v7' (AF) or 'v8' (KFS)\n","        project_root: Project root path\n","\n","    Returns:\n","        dict: Configuration for selected test dataset\n","    \"\"\"\n","    if version == 'v7':\n","        config = {\n","            'version': 'v7',\n","            'variant': 'AF',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/preprocessed_v7\",\n","            'preprocessing_summary': 'preprocessing_summary.json',\n","            'description': 'Apex Frame with Face-Aware Preprocessing',\n","            'expected_samples': 28,\n","            'frame_strategy': 'apex_frame',\n","            'evaluation_mode': 'frame_level',\n","            'aggregation': None\n","        }\n","    elif version == 'v8':\n","        config = {\n","            'version': 'v8',\n","            'variant': 'KFS',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/preprocessed_v8\",\n","            'preprocessing_summary': 'preprocessing_summary.json',\n","            'description': 'Key Frame Sequence with Face-Aware Preprocessing',\n","            'expected_frames': 84,\n","            'expected_videos': 28,\n","            'frame_strategy': 'key_frame_sequence',\n","            'frame_types': ['onset', 'apex', 'offset'],\n","            'evaluation_mode': 'video_level',\n","            'aggregation': 'late_fusion'\n","        }\n","    else:\n","        raise ValueError(f\"Invalid version: {version}. Must be 'v7' or 'v8'\")\n","\n","    return config\n","\n","# =====================================================\n","# VIDEO ID EXTRACTION FOR KFS LATE FUSION\n","# =====================================================\n","\n","def extract_video_id_from_filename(filename):\n","    \"\"\"\n","    Extract video ID from KFS filename by removing frame type suffix\n","\n","    Expected format: sub01_EP02_01f_happiness_onset.jpg\n","    Video ID: sub01_EP02_01f_happiness\n","\n","    Args:\n","        filename: Image filename with frame type\n","\n","    Returns:\n","        str: Video ID without frame type\n","    \"\"\"\n","    # Remove file extension\n","    name_without_ext = filename.rsplit('.', 1)[0]\n","\n","    # Remove frame type suffix (onset, apex, offset)\n","    for frame_type in ['onset', 'apex', 'offset']:\n","        if name_without_ext.endswith(f'_{frame_type}'):\n","            video_id = name_without_ext.rsplit(f'_{frame_type}', 1)[0]\n","            return video_id\n","\n","    # If no frame type found, return as is\n","    return name_without_ext\n","\n","# =====================================================\n","# ENHANCED TEST DATASET CLASS\n","# =====================================================\n","\n","class CASME2DatasetEvaluation(Dataset):\n","    \"\"\"Enhanced CASME II test dataset with comprehensive evaluation support\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","        self.emotions = []\n","        self.video_ids = []\n","        self.cached_images = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading CASME II {split} dataset for evaluation...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        loaded_count = 0\n","\n","        for filename in sorted(all_files):\n","            emotion_found = None\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                video_id = extract_video_id_from_filename(filename)\n","\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                self.emotions.append(emotion_found)\n","                self.video_ids.append(video_id)\n","                loaded_count += 1\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples for evaluation\")\n","        self._print_evaluation_distribution()\n","\n","        if self.use_ram_cache and len(self.images) > 0:\n","            self._preload_to_ram_evaluation()\n","\n","    def _print_evaluation_distribution(self):\n","        \"\"\"Print comprehensive distribution for evaluation analysis\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"No test samples found!\")\n","            return\n","\n","        label_counts = {}\n","        unique_videos = set(self.video_ids)\n","\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        print(\"Test set class distribution:\")\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","        print(f\"Unique video IDs: {len(unique_videos)}\")\n","\n","        missing_classes = []\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i not in label_counts:\n","                missing_classes.append(class_name)\n","\n","        if missing_classes:\n","            print(f\"Missing classes in test set: {missing_classes}\")\n","\n","    def _preload_to_ram_evaluation(self):\n","        \"\"\"RAM preloading optimized for evaluation\"\"\"\n","        if len(self.images) == 0:\n","            return\n","\n","        print(f\"Preloading {len(self.images)} test images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (224, 224), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=\"Loading test set to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * 224 * 224 * 3 * 4 / 1e9\n","        print(f\"TEST RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (224, 224), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","# =====================================================\n","# MODEL LOADING\n","# =====================================================\n","\n","def load_trained_model_casme2(checkpoint_path, device):\n","    \"\"\"Load trained Swin Transformer model from checkpoint with enhanced validation\"\"\"\n","    print(f\"\\nValidating checkpoint availability...\")\n","    print(f\"Expected checkpoint: {os.path.basename(checkpoint_path)}\")\n","    print(f\"Full path: {checkpoint_path}\")\n","\n","    # Check if checkpoint directory exists\n","    checkpoint_dir = os.path.dirname(checkpoint_path)\n","    if not os.path.exists(checkpoint_dir):\n","        print(f\"\\nERROR: Checkpoint directory not found!\")\n","        print(f\"Directory: {checkpoint_dir}\")\n","        print(\"\\nTroubleshooting:\")\n","        print(\"1. Make sure Cell 2 (Training) has been executed successfully\")\n","        print(\"2. Check if training completed without errors\")\n","        print(\"3. Verify the checkpoint was saved during training\")\n","        raise FileNotFoundError(f\"Checkpoint directory does not exist: {checkpoint_dir}\")\n","\n","    # List available checkpoints in directory\n","    available_checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n","\n","    if not os.path.exists(checkpoint_path):\n","        print(f\"\\nERROR: Checkpoint file not found!\")\n","        print(f\"Expected: {os.path.basename(checkpoint_path)}\")\n","\n","        if available_checkpoints:\n","            print(f\"\\nAvailable checkpoints in directory:\")\n","            for ckpt in available_checkpoints:\n","                ckpt_path = os.path.join(checkpoint_dir, ckpt)\n","                size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n","                print(f\"  - {ckpt} ({size_mb:.1f} MB)\")\n","            print(\"\\nPossible solutions:\")\n","            print(\"1. Check if training saved checkpoint with different name\")\n","            print(\"2. Re-run Cell 2 (Training) to generate checkpoint\")\n","        else:\n","            print(f\"\\nNo checkpoints found in directory!\")\n","            print(\"\\nRequired actions:\")\n","            print(\"1. Execute Cell 2 (Training Pipeline) first\")\n","            print(\"2. Wait for training to complete (~2.5-3 hours)\")\n","            print(\"3. Verify checkpoint is saved successfully\")\n","            print(\"4. Then run this Cell 3 (Evaluation)\")\n","\n","        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n","\n","    # Validate checkpoint file size\n","    checkpoint_size = os.path.getsize(checkpoint_path) / (1024 * 1024)\n","    print(f\"Checkpoint found: {checkpoint_size:.1f} MB\")\n","\n","    if checkpoint_size < 10:\n","        print(f\"WARNING: Checkpoint size is unusually small ({checkpoint_size:.1f} MB)\")\n","        print(\"This might indicate a corrupted or incomplete checkpoint\")\n","\n","    # Load checkpoint\n","    print(f\"Loading checkpoint from disk...\")\n","    try:\n","        checkpoint = torch.load(checkpoint_path, map_location=device)\n","    except Exception as e:\n","        print(f\"ERROR: Failed to load checkpoint: {e}\")\n","        print(\"\\nPossible causes:\")\n","        print(\"1. Corrupted checkpoint file\")\n","        print(\"2. Incompatible PyTorch version\")\n","        print(\"3. Disk I/O error during save\")\n","        raise\n","\n","    # Initialize model\n","    print(f\"Initializing Swin Transformer model...\")\n","    model = SwinCASME2Baseline(\n","        num_classes=7,\n","        dropout_rate=CASME2_SWIN_CONFIG['dropout_rate']\n","    ).to(device)\n","\n","    # Load model weights\n","    try:\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","    except Exception as e:\n","        print(f\"ERROR: Failed to load model weights: {e}\")\n","        print(\"\\nPossible causes:\")\n","        print(\"1. Model architecture mismatch\")\n","        print(\"2. Checkpoint from different model variant\")\n","        raise\n","\n","    model.eval()\n","\n","    print(f\"Model loaded successfully from epoch {checkpoint['epoch']}\")\n","    print(f\"Best validation F1: {checkpoint['best_metrics']['f1']:.4f}\")\n","\n","    training_info = {\n","        'best_epoch': checkpoint['epoch'],\n","        'best_val_f1': checkpoint['best_metrics']['f1'],\n","        'best_val_loss': checkpoint['best_metrics']['loss'],\n","        'best_val_accuracy': checkpoint['best_metrics']['accuracy']\n","    }\n","\n","    return model, training_info\n","\n","# =====================================================\n","# FRAME-LEVEL INFERENCE (for v7 AF)\n","# =====================================================\n","\n","def run_frame_level_inference(model, dataloader, device):\n","    \"\"\"Run frame-level inference for AF evaluation\"\"\"\n","    model.eval()\n","\n","    all_predictions = []\n","    all_labels = []\n","    all_filenames = []\n","    all_probabilities = []\n","\n","    print(\"Running frame-level inference...\")\n","    start_time = time.time()\n","\n","    with torch.no_grad():\n","        for images, labels, filenames in tqdm(dataloader, desc=\"Frame-level inference\"):\n","            images = images.to(device, non_blocking=True)\n","\n","            outputs = model(images)\n","            probabilities = torch.softmax(outputs, dim=1)\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","            all_filenames.extend(filenames)\n","            all_probabilities.extend(probabilities.cpu().numpy())\n","\n","    inference_time = time.time() - start_time\n","\n","    print(f\"Frame-level inference completed in {inference_time:.2f}s\")\n","    print(f\"Processed {len(all_predictions)} frames\")\n","\n","    return {\n","        'predictions': np.array(all_predictions),\n","        'labels': np.array(all_labels),\n","        'filenames': all_filenames,\n","        'probabilities': np.array(all_probabilities),\n","        'inference_time': inference_time,\n","        'evaluation_mode': 'frame_level'\n","    }\n","\n","# =====================================================\n","# VIDEO-LEVEL INFERENCE WITH LATE FUSION (for v8 KFS)\n","# =====================================================\n","\n","def run_video_level_inference_late_fusion(model, dataloader, device):\n","    \"\"\"Run video-level inference with late fusion for KFS evaluation\"\"\"\n","    model.eval()\n","\n","    frame_predictions = []\n","    frame_labels = []\n","    frame_filenames = []\n","    frame_probabilities = []\n","    frame_video_ids = []\n","\n","    print(\"Running frame-level predictions for late fusion...\")\n","    start_time = time.time()\n","\n","    with torch.no_grad():\n","        for images, labels, filenames in tqdm(dataloader, desc=\"Frame predictions\"):\n","            images = images.to(device, non_blocking=True)\n","\n","            outputs = model(images)\n","            probabilities = torch.softmax(outputs, dim=1)\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            frame_predictions.extend(predicted.cpu().numpy())\n","            frame_labels.extend(labels.numpy())\n","            frame_filenames.extend(filenames)\n","            frame_probabilities.extend(probabilities.cpu().numpy())\n","\n","            for filename in filenames:\n","                video_id = extract_video_id_from_filename(filename)\n","                frame_video_ids.append(video_id)\n","\n","    print(f\"\\nAggregating frame predictions to video level...\")\n","\n","    video_data = defaultdict(lambda: {\n","        'frame_predictions': [],\n","        'frame_probabilities': [],\n","        'frame_filenames': [],\n","        'true_label': None\n","    })\n","\n","    for i, video_id in enumerate(frame_video_ids):\n","        video_data[video_id]['frame_predictions'].append(frame_predictions[i])\n","        video_data[video_id]['frame_probabilities'].append(frame_probabilities[i])\n","        video_data[video_id]['frame_filenames'].append(frame_filenames[i])\n","        if video_data[video_id]['true_label'] is None:\n","            video_data[video_id]['true_label'] = frame_labels[i]\n","\n","    video_predictions = []\n","    video_labels = []\n","    video_ids_list = []\n","\n","    for video_id, data in video_data.items():\n","        avg_probabilities = np.mean(data['frame_probabilities'], axis=0)\n","        video_prediction = np.argmax(avg_probabilities)\n","\n","        video_predictions.append(video_prediction)\n","        video_labels.append(data['true_label'])\n","        video_ids_list.append(video_id)\n","\n","    inference_time = time.time() - start_time\n","\n","    print(f\"Late fusion completed in {inference_time:.2f}s\")\n","    print(f\"Aggregated {len(frame_predictions)} frames into {len(video_predictions)} videos\")\n","\n","    return {\n","        'predictions': np.array(video_predictions),\n","        'labels': np.array(video_labels),\n","        'filenames': video_ids_list,\n","        'probabilities': None,\n","        'inference_time': inference_time,\n","        'evaluation_mode': 'video_level',\n","        'kfs_late_fusion_info': {\n","            'total_frames': len(frame_predictions),\n","            'total_videos': len(video_predictions),\n","            'aggregation_method': 'average_probabilities'\n","        }\n","    }\n","\n","# =====================================================\n","# COMPREHENSIVE METRICS CALCULATION\n","# =====================================================\n","\n","def calculate_comprehensive_metrics(inference_results):\n","    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n","    predictions = inference_results['predictions']\n","    labels = inference_results['labels']\n","    filenames = inference_results['filenames']\n","\n","    unique_labels = np.unique(labels)\n","    missing_classes = [i for i in range(len(CASME2_CLASSES)) if i not in unique_labels]\n","    available_classes = [i for i in range(len(CASME2_CLASSES)) if i in unique_labels]\n","\n","    accuracy = accuracy_score(labels, predictions)\n","\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions,\n","        labels=available_classes,\n","        average='macro',\n","        zero_division=0\n","    )\n","\n","    cm = confusion_matrix(labels, predictions, labels=list(range(len(CASME2_CLASSES))))\n","\n","    per_class_metrics = {}\n","\n","    for i, class_name in enumerate(CASME2_CLASSES):\n","        if i in available_classes:\n","            class_mask = (labels == i)\n","\n","            if np.sum(class_mask) > 0:\n","                # Create binary classification: current class vs all others\n","                binary_labels = (labels == i).astype(int)\n","                binary_predictions = (predictions == i).astype(int)\n","\n","                class_precision, class_recall, class_f1, _ = precision_recall_fscore_support(\n","                    binary_labels, binary_predictions,\n","                    average='binary',\n","                    pos_label=1,\n","                    zero_division=0\n","                )\n","\n","                labels_binary = label_binarize(labels, classes=list(range(len(CASME2_CLASSES))))\n","                if inference_results['probabilities'] is not None:\n","                    probs = inference_results['probabilities'][:, i]\n","                    try:\n","                        fpr, tpr, _ = roc_curve(labels_binary[:, i], probs)\n","                        class_auc = auc(fpr, tpr)\n","                    except:\n","                        class_auc = 0.0\n","                else:\n","                    class_auc = 0.0\n","\n","                per_class_metrics[class_name] = {\n","                    'precision': float(class_precision),\n","                    'recall': float(class_recall),\n","                    'f1_score': float(class_f1),\n","                    'auc': float(class_auc),\n","                    'support': int(np.sum(class_mask)),\n","                    'in_test_set': True\n","                }\n","            else:\n","                per_class_metrics[class_name] = {\n","                    'precision': 0.0,\n","                    'recall': 0.0,\n","                    'f1_score': 0.0,\n","                    'auc': 0.0,\n","                    'support': 0,\n","                    'in_test_set': True\n","                }\n","        else:\n","            per_class_metrics[class_name] = {\n","                'precision': 0.0,\n","                'recall': 0.0,\n","                'f1_score': 0.0,\n","                'auc': 0.0,\n","                'support': 0,\n","                'in_test_set': False\n","            }\n","\n","    macro_auc = np.mean([m['auc'] for m in per_class_metrics.values() if m['in_test_set']])\n","\n","    results = {\n","        'overall_performance': {\n","            'accuracy': float(accuracy),\n","            'macro_precision': float(precision),\n","            'macro_recall': float(recall),\n","            'macro_f1': float(f1),\n","            'macro_auc': float(macro_auc)\n","        },\n","        'per_class_performance': per_class_metrics,\n","        'confusion_matrix': cm.tolist(),\n","        'evaluation_metadata': {\n","            'dataset': 'CASME_II',\n","            'model_type': 'SwinCASME2Baseline',\n","            'test_samples': len(predictions),\n","            'class_names': CASME2_CLASSES,\n","            'missing_classes': [CASME2_CLASSES[i] for i in missing_classes],\n","            'available_classes': [CASME2_CLASSES[i] for i in available_classes],\n","            'evaluation_timestamp': datetime.now().isoformat(),\n","            'evaluation_mode': inference_results['evaluation_mode']\n","        },\n","        'inference_performance': {\n","            'total_time_seconds': float(inference_results['inference_time']),\n","            'average_time_ms_per_sample': float(inference_results['inference_time'] * 1000 / len(predictions))\n","        }\n","    }\n","\n","    if 'kfs_late_fusion_info' in inference_results:\n","        results['kfs_late_fusion_info'] = inference_results['kfs_late_fusion_info']\n","\n","    return results\n","\n","# =====================================================\n","# WRONG PREDICTIONS ANALYSIS\n","# =====================================================\n","\n","def analyze_wrong_predictions(inference_results):\n","    \"\"\"Analyze wrong predictions for error pattern identification\"\"\"\n","    predictions = inference_results['predictions']\n","    labels = inference_results['labels']\n","    filenames = inference_results['filenames']\n","\n","    wrong_predictions = []\n","    wrong_by_class = defaultdict(int)\n","    confusion_patterns = defaultdict(int)\n","\n","    for i in range(len(predictions)):\n","        if predictions[i] != labels[i]:\n","            true_class = CASME2_CLASSES[labels[i]]\n","            pred_class = CASME2_CLASSES[predictions[i]]\n","\n","            wrong_predictions.append({\n","                'filename': filenames[i],\n","                'true_label': int(labels[i]),\n","                'true_class': true_class,\n","                'predicted_label': int(predictions[i]),\n","                'predicted_class': pred_class\n","            })\n","\n","            wrong_by_class[true_class] += 1\n","            confusion_patterns[f\"{true_class} -> {pred_class}\"] += 1\n","\n","    error_summary = {}\n","    for class_name in CASME2_CLASSES:\n","        total_samples = np.sum(labels == CLASS_TO_IDX[class_name])\n","        errors = wrong_by_class.get(class_name, 0)\n","        error_summary[class_name] = {\n","            'total_samples': int(total_samples),\n","            'wrong_predictions': int(errors),\n","            'error_rate': float(errors / total_samples * 100) if total_samples > 0 else 0.0\n","        }\n","\n","    results = {\n","        'analysis_metadata': {\n","            'total_samples': len(predictions),\n","            'total_wrong_predictions': len(wrong_predictions),\n","            'overall_error_rate': (len(wrong_predictions) / len(predictions) * 100) if len(predictions) > 0 else 0.0\n","        },\n","        'wrong_predictions': wrong_predictions,\n","        'wrong_predictions_by_class': dict(wrong_by_class),\n","        'error_summary': error_summary,\n","        'confusion_patterns': dict(confusion_patterns)\n","    }\n","\n","    return results\n","\n","# =====================================================\n","# SAVE EVALUATION RESULTS\n","# =====================================================\n","\n","def save_evaluation_results(evaluation_results, wrong_predictions_results, results_dir, test_version):\n","    \"\"\"Save comprehensive evaluation results\"\"\"\n","    os.makedirs(results_dir, exist_ok=True)\n","\n","    results_file = f\"{results_dir}/casme2_swint_evaluation_results_{test_version}.json\"\n","    with open(results_file, 'w') as f:\n","        json.dump(evaluation_results, f, indent=2, default=str)\n","\n","    wrong_predictions_file = f\"{results_dir}/casme2_swint_wrong_predictions_{test_version}.json\"\n","    with open(wrong_predictions_file, 'w') as f:\n","        json.dump(wrong_predictions_results, f, indent=2, default=str)\n","\n","    print(f\"Evaluation results saved:\")\n","    print(f\"  Main results: {os.path.basename(results_file)}\")\n","    print(f\"  Wrong predictions: {os.path.basename(wrong_predictions_file)}\")\n","\n","    return results_file, wrong_predictions_file\n","\n","# =====================================================\n","# MAIN EVALUATION EXECUTION\n","# =====================================================\n","\n","all_evaluation_results = {}\n","\n","for dataset_version in EVALUATE_DATASETS:\n","    print(\"\\n\" + \"=\" * 70)\n","    print(f\"EVALUATING DATASET: {dataset_version.upper()}\")\n","    print(\"=\" * 70)\n","\n","    try:\n","        # Get dataset configuration\n","        test_config = get_test_dataset_config(dataset_version, PROJECT_ROOT)\n","\n","        print(f\"\\nTest Dataset Configuration:\")\n","        print(f\"  Version: {test_config['version']}\")\n","        print(f\"  Variant: {test_config['variant']}\")\n","        print(f\"  Description: {test_config['description']}\")\n","        print(f\"  Frame strategy: {test_config['frame_strategy']}\")\n","        print(f\"  Evaluation mode: {test_config['evaluation_mode']}\")\n","        if 'aggregation' in test_config and test_config['aggregation']:\n","            print(f\"  Aggregation: {test_config['aggregation']}\")\n","        print(f\"  Dataset path: {test_config['dataset_path']}\")\n","\n","        # Create test dataset\n","        print(f\"\\nCreating CASME II test dataset from {test_config['variant']}...\")\n","        test_dataset = CASME2DatasetEvaluation(\n","            dataset_root=test_config['dataset_path'],\n","            split='test',\n","            transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","            use_ram_cache=True\n","        )\n","\n","        if len(test_dataset) == 0:\n","            raise ValueError(f\"No test samples found for {dataset_version}!\")\n","\n","        test_loader = DataLoader(\n","            test_dataset,\n","            batch_size=CASME2_SWIN_CONFIG['batch_size'],\n","            shuffle=False,\n","            num_workers=CASME2_SWIN_CONFIG['num_workers'],\n","            pin_memory=True\n","        )\n","\n","        # Load trained model\n","        checkpoint_path = f\"{GLOBAL_CONFIG_CASME2['checkpoint_root']}/casme2_swint_mfs_best_f1.pth\"\n","        model, training_info = load_trained_model_casme2(checkpoint_path, GLOBAL_CONFIG_CASME2['device'])\n","\n","        # Run inference based on evaluation mode\n","        if test_config['evaluation_mode'] == 'frame_level':\n","            print(f\"\\nRunning frame-level evaluation for {test_config['variant']}...\")\n","            inference_results = run_frame_level_inference(model, test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","        elif test_config['evaluation_mode'] == 'video_level':\n","            print(f\"\\nRunning video-level evaluation with late fusion for {test_config['variant']}...\")\n","            inference_results = run_video_level_inference_late_fusion(model, test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","        else:\n","            raise ValueError(f\"Unknown evaluation mode: {test_config['evaluation_mode']}\")\n","\n","        # Calculate comprehensive metrics\n","        evaluation_results = calculate_comprehensive_metrics(inference_results)\n","\n","        # Analyze wrong predictions\n","        wrong_predictions_results = analyze_wrong_predictions(inference_results)\n","\n","        # Add training information\n","        evaluation_results['training_information'] = training_info\n","        evaluation_results['test_configuration'] = test_config\n","\n","        # Save results\n","        results_dir = f\"{GLOBAL_CONFIG_CASME2['results_root']}/evaluation_results\"\n","        save_evaluation_results(\n","            evaluation_results, wrong_predictions_results, results_dir, test_config['version']\n","        )\n","\n","        # Store for comparison\n","        all_evaluation_results[dataset_version] = {\n","            'evaluation': evaluation_results,\n","            'wrong_predictions': wrong_predictions_results,\n","            'config': test_config\n","        }\n","\n","        # Display results\n","        print(\"\\n\" + \"=\" * 60)\n","        print(f\"EVALUATION RESULTS - {test_config['variant']} ({dataset_version})\")\n","        print(\"=\" * 60)\n","\n","        overall = evaluation_results['overall_performance']\n","        print(f\"\\nOverall Performance:\")\n","        print(f\"  Accuracy:  {overall['accuracy']:.4f}\")\n","        print(f\"  Precision: {overall['macro_precision']:.4f}\")\n","        print(f\"  Recall:    {overall['macro_recall']:.4f}\")\n","        print(f\"  F1 Score:  {overall['macro_f1']:.4f}\")\n","        print(f\"  AUC:       {overall['macro_auc']:.4f}\")\n","\n","        if 'kfs_late_fusion_info' in evaluation_results:\n","            fusion_info = evaluation_results['kfs_late_fusion_info']\n","            print(f\"\\nLate Fusion Info:\")\n","            print(f\"  Total frames processed: {fusion_info['total_frames']}\")\n","            print(f\"  Video-level predictions: {fusion_info['total_videos']}\")\n","            print(f\"  Aggregation method: {fusion_info['aggregation_method']}\")\n","\n","        print(f\"\\nPer-Class Performance:\")\n","        for class_name, metrics in evaluation_results['per_class_performance'].items():\n","            in_test = \"Present\" if metrics['in_test_set'] else \"Missing\"\n","            print(f\"  {class_name} [{in_test}]: F1={metrics['f1_score']:.4f}, \"\n","                  f\"Support={metrics['support']}\")\n","\n","        wrong_meta = wrong_predictions_results['analysis_metadata']\n","        print(f\"\\nWrong Predictions Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']} / {wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        print(f\"\\nInference Performance:\")\n","        print(f\"  Total time: {inference_results['inference_time']:.2f}s\")\n","        print(f\"  Speed: {evaluation_results['inference_performance']['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","    except Exception as e:\n","        print(f\"Evaluation failed for {dataset_version}: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        continue\n","\n","# =====================================================\n","# COMPARATIVE ANALYSIS (if both datasets evaluated)\n","# =====================================================\n","\n","if len(all_evaluation_results) == 2 and 'v7' in all_evaluation_results and 'v8' in all_evaluation_results:\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"COMPARATIVE ANALYSIS: AF (v7) vs KFS (v8)\")\n","    print(\"=\" * 70)\n","\n","    v7_results = all_evaluation_results['v7']['evaluation']\n","    v8_results = all_evaluation_results['v8']['evaluation']\n","\n","    print(\"\\nOverall Performance Comparison:\")\n","    print(f\"{'Metric':<20} {'AF (v7)':<15} {'KFS (v8)':<15} {'Difference':<15}\")\n","    print(\"-\" * 65)\n","\n","    metrics_to_compare = ['accuracy', 'macro_precision', 'macro_recall', 'macro_f1', 'macro_auc']\n","\n","    for metric in metrics_to_compare:\n","        v7_val = v7_results['overall_performance'][metric]\n","        v8_val = v8_results['overall_performance'][metric]\n","        diff = v8_val - v7_val\n","\n","        print(f\"{metric:<20} {v7_val:<15.4f} {v8_val:<15.4f} {diff:+.4f}\")\n","\n","    print(f\"\\nEvaluation Modes:\")\n","    print(f\"  AF (v7): {v7_results['evaluation_metadata']['evaluation_mode']}\")\n","    print(f\"  KFS (v8): {v8_results['evaluation_metadata']['evaluation_mode']}\")\n","\n","    if 'kfs_late_fusion_info' in v8_results:\n","        print(f\"\\nKFS Late Fusion Strategy:\")\n","        print(f\"  Frames used: {v8_results['kfs_late_fusion_info']['total_frames']}\")\n","        print(f\"  Video predictions: {v8_results['kfs_late_fusion_info']['total_videos']}\")\n","        print(f\"  Aggregation: {v8_results['kfs_late_fusion_info']['aggregation_method']}\")\n","\n","# Memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II SWIN TRANSFORMER EVALUATION COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Evaluated datasets: {EVALUATE_DATASETS}\")\n","print(\"Next: Cell 4 - Generate confusion matrices and visualization\")"],"metadata":{"cellView":"form","id":"i7DsPn9iIWb2","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1761282725202,"user_tz":-420,"elapsed":14629,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"79306ba7-035e-40b9-96ab-b5ae5e4d356d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II Swin Transformer Evaluation Framework with Dual Dataset Support\n","============================================================\n","Datasets to evaluate: ['v7', 'v8']\n","============================================================\n","\n","======================================================================\n","EVALUATING DATASET: V7\n","======================================================================\n","\n","Test Dataset Configuration:\n","  Version: v7\n","  Variant: AF\n","  Description: Apex Frame with Face-Aware Preprocessing\n","  Frame strategy: apex_frame\n","  Evaluation mode: frame_level\n","  Dataset path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7\n","\n","Creating CASME II test dataset from AF...\n","Loading CASME II test dataset for evaluation...\n","Found 28 image files in directory\n","Loaded 28 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 10 samples (35.7%)\n","  disgust: 7 samples (25.0%)\n","  happiness: 4 samples (14.3%)\n","  repression: 3 samples (10.7%)\n","  surprise: 3 samples (10.7%)\n","  sadness: 1 samples (3.6%)\n","Unique video IDs: 28\n","Missing classes in test set: ['fear']\n","Preloading 28 test images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test set to RAM: 100%|██████████| 28/28 [00:01<00:00, 15.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TEST RAM caching completed: 28/28 images, ~0.02GB\n","\n","Validating checkpoint availability...\n","Expected checkpoint: casme2_swint_mfs_best_f1.pth\n","Full path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/07_02_swint_casme2_mfs_prep/casme2_swint_mfs_best_f1.pth\n","Checkpoint found: 1000.5 MB\n","Loading checkpoint from disk...\n","Initializing Swin Transformer model...\n","Swin feature dimension (final stage): 1024\n","Base embed_dim: 128, Stages: 4\n","Classification head: 1024 -> GAP -> 512 -> 128 -> 7\n","Dropout rate: 0.3 (balanced for large dataset)\n","Model loaded successfully from epoch 1\n","Best validation F1: 0.3059\n","\n","Running frame-level evaluation for AF...\n","Running frame-level inference...\n"]},{"output_type":"stream","name":"stderr","text":["Frame-level inference: 100%|██████████| 2/2 [00:00<00:00,  4.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Frame-level inference completed in 0.48s\n","Processed 28 frames\n","Evaluation results saved:\n","  Main results: casme2_swint_evaluation_results_v7.json\n","  Wrong predictions: casme2_swint_wrong_predictions_v7.json\n","\n","============================================================\n","EVALUATION RESULTS - AF (v7)\n","============================================================\n","\n","Overall Performance:\n","  Accuracy:  0.4643\n","  Precision: 0.4380\n","  Recall:    0.3480\n","  F1 Score:  0.3639\n","  AUC:       0.5519\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.6000, Support=10\n","  disgust [Present]: F1=0.5000, Support=7\n","  happiness [Present]: F1=0.2500, Support=4\n","  repression [Present]: F1=0.3333, Support=3\n","  surprise [Present]: F1=0.5000, Support=3\n","  sadness [Present]: F1=0.0000, Support=1\n","  fear [Missing]: F1=0.0000, Support=0\n","\n","Wrong Predictions Analysis:\n","  Total errors: 15 / 28\n","  Error rate: 53.57%\n","\n","Inference Performance:\n","  Total time: 0.48s\n","  Speed: 17.1 ms/sample\n","\n","======================================================================\n","EVALUATING DATASET: V8\n","======================================================================\n","\n","Test Dataset Configuration:\n","  Version: v8\n","  Variant: KFS\n","  Description: Key Frame Sequence with Face-Aware Preprocessing\n","  Frame strategy: key_frame_sequence\n","  Evaluation mode: video_level\n","  Aggregation: late_fusion\n","  Dataset path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8\n","\n","Creating CASME II test dataset from KFS...\n","Loading CASME II test dataset for evaluation...\n","Found 84 image files in directory\n","Loaded 84 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 30 samples (35.7%)\n","  disgust: 21 samples (25.0%)\n","  happiness: 12 samples (14.3%)\n","  repression: 9 samples (10.7%)\n","  surprise: 9 samples (10.7%)\n","  sadness: 3 samples (3.6%)\n","Unique video IDs: 84\n","Missing classes in test set: ['fear']\n","Preloading 84 test images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test set to RAM: 100%|██████████| 84/84 [00:03<00:00, 21.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TEST RAM caching completed: 84/84 images, ~0.05GB\n","\n","Validating checkpoint availability...\n","Expected checkpoint: casme2_swint_mfs_best_f1.pth\n","Full path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/07_02_swint_casme2_mfs_prep/casme2_swint_mfs_best_f1.pth\n","Checkpoint found: 1000.5 MB\n","Loading checkpoint from disk...\n","Initializing Swin Transformer model...\n","Swin feature dimension (final stage): 1024\n","Base embed_dim: 128, Stages: 4\n","Classification head: 1024 -> GAP -> 512 -> 128 -> 7\n","Dropout rate: 0.3 (balanced for large dataset)\n","Model loaded successfully from epoch 1\n","Best validation F1: 0.3059\n","\n","Running video-level evaluation with late fusion for KFS...\n","Running frame-level predictions for late fusion...\n"]},{"output_type":"stream","name":"stderr","text":["Frame predictions: 100%|██████████| 6/6 [00:00<00:00,  7.09it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Aggregating frame predictions to video level...\n","Late fusion completed in 0.85s\n","Aggregated 84 frames into 84 videos\n","Evaluation results saved:\n","  Main results: casme2_swint_evaluation_results_v8.json\n","  Wrong predictions: casme2_swint_wrong_predictions_v8.json\n","\n","============================================================\n","EVALUATION RESULTS - KFS (v8)\n","============================================================\n","\n","Overall Performance:\n","  Accuracy:  0.4524\n","  Precision: 0.3643\n","  Recall:    0.3554\n","  F1 Score:  0.3558\n","  AUC:       0.0000\n","\n","Late Fusion Info:\n","  Total frames processed: 84\n","  Video-level predictions: 84\n","  Aggregation method: average_probabilities\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.5424, Support=30\n","  disgust [Present]: F1=0.5106, Support=21\n","  happiness [Present]: F1=0.2609, Support=12\n","  repression [Present]: F1=0.4211, Support=9\n","  surprise [Present]: F1=0.4000, Support=9\n","  sadness [Present]: F1=0.0000, Support=3\n","  fear [Missing]: F1=0.0000, Support=0\n","\n","Wrong Predictions Analysis:\n","  Total errors: 46 / 84\n","  Error rate: 54.76%\n","\n","Inference Performance:\n","  Total time: 0.85s\n","  Speed: 10.1 ms/sample\n","\n","======================================================================\n","COMPARATIVE ANALYSIS: AF (v7) vs KFS (v8)\n","======================================================================\n","\n","Overall Performance Comparison:\n","Metric               AF (v7)         KFS (v8)        Difference     \n","-----------------------------------------------------------------\n","accuracy             0.4643          0.4524          -0.0119\n","macro_precision      0.4380          0.3643          -0.0736\n","macro_recall         0.3480          0.3554          +0.0074\n","macro_f1             0.3639          0.3558          -0.0081\n","macro_auc            0.5519          0.0000          -0.5519\n","\n","Evaluation Modes:\n","  AF (v7): frame_level\n","  KFS (v8): video_level\n","\n","KFS Late Fusion Strategy:\n","  Frames used: 84\n","  Video predictions: 84\n","  Aggregation: average_probabilities\n","\n","======================================================================\n","CASME II SWIN TRANSFORMER EVALUATION COMPLETED\n","======================================================================\n","Evaluated datasets: ['v7', 'v8']\n","Next: Cell 4 - Generate confusion matrices and visualization\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title Cell 4: CASME II Swin Transformer Confusion Matrix Generation\n","\n","# File: 07_02_SwinT_CASME2_MFS_Cell4.py\n","# Location: experiments/07_02_SwinT_CASME2-MFS-PREP.ipynb\n","# Purpose: Generate professional confusion matrix visualization for AF and KFS evaluations\n","\n","import json\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import seaborn as sns\n","from datetime import datetime\n","\n","print(\"CASME II Swin Transformer Confusion Matrix Generation\")\n","print(\"=\" * 60)\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/07_02_swint_casme2_mfs_prep\"\n","\n","def find_evaluation_json_files(results_path):\n","    \"\"\"Find evaluation JSON files with version detection\"\"\"\n","    json_files = {}\n","    eval_dir = f\"{results_path}/evaluation_results\"\n","\n","    if os.path.exists(eval_dir):\n","        for version in ['v7', 'v8']:\n","            eval_pattern = f\"{eval_dir}/casme2_swint_evaluation_results_{version}.json\"\n","            eval_files = glob.glob(eval_pattern)\n","\n","            if eval_files:\n","                json_files[f'main_{version}'] = eval_files[0]\n","                print(f\"Found {version.upper()} evaluation file: {os.path.basename(eval_files[0])}\")\n","\n","            wrong_pattern = f\"{eval_dir}/casme2_swint_wrong_predictions_{version}.json\"\n","            wrong_files = glob.glob(wrong_pattern)\n","\n","            if wrong_files:\n","                json_files[f'wrong_{version}'] = wrong_files[0]\n","                print(f\"Found {version.upper()} wrong predictions: {os.path.basename(wrong_files[0])}\")\n","\n","        if not json_files:\n","            print(f\"WARNING: No evaluation results found in {eval_dir}\")\n","            print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","    else:\n","        print(f\"ERROR: Evaluation directory not found: {eval_dir}\")\n","\n","    return json_files\n","\n","def load_evaluation_results(json_path):\n","    \"\"\"Load and parse evaluation results JSON\"\"\"\n","    try:\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"Successfully loaded: {os.path.basename(json_path)}\")\n","        return data\n","    except Exception as e:\n","        print(f\"ERROR loading {json_path}: {str(e)}\")\n","        return None\n","\n","def calculate_weighted_f1(per_class_performance):\n","    \"\"\"Calculate weighted F1 score\"\"\"\n","    total_support = sum([class_data['support'] for class_data in per_class_performance.values()\n","                        if class_data['support'] > 0])\n","\n","    if total_support == 0:\n","        return 0.0\n","\n","    weighted_f1 = 0.0\n","\n","    for class_name, class_data in per_class_performance.items():\n","        if class_data['support'] > 0:\n","            weight = class_data['support'] / total_support\n","            weighted_f1 += class_data['f1_score'] * weight\n","\n","    return weighted_f1\n","\n","def calculate_balanced_accuracy(confusion_matrix):\n","    \"\"\"Calculate balanced accuracy handling classes with zero support\"\"\"\n","    cm = np.array(confusion_matrix)\n","    n_classes = cm.shape[0]\n","\n","    per_class_balanced_acc = []\n","    classes_with_samples = []\n","\n","    for i in range(n_classes):\n","        if cm[i, :].sum() > 0:\n","            classes_with_samples.append(i)\n","\n","    for i in classes_with_samples:\n","        tp = cm[i, i]\n","        fn = cm[i, :].sum() - tp\n","        fp = cm[:, i].sum() - tp\n","        tn = cm.sum() - tp - fn - fp\n","\n","        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n","\n","        class_balanced_acc = (sensitivity + specificity) / 2\n","        per_class_balanced_acc.append(class_balanced_acc)\n","\n","    balanced_acc = np.mean(per_class_balanced_acc) if per_class_balanced_acc else 0.0\n","\n","    return balanced_acc\n","\n","def determine_text_color(color_value, threshold=0.5):\n","    \"\"\"Determine optimal text color based on background intensity\"\"\"\n","    return 'white' if color_value > threshold else 'black'\n","\n","def create_confusion_matrix_plot(data, output_path, test_version):\n","    \"\"\"Create professional confusion matrix visualization for CASME II micro-expression recognition\"\"\"\n","\n","    meta = data['evaluation_metadata']\n","    class_names = meta['class_names']\n","    cm = np.array(data['confusion_matrix'], dtype=int)\n","    overall = data['overall_performance']\n","    per_class = data['per_class_performance']\n","\n","    test_config = data.get('test_configuration', {})\n","    variant = test_config.get('variant', test_version.upper())\n","    description = test_config.get('description', f'{test_version} preprocessing')\n","    eval_mode = meta.get('evaluation_mode', 'frame_level')\n","\n","    print(f\"Processing confusion matrix for {variant} ({test_version})\")\n","    print(f\"Dataset: {description}\")\n","    print(f\"Evaluation mode: {eval_mode}\")\n","    print(f\"Confusion matrix shape: {cm.shape}\")\n","\n","    macro_f1 = overall.get('macro_f1', 0.0)\n","    accuracy = overall.get('accuracy', 0.0)\n","    weighted_f1 = calculate_weighted_f1(per_class)\n","    balanced_acc = calculate_balanced_accuracy(cm)\n","\n","    print(f\"Calculated metrics - Macro F1: {macro_f1:.4f}, Weighted F1: {weighted_f1:.4f}, \"\n","          f\"Balanced Acc: {balanced_acc:.4f}, Accuracy: {accuracy:.4f}\")\n","\n","    row_sums = cm.sum(axis=1, keepdims=True)\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        cm_pct = np.divide(cm, row_sums, where=(row_sums!=0))\n","        cm_pct = np.nan_to_num(cm_pct)\n","\n","    fig, ax = plt.subplots(figsize=(12, 10))\n","\n","    cmap = 'Blues'\n","    im = ax.imshow(cm_pct, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=0.8)\n","\n","    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","    cbar.set_label('True Class Percentage', rotation=270, labelpad=15, fontsize=11)\n","\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            count = cm[i, j]\n","\n","            if row_sums[i, 0] > 0:\n","                percentage = cm_pct[i, j] * 100\n","                text = f\"{count}\\n{percentage:.1f}%\"\n","            else:\n","                text = f\"{count}\\n(N/A)\"\n","\n","            cell_value = cm_pct[i, j]\n","            text_color = determine_text_color(cell_value, threshold=0.4)\n","\n","            ax.text(j, i, text, ha=\"center\", va=\"center\",\n","                   color=text_color, fontsize=9, fontweight='bold')\n","\n","    ax.set_xticks(np.arange(len(class_names)))\n","    ax.set_yticks(np.arange(len(class_names)))\n","    ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=10)\n","    ax.set_yticklabels(class_names, fontsize=10)\n","    ax.set_xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n","    ax.set_ylabel(\"True Label\", fontsize=12, fontweight='bold')\n","\n","    preprocessing_note = f\"Preprocessing: {description}\\n\"\n","    preprocessing_note += f\"Dataset: {test_version}\\n\"\n","    preprocessing_note += f\"Evaluation: {eval_mode.replace('_', ' ').title()}\"\n","\n","    if 'kfs_late_fusion_info' in data:\n","        fusion_info = data['kfs_late_fusion_info']\n","        preprocessing_note += f\"\\nFrames: {fusion_info['total_frames']}, Videos: {fusion_info['total_videos']}\"\n","\n","    missing_classes = meta.get('missing_classes', [])\n","    if missing_classes:\n","        preprocessing_note += f\"\\nMissing: {', '.join(missing_classes)}\"\n","\n","    ax.text(0.02, 0.98, preprocessing_note, transform=ax.transAxes, fontsize=8,\n","            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n","\n","    title = f\"CASME II {variant} Micro-Expression Recognition - Swin Transformer\\n\"\n","    title += f\"Acc: {accuracy:.4f}  |  Macro F1: {macro_f1:.4f}  |  Weighted F1: {weighted_f1:.4f}  |  Balanced Acc: {balanced_acc:.4f}\"\n","    ax.set_title(title, fontsize=12, pad=25, fontweight='bold')\n","\n","    plt.tight_layout()\n","    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n","    plt.close(fig)\n","\n","    print(f\"Confusion matrix saved to: {os.path.basename(output_path)}\")\n","\n","    return {\n","        'accuracy': accuracy,\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        'balanced_accuracy': balanced_acc,\n","        'missing_classes': missing_classes\n","    }\n","\n","def generate_performance_summary(evaluation_data, wrong_predictions_data=None):\n","    \"\"\"Generate comprehensive performance summary\"\"\"\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\")\n","    print(\"=\" * 60)\n","\n","    overall = evaluation_data['overall_performance']\n","    meta = evaluation_data['evaluation_metadata']\n","    test_config = evaluation_data.get('test_configuration', {})\n","\n","    variant = test_config.get('variant', 'N/A')\n","\n","    print(f\"Dataset: {meta['dataset']}\")\n","    print(f\"Variant: {variant}\")\n","    print(f\"Dataset version: {test_config.get('version', 'N/A')}\")\n","    print(f\"Preprocessing: {test_config.get('description', 'N/A')}\")\n","    print(f\"Test samples: {meta['test_samples']}\")\n","    print(f\"Model: {meta['model_type']}\")\n","    print(f\"Evaluation date: {meta['evaluation_timestamp']}\")\n","\n","    if 'kfs_late_fusion_info' in evaluation_data:\n","        fusion_info = evaluation_data['kfs_late_fusion_info']\n","        print(f\"\\nLate Fusion Information:\")\n","        print(f\"  Total frames: {fusion_info['total_frames']}\")\n","        print(f\"  Video predictions: {fusion_info['total_videos']}\")\n","        print(f\"  Aggregation: {fusion_info['aggregation_method']}\")\n","\n","    print(f\"\\nOverall Performance:\")\n","    print(f\"  Accuracy:         {overall['accuracy']:.4f}\")\n","    print(f\"  Macro Precision:  {overall['macro_precision']:.4f}\")\n","    print(f\"  Macro Recall:     {overall['macro_recall']:.4f}\")\n","    print(f\"  Macro F1:         {overall['macro_f1']:.4f}\")\n","    print(f\"  Macro AUC:        {overall['macro_auc']:.4f}\")\n","\n","    print(f\"\\nPer-Class Performance:\")\n","    per_class = evaluation_data['per_class_performance']\n","\n","    print(f\"{'Class':<12} {'F1':<8} {'Precision':<10} {'Recall':<8} {'AUC':<8} {'Support':<8} {'In Test'}\")\n","    print(\"-\" * 65)\n","\n","    for class_name, metrics in per_class.items():\n","        in_test = \"Yes\" if metrics['in_test_set'] else \"No\"\n","        print(f\"{class_name:<12} {metrics['f1_score']:<8.4f} {metrics['precision']:<10.4f} \"\n","              f\"{metrics['recall']:<8.4f} {metrics['auc']:<8.4f} {metrics['support']:<8} {in_test}\")\n","\n","    if 'training_information' in evaluation_data:\n","        training = evaluation_data['training_information']\n","        print(f\"\\nTraining vs Test Comparison:\")\n","        print(f\"  Training Val F1:  {training['best_val_f1']:.4f}\")\n","        print(f\"  Test F1:          {overall['macro_f1']:.4f}\")\n","        print(f\"  Performance Gap:  {training['best_val_f1'] - overall['macro_f1']:+.4f}\")\n","        print(f\"  Best Epoch:       {training['best_epoch']}\")\n","\n","    missing_classes = meta.get('missing_classes', [])\n","    available_classes = meta.get('available_classes', [])\n","\n","    print(f\"\\nClass Availability Analysis:\")\n","    print(f\"  Available classes: {len(available_classes)}/7\")\n","    print(f\"  Missing classes: {missing_classes if missing_classes else 'None'}\")\n","\n","    if wrong_predictions_data:\n","        wrong_meta = wrong_predictions_data['analysis_metadata']\n","        print(f\"\\nError Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']}/{wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        patterns = wrong_predictions_data.get('confusion_patterns', {})\n","        if patterns:\n","            print(f\"\\nTop Confusion Patterns:\")\n","            sorted_patterns = sorted(patterns.items(), key=lambda x: x[1], reverse=True)[:3]\n","            for pattern, count in sorted_patterns:\n","                print(f\"  {pattern}: {count} cases\")\n","\n","    print(f\"\\nInference Performance:\")\n","    inference = evaluation_data['inference_performance']\n","    print(f\"  Total time: {inference['total_time_seconds']:.2f}s\")\n","    print(f\"  Speed: {inference['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","json_files = find_evaluation_json_files(RESULTS_ROOT)\n","\n","if not json_files:\n","    print(f\"ERROR: No evaluation JSON files found in {RESULTS_ROOT}\")\n","    print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","else:\n","    print(f\"\\nFound {len([k for k in json_files.keys() if k.startswith('main_')])} evaluation result(s)\")\n","\n","output_dir = f\"{RESULTS_ROOT}/confusion_matrix_analysis\"\n","Path(output_dir).mkdir(parents=True, exist_ok=True)\n","\n","results_summary = {}\n","generated_files = []\n","\n","for version in ['v7', 'v8']:\n","    main_key = f'main_{version}'\n","    wrong_key = f'wrong_{version}'\n","\n","    if main_key in json_files:\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Processing {version.upper()} Evaluation Results\")\n","        print(f\"{'='*60}\")\n","\n","        eval_data = load_evaluation_results(json_files[main_key])\n","\n","        wrong_data = None\n","        if wrong_key in json_files:\n","            wrong_data = load_evaluation_results(json_files[wrong_key])\n","\n","        if eval_data is not None:\n","            try:\n","                cm_output_path = os.path.join(output_dir, f\"confusion_matrix_CASME2_SwinT_{version}.png\")\n","                metrics = create_confusion_matrix_plot(eval_data, cm_output_path, version)\n","                generated_files.append(cm_output_path)\n","\n","                results_summary[version] = metrics\n","\n","                print(f\"\\nSUCCESS: {version.upper()} confusion matrix generated successfully\")\n","                print(f\"Output file: {os.path.basename(cm_output_path)}\")\n","\n","                print(f\"\\nPerformance Metrics Summary:\")\n","                print(f\"  Accuracy:        {metrics['accuracy']:.4f}\")\n","                print(f\"  Macro F1:        {metrics['macro_f1']:.4f}\")\n","                print(f\"  Weighted F1:     {metrics['weighted_f1']:.4f}\")\n","                print(f\"  Balanced Acc:    {metrics['balanced_accuracy']:.4f}\")\n","\n","                if metrics['missing_classes']:\n","                    print(f\"  Missing classes: {metrics['missing_classes']}\")\n","\n","            except Exception as e:\n","                print(f\"ERROR: Failed to generate {version.upper()} confusion matrix: {str(e)}\")\n","                import traceback\n","                traceback.print_exc()\n","\n","            generate_performance_summary(eval_data, wrong_data)\n","        else:\n","            print(f\"ERROR: Could not load {version.upper()} evaluation data\")\n","\n","if generated_files:\n","    print(f\"\\n\" + \"=\" * 60)\n","    print(\"CASME II CONFUSION MATRIX GENERATION COMPLETED\")\n","    print(\"=\" * 60)\n","\n","    print(f\"\\nGenerated visualization files:\")\n","    for file_path in generated_files:\n","        filename = os.path.basename(file_path)\n","        print(f\"  {filename}\")\n","\n","    for version in ['v7', 'v8']:\n","        if version in results_summary:\n","            variant = 'AF' if version == 'v7' else 'KFS'\n","            print(f\"\\n{variant} ({version.upper()}) Performance Summary:\")\n","            metrics = results_summary[version]\n","            print(f\"  Accuracy:       {metrics['accuracy']:.4f}\")\n","            print(f\"  Macro F1:       {metrics['macro_f1']:.4f}\")\n","            print(f\"  Weighted F1:    {metrics['weighted_f1']:.4f}\")\n","            print(f\"  Balanced Acc:   {metrics['balanced_accuracy']:.4f}\")\n","\n","    print(f\"\\nFiles saved in: {output_dir}\")\n","    print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","else:\n","    print(f\"\\nERROR: No visualizations were generated\")\n","    print(\"Please check:\")\n","    print(\"1. Cell 3 evaluation results exist\")\n","    print(\"2. JSON file structure is correct\")\n","    print(\"3. No file permission issues\")\n","\n","print(\"\\nCell 4 completed - CASME II confusion matrix analysis generated\")"],"metadata":{"cellView":"form","id":"DyuL_W3YJWLM","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1761282728406,"user_tz":-420,"elapsed":3201,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"a56060a1-3709-4359-ec77-9923f7a78391"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II Swin Transformer Confusion Matrix Generation\n","============================================================\n","Found V7 evaluation file: casme2_swint_evaluation_results_v7.json\n","Found V7 wrong predictions: casme2_swint_wrong_predictions_v7.json\n","Found V8 evaluation file: casme2_swint_evaluation_results_v8.json\n","Found V8 wrong predictions: casme2_swint_wrong_predictions_v8.json\n","\n","Found 2 evaluation result(s)\n","\n","============================================================\n","Processing V7 Evaluation Results\n","============================================================\n","Successfully loaded: casme2_swint_evaluation_results_v7.json\n","Successfully loaded: casme2_swint_wrong_predictions_v7.json\n","Processing confusion matrix for AF (v7)\n","Dataset: Apex Frame with Face-Aware Preprocessing\n","Evaluation mode: frame_level\n","Confusion matrix shape: (7, 7)\n","Calculated metrics - Macro F1: 0.3639, Weighted F1: 0.4643, Balanced Acc: 0.6155, Accuracy: 0.4643\n","Confusion matrix saved to: confusion_matrix_CASME2_SwinT_v7.png\n","\n","SUCCESS: V7 confusion matrix generated successfully\n","Output file: confusion_matrix_CASME2_SwinT_v7.png\n","\n","Performance Metrics Summary:\n","  Accuracy:        0.4643\n","  Macro F1:        0.3639\n","  Weighted F1:     0.4643\n","  Balanced Acc:    0.6155\n","  Missing classes: ['fear']\n","\n","============================================================\n","CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\n","============================================================\n","Dataset: CASME_II\n","Variant: AF\n","Dataset version: v7\n","Preprocessing: Apex Frame with Face-Aware Preprocessing\n","Test samples: 28\n","Model: SwinCASME2Baseline\n","Evaluation date: 2025-10-24T05:11:56.606545\n","\n","Overall Performance:\n","  Accuracy:         0.4643\n","  Macro Precision:  0.4380\n","  Macro Recall:     0.3480\n","  Macro F1:         0.3639\n","  Macro AUC:        0.5519\n","\n","Per-Class Performance:\n","Class        F1       Precision  Recall   AUC      Support  In Test\n","-----------------------------------------------------------------\n","others       0.6000   0.6000     0.6000   0.7278   10       Yes\n","disgust      0.5000   0.4444     0.5714   0.6735   7        Yes\n","happiness    0.2500   0.2500     0.2500   0.5104   4        Yes\n","repression   0.3333   0.3333     0.3333   0.6533   3        Yes\n","surprise     0.5000   1.0000     0.3333   0.7467   3        Yes\n","sadness      0.0000   0.0000     0.0000   0.0000   1        Yes\n","fear         0.0000   0.0000     0.0000   0.0000   0        No\n","\n","Training vs Test Comparison:\n","  Training Val F1:  0.3059\n","  Test F1:          0.3639\n","  Performance Gap:  -0.0580\n","  Best Epoch:       1\n","\n","Class Availability Analysis:\n","  Available classes: 6/7\n","  Missing classes: ['fear']\n","\n","Error Analysis:\n","  Total errors: 15/28\n","  Error rate: 53.57%\n","\n","Top Confusion Patterns:\n","  others -> disgust: 3 cases\n","  disgust -> others: 2 cases\n","  happiness -> repression: 2 cases\n","\n","Inference Performance:\n","  Total time: 0.48s\n","  Speed: 17.1 ms/sample\n","\n","============================================================\n","Processing V8 Evaluation Results\n","============================================================\n","Successfully loaded: casme2_swint_evaluation_results_v8.json\n","Successfully loaded: casme2_swint_wrong_predictions_v8.json\n","Processing confusion matrix for KFS (v8)\n","Dataset: Key Frame Sequence with Face-Aware Preprocessing\n","Evaluation mode: video_level\n","Confusion matrix shape: (7, 7)\n","Calculated metrics - Macro F1: 0.3558, Weighted F1: 0.4466, Balanced Acc: 0.6178, Accuracy: 0.4524\n","Confusion matrix saved to: confusion_matrix_CASME2_SwinT_v8.png\n","\n","SUCCESS: V8 confusion matrix generated successfully\n","Output file: confusion_matrix_CASME2_SwinT_v8.png\n","\n","Performance Metrics Summary:\n","  Accuracy:        0.4524\n","  Macro F1:        0.3558\n","  Weighted F1:     0.4466\n","  Balanced Acc:    0.6178\n","  Missing classes: ['fear']\n","\n","============================================================\n","CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\n","============================================================\n","Dataset: CASME_II\n","Variant: KFS\n","Dataset version: v8\n","Preprocessing: Key Frame Sequence with Face-Aware Preprocessing\n","Test samples: 84\n","Model: SwinCASME2Baseline\n","Evaluation date: 2025-10-24T05:12:05.249699\n","\n","Late Fusion Information:\n","  Total frames: 84\n","  Video predictions: 84\n","  Aggregation: average_probabilities\n","\n","Overall Performance:\n","  Accuracy:         0.4524\n","  Macro Precision:  0.3643\n","  Macro Recall:     0.3554\n","  Macro F1:         0.3558\n","  Macro AUC:        0.0000\n","\n","Per-Class Performance:\n","Class        F1       Precision  Recall   AUC      Support  In Test\n","-----------------------------------------------------------------\n","others       0.5424   0.5517     0.5333   0.0000   30       Yes\n","disgust      0.5106   0.4615     0.5714   0.0000   21       Yes\n","happiness    0.2609   0.2727     0.2500   0.0000   12       Yes\n","repression   0.4211   0.4000     0.4444   0.0000   9        Yes\n","surprise     0.4000   0.5000     0.3333   0.0000   9        Yes\n","sadness      0.0000   0.0000     0.0000   0.0000   3        Yes\n","fear         0.0000   0.0000     0.0000   0.0000   0        No\n","\n","Training vs Test Comparison:\n","  Training Val F1:  0.3059\n","  Test F1:          0.3558\n","  Performance Gap:  -0.0499\n","  Best Epoch:       1\n","\n","Class Availability Analysis:\n","  Available classes: 6/7\n","  Missing classes: ['fear']\n","\n","Error Analysis:\n","  Total errors: 46/84\n","  Error rate: 54.76%\n","\n","Top Confusion Patterns:\n","  others -> disgust: 9 cases\n","  disgust -> others: 5 cases\n","  happiness -> repression: 4 cases\n","\n","Inference Performance:\n","  Total time: 0.85s\n","  Speed: 10.1 ms/sample\n","\n","============================================================\n","CASME II CONFUSION MATRIX GENERATION COMPLETED\n","============================================================\n","\n","Generated visualization files:\n","  confusion_matrix_CASME2_SwinT_v7.png\n","  confusion_matrix_CASME2_SwinT_v8.png\n","\n","AF (V7) Performance Summary:\n","  Accuracy:       0.4643\n","  Macro F1:       0.3639\n","  Weighted F1:    0.4643\n","  Balanced Acc:   0.6155\n","\n","KFS (V8) Performance Summary:\n","  Accuracy:       0.4524\n","  Macro F1:       0.3558\n","  Weighted F1:    0.4466\n","  Balanced Acc:   0.6178\n","\n","Files saved in: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/07_02_swint_casme2_mfs_prep/confusion_matrix_analysis\n","Analysis completed at: 2025-10-24 05:12:08\n","\n","Cell 4 completed - CASME II confusion matrix analysis generated\n"]}]}]}