{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1e8447ddc3d34d5ab0f195069555399f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cc2f6f9ef3c74f3aafaa3110a36f609a","IPY_MODEL_119ea354fdc34d23a4fbb989a1f6d08f","IPY_MODEL_3e8fc997187b45d5a9b44d613db48a56"],"layout":"IPY_MODEL_65030dbd86ae4d868222fa97f7b1d57a"}},"cc2f6f9ef3c74f3aafaa3110a36f609a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4253356b0354cffa1b00e0f5a2241dc","placeholder":"​","style":"IPY_MODEL_d40e14a174ac436696c27f8bbb726e80","value":"preprocessor_config.json: 100%"}},"119ea354fdc34d23a4fbb989a1f6d08f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e331f98ee6b4d349ebf7c8db525fd1d","max":283,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e18dbdd9043444f6a981ff0fb73298c0","value":283}},"3e8fc997187b45d5a9b44d613db48a56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_732a7dd3cbe84b90a6bd85a1f9052857","placeholder":"​","style":"IPY_MODEL_187219db0f844280978542b25df45749","value":" 283/283 [00:00&lt;00:00, 35.2kB/s]"}},"65030dbd86ae4d868222fa97f7b1d57a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4253356b0354cffa1b00e0f5a2241dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d40e14a174ac436696c27f8bbb726e80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e331f98ee6b4d349ebf7c8db525fd1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e18dbdd9043444f6a981ff0fb73298c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"732a7dd3cbe84b90a6bd85a1f9052857":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"187219db0f844280978542b25df45749":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"887ed5d867d34991837ebec149f6eb9a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba82d944225648dbb1eb7b93510d13eb","IPY_MODEL_2c02729f765a47588bc357f6ca74946b","IPY_MODEL_e8844aaa9bd24de48e659538c536a75c"],"layout":"IPY_MODEL_2bfed666dd93457c8b86ecea1029f331"}},"ba82d944225648dbb1eb7b93510d13eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34f83bf80f1a4cb1b423b039a0ed1f34","placeholder":"​","style":"IPY_MODEL_449dd1a438114deb987467846ec62ade","value":"config.json: "}},"2c02729f765a47588bc357f6ca74946b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b1bbfe53a9c413cb2fc42c8cfd38021","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a2db4918481a4c4387da74520784adb4","value":1}},"e8844aaa9bd24de48e659538c536a75c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef297897660e4b81b6c13615b28b48af","placeholder":"​","style":"IPY_MODEL_1798bda9eff14f309954ed0c8b684341","value":" 69.9k/? [00:00&lt;00:00, 7.36MB/s]"}},"2bfed666dd93457c8b86ecea1029f331":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34f83bf80f1a4cb1b423b039a0ed1f34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"449dd1a438114deb987467846ec62ade":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b1bbfe53a9c413cb2fc42c8cfd38021":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a2db4918481a4c4387da74520784adb4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef297897660e4b81b6c13615b28b48af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1798bda9eff14f309954ed0c8b684341":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9b0f7598fea49cf9e72d09d13bc63c5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42359f58768447c1bfdba50f8077c9ad","IPY_MODEL_c243f6b9155943bc9279b44132df2c02","IPY_MODEL_84af7d7d9192441394f8bf9f39822c62"],"layout":"IPY_MODEL_6e78776167e840d1b28babe8933252db"}},"42359f58768447c1bfdba50f8077c9ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26d36357e43848a899492c671f760668","placeholder":"​","style":"IPY_MODEL_8d7c317eff1747e38c52c7edf4b42f49","value":"model.safetensors: 100%"}},"c243f6b9155943bc9279b44132df2c02":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_031d1fff8108430ba067bae7d500b543","max":293950574,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b2bf407890584e5792907917f1452ed8","value":293950574}},"84af7d7d9192441394f8bf9f39822c62":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f983aff86b8e4209a363cb31b7d955aa","placeholder":"​","style":"IPY_MODEL_dfea51b9033e4bf086a659685010bc4f","value":" 294M/294M [00:01&lt;00:00, 210MB/s]"}},"6e78776167e840d1b28babe8933252db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26d36357e43848a899492c671f760668":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d7c317eff1747e38c52c7edf4b42f49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"031d1fff8108430ba067bae7d500b543":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2bf407890584e5792907917f1452ed8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f983aff86b8e4209a363cb31b7d955aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfea51b9033e4bf086a659685010bc4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"QX-4O9RH28RZ","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1e8447ddc3d34d5ab0f195069555399f","cc2f6f9ef3c74f3aafaa3110a36f609a","119ea354fdc34d23a4fbb989a1f6d08f","3e8fc997187b45d5a9b44d613db48a56","65030dbd86ae4d868222fa97f7b1d57a","a4253356b0354cffa1b00e0f5a2241dc","d40e14a174ac436696c27f8bbb726e80","1e331f98ee6b4d349ebf7c8db525fd1d","e18dbdd9043444f6a981ff0fb73298c0","732a7dd3cbe84b90a6bd85a1f9052857","187219db0f844280978542b25df45749","887ed5d867d34991837ebec149f6eb9a","ba82d944225648dbb1eb7b93510d13eb","2c02729f765a47588bc357f6ca74946b","e8844aaa9bd24de48e659538c536a75c","2bfed666dd93457c8b86ecea1029f331","34f83bf80f1a4cb1b423b039a0ed1f34","449dd1a438114deb987467846ec62ade","5b1bbfe53a9c413cb2fc42c8cfd38021","a2db4918481a4c4387da74520784adb4","ef297897660e4b81b6c13615b28b48af","1798bda9eff14f309954ed0c8b684341","e9b0f7598fea49cf9e72d09d13bc63c5","42359f58768447c1bfdba50f8077c9ad","c243f6b9155943bc9279b44132df2c02","84af7d7d9192441394f8bf9f39822c62","6e78776167e840d1b28babe8933252db","26d36357e43848a899492c671f760668","8d7c317eff1747e38c52c7edf4b42f49","031d1fff8108430ba067bae7d500b543","b2bf407890584e5792907917f1452ed8","f983aff86b8e4209a363cb31b7d955aa","dfea51b9033e4bf086a659685010bc4f"]},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1761288776095,"user_tz":-420,"elapsed":49772,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"5a130588-0363-4156-b272-57937b458be6"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","CASME II MULTI-FRAME SEQUENCE POOLFORMER WITH FACE-AWARE PREPROCESSING\n","============================================================\n","\n","[1] Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully\n","\n","[2] Importing required libraries...\n","CASME II Multi-Frame Sequence PoolFormer - Face-Aware Preprocessing Infrastructure\n","============================================================\n","Loading CASME II v9 preprocessing metadata...\n","Dataset variant: MFS\n","Processing date: 2025-10-19T08:20:12.098301\n","Preprocessing method: face_bbox_expansion_all_directions\n","Total images processed: 2774\n","Face detection rate: 100.00%\n","Target size: 224x224px\n","BBox expansion: 20px (all directions)\n","\n","Dataset split information:\n","  Train samples: 2613\n","  Validation samples: 78\n","  Test samples: 83\n","Using PoolFormer-M48 for enhanced micro-expression recognition (73M parameters)\n","\n","==================================================\n","EXPERIMENT CONFIGURATION - MULTI-FRAME SEQUENCE FACE-AWARE\n","==================================================\n","Dataset: v9 Multi-Frame Sequence with Face-Aware Preprocessing\n","Frame strategy: Multiple frames per video (dense sampling)\n","Training approach: Frame-level independent learning\n","Loss Function: Focal Loss\n","  Gamma: 2.0\n","  Alpha Weights (per-class): [0.048, 0.06, 0.085, 0.093, 0.095, 0.191, 0.427]\n","  Alpha Sum Validation: 0.999\n","PoolFormer Model Variant: M48\n","  Model: sail/poolformer_m48\n","  Parameters: 73M\n","  Feature Dimension: 768\n","Input Resolution: 224x224px (native from v9 preprocessing)\n","Image Format: Grayscale converted to RGB (3-channel)\n","==================================================\n","\n","Device: cuda\n","GPU: NVIDIA L4 (23.8 GB)\n","GPU optimization enabled for NVIDIA L4\n","Large dataset configuration: Batch size 16 (optimal for 2613 samples at 224px)\n","Iterations per epoch: 163 (~163 iterations per epoch)\n","RAM preload workers: 32 (parallel image loading)\n","\n","Loading v9 class distribution...\n","\n","v9 Train distribution: [1027, 650, 325, 273, 260, 65, 13]\n","v9 Val distribution: [30, 18, 9, 9, 6, 3, 3]\n","v9 Test distribution: [30, 21, 12, 8, 9, 3, 0]\n","Applied Focal Loss alpha weights: [0.048 0.06  0.085 0.093 0.095 0.191 0.427]\n","Alpha weights sum: 0.999\n","\n","Setting up PoolFormer Image Processor for 224px input...\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/283 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e8447ddc3d34d5ab0f195069555399f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["PoolFormer Image Processor configured for 224px with token mixing\n","Transform functions with enhanced error handling and validation\n","\n","Dataset paths:\n","Train: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/train\n","Validation: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/val\n","Test: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/test\n","\n","PoolFormer CASME II architecture validation...\n"]},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"887ed5d867d34991837ebec149f6eb9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/294M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9b0f7598fea49cf9e72d09d13bc63c5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["PoolFormer feature dimension: 768\n","Classification head: 768 -> GAP2D -> 512 -> 128 -> 7\n","Dropout rate: 0.3 (balanced for large dataset)\n","Test input shape: torch.Size([1, 3, 224, 224])\n","Test output shape: torch.Size([1, 7])\n","\n","Validation successful: Output shape torch.Size([1, 7])\n","PoolFormer M48 with 73M parameters\n","Token mixing with pooling operations\n","PoolFormer M48 architecture validated successfully\n","\n","============================================================\n","CASME II MULTI-FRAME SEQUENCE POOLFORMER CONFIGURATION COMPLETE\n","============================================================\n","Loss Configuration:\n","  Function: Optimized Focal Loss\n","  Gamma: 2.0\n","  Per-class Alpha: [0.048, 0.06, 0.085, 0.093, 0.095, 0.191, 0.427]\n","  Alpha Sum: 0.999\n","\n","Model Configuration:\n","  Architecture: PoolFormer\n","  Variant: M48\n","  Model: sail/poolformer_m48\n","  Parameters: 73M\n","  Input Resolution: 224px (native from v9 preprocessing)\n","  Feature Dimension: 768\n","  Token Mixing: Pooling operations (attention-free)\n","  Classification Head: 768 -> GAP2D -> 512 -> 128 -> 7\n","\n","Dataset Configuration:\n","  Version: v9\n","  Classes: 7\n","  Frame strategy: multi_frame_sequence\n","  Training approach: frame_level_independent\n","  Inference strategy: late_fusion_aggregation\n","  Weight Optimization: Per-class Alpha\n","\n","Training Configuration:\n","  Train samples: 2613 frames\n","  Validation samples: 78 frames\n","  Test samples: 83 frames\n","  Batch size: 16\n","  Learning rate: 1e-05\n","  Dropout rate: 0.3\n","\n","Next: Cell 2 - Dataset Loading and Multi-Frame PoolFormer Training Pipeline\n"]}],"source":["# @title Cell 1: CASME II Multi-Frame Sequence PoolFormer Infrastructure Configuration\n","\n","# File: 07_03_PoolFormer_CASME2_MFS_Cell1_FIXED.py\n","# Location: experiments/07_03_PoolFormer_CASME2-MFS-PREP.ipynb\n","# Purpose: PoolFormer for CASME II micro-expression recognition with multi-frame sequence strategy and face-aware preprocessing\n","# Fix: Corrected forward function to handle PoolFormer 4D output format\n","\n","from google.colab import drive\n","print(\"=\" * 60)\n","print(\"CASME II MULTI-FRAME SEQUENCE POOLFORMER WITH FACE-AWARE PREPROCESSING\")\n","print(\"=\" * 60)\n","print(\"\\n[1] Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Google Drive mounted successfully\")\n","\n","print(\"\\n[2] Importing required libraries...\")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import json\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import time\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Project paths configuration\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","DATASET_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/preprocessed_v9\"\n","CHECKPOINT_ROOT = f\"{PROJECT_ROOT}/models/07_03_poolformer_casme2_mfs_prep\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/07_03_poolformer_casme2_mfs_prep\"\n","\n","# Load CASME II v9 preprocessing metadata\n","PREPROCESSING_SUMMARY = f\"{DATASET_ROOT}/preprocessing_summary.json\"\n","\n","print(\"CASME II Multi-Frame Sequence PoolFormer - Face-Aware Preprocessing Infrastructure\")\n","print(\"=\" * 60)\n","\n","# Validate preprocessing metadata exists\n","if not os.path.exists(PREPROCESSING_SUMMARY):\n","    raise FileNotFoundError(f\"v9 preprocessing summary not found: {PREPROCESSING_SUMMARY}\")\n","\n","# Load v9 preprocessing metadata\n","print(\"Loading CASME II v9 preprocessing metadata...\")\n","with open(PREPROCESSING_SUMMARY, 'r') as f:\n","    preprocessing_info = json.load(f)\n","\n","print(f\"Dataset variant: {preprocessing_info['variant']}\")\n","print(f\"Processing date: {preprocessing_info['processing_date']}\")\n","print(f\"Preprocessing method: {preprocessing_info['preprocessing_method']}\")\n","print(f\"Total images processed: {preprocessing_info['total_processed']}\")\n","print(f\"Face detection rate: {preprocessing_info['face_detection_stats']['detection_rate']:.2%}\")\n","\n","# Extract preprocessing parameters\n","preproc_params = preprocessing_info['preprocessing_parameters']\n","print(f\"Target size: {preproc_params['target_size']}x{preproc_params['target_size']}px\")\n","print(f\"BBox expansion: {preproc_params['bbox_expansion']}px (all directions)\")\n","\n","# Display split information\n","print(f\"\\nDataset split information:\")\n","print(f\"  Train samples: {preprocessing_info['splits']['train']['total_images']}\")\n","print(f\"  Validation samples: {preprocessing_info['splits']['val']['total_images']}\")\n","print(f\"  Test samples: {preprocessing_info['splits']['test']['total_images']}\")\n","\n","# EXPERIMENT CONFIGURATION - Multi-Frame Sequence with Face-Aware Preprocessing\n","# This configuration supports 4 experiment scenarios:\n","# 1. PoolFormer-M36 + CrossEntropy Loss\n","# 2. PoolFormer-M36 + Focal Loss\n","# 3. PoolFormer-M48 + CrossEntropy Loss\n","# 4. PoolFormer-M48 + Focal Loss\n","#\n","# Toggle POOLFORMER_MODEL_VARIANT for model selection: 'm36' or 'm48'\n","# Toggle USE_FOCAL_LOSS for loss function: False (CrossEntropy) or True (Focal)\n","\n","# FOCAL LOSS CONFIGURATION - Toggle for experimentation\n","USE_FOCAL_LOSS = True\n","FOCAL_LOSS_GAMMA = 2.0\n","\n","# OPTIMIZED CLASS WEIGHTS CONFIGURATION\n","# CASME II classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","# v9 Train distribution: [1027, 650, 325, 273, 260, 65, 13]\n","\n","# CrossEntropy Loss - Optimized inverse square root frequency weights\n","CROSSENTROPY_CLASS_WEIGHTS = [1.00, 1.26, 1.78, 1.94, 1.99, 3.98, 8.90]\n","\n","# Focal Loss - Normalized per-class alpha values\n","FOCAL_LOSS_ALPHA_WEIGHTS = [0.048, 0.060, 0.085, 0.093, 0.095, 0.191, 0.427]\n","\n","# POOLFORMER MODEL CONFIGURATION\n","POOLFORMER_MODEL_VARIANT = 'm48'\n","\n","# Dynamic PoolFormer model selection based on variant\n","if POOLFORMER_MODEL_VARIANT == 'm36':\n","    POOLFORMER_MODEL_NAME = 'sail/poolformer_m36'\n","    EXPECTED_FEATURE_DIM = 768\n","    MODEL_PARAMS = '56M'\n","    print(\"Using PoolFormer-M36 for efficient token mixing analysis (56M parameters)\")\n","elif POOLFORMER_MODEL_VARIANT == 'm48':\n","    POOLFORMER_MODEL_NAME = 'sail/poolformer_m48'\n","    EXPECTED_FEATURE_DIM = 768\n","    MODEL_PARAMS = '73M'\n","    print(\"Using PoolFormer-M48 for enhanced micro-expression recognition (73M parameters)\")\n","else:\n","    raise ValueError(f\"Unsupported POOLFORMER_MODEL_VARIANT: {POOLFORMER_MODEL_VARIANT}\")\n","\n","# Display experiment configuration\n","print(\"\\n\" + \"=\" * 50)\n","print(\"EXPERIMENT CONFIGURATION - MULTI-FRAME SEQUENCE FACE-AWARE\")\n","print(\"=\" * 50)\n","print(f\"Dataset: v9 Multi-Frame Sequence with Face-Aware Preprocessing\")\n","print(f\"Frame strategy: Multiple frames per video (dense sampling)\")\n","print(f\"Training approach: Frame-level independent learning\")\n","print(f\"Loss Function: {'Focal Loss' if USE_FOCAL_LOSS else 'CrossEntropy Loss'}\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Alpha Weights (per-class): {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum Validation: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Class Weights (inverse sqrt freq): {CROSSENTROPY_CLASS_WEIGHTS}\")\n","print(f\"PoolFormer Model Variant: {POOLFORMER_MODEL_VARIANT.upper()}\")\n","print(f\"  Model: {POOLFORMER_MODEL_NAME}\")\n","print(f\"  Parameters: {MODEL_PARAMS}\")\n","print(f\"  Feature Dimension: {EXPECTED_FEATURE_DIM}\")\n","print(f\"Input Resolution: 224x224px (native from v9 preprocessing)\")\n","print(f\"Image Format: Grayscale converted to RGB (3-channel)\")\n","print(\"=\" * 50)\n","\n","# Enhanced GPU configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n","gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n","\n","print(f\"\\nDevice: {device}\")\n","print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n","\n","# Fixed batch size configuration for large dataset\n","BATCH_SIZE = 16\n","NUM_WORKERS = 4\n","\n","if 'A100' in gpu_name or 'L4' in gpu_name:\n","    torch.backends.cudnn.benchmark = True\n","    print(f\"GPU optimization enabled for {gpu_name}\")\n","\n","print(f\"Large dataset configuration: Batch size {BATCH_SIZE} (optimal for 2613 samples at 224px)\")\n","print(f\"Iterations per epoch: {2613 // BATCH_SIZE} (~163 iterations per epoch)\")\n","\n","# RAM preloading workers\n","RAM_PRELOAD_WORKERS = 32\n","print(f\"RAM preload workers: {RAM_PRELOAD_WORKERS} (parallel image loading)\")\n","\n","# CASME II class mapping and analysis\n","CASME2_CLASSES = ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CASME2_CLASSES)}\n","\n","# Extract class distribution from v9 preprocessing metadata\n","print(\"\\nLoading v9 class distribution...\")\n","train_dist = preprocessing_info['splits']['train']['emotion_distribution']\n","val_dist = preprocessing_info['splits']['val']['emotion_distribution']\n","test_dist = preprocessing_info['splits']['test']['emotion_distribution']\n","\n","# Convert to ordered list matching CASME2_CLASSES\n","def emotion_dist_to_list(emotion_dict, class_names):\n","    \"\"\"Convert emotion distribution dict to ordered list\"\"\"\n","    return [emotion_dict.get(cls, 0) for cls in class_names]\n","\n","train_dist_list = emotion_dist_to_list(train_dist, CASME2_CLASSES)\n","val_dist_list = emotion_dist_to_list(val_dist, CASME2_CLASSES)\n","test_dist_list = emotion_dist_to_list(test_dist, CASME2_CLASSES)\n","\n","print(f\"\\nv9 Train distribution: {train_dist_list}\")\n","print(f\"v9 Val distribution: {val_dist_list}\")\n","print(f\"v9 Test distribution: {test_dist_list}\")\n","\n","# Apply optimized class weights based on loss function selection\n","if USE_FOCAL_LOSS:\n","    class_weights = torch.tensor(FOCAL_LOSS_ALPHA_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied Focal Loss alpha weights: {class_weights.cpu().numpy()}\")\n","    print(f\"Alpha weights sum: {class_weights.sum().item():.3f}\")\n","else:\n","    class_weights = torch.tensor(CROSSENTROPY_CLASS_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied CrossEntropy class weights: {class_weights.cpu().numpy()}\")\n","\n","# CASME II PoolFormer Configuration\n","CASME2_POOLFORMER_CONFIG = {\n","    # Architecture configuration\n","    'poolformer_model': POOLFORMER_MODEL_NAME,\n","    'model_variant': POOLFORMER_MODEL_VARIANT,\n","    'model_params': MODEL_PARAMS,\n","    'input_size': 224,\n","    'num_classes': 7,\n","    'dropout_rate': 0.3,\n","    'expected_feature_dim': EXPECTED_FEATURE_DIM,\n","\n","    # Training configuration\n","    'learning_rate': 2e-5,\n","    'weight_decay': 1e-5,\n","    'gradient_clip': 1.0,\n","    'num_epochs': 50,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","\n","    # Scheduler configuration\n","    'scheduler_type': 'plateau',\n","    'scheduler_mode': 'max',\n","    'scheduler_monitor': 'validation_f1',\n","    'scheduler_factor': 0.5,\n","    'scheduler_patience': 3,\n","    'scheduler_min_lr': 1e-7,\n","\n","    # Dataset configuration\n","    'dataset_version': 'v9',\n","    'preprocessing_method': 'face_aware_bbox_expansion',\n","    'frame_strategy': 'multi_frame_sequence',\n","    'training_approach': 'frame_level_independent',\n","    'inference_strategy': 'late_fusion_aggregation',\n","\n","    # Loss configuration\n","    'use_focal_loss': USE_FOCAL_LOSS,\n","    'focal_loss_gamma': FOCAL_LOSS_GAMMA,\n","    'focal_loss_alpha_weights': FOCAL_LOSS_ALPHA_WEIGHTS,\n","    'crossentropy_class_weights': CROSSENTROPY_CLASS_WEIGHTS\n","}\n","\n","# Optimized Focal Loss implementation\n","class OptimizedFocalLoss(nn.Module):\n","    \"\"\"Optimized Focal Loss with per-class alpha weights\"\"\"\n","\n","    def __init__(self, alpha=None, gamma=2.0):\n","        super(OptimizedFocalLoss, self).__init__()\n","        self.gamma = gamma\n","        if alpha is not None:\n","            self.alpha = torch.tensor(alpha, dtype=torch.float32)\n","        else:\n","            self.alpha = None\n","\n","    def forward(self, inputs, targets):\n","        if self.alpha is not None and self.alpha.device != inputs.device:\n","            self.alpha = self.alpha.to(inputs.device)\n","\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = (1 - pt) ** self.gamma * ce_loss\n","\n","        if self.alpha is not None:\n","            alpha_t = self.alpha[targets]\n","            focal_loss = alpha_t * focal_loss\n","\n","        return focal_loss.mean()\n","\n","# PoolFormer CASME II Baseline Model - FIXED VERSION\n","class PoolFormerCASME2Baseline(nn.Module):\n","    \"\"\"PoolFormer baseline model for CASME II micro-expression recognition with corrected forward pass\"\"\"\n","\n","    def __init__(self, num_classes=7, dropout_rate=0.3):\n","        super(PoolFormerCASME2Baseline, self).__init__()\n","\n","        from transformers import PoolFormerModel\n","\n","        self.poolformer = PoolFormerModel.from_pretrained(\n","            CASME2_POOLFORMER_CONFIG['poolformer_model']\n","        )\n","\n","        hidden_size = CASME2_POOLFORMER_CONFIG['expected_feature_dim']\n","\n","        # Classifier with 2D adaptive pooling for PoolFormer 4D output\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(hidden_size, 512),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(512, 128),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(128, num_classes)\n","        )\n","\n","        print(f\"PoolFormer feature dimension: {hidden_size}\")\n","        print(f\"Classification head: {hidden_size} -> GAP2D -> 512 -> 128 -> {num_classes}\")\n","        print(f\"Dropout rate: {dropout_rate} (balanced for large dataset)\")\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass with proper handling of PoolFormer output format\n","\n","        PoolFormer returns last_hidden_state which can be:\n","        - 3D: [batch, seq_len, hidden_dim]\n","        - 4D: [batch, hidden_dim, height, width]\n","\n","        This implementation handles both formats\n","        \"\"\"\n","        outputs = self.poolformer(pixel_values=x)\n","        pooled_output = outputs.last_hidden_state\n","\n","        # Handle different output formats from PoolFormer\n","        if pooled_output.dim() == 4:\n","            # 4D output: [batch, channels, H, W]\n","            # Apply 2D adaptive pooling\n","            pooled_output = self.adaptive_pool(pooled_output)\n","        elif pooled_output.dim() == 3:\n","            # 3D output: [batch, seq_len, hidden_dim]\n","            # Reshape to 4D for consistent processing\n","            batch_size, seq_len, hidden_dim = pooled_output.shape\n","            # Assume square spatial dimensions\n","            spatial_size = int(np.sqrt(seq_len))\n","            if spatial_size * spatial_size == seq_len:\n","                pooled_output = pooled_output.permute(0, 2, 1).reshape(batch_size, hidden_dim, spatial_size, spatial_size)\n","                pooled_output = self.adaptive_pool(pooled_output)\n","            else:\n","                # Fallback: global average pooling over sequence dimension\n","                pooled_output = pooled_output.mean(dim=1, keepdim=True).unsqueeze(-1)\n","        else:\n","            raise ValueError(f\"Unexpected PoolFormer output shape: {pooled_output.shape}\")\n","\n","        # Pass through classifier\n","        logits = self.classifier(pooled_output)\n","\n","        return logits\n","\n","# Enhanced optimizer and scheduler factory\n","def create_optimizer_scheduler_casme2(model, config):\n","    \"\"\"Create optimizer and scheduler for CASME II PoolFormer training\"\"\"\n","\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=config['learning_rate'],\n","        weight_decay=config['weight_decay'],\n","        betas=(0.9, 0.999)\n","    )\n","\n","    if config['scheduler_type'] == 'plateau':\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer,\n","            mode=config['scheduler_mode'],\n","            factor=config['scheduler_factor'],\n","            patience=config['scheduler_patience'],\n","            min_lr=config['scheduler_min_lr']\n","        )\n","        print(f\"Scheduler: ReduceLROnPlateau monitoring {config['scheduler_monitor']}\")\n","    else:\n","        scheduler = None\n","\n","    return optimizer, scheduler\n","\n","# PoolFormer Image Processor setup for 224px input\n","from transformers import PoolFormerImageProcessor\n","\n","print(\"\\nSetting up PoolFormer Image Processor for 224px input...\")\n","\n","poolformer_processor = PoolFormerImageProcessor.from_pretrained(\n","    CASME2_POOLFORMER_CONFIG['poolformer_model'],\n","    do_resize=True,\n","    size={'height': 224, 'width': 224},\n","    do_normalize=True,\n","    do_rescale=True,\n","    do_center_crop=False\n",")\n","\n","# Transform functions for PoolFormer with proper error handling\n","def poolformer_transform_train(image):\n","    \"\"\"Training transform with PoolFormer Image Processor\"\"\"\n","    try:\n","        if not isinstance(image, Image.Image):\n","            raise TypeError(f\"Expected PIL Image, got {type(image)}\")\n","\n","        inputs = poolformer_processor(images=image, return_tensors=\"pt\")\n","        pixel_values = inputs['pixel_values']\n","\n","        # Remove batch dimension: [1, C, H, W] -> [C, H, W]\n","        if pixel_values.dim() == 4 and pixel_values.size(0) == 1:\n","            pixel_values = pixel_values.squeeze(0)\n","\n","        return pixel_values\n","\n","    except Exception as e:\n","        print(f\"Error in poolformer_transform_train: {e}\")\n","        print(f\"Image type: {type(image)}\")\n","        if isinstance(image, Image.Image):\n","            print(f\"Image size: {image.size}\")\n","        raise\n","\n","def poolformer_transform_val(image):\n","    \"\"\"Validation transform with PoolFormer Image Processor\"\"\"\n","    try:\n","        if not isinstance(image, Image.Image):\n","            raise TypeError(f\"Expected PIL Image, got {type(image)}\")\n","\n","        inputs = poolformer_processor(images=image, return_tensors=\"pt\")\n","        pixel_values = inputs['pixel_values']\n","\n","        # Remove batch dimension: [1, C, H, W] -> [C, H, W]\n","        if pixel_values.dim() == 4 and pixel_values.size(0) == 1:\n","            pixel_values = pixel_values.squeeze(0)\n","\n","        return pixel_values\n","\n","    except Exception as e:\n","        print(f\"Error in poolformer_transform_val: {e}\")\n","        print(f\"Image type: {type(image)}\")\n","        if isinstance(image, Image.Image):\n","            print(f\"Image size: {image.size}\")\n","        raise\n","\n","print(\"PoolFormer Image Processor configured for 224px with token mixing\")\n","print(\"Transform functions with enhanced error handling and validation\")\n","\n","# Custom Dataset class for CASME II\n","class CASME2Dataset(Dataset):\n","    \"\"\"Custom dataset class for CASME II with flexible file loading\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading CASME II {split} dataset...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        if len(all_files) > 0:\n","            print(f\"Sample filename: {all_files[0]}\")\n","\n","        loaded_count = 0\n","        skipped_count = 0\n","\n","        for filename in sorted(all_files):\n","            emotion_found = None\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                loaded_count += 1\n","            else:\n","                skipped_count += 1\n","                if skipped_count <= 3:\n","                    print(f\"  Skipped (no emotion found): {filename}\")\n","\n","        print(f\"Loaded {len(self.images)} samples for {split} split\")\n","        if skipped_count > 0:\n","            print(f\"  Skipped {skipped_count} files (no recognizable emotion)\")\n","\n","        if len(self.images) == 0:\n","            print(f\"ERROR: No samples loaded! Check filename format and emotion labels.\")\n","            print(f\"Expected emotions in filenames: {CASME2_CLASSES}\")\n","\n","        self._print_distribution()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"  No samples to display distribution\")\n","            return\n","\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.images[idx]\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","# Create directories\n","os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/training_logs\", exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/evaluation_results\", exist_ok=True)\n","\n","print(f\"\\nDataset paths:\")\n","print(f\"Train: {DATASET_ROOT}/train\")\n","print(f\"Validation: {DATASET_ROOT}/val\")\n","print(f\"Test: {DATASET_ROOT}/test\")\n","\n","# Architecture validation with enhanced error handling\n","print(\"\\nPoolFormer CASME II architecture validation...\")\n","\n","try:\n","    test_model = PoolFormerCASME2Baseline(num_classes=7, dropout_rate=0.3).to(device)\n","    test_input = torch.randn(1, 3, 224, 224).to(device)\n","\n","    print(f\"Test input shape: {test_input.shape}\")\n","    test_output = test_model(test_input)\n","    print(f\"Test output shape: {test_output.shape}\")\n","\n","    print(f\"\\nValidation successful: Output shape {test_output.shape}\")\n","    print(f\"PoolFormer {POOLFORMER_MODEL_VARIANT.upper()} with {MODEL_PARAMS} parameters\")\n","    print(f\"Token mixing with pooling operations\")\n","    print(f\"PoolFormer {POOLFORMER_MODEL_VARIANT.upper()} architecture validated successfully\")\n","\n","    del test_model, test_input, test_output\n","    torch.cuda.empty_cache()\n","\n","except Exception as e:\n","    print(f\"Validation failed: {e}\")\n","    import traceback\n","    traceback.print_exc()\n","\n","# Loss function factory\n","def create_criterion_casme2(weights, use_focal_loss=False, alpha_weights=None, gamma=2.0):\n","    \"\"\"\n","    Factory function to create loss criterion based on configuration\n","\n","    Args:\n","        weights: Class weights for CrossEntropy\n","        use_focal_loss: Whether to use Focal Loss or CrossEntropy\n","        alpha_weights: Per-class alpha weights for Focal Loss\n","        gamma: Focal loss gamma parameter\n","\n","    Returns:\n","        Loss function\n","    \"\"\"\n","    if use_focal_loss:\n","        print(f\"Using Optimized Focal Loss with gamma={gamma}\")\n","        if alpha_weights:\n","            print(f\"Per-class alpha weights: {alpha_weights}\")\n","            print(f\"Alpha sum: {sum(alpha_weights):.3f}\")\n","        return OptimizedFocalLoss(alpha=alpha_weights, gamma=gamma)\n","    else:\n","        print(f\"Using CrossEntropy Loss with optimized class weights\")\n","        print(f\"Class weights: {weights.cpu().numpy()}\")\n","        return nn.CrossEntropyLoss(weight=weights)\n","\n","# Global configuration for training pipeline\n","GLOBAL_CONFIG_CASME2 = {\n","    'device': device,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'num_classes': 7,\n","    'class_weights': class_weights,\n","    'class_names': CASME2_CLASSES,\n","    'class_to_idx': CLASS_TO_IDX,\n","    'transform_train': poolformer_transform_train,\n","    'transform_val': poolformer_transform_val,\n","    'poolformer_config': CASME2_POOLFORMER_CONFIG,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'results_root': RESULTS_ROOT,\n","    'dataset_root': DATASET_ROOT,\n","    'optimizer_scheduler_factory': create_optimizer_scheduler_casme2,\n","    'criterion_factory': create_criterion_casme2\n","}\n","\n","# Configuration validation and summary\n","print(\"\\n\" + \"=\" * 60)\n","print(\"CASME II MULTI-FRAME SEQUENCE POOLFORMER CONFIGURATION COMPLETE\")\n","print(\"=\" * 60)\n","\n","print(f\"Loss Configuration:\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Function: Optimized Focal Loss\")\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Per-class Alpha: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Function: CrossEntropy with Optimized Weights\")\n","    print(f\"  Class Weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","\n","print(f\"\\nModel Configuration:\")\n","print(f\"  Architecture: PoolFormer\")\n","print(f\"  Variant: {POOLFORMER_MODEL_VARIANT.upper()}\")\n","print(f\"  Model: {POOLFORMER_MODEL_NAME}\")\n","print(f\"  Parameters: {MODEL_PARAMS}\")\n","print(f\"  Input Resolution: 224px (native from v9 preprocessing)\")\n","print(f\"  Feature Dimension: {EXPECTED_FEATURE_DIM}\")\n","print(f\"  Token Mixing: Pooling operations (attention-free)\")\n","print(f\"  Classification Head: {EXPECTED_FEATURE_DIM} -> GAP2D -> 512 -> 128 -> 7\")\n","\n","print(f\"\\nDataset Configuration:\")\n","print(f\"  Version: {CASME2_POOLFORMER_CONFIG['dataset_version']}\")\n","print(f\"  Classes: {len(CASME2_CLASSES)}\")\n","print(f\"  Frame strategy: {CASME2_POOLFORMER_CONFIG['frame_strategy']}\")\n","print(f\"  Training approach: {CASME2_POOLFORMER_CONFIG['training_approach']}\")\n","print(f\"  Inference strategy: {CASME2_POOLFORMER_CONFIG['inference_strategy']}\")\n","print(f\"  Weight Optimization: {'Per-class Alpha' if USE_FOCAL_LOSS else 'Inverse Sqrt Frequency'}\")\n","\n","print(f\"\\nTraining Configuration:\")\n","print(f\"  Train samples: {preprocessing_info['splits']['train']['total_images']} frames\")\n","print(f\"  Validation samples: {preprocessing_info['splits']['val']['total_images']} frames\")\n","print(f\"  Test samples: {preprocessing_info['splits']['test']['total_images']} frames\")\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","print(f\"  Learning rate: {CASME2_POOLFORMER_CONFIG['learning_rate']}\")\n","print(f\"  Dropout rate: {CASME2_POOLFORMER_CONFIG['dropout_rate']}\")\n","\n","print(\"\\nNext: Cell 2 - Dataset Loading and Multi-Frame PoolFormer Training Pipeline\")"]},{"cell_type":"code","source":["# @title Cell 2: CASME II Multi-Frame Sequence PoolFormer Training Pipeline\n","\n","# File: 07_03_PoolFormer_CASME2_MFS_Cell2_FIXED.py\n","# Location: experiments/07_03_PoolFormer_CASME2-MFS-PREP.ipynb\n","# Purpose: Enhanced training pipeline for CASME II Multi-Frame Sequence PoolFormer with optimized RAM caching\n","# Fix: Added custom collate function for PoolFormer processor compatibility\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing as mp\n","import shutil\n","import tempfile\n","\n","print(\"CASME II Multi-Frame Sequence PoolFormer Training Pipeline\")\n","print(\"=\" * 70)\n","print(f\"Loss Function: {'Optimized Focal Loss' if CASME2_POOLFORMER_CONFIG['use_focal_loss'] else 'CrossEntropy Loss'}\")\n","if CASME2_POOLFORMER_CONFIG['use_focal_loss']:\n","    print(f\"Focal Loss Parameters:\")\n","    print(f\"  Gamma: {CASME2_POOLFORMER_CONFIG['focal_loss_gamma']}\")\n","    print(f\"  Per-class Alpha: {CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights']}\")\n","    print(f\"  Alpha Sum: {sum(CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","else:\n","    print(f\"CrossEntropy Parameters:\")\n","    print(f\"  Optimized Class Weights: {CASME2_POOLFORMER_CONFIG['crossentropy_class_weights']}\")\n","print(f\"Dataset version: {CASME2_POOLFORMER_CONFIG['dataset_version']}\")\n","print(f\"Frame strategy: {CASME2_POOLFORMER_CONFIG['frame_strategy']}\")\n","print(f\"Training approach: {CASME2_POOLFORMER_CONFIG['training_approach']}\")\n","print(f\"PoolFormer variant: {CASME2_POOLFORMER_CONFIG['model_variant'].upper()}\")\n","print(f\"Model parameters: {CASME2_POOLFORMER_CONFIG['model_params']}\")\n","print(f\"Training epochs: {CASME2_POOLFORMER_CONFIG['num_epochs']}\")\n","print(f\"Batch size: {CASME2_POOLFORMER_CONFIG['batch_size']}\")\n","print(f\"Scheduler patience: {CASME2_POOLFORMER_CONFIG['scheduler_patience']}\")\n","\n","# Enhanced CASME II Dataset with optimized RAM caching for large dataset\n","class CASME2DatasetTraining(Dataset):\n","    \"\"\"Enhanced CASME II dataset for training with RAM caching optimization for large dataset\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","        self.cached_images = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading CASME II {split} dataset for training...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        if len(all_files) > 0:\n","            print(f\"Sample filename: {all_files[0]}\")\n","\n","        loaded_count = 0\n","        skipped_count = 0\n","\n","        for filename in sorted(all_files):\n","            emotion_found = None\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                loaded_count += 1\n","            else:\n","                skipped_count += 1\n","                if skipped_count <= 3:\n","                    print(f\"  Skipped (no emotion found): {filename}\")\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples\")\n","        if skipped_count > 0:\n","            print(f\"  Skipped {skipped_count} files (no recognizable emotion)\")\n","\n","        if len(self.images) == 0:\n","            print(f\"ERROR: No samples loaded! Check filename format and emotion labels.\")\n","            print(f\"Expected emotions in filenames: {CASME2_CLASSES}\")\n","\n","        self._print_distribution()\n","\n","        if self.use_ram_cache and len(self.images) > 0:\n","            self._preload_to_ram()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"  No samples to display distribution\")\n","            return\n","\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def _preload_to_ram(self):\n","        \"\"\"RAM preloading with parallel loading for training efficiency\"\"\"\n","        if len(self.images) == 0:\n","            print(f\"Skipping RAM preload: No images to load\")\n","            return\n","\n","        print(f\"Preloading {len(self.images)} {self.split} images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            \"\"\"Load single image with error handling\"\"\"\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (224, 224), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=f\"Loading {self.split} to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * 224 * 224 * 3 * 4 / 1e9\n","        print(f\"{self.split.upper()} RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (224, 224), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","# Custom collate function for PoolFormer processor compatibility\n","def poolformer_collate_fn(batch):\n","    \"\"\"\n","    Custom collate function to handle PoolFormer processor output\n","\n","    PoolFormer processor may return tensors with unexpected shapes or additional metadata\n","    This function ensures proper batching of (image, label, filename) tuples\n","\n","    Args:\n","        batch: List of tuples from dataset __getitem__\n","\n","    Returns:\n","        tuple: (batched_images, batched_labels, filenames_list)\n","    \"\"\"\n","    images = []\n","    labels = []\n","    filenames = []\n","\n","    for item in batch:\n","        # Ensure we extract exactly 3 elements from each batch item\n","        if len(item) == 3:\n","            img, lbl, fname = item\n","            images.append(img)\n","            labels.append(lbl)\n","            filenames.append(fname)\n","        else:\n","            # Log unexpected batch structure but continue processing\n","            print(f\"Warning: Unexpected batch item with {len(item)} elements, skipping...\")\n","            continue\n","\n","    # Validate we have data to batch\n","    if len(images) == 0:\n","        raise ValueError(\"No valid samples in batch after filtering\")\n","\n","    # Stack images into batch tensor\n","    # PoolFormer expects [batch_size, channels, height, width]\n","    images = torch.stack(images, dim=0)\n","\n","    # Convert labels to tensor\n","    labels = torch.tensor(labels, dtype=torch.long)\n","\n","    return images, labels, filenames\n","\n","# Enhanced metrics calculation with comprehensive error handling\n","def calculate_metrics_safe_robust(predictions, labels, class_names, average='macro'):\n","    \"\"\"\n","    Calculate metrics with enhanced error handling and validation\n","\n","    Args:\n","        predictions: Predicted labels\n","        labels: True labels\n","        class_names: List of class names\n","        average: Averaging method for metrics\n","\n","    Returns:\n","        dict: Computed metrics\n","    \"\"\"\n","    try:\n","        predictions = np.array(predictions)\n","        labels = np.array(labels)\n","\n","        if len(predictions) == 0 or len(labels) == 0:\n","            return {\n","                'accuracy': 0.0,\n","                'precision': 0.0,\n","                'recall': 0.0,\n","                'f1_score': 0.0\n","            }\n","\n","        if len(predictions) != len(labels):\n","            print(f\"Warning: Prediction and label length mismatch: {len(predictions)} vs {len(labels)}\")\n","            min_len = min(len(predictions), len(labels))\n","            predictions = predictions[:min_len]\n","            labels = labels[:min_len]\n","\n","        accuracy = accuracy_score(labels, predictions)\n","\n","        unique_labels = np.unique(np.concatenate([labels, predictions]))\n","        labels_present = [i for i in range(len(class_names)) if i in unique_labels]\n","\n","        precision, recall, f1, _ = precision_recall_fscore_support(\n","            labels, predictions,\n","            labels=labels_present,\n","            average=average,\n","            zero_division=0\n","        )\n","\n","        return {\n","            'accuracy': float(accuracy),\n","            'precision': float(precision),\n","            'recall': float(recall),\n","            'f1_score': float(f1)\n","        }\n","\n","    except Exception as e:\n","        print(f\"Error in metrics calculation: {e}\")\n","        return {\n","            'accuracy': 0.0,\n","            'precision': 0.0,\n","            'recall': 0.0,\n","            'f1_score': 0.0\n","        }\n","\n","# Enhanced training epoch with comprehensive validation\n","def train_epoch(model, dataloader, criterion, optimizer, device, epoch, total_epochs):\n","    \"\"\"Enhanced training epoch with comprehensive validation and progress tracking\"\"\"\n","    model.train()\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_labels = []\n","    all_filenames = []\n","    batch_count = 0\n","\n","    pbar = tqdm(dataloader, desc=f\"Train Epoch {epoch+1}/{total_epochs}\")\n","\n","    for batch_data in pbar:\n","        try:\n","            # Unpack batch data with validation\n","            if len(batch_data) != 3:\n","                print(f\"Warning: Expected 3 values from dataloader, got {len(batch_data)}\")\n","                print(f\"Batch data types: {[type(x) for x in batch_data]}\")\n","                continue\n","\n","            images, labels, filenames = batch_data\n","\n","            images = images.to(device, non_blocking=True)\n","            labels = labels.to(device, non_blocking=True)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","\n","            if outputs is None or torch.isnan(outputs).any() or torch.isinf(outputs).any():\n","                print(f\"Warning: Invalid model outputs detected at batch {batch_count}\")\n","                continue\n","\n","            loss = criterion(outputs, labels)\n","\n","            if torch.isnan(loss) or torch.isinf(loss):\n","                print(f\"Warning: Invalid loss detected at batch {batch_count}\")\n","                continue\n","\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            batch_count += 1\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","            all_filenames.extend(filenames)\n","\n","            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n","\n","        except Exception as e:\n","            print(f\"Error in training batch {batch_count}: {e}\")\n","            continue\n","\n","    avg_loss = running_loss / max(batch_count, 1)\n","\n","    metrics = calculate_metrics_safe_robust(\n","        all_predictions, all_labels, CASME2_CLASSES, average='macro'\n","    )\n","\n","    return avg_loss, metrics, all_filenames\n","\n","# Enhanced validation epoch\n","def validate_epoch(model, dataloader, criterion, device, epoch, total_epochs):\n","    \"\"\"Enhanced validation epoch with comprehensive metrics\"\"\"\n","    model.eval()\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_labels = []\n","    all_filenames = []\n","    batch_count = 0\n","\n","    pbar = tqdm(dataloader, desc=f\"Val Epoch {epoch+1}/{total_epochs}\")\n","\n","    with torch.no_grad():\n","        for images, labels, filenames in pbar:\n","            try:\n","                images = images.to(device, non_blocking=True)\n","                labels = labels.to(device, non_blocking=True)\n","\n","                outputs = model(images)\n","\n","                if outputs is None or torch.isnan(outputs).any() or torch.isinf(outputs).any():\n","                    print(f\"Warning: Invalid validation outputs at batch {batch_count}\")\n","                    continue\n","\n","                loss = criterion(outputs, labels)\n","\n","                if torch.isnan(loss) or torch.isinf(loss):\n","                    print(f\"Warning: Invalid validation loss at batch {batch_count}\")\n","                    continue\n","\n","                running_loss += loss.item()\n","                batch_count += 1\n","\n","                _, predicted = torch.max(outputs, 1)\n","\n","                all_predictions.extend(predicted.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","                all_filenames.extend(filenames)\n","\n","                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n","\n","            except Exception as e:\n","                print(f\"Error in validation batch {batch_count}: {e}\")\n","                continue\n","\n","    avg_loss = running_loss / max(batch_count, 1)\n","\n","    metrics = calculate_metrics_safe_robust(\n","        all_predictions, all_labels, CASME2_CLASSES, average='macro'\n","    )\n","\n","    return avg_loss, metrics, all_filenames\n","\n","# Enhanced atomic checkpoint saving\n","def save_checkpoint_robust(model, optimizer, scheduler, epoch, train_metrics, val_metrics,\n","                          checkpoint_root, best_metrics, config):\n","    \"\"\"Enhanced atomic checkpoint saving with validation\"\"\"\n","    try:\n","        checkpoint_data = {\n","            'epoch': epoch + 1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","            'train_metrics': train_metrics,\n","            'val_metrics': val_metrics,\n","            'best_metrics': best_metrics,\n","            'config': config\n","        }\n","\n","        checkpoint_path = os.path.join(checkpoint_root, 'casme2_poolformer_mfs_best_f1.pth')\n","\n","        with tempfile.NamedTemporaryFile(mode='wb', delete=False, dir=checkpoint_root, suffix='.tmp') as tmp_file:\n","            torch.save(checkpoint_data, tmp_file.name)\n","            tmp_path = tmp_file.name\n","\n","        if os.path.exists(checkpoint_path):\n","            backup_path = checkpoint_path + '.backup'\n","            shutil.copy2(checkpoint_path, backup_path)\n","\n","        shutil.move(tmp_path, checkpoint_path)\n","\n","        if os.path.exists(checkpoint_path + '.backup'):\n","            os.remove(checkpoint_path + '.backup')\n","\n","        checkpoint_size = os.path.getsize(checkpoint_path) / (1024 * 1024)\n","\n","        if checkpoint_size < 10:\n","            print(f\"Warning: Checkpoint size unusually small ({checkpoint_size:.1f}MB)\")\n","            return None\n","\n","        return checkpoint_path\n","\n","    except Exception as e:\n","        print(f\"ERROR: Checkpoint save failed: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","        if 'tmp_path' in locals() and os.path.exists(tmp_path):\n","            os.remove(tmp_path)\n","\n","        return None\n","\n","# JSON serialization helper\n","def safe_json_serialize(obj):\n","    \"\"\"Convert numpy types to Python native types for JSON serialization\"\"\"\n","    if isinstance(obj, dict):\n","        return {key: safe_json_serialize(value) for key, value in obj.items()}\n","    elif isinstance(obj, list):\n","        return [safe_json_serialize(item) for item in obj]\n","    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n","        return int(obj)\n","    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n","        return float(obj)\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    elif isinstance(obj, torch.Tensor):\n","        return obj.cpu().numpy().tolist()\n","    else:\n","        return obj\n","\n","# Dataset loading with RAM caching\n","print(\"\\n\" + \"=\" * 70)\n","print(\"LOADING CASME II DATASETS WITH RAM CACHING\")\n","print(\"=\" * 70)\n","\n","train_dataset = CASME2DatasetTraining(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='train',\n","    transform=GLOBAL_CONFIG_CASME2['transform_train'],\n","    use_ram_cache=True\n",")\n","\n","val_dataset = CASME2DatasetTraining(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='val',\n","    transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","    use_ram_cache=True\n",")\n","\n","# DataLoader with custom collate function for PoolFormer compatibility\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=GLOBAL_CONFIG_CASME2['batch_size'],\n","    shuffle=True,\n","    num_workers=GLOBAL_CONFIG_CASME2['num_workers'],\n","    pin_memory=True,\n","    persistent_workers=True if GLOBAL_CONFIG_CASME2['num_workers'] > 0 else False,\n","    collate_fn=poolformer_collate_fn\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=GLOBAL_CONFIG_CASME2['batch_size'],\n","    shuffle=False,\n","    num_workers=GLOBAL_CONFIG_CASME2['num_workers'],\n","    pin_memory=True,\n","    persistent_workers=True if GLOBAL_CONFIG_CASME2['num_workers'] > 0 else False,\n","    collate_fn=poolformer_collate_fn\n",")\n","\n","print(f\"\\nDataset loading completed:\")\n","print(f\"Train samples: {len(train_dataset)}, batches: {len(train_loader)}\")\n","print(f\"Validation samples: {len(val_dataset)}, batches: {len(val_loader)}\")\n","print(f\"Custom collate function applied for PoolFormer processor compatibility\")\n","\n","# Model initialization\n","print(\"\\n\" + \"=\" * 70)\n","print(\"INITIALIZING POOLFORMER MODEL\")\n","print(\"=\" * 70)\n","\n","model = PoolFormerCASME2Baseline(\n","    num_classes=GLOBAL_CONFIG_CASME2['num_classes'],\n","    dropout_rate=CASME2_POOLFORMER_CONFIG['dropout_rate']\n",").to(GLOBAL_CONFIG_CASME2['device'])\n","\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f\"Model: PoolFormerCASME2Baseline\")\n","print(f\"Variant: {POOLFORMER_MODEL_VARIANT.upper()}\")\n","print(f\"Total parameters: {total_params:,}\")\n","print(f\"Trainable parameters: {trainable_params:,}\")\n","print(f\"Feature dimension: {EXPECTED_FEATURE_DIM}\")\n","\n","# Optimizer and criterion setup\n","optimizer, scheduler = GLOBAL_CONFIG_CASME2['optimizer_scheduler_factory'](\n","    model, CASME2_POOLFORMER_CONFIG\n",")\n","\n","criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","    weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","    use_focal_loss=CASME2_POOLFORMER_CONFIG['use_focal_loss'],\n","    alpha_weights=CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights'],\n","    gamma=CASME2_POOLFORMER_CONFIG['focal_loss_gamma']\n",")\n","\n","print(f\"Optimizer: {optimizer.__class__.__name__}\")\n","print(f\"Learning rate: {CASME2_POOLFORMER_CONFIG['learning_rate']}\")\n","print(f\"Weight decay: {CASME2_POOLFORMER_CONFIG['weight_decay']}\")\n","print(f\"Scheduler: ReduceLROnPlateau (patience={CASME2_POOLFORMER_CONFIG['scheduler_patience']})\")\n","\n","# Training initialization\n","print(\"\\n\" + \"=\" * 70)\n","print(\"STARTING TRAINING\")\n","print(\"=\" * 70)\n","\n","training_history = {\n","    'train_loss': [],\n","    'val_loss': [],\n","    'train_f1': [],\n","    'val_f1': [],\n","    'train_acc': [],\n","    'val_acc': [],\n","    'learning_rate': [],\n","    'epoch_time': []\n","}\n","\n","best_metrics = {\n","    'f1': 0.0,\n","    'loss': float('inf'),\n","    'accuracy': 0.0,\n","    'epoch': 0\n","}\n","\n","start_time = time.time()\n","\n","for epoch in range(CASME2_POOLFORMER_CONFIG['num_epochs']):\n","    epoch_start_time = time.time()\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"Epoch {epoch+1}/{CASME2_POOLFORMER_CONFIG['num_epochs']}\")\n","    print(f\"{'='*70}\")\n","\n","    # Training phase\n","    train_loss, train_metrics, train_filenames = train_epoch(\n","        model, train_loader, criterion, optimizer,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_POOLFORMER_CONFIG['num_epochs']\n","    )\n","\n","    # Validation phase\n","    val_loss, val_metrics, val_filenames = validate_epoch(\n","        model, val_loader, criterion,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_POOLFORMER_CONFIG['num_epochs']\n","    )\n","\n","    # Update scheduler\n","    if scheduler:\n","        scheduler.step(val_metrics['f1_score'])\n","\n","    # Record training history\n","    epoch_time = time.time() - epoch_start_time\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    training_history['train_loss'].append(float(train_loss))\n","    training_history['val_loss'].append(float(val_loss))\n","    training_history['train_f1'].append(float(train_metrics['f1_score']))\n","    training_history['val_f1'].append(float(val_metrics['f1_score']))\n","    training_history['train_acc'].append(float(train_metrics['accuracy']))\n","    training_history['val_acc'].append(float(val_metrics['accuracy']))\n","    training_history['learning_rate'].append(float(current_lr))\n","    training_history['epoch_time'].append(float(epoch_time))\n","\n","    # Print epoch summary\n","    print(f\"Train - Loss: {train_loss:.4f}, F1: {train_metrics['f1_score']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n","    print(f\"Val   - Loss: {val_loss:.4f}, F1: {val_metrics['f1_score']:.4f}, Acc: {val_metrics['accuracy']:.4f}\")\n","    print(f\"Time  - Epoch: {epoch_time:.1f}s, LR: {current_lr:.2e}\")\n","\n","    # Enhanced multi-criteria checkpoint saving logic\n","    save_model = False\n","    improvement_reason = \"\"\n","\n","    if val_metrics['f1_score'] > best_metrics['f1']:\n","        save_model = True\n","        improvement_reason = \"Higher F1\"\n","    elif val_metrics['f1_score'] == best_metrics['f1']:\n","        if val_loss < best_metrics['loss']:\n","            save_model = True\n","            improvement_reason = \"Same F1, Lower Loss\"\n","        elif val_loss == best_metrics['loss'] and val_metrics['accuracy'] > best_metrics['accuracy']:\n","            save_model = True\n","            improvement_reason = \"Same F1&Loss, Higher Accuracy\"\n","\n","    if save_model:\n","        best_metrics['f1'] = val_metrics['f1_score']\n","        best_metrics['loss'] = val_loss\n","        best_metrics['accuracy'] = val_metrics['accuracy']\n","        best_metrics['epoch'] = epoch + 1\n","\n","        best_model_path = save_checkpoint_robust(\n","            model, optimizer, scheduler, epoch,\n","            train_metrics, val_metrics, GLOBAL_CONFIG_CASME2['checkpoint_root'],\n","            best_metrics, CASME2_POOLFORMER_CONFIG\n","        )\n","\n","        if best_model_path:\n","            print(f\"New best model: {improvement_reason} - F1: {best_metrics['f1']:.4f}\")\n","        else:\n","            print(f\"Warning: Failed to save checkpoint despite improvement\")\n","\n","    # Progress tracking\n","    elapsed_time = time.time() - start_time\n","    estimated_total = (elapsed_time / (epoch + 1)) * CASME2_POOLFORMER_CONFIG['num_epochs']\n","    remaining_time = estimated_total - elapsed_time\n","    progress_pct = ((epoch + 1) / CASME2_POOLFORMER_CONFIG['num_epochs']) * 100\n","\n","    print(f\"Progress: {progress_pct:.1f}% | Best F1: {best_metrics['f1']:.4f} | ETA: {remaining_time/60:.1f}min\")\n","\n","# Training completion\n","total_time = time.time() - start_time\n","actual_epochs = CASME2_POOLFORMER_CONFIG['num_epochs']\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II MULTI-FRAME SEQUENCE POOLFORMER TRAINING COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Training time: {total_time/60:.1f} minutes\")\n","print(f\"Epochs completed: {actual_epochs}\")\n","print(f\"Best validation F1: {best_metrics['f1']:.4f} (epoch {best_metrics['epoch']})\")\n","print(f\"Final train F1: {training_history['train_f1'][-1]:.4f}\")\n","print(f\"Final validation F1: {training_history['val_f1'][-1]:.4f}\")\n","\n","# Enhanced training documentation export\n","results_dir = GLOBAL_CONFIG_CASME2['results_root']\n","os.makedirs(f\"{results_dir}/training_logs\", exist_ok=True)\n","\n","training_history_path = f\"{results_dir}/training_logs/casme2_poolformer_mfs_training_history.json\"\n","\n","print(\"\\nExporting enhanced training documentation...\")\n","\n","try:\n","    training_summary = {\n","        'experiment_type': 'CASME2_PoolFormer_MultiFrameSequence',\n","        'experiment_configuration': {\n","            'dataset_version': CASME2_POOLFORMER_CONFIG['dataset_version'],\n","            'frame_strategy': CASME2_POOLFORMER_CONFIG['frame_strategy'],\n","            'training_approach': CASME2_POOLFORMER_CONFIG['training_approach'],\n","            'inference_strategy': CASME2_POOLFORMER_CONFIG['inference_strategy'],\n","            'loss_function': 'Optimized Focal Loss' if CASME2_POOLFORMER_CONFIG['use_focal_loss'] else 'CrossEntropy',\n","            'weight_approach': 'Per-class Alpha (sum=1.0)' if CASME2_POOLFORMER_CONFIG['use_focal_loss'] else 'Inverse Sqrt Frequency',\n","            'focal_loss_gamma': CASME2_POOLFORMER_CONFIG['focal_loss_gamma'],\n","            'focal_loss_alpha_weights': CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights'],\n","            'crossentropy_class_weights': CASME2_POOLFORMER_CONFIG['crossentropy_class_weights'],\n","            'poolformer_model': CASME2_POOLFORMER_CONFIG['poolformer_model'],\n","            'model_variant': CASME2_POOLFORMER_CONFIG['model_variant'],\n","            'model_params': CASME2_POOLFORMER_CONFIG['model_params']\n","        },\n","        'training_history': safe_json_serialize(training_history),\n","        'best_val_f1': float(best_metrics['f1']),\n","        'best_val_loss': float(best_metrics['loss']),\n","        'best_val_accuracy': float(best_metrics['accuracy']),\n","        'best_epoch': int(best_metrics['epoch']),\n","        'total_epochs': int(actual_epochs),\n","        'total_time_minutes': float(total_time / 60),\n","        'average_epoch_time_seconds': float(np.mean(training_history['epoch_time'])),\n","        'config': safe_json_serialize(CASME2_POOLFORMER_CONFIG),\n","        'final_train_f1': float(training_history['train_f1'][-1]),\n","        'final_val_f1': float(training_history['val_f1'][-1]),\n","        'model_checkpoint': 'casme2_poolformer_mfs_best_f1.pth',\n","        'dataset_info': {\n","            'name': 'CASME_II',\n","            'version': CASME2_POOLFORMER_CONFIG['dataset_version'],\n","            'frame_strategy': CASME2_POOLFORMER_CONFIG['frame_strategy'],\n","            'train_samples': len(train_dataset),\n","            'val_samples': len(val_dataset),\n","            'num_classes': 7,\n","            'class_names': CASME2_CLASSES\n","        },\n","        'architecture_info': {\n","            'model_type': 'PoolFormerCASME2Baseline',\n","            'backbone': CASME2_POOLFORMER_CONFIG['poolformer_model'],\n","            'variant': CASME2_POOLFORMER_CONFIG['model_variant'],\n","            'input_size': f\"{CASME2_POOLFORMER_CONFIG['input_size']}x{CASME2_POOLFORMER_CONFIG['input_size']}\",\n","            'expected_feature_dim': CASME2_POOLFORMER_CONFIG['expected_feature_dim'],\n","            'classification_head': f\"{CASME2_POOLFORMER_CONFIG['expected_feature_dim']}->GAP->512->128->7\",\n","            'token_mixing': 'pooling_operations'\n","        },\n","        'enhanced_features': {\n","            'hardened_checkpoint_system': True,\n","            'atomic_checkpoint_save': True,\n","            'checkpoint_validation': True,\n","            'model_output_validation': True,\n","            'enhanced_error_handling': True,\n","            'multi_criteria_checkpoint_logic': True,\n","            'memory_optimized_training': True,\n","            'ram_caching': True,\n","            'attention_free_token_mixing': True,\n","            'custom_collate_function': True,\n","            'poolformer_processor_compatibility': True\n","        }\n","    }\n","\n","    with open(training_history_path, 'w') as f:\n","        json.dump(training_summary, f, indent=2)\n","\n","    print(f\"Enhanced training documentation saved successfully: {training_history_path}\")\n","    print(f\"Experiment details: {training_summary['experiment_configuration']['loss_function']} loss\")\n","    if CASME2_POOLFORMER_CONFIG['use_focal_loss']:\n","        print(f\"  Gamma: {CASME2_POOLFORMER_CONFIG['focal_loss_gamma']}, Alpha Sum: {sum(CASME2_POOLFORMER_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","    print(f\"Model variant: {CASME2_POOLFORMER_CONFIG['model_variant'].upper()}\")\n","    print(f\"Model parameters: {CASME2_POOLFORMER_CONFIG['model_params']}\")\n","    print(f\"Dataset version: {CASME2_POOLFORMER_CONFIG['dataset_version']}\")\n","\n","except Exception as e:\n","    print(f\"Warning: Could not save training documentation: {e}\")\n","    print(\"Training completed successfully, but documentation export failed\")\n","\n","# Enhanced memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\nNext: Cell 3 - CASME II Multi-Frame Sequence PoolFormer Evaluation\")\n","print(\"Enhanced training pipeline completed successfully!\")"],"metadata":{"cellView":"form","id":"LVHIcgKFPfw7","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1761290935513,"user_tz":-420,"elapsed":2159378,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"15b55525-66fe-46d4-9729-8add1acabf5d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II Multi-Frame Sequence PoolFormer Training Pipeline\n","======================================================================\n","Loss Function: Optimized Focal Loss\n","Focal Loss Parameters:\n","  Gamma: 2.0\n","  Per-class Alpha: [0.048, 0.06, 0.085, 0.093, 0.095, 0.191, 0.427]\n","  Alpha Sum: 0.999\n","Dataset version: v9\n","Frame strategy: multi_frame_sequence\n","Training approach: frame_level_independent\n","PoolFormer variant: M48\n","Model parameters: 73M\n","Training epochs: 50\n","Batch size: 16\n","Scheduler patience: 3\n","\n","======================================================================\n","LOADING CASME II DATASETS WITH RAM CACHING\n","======================================================================\n","Loading CASME II train dataset for training...\n","Found 2613 image files in directory\n","Sample filename: sub13_EP01_01_apex_p+1_others.jpg\n","Loaded 2613 CASME II train samples\n","  others: 1027 samples (39.3%)\n","  disgust: 650 samples (24.9%)\n","  happiness: 325 samples (12.4%)\n","  repression: 273 samples (10.4%)\n","  surprise: 260 samples (10.0%)\n","  sadness: 65 samples (2.5%)\n","  fear: 13 samples (0.5%)\n","Preloading 2613 train images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading train to RAM: 100%|██████████| 2613/2613 [00:47<00:00, 54.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TRAIN RAM caching completed: 2613/2613 images, ~1.57GB\n","Loading CASME II val dataset for training...\n","Found 78 image files in directory\n","Sample filename: sub01_EP03_02_onset_others.jpg\n","Loaded 78 CASME II val samples\n","  others: 30 samples (38.5%)\n","  disgust: 18 samples (23.1%)\n","  happiness: 9 samples (11.5%)\n","  repression: 9 samples (11.5%)\n","  surprise: 6 samples (7.7%)\n","  sadness: 3 samples (3.8%)\n","  fear: 3 samples (3.8%)\n","Preloading 78 val images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading val to RAM: 100%|██████████| 78/78 [00:01<00:00, 49.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["VAL RAM caching completed: 78/78 images, ~0.05GB\n","\n","Dataset loading completed:\n","Train samples: 2613, batches: 164\n","Validation samples: 78, batches: 5\n","Custom collate function applied for PoolFormer processor compatibility\n","\n","======================================================================\n","INITIALIZING POOLFORMER MODEL\n","======================================================================\n","PoolFormer feature dimension: 768\n","Classification head: 768 -> GAP2D -> 512 -> 128 -> 7\n","Dropout rate: 0.3 (balanced for large dataset)\n","Model: PoolFormerCASME2Baseline\n","Variant: M48\n","Total parameters: 73,163,207\n","Trainable parameters: 73,163,207\n","Feature dimension: 768\n","Scheduler: ReduceLROnPlateau monitoring validation_f1\n","Using Optimized Focal Loss with gamma=2.0\n","Per-class alpha weights: [0.048, 0.06, 0.085, 0.093, 0.095, 0.191, 0.427]\n","Alpha sum: 0.999\n","Optimizer: AdamW\n","Learning rate: 1e-05\n","Weight decay: 1e-05\n","Scheduler: ReduceLROnPlateau (patience=3)\n","\n","======================================================================\n","STARTING TRAINING\n","======================================================================\n","\n","======================================================================\n","Epoch 1/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 1/50: 100%|██████████| 164/164 [00:43<00:00,  3.81it/s, loss=0.1260]\n","Val Epoch 1/50: 100%|██████████| 5/5 [00:00<00:00,  6.53it/s, loss=0.1134]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.2384, F1: 0.2362, Acc: 0.3360\n","Val   - Loss: 0.1445, F1: 0.1843, Acc: 0.2692\n","Time  - Epoch: 43.9s, LR: 1.00e-05\n","New best model: Higher F1 - F1: 0.1843\n","Progress: 2.0% | Best F1: 0.1843 | ETA: 37.7min\n","\n","======================================================================\n","Epoch 2/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 2/50: 100%|██████████| 164/164 [00:41<00:00,  3.97it/s, loss=0.0638]\n","Val Epoch 2/50: 100%|██████████| 5/5 [00:00<00:00, 11.04it/s, loss=0.1235]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0628, F1: 0.4612, Acc: 0.5561\n","Val   - Loss: 0.1425, F1: 0.1549, Acc: 0.2692\n","Time  - Epoch: 41.7s, LR: 1.00e-05\n","Progress: 4.0% | Best F1: 0.1843 | ETA: 35.1min\n","\n","======================================================================\n","Epoch 3/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 3/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0005]\n","Val Epoch 3/50: 100%|██████████| 5/5 [00:00<00:00, 10.67it/s, loss=0.1335]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0324, F1: 0.6635, Acc: 0.7290\n","Val   - Loss: 0.1744, F1: 0.1052, Acc: 0.2308\n","Time  - Epoch: 42.1s, LR: 1.00e-05\n","Progress: 6.0% | Best F1: 0.1843 | ETA: 33.9min\n","\n","======================================================================\n","Epoch 4/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 4/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0040]\n","Val Epoch 4/50: 100%|██████████| 5/5 [00:00<00:00, 10.58it/s, loss=0.1668]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0187, F1: 0.7581, Acc: 0.8025\n","Val   - Loss: 0.2052, F1: 0.1502, Acc: 0.3077\n","Time  - Epoch: 41.9s, LR: 1.00e-05\n","Progress: 8.0% | Best F1: 0.1843 | ETA: 32.9min\n","\n","======================================================================\n","Epoch 5/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 5/50: 100%|██████████| 164/164 [00:41<00:00,  3.95it/s, loss=0.0001]\n","Val Epoch 5/50: 100%|██████████| 5/5 [00:00<00:00, 11.01it/s, loss=0.1480]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0131, F1: 0.8395, Acc: 0.8595\n","Val   - Loss: 0.2050, F1: 0.1493, Acc: 0.2692\n","Time  - Epoch: 41.9s, LR: 5.00e-06\n","Progress: 10.0% | Best F1: 0.1843 | ETA: 32.1min\n","\n","======================================================================\n","Epoch 6/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 6/50: 100%|██████████| 164/164 [00:41<00:00,  3.95it/s, loss=0.0010]\n","Val Epoch 6/50: 100%|██████████| 5/5 [00:00<00:00, 10.85it/s, loss=0.1524]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0077, F1: 0.9046, Acc: 0.9215\n","Val   - Loss: 0.2185, F1: 0.1267, Acc: 0.2821\n","Time  - Epoch: 42.0s, LR: 5.00e-06\n","Progress: 12.0% | Best F1: 0.1843 | ETA: 31.3min\n","\n","======================================================================\n","Epoch 7/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 7/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0000]\n","Val Epoch 7/50: 100%|██████████| 5/5 [00:00<00:00, 10.99it/s, loss=0.1939]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0052, F1: 0.9294, Acc: 0.9430\n","Val   - Loss: 0.2546, F1: 0.1264, Acc: 0.2692\n","Time  - Epoch: 41.9s, LR: 5.00e-06\n","Progress: 14.0% | Best F1: 0.1843 | ETA: 30.5min\n","\n","======================================================================\n","Epoch 8/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 8/50: 100%|██████████| 164/164 [00:41<00:00,  3.95it/s, loss=0.0023]\n","Val Epoch 8/50: 100%|██████████| 5/5 [00:00<00:00, 10.93it/s, loss=0.1993]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0035, F1: 0.9411, Acc: 0.9545\n","Val   - Loss: 0.2669, F1: 0.1244, Acc: 0.3077\n","Time  - Epoch: 42.0s, LR: 5.00e-06\n","Progress: 16.0% | Best F1: 0.1843 | ETA: 29.7min\n","\n","======================================================================\n","Epoch 9/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 9/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0000]\n","Val Epoch 9/50: 100%|██████████| 5/5 [00:00<00:00, 10.67it/s, loss=0.2356]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0025, F1: 0.9441, Acc: 0.9694\n","Val   - Loss: 0.2776, F1: 0.1766, Acc: 0.3077\n","Time  - Epoch: 41.9s, LR: 2.50e-06\n","Progress: 18.0% | Best F1: 0.1843 | ETA: 29.0min\n","\n","======================================================================\n","Epoch 10/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 10/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0027]\n","Val Epoch 10/50: 100%|██████████| 5/5 [00:00<00:00, 10.58it/s, loss=0.2123]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0019, F1: 0.9533, Acc: 0.9759\n","Val   - Loss: 0.2745, F1: 0.1603, Acc: 0.3205\n","Time  - Epoch: 41.9s, LR: 2.50e-06\n","Progress: 20.0% | Best F1: 0.1843 | ETA: 28.2min\n","\n","======================================================================\n","Epoch 11/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 11/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0016]\n","Val Epoch 11/50: 100%|██████████| 5/5 [00:00<00:00, 10.87it/s, loss=0.2298]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0019, F1: 0.9673, Acc: 0.9759\n","Val   - Loss: 0.2825, F1: 0.1535, Acc: 0.3077\n","Time  - Epoch: 41.9s, LR: 2.50e-06\n","Progress: 22.0% | Best F1: 0.1843 | ETA: 27.5min\n","\n","======================================================================\n","Epoch 12/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 12/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0001]\n","Val Epoch 12/50: 100%|██████████| 5/5 [00:00<00:00, 10.97it/s, loss=0.2307]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0014, F1: 0.9692, Acc: 0.9801\n","Val   - Loss: 0.3074, F1: 0.1896, Acc: 0.3590\n","Time  - Epoch: 41.9s, LR: 2.50e-06\n","New best model: Higher F1 - F1: 0.1896\n","Progress: 24.0% | Best F1: 0.1896 | ETA: 27.0min\n","\n","======================================================================\n","Epoch 13/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 13/50: 100%|██████████| 164/164 [00:41<00:00,  3.95it/s, loss=0.0000]\n","Val Epoch 13/50: 100%|██████████| 5/5 [00:00<00:00, 10.65it/s, loss=0.2223]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0012, F1: 0.9877, Acc: 0.9843\n","Val   - Loss: 0.2918, F1: 0.1529, Acc: 0.3077\n","Time  - Epoch: 42.1s, LR: 2.50e-06\n","Progress: 26.0% | Best F1: 0.1896 | ETA: 26.3min\n","\n","======================================================================\n","Epoch 14/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 14/50: 100%|██████████| 164/164 [00:41<00:00,  3.93it/s, loss=0.0003]\n","Val Epoch 14/50: 100%|██████████| 5/5 [00:00<00:00, 10.97it/s, loss=0.2293]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0011, F1: 0.9868, Acc: 0.9866\n","Val   - Loss: 0.2998, F1: 0.1616, Acc: 0.3077\n","Time  - Epoch: 42.2s, LR: 2.50e-06\n","Progress: 28.0% | Best F1: 0.1896 | ETA: 25.6min\n","\n","======================================================================\n","Epoch 15/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 15/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0000]\n","Val Epoch 15/50: 100%|██████████| 5/5 [00:00<00:00, 10.76it/s, loss=0.2469]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0012, F1: 0.9744, Acc: 0.9843\n","Val   - Loss: 0.3095, F1: 0.1572, Acc: 0.3205\n","Time  - Epoch: 42.1s, LR: 2.50e-06\n","Progress: 30.0% | Best F1: 0.1896 | ETA: 24.8min\n","\n","======================================================================\n","Epoch 16/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 16/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0001]\n","Val Epoch 16/50: 100%|██████████| 5/5 [00:00<00:00, 11.21it/s, loss=0.2669]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0012, F1: 0.9852, Acc: 0.9839\n","Val   - Loss: 0.3221, F1: 0.1584, Acc: 0.3205\n","Time  - Epoch: 42.0s, LR: 1.25e-06\n","Progress: 32.0% | Best F1: 0.1896 | ETA: 24.1min\n","\n","======================================================================\n","Epoch 17/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 17/50: 100%|██████████| 164/164 [00:41<00:00,  3.95it/s, loss=0.0000]\n","Val Epoch 17/50: 100%|██████████| 5/5 [00:00<00:00, 11.11it/s, loss=0.2691]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0013, F1: 0.9805, Acc: 0.9851\n","Val   - Loss: 0.3217, F1: 0.1699, Acc: 0.3205\n","Time  - Epoch: 42.0s, LR: 1.25e-06\n","Progress: 34.0% | Best F1: 0.1896 | ETA: 23.4min\n","\n","======================================================================\n","Epoch 18/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 18/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0000]\n","Val Epoch 18/50: 100%|██████████| 5/5 [00:00<00:00, 11.14it/s, loss=0.2532]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9883, Acc: 0.9878\n","Val   - Loss: 0.3239, F1: 0.1495, Acc: 0.3205\n","Time  - Epoch: 42.1s, LR: 1.25e-06\n","Progress: 36.0% | Best F1: 0.1896 | ETA: 22.7min\n","\n","======================================================================\n","Epoch 19/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 19/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0207]\n","Val Epoch 19/50: 100%|██████████| 5/5 [00:00<00:00, 11.12it/s, loss=0.2539]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9928, Acc: 0.9916\n","Val   - Loss: 0.3291, F1: 0.1213, Acc: 0.2949\n","Time  - Epoch: 42.1s, LR: 1.25e-06\n","Progress: 38.0% | Best F1: 0.1896 | ETA: 21.9min\n","\n","======================================================================\n","Epoch 20/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 20/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0014]\n","Val Epoch 20/50: 100%|██████████| 5/5 [00:00<00:00, 11.25it/s, loss=0.2401]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9912, Acc: 0.9897\n","Val   - Loss: 0.3127, F1: 0.1753, Acc: 0.3333\n","Time  - Epoch: 42.0s, LR: 1.00e-06\n","Progress: 40.0% | Best F1: 0.1896 | ETA: 21.2min\n","\n","======================================================================\n","Epoch 21/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 21/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0000]\n","Val Epoch 21/50: 100%|██████████| 5/5 [00:00<00:00, 11.06it/s, loss=0.2479]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9826, Acc: 0.9897\n","Val   - Loss: 0.3162, F1: 0.1561, Acc: 0.3205\n","Time  - Epoch: 42.1s, LR: 1.00e-06\n","Progress: 42.0% | Best F1: 0.1896 | ETA: 20.5min\n","\n","======================================================================\n","Epoch 22/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 22/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0001]\n","Val Epoch 22/50: 100%|██████████| 5/5 [00:00<00:00, 11.16it/s, loss=0.2345]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9817, Acc: 0.9878\n","Val   - Loss: 0.3153, F1: 0.1825, Acc: 0.3333\n","Time  - Epoch: 42.1s, LR: 1.00e-06\n","Progress: 44.0% | Best F1: 0.1896 | ETA: 19.8min\n","\n","======================================================================\n","Epoch 23/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 23/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0000]\n","Val Epoch 23/50: 100%|██████████| 5/5 [00:00<00:00, 11.09it/s, loss=0.2600]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0007, F1: 0.9903, Acc: 0.9897\n","Val   - Loss: 0.3191, F1: 0.1749, Acc: 0.3462\n","Time  - Epoch: 42.0s, LR: 1.00e-06\n","Progress: 46.0% | Best F1: 0.1896 | ETA: 19.1min\n","\n","======================================================================\n","Epoch 24/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 24/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0002]\n","Val Epoch 24/50: 100%|██████████| 5/5 [00:00<00:00, 11.19it/s, loss=0.2562]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0007, F1: 0.9897, Acc: 0.9916\n","Val   - Loss: 0.3163, F1: 0.1716, Acc: 0.3333\n","Time  - Epoch: 42.1s, LR: 1.00e-06\n","Progress: 48.0% | Best F1: 0.1896 | ETA: 18.4min\n","\n","======================================================================\n","Epoch 25/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 25/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0000]\n","Val Epoch 25/50: 100%|██████████| 5/5 [00:00<00:00, 11.05it/s, loss=0.2634]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 0.9889, Acc: 0.9908\n","Val   - Loss: 0.3205, F1: 0.1652, Acc: 0.3205\n","Time  - Epoch: 42.1s, LR: 1.00e-06\n","Progress: 50.0% | Best F1: 0.1896 | ETA: 17.6min\n","\n","======================================================================\n","Epoch 26/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 26/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0000]\n","Val Epoch 26/50: 100%|██████████| 5/5 [00:00<00:00, 11.00it/s, loss=0.2621]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0010, F1: 0.9793, Acc: 0.9908\n","Val   - Loss: 0.3180, F1: 0.1652, Acc: 0.3205\n","Time  - Epoch: 42.1s, LR: 1.00e-06\n","Progress: 52.0% | Best F1: 0.1896 | ETA: 16.9min\n","\n","======================================================================\n","Epoch 27/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 27/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0000]\n","Val Epoch 27/50: 100%|██████████| 5/5 [00:00<00:00, 11.05it/s, loss=0.2621]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 0.9875, Acc: 0.9920\n","Val   - Loss: 0.3223, F1: 0.1442, Acc: 0.3077\n","Time  - Epoch: 42.1s, LR: 1.00e-06\n","Progress: 54.0% | Best F1: 0.1896 | ETA: 16.2min\n","\n","======================================================================\n","Epoch 28/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 28/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0000]\n","Val Epoch 28/50: 100%|██████████| 5/5 [00:00<00:00, 10.62it/s, loss=0.2653]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 0.9920, Acc: 0.9923\n","Val   - Loss: 0.3209, F1: 0.1426, Acc: 0.2949\n","Time  - Epoch: 41.9s, LR: 1.00e-06\n","Progress: 56.0% | Best F1: 0.1896 | ETA: 15.5min\n","\n","======================================================================\n","Epoch 29/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 29/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0002]\n","Val Epoch 29/50: 100%|██████████| 5/5 [00:00<00:00, 10.89it/s, loss=0.2685]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 0.9870, Acc: 0.9908\n","Val   - Loss: 0.3174, F1: 0.1548, Acc: 0.3077\n","Time  - Epoch: 41.9s, LR: 1.00e-06\n","Progress: 58.0% | Best F1: 0.1896 | ETA: 14.8min\n","\n","======================================================================\n","Epoch 30/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 30/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0000]\n","Val Epoch 30/50: 100%|██████████| 5/5 [00:00<00:00, 10.80it/s, loss=0.2695]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 0.9889, Acc: 0.9931\n","Val   - Loss: 0.3301, F1: 0.1024, Acc: 0.2564\n","Time  - Epoch: 41.9s, LR: 1.00e-06\n","Progress: 60.0% | Best F1: 0.1896 | ETA: 14.1min\n","\n","======================================================================\n","Epoch 31/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 31/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0000]\n","Val Epoch 31/50: 100%|██████████| 5/5 [00:00<00:00, 10.98it/s, loss=0.2712]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 0.9885, Acc: 0.9920\n","Val   - Loss: 0.3279, F1: 0.1612, Acc: 0.3077\n","Time  - Epoch: 41.9s, LR: 1.00e-06\n","Progress: 62.0% | Best F1: 0.1896 | ETA: 13.4min\n","\n","======================================================================\n","Epoch 32/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 32/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0006]\n","Val Epoch 32/50: 100%|██████████| 5/5 [00:00<00:00, 10.18it/s, loss=0.2493]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0007, F1: 0.9918, Acc: 0.9927\n","Val   - Loss: 0.3217, F1: 0.1738, Acc: 0.3205\n","Time  - Epoch: 41.9s, LR: 1.00e-06\n","Progress: 64.0% | Best F1: 0.1896 | ETA: 12.7min\n","\n","======================================================================\n","Epoch 33/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 33/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0004]\n","Val Epoch 33/50: 100%|██████████| 5/5 [00:00<00:00, 10.82it/s, loss=0.2494]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 0.9850, Acc: 0.9897\n","Val   - Loss: 0.3204, F1: 0.1565, Acc: 0.2949\n","Time  - Epoch: 41.9s, LR: 1.00e-06\n","Progress: 66.0% | Best F1: 0.1896 | ETA: 12.0min\n","\n","======================================================================\n","Epoch 34/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 34/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0000]\n","Val Epoch 34/50: 100%|██████████| 5/5 [00:00<00:00, 10.83it/s, loss=0.2573]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 0.9881, Acc: 0.9916\n","Val   - Loss: 0.3285, F1: 0.1560, Acc: 0.3077\n","Time  - Epoch: 41.9s, LR: 1.00e-06\n","Progress: 68.0% | Best F1: 0.1896 | ETA: 11.3min\n","\n","======================================================================\n","Epoch 35/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 35/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0005]\n","Val Epoch 35/50: 100%|██████████| 5/5 [00:00<00:00, 10.49it/s, loss=0.2799]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 0.9952, Acc: 0.9962\n","Val   - Loss: 0.3340, F1: 0.1581, Acc: 0.3205\n","Time  - Epoch: 42.1s, LR: 1.00e-06\n","Progress: 70.0% | Best F1: 0.1896 | ETA: 10.6min\n","\n","======================================================================\n","Epoch 36/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 36/50: 100%|██████████| 164/164 [00:41<00:00,  3.95it/s, loss=0.0000]\n","Val Epoch 36/50: 100%|██████████| 5/5 [00:00<00:00, 10.74it/s, loss=0.2821]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 0.9940, Acc: 0.9939\n","Val   - Loss: 0.3358, F1: 0.1789, Acc: 0.3462\n","Time  - Epoch: 42.0s, LR: 1.00e-06\n","Progress: 72.0% | Best F1: 0.1896 | ETA: 9.9min\n","\n","======================================================================\n","Epoch 37/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 37/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0000]\n","Val Epoch 37/50: 100%|██████████| 5/5 [00:00<00:00, 10.84it/s, loss=0.2932]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 0.9913, Acc: 0.9943\n","Val   - Loss: 0.3347, F1: 0.1644, Acc: 0.3333\n","Time  - Epoch: 42.1s, LR: 1.00e-06\n","Progress: 74.0% | Best F1: 0.1896 | ETA: 9.2min\n","\n","======================================================================\n","Epoch 38/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 38/50: 100%|██████████| 164/164 [00:41<00:00,  3.95it/s, loss=0.0002]\n","Val Epoch 38/50: 100%|██████████| 5/5 [00:00<00:00, 10.99it/s, loss=0.2803]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 0.9964, Acc: 0.9962\n","Val   - Loss: 0.3411, F1: 0.1625, Acc: 0.3333\n","Time  - Epoch: 42.0s, LR: 1.00e-06\n","Progress: 76.0% | Best F1: 0.1896 | ETA: 8.4min\n","\n","======================================================================\n","Epoch 39/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 39/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0000]\n","Val Epoch 39/50: 100%|██████████| 5/5 [00:00<00:00, 10.75it/s, loss=0.2743]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 0.9883, Acc: 0.9939\n","Val   - Loss: 0.3361, F1: 0.1658, Acc: 0.3333\n","Time  - Epoch: 42.1s, LR: 1.00e-06\n","Progress: 78.0% | Best F1: 0.1896 | ETA: 7.7min\n","\n","======================================================================\n","Epoch 40/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 40/50: 100%|██████████| 164/164 [00:41<00:00,  3.92it/s, loss=0.0000]\n","Val Epoch 40/50: 100%|██████████| 5/5 [00:00<00:00, 10.82it/s, loss=0.2943]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 0.9964, Acc: 0.9962\n","Val   - Loss: 0.3483, F1: 0.1643, Acc: 0.3205\n","Time  - Epoch: 42.3s, LR: 1.00e-06\n","Progress: 80.0% | Best F1: 0.1896 | ETA: 7.0min\n","\n","======================================================================\n","Epoch 41/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 41/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0001]\n","Val Epoch 41/50: 100%|██████████| 5/5 [00:00<00:00, 10.58it/s, loss=0.2830]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 0.9972, Acc: 0.9969\n","Val   - Loss: 0.3396, F1: 0.1889, Acc: 0.3462\n","Time  - Epoch: 42.1s, LR: 1.00e-06\n","Progress: 82.0% | Best F1: 0.1896 | ETA: 6.3min\n","\n","======================================================================\n","Epoch 42/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 42/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0000]\n","Val Epoch 42/50: 100%|██████████| 5/5 [00:00<00:00, 10.66it/s, loss=0.2893]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 0.9962, Acc: 0.9958\n","Val   - Loss: 0.3500, F1: 0.1656, Acc: 0.3333\n","Time  - Epoch: 42.1s, LR: 1.00e-06\n","Progress: 84.0% | Best F1: 0.1896 | ETA: 5.6min\n","\n","======================================================================\n","Epoch 43/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 43/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0000]\n","Val Epoch 43/50: 100%|██████████| 5/5 [00:00<00:00, 10.67it/s, loss=0.2908]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 0.9955, Acc: 0.9954\n","Val   - Loss: 0.3473, F1: 0.1594, Acc: 0.3205\n","Time  - Epoch: 42.1s, LR: 1.00e-06\n","Progress: 86.0% | Best F1: 0.1896 | ETA: 4.9min\n","\n","======================================================================\n","Epoch 44/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 44/50: 100%|██████████| 164/164 [00:41<00:00,  3.94it/s, loss=0.0000]\n","Val Epoch 44/50: 100%|██████████| 5/5 [00:00<00:00, 10.72it/s, loss=0.2938]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 0.9982, Acc: 0.9973\n","Val   - Loss: 0.3500, F1: 0.1728, Acc: 0.3462\n","Time  - Epoch: 42.1s, LR: 1.00e-06\n","Progress: 88.0% | Best F1: 0.1896 | ETA: 4.2min\n","\n","======================================================================\n","Epoch 45/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 45/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0000]\n","Val Epoch 45/50: 100%|██████████| 5/5 [00:00<00:00, 10.81it/s, loss=0.2702]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 0.9965, Acc: 0.9969\n","Val   - Loss: 0.3420, F1: 0.1658, Acc: 0.3333\n","Time  - Epoch: 41.9s, LR: 1.00e-06\n","Progress: 90.0% | Best F1: 0.1896 | ETA: 3.5min\n","\n","======================================================================\n","Epoch 46/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 46/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0001]\n","Val Epoch 46/50: 100%|██████████| 5/5 [00:00<00:00, 10.57it/s, loss=0.2740]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 0.9964, Acc: 0.9969\n","Val   - Loss: 0.3549, F1: 0.1588, Acc: 0.3077\n","Time  - Epoch: 41.9s, LR: 1.00e-06\n","Progress: 92.0% | Best F1: 0.1896 | ETA: 2.8min\n","\n","======================================================================\n","Epoch 47/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 47/50: 100%|██████████| 164/164 [00:41<00:00,  3.95it/s, loss=0.0000]\n","Val Epoch 47/50: 100%|██████████| 5/5 [00:00<00:00, 10.67it/s, loss=0.2948]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 0.9968, Acc: 0.9962\n","Val   - Loss: 0.3574, F1: 0.1445, Acc: 0.3077\n","Time  - Epoch: 42.0s, LR: 1.00e-06\n","Progress: 94.0% | Best F1: 0.1896 | ETA: 2.1min\n","\n","======================================================================\n","Epoch 48/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 48/50: 100%|██████████| 164/164 [00:41<00:00,  3.95it/s, loss=0.0000]\n","Val Epoch 48/50: 100%|██████████| 5/5 [00:00<00:00, 11.05it/s, loss=0.2966]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 0.9976, Acc: 0.9973\n","Val   - Loss: 0.3526, F1: 0.1652, Acc: 0.3333\n","Time  - Epoch: 42.0s, LR: 1.00e-06\n","Progress: 96.0% | Best F1: 0.1896 | ETA: 1.4min\n","\n","======================================================================\n","Epoch 49/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 49/50: 100%|██████████| 164/164 [00:41<00:00,  3.96it/s, loss=0.0000]\n","Val Epoch 49/50: 100%|██████████| 5/5 [00:00<00:00, 10.88it/s, loss=0.2780]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 0.9931, Acc: 0.9946\n","Val   - Loss: 0.3474, F1: 0.1658, Acc: 0.3333\n","Time  - Epoch: 41.9s, LR: 1.00e-06\n","Progress: 98.0% | Best F1: 0.1896 | ETA: 0.7min\n","\n","======================================================================\n","Epoch 50/50\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 50/50: 100%|██████████| 164/164 [00:41<00:00,  3.95it/s, loss=0.0000]\n","Val Epoch 50/50: 100%|██████████| 5/5 [00:00<00:00, 10.89it/s, loss=0.2998]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 0.9970, Acc: 0.9969\n","Val   - Loss: 0.3670, F1: 0.1191, Acc: 0.2949\n","Time  - Epoch: 41.9s, LR: 1.00e-06\n","Progress: 100.0% | Best F1: 0.1896 | ETA: 0.0min\n","\n","======================================================================\n","CASME II MULTI-FRAME SEQUENCE POOLFORMER TRAINING COMPLETED\n","======================================================================\n","Training time: 35.2 minutes\n","Epochs completed: 50\n","Best validation F1: 0.1896 (epoch 12)\n","Final train F1: 0.9970\n","Final validation F1: 0.1191\n","\n","Exporting enhanced training documentation...\n","Enhanced training documentation saved successfully: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/07_03_poolformer_casme2_mfs_prep/training_logs/casme2_poolformer_mfs_training_history.json\n","Experiment details: Optimized Focal Loss loss\n","  Gamma: 2.0, Alpha Sum: 0.999\n","Model variant: M48\n","Model parameters: 73M\n","Dataset version: v9\n","\n","Next: Cell 3 - CASME II Multi-Frame Sequence PoolFormer Evaluation\n","Enhanced training pipeline completed successfully!\n"]}]},{"cell_type":"code","source":["# @title Cell 3: CASME II PoolFormer Evaluation with Dual Dataset Support\n","\n","# File: 07_03_PoolFormer_CASME2_MFS_Cell3.py\n","# Location: experiments/07_03_PoolFormer_CASME2-MFS-PREP.ipynb\n","# Purpose: Comprehensive evaluation framework with support for AF (v7) and KFS (v8) test datasets\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from datetime import datetime\n","from collections import defaultdict\n","\n","from sklearn.metrics import (\n","    accuracy_score, precision_recall_fscore_support,\n","    classification_report, confusion_matrix,\n","    roc_curve, auc\n",")\n","from sklearn.preprocessing import label_binarize\n","from concurrent.futures import ThreadPoolExecutor\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =====================================================\n","# DUAL DATASET EVALUATION CONFIGURATION\n","# =====================================================\n","# Configure which test datasets to evaluate:\n","# 'v7' = Apex Frame preprocessing (28 samples, frame-level evaluation)\n","# 'v8' = Key Frame Sequence preprocessing (84 frames -> 28 videos with late fusion)\n","\n","EVALUATE_DATASETS = ['v7', 'v8']  # Can be ['v7'], ['v8'], or ['v7', 'v8']\n","\n","print(\"CASME II PoolFormer Evaluation Framework with Dual Dataset Support\")\n","print(\"=\" * 60)\n","print(f\"Datasets to evaluate: {EVALUATE_DATASETS}\")\n","print(\"=\" * 60)\n","\n","# =====================================================\n","# DATASET CONFIGURATION FUNCTION\n","# =====================================================\n","\n","def get_test_dataset_config(version, project_root):\n","    \"\"\"\n","    Get test dataset configuration based on version\n","\n","    Args:\n","        version: 'v7' (AF) or 'v8' (KFS)\n","        project_root: Project root path\n","\n","    Returns:\n","        dict: Configuration for selected test dataset\n","    \"\"\"\n","    if version == 'v7':\n","        config = {\n","            'version': 'v7',\n","            'variant': 'AF',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/preprocessed_v7\",\n","            'preprocessing_summary': 'preprocessing_summary.json',\n","            'description': 'Apex Frame with Face-Aware Preprocessing',\n","            'expected_samples': 28,\n","            'frame_strategy': 'apex_frame',\n","            'evaluation_mode': 'frame_level',\n","            'aggregation': None\n","        }\n","    elif version == 'v8':\n","        config = {\n","            'version': 'v8',\n","            'variant': 'KFS',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/preprocessed_v8\",\n","            'preprocessing_summary': 'preprocessing_summary.json',\n","            'description': 'Key Frame Sequence with Face-Aware Preprocessing',\n","            'expected_frames': 84,\n","            'expected_videos': 28,\n","            'frame_strategy': 'key_frame_sequence',\n","            'frame_types': ['onset', 'apex', 'offset'],\n","            'evaluation_mode': 'video_level',\n","            'aggregation': 'late_fusion'\n","        }\n","    else:\n","        raise ValueError(f\"Invalid version: {version}. Must be 'v7' or 'v8'\")\n","\n","    return config\n","\n","# =====================================================\n","# VIDEO ID EXTRACTION FOR KFS LATE FUSION\n","# =====================================================\n","\n","def extract_video_id_from_filename(filename):\n","    \"\"\"\n","    Extract video ID from KFS filename by removing frame type suffix\n","\n","    Expected format: sub01_EP02_01f_happiness_onset.jpg\n","    Video ID: sub01_EP02_01f_happiness\n","\n","    Args:\n","        filename: Image filename with frame type\n","\n","    Returns:\n","        str: Video ID without frame type\n","    \"\"\"\n","    # Remove file extension\n","    name_without_ext = filename.rsplit('.', 1)[0]\n","\n","    # Remove frame type suffix (onset, apex, offset)\n","    for frame_type in ['onset', 'apex', 'offset']:\n","        if name_without_ext.endswith(f'_{frame_type}'):\n","            video_id = name_without_ext.rsplit(f'_{frame_type}', 1)[0]\n","            return video_id\n","\n","    # If no frame type found, return as is\n","    return name_without_ext\n","\n","# =====================================================\n","# ENHANCED TEST DATASET CLASS\n","# =====================================================\n","\n","class CASME2DatasetEvaluation(Dataset):\n","    \"\"\"Enhanced CASME II test dataset with comprehensive evaluation support\"\"\"\n","\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","        self.emotions = []\n","        self.video_ids = []\n","        self.cached_images = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading CASME II {split} dataset for evaluation...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        loaded_count = 0\n","\n","        for filename in sorted(all_files):\n","            emotion_found = None\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                video_id = extract_video_id_from_filename(filename)\n","\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                self.emotions.append(emotion_found)\n","                self.video_ids.append(video_id)\n","                loaded_count += 1\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples for evaluation\")\n","        self._print_evaluation_distribution()\n","\n","        if self.use_ram_cache and len(self.images) > 0:\n","            self._preload_to_ram_evaluation()\n","\n","    def _print_evaluation_distribution(self):\n","        \"\"\"Print comprehensive distribution for evaluation analysis\"\"\"\n","        if len(self.labels) == 0:\n","            print(\"No test samples found!\")\n","            return\n","\n","        label_counts = {}\n","        unique_videos = set(self.video_ids)\n","\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        print(\"Test set class distribution:\")\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","        print(f\"Unique video IDs: {len(unique_videos)}\")\n","\n","        missing_classes = []\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i not in label_counts:\n","                missing_classes.append(class_name)\n","\n","        if missing_classes:\n","            print(f\"Missing classes in test set: {missing_classes}\")\n","\n","    def _preload_to_ram_evaluation(self):\n","        \"\"\"RAM preloading optimized for evaluation\"\"\"\n","        if len(self.images) == 0:\n","            return\n","\n","        print(f\"Preloading {len(self.images)} test images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (224, 224), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=\"Loading test set to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * 224 * 224 * 3 * 4 / 1e9\n","        print(f\"TEST RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (224, 224), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","# =====================================================\n","# MODEL LOADING\n","# =====================================================\n","\n","def load_trained_model_casme2(checkpoint_path, device):\n","    \"\"\"Load trained PoolFormer model from checkpoint with enhanced validation\"\"\"\n","    print(f\"\\nValidating checkpoint availability...\")\n","    print(f\"Expected checkpoint: {os.path.basename(checkpoint_path)}\")\n","    print(f\"Full path: {checkpoint_path}\")\n","\n","    # Check if checkpoint directory exists\n","    checkpoint_dir = os.path.dirname(checkpoint_path)\n","    if not os.path.exists(checkpoint_dir):\n","        print(f\"\\nERROR: Checkpoint directory not found!\")\n","        print(f\"Directory: {checkpoint_dir}\")\n","        print(\"\\nTroubleshooting:\")\n","        print(\"1. Make sure Cell 2 (Training) has been executed successfully\")\n","        print(\"2. Check if training completed without errors\")\n","        print(\"3. Verify the checkpoint was saved during training\")\n","        raise FileNotFoundError(f\"Checkpoint directory does not exist: {checkpoint_dir}\")\n","\n","    # List available checkpoints in directory\n","    available_checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n","\n","    if not os.path.exists(checkpoint_path):\n","        print(f\"\\nERROR: Checkpoint file not found!\")\n","        print(f\"Expected: {os.path.basename(checkpoint_path)}\")\n","\n","        if available_checkpoints:\n","            print(f\"\\nAvailable checkpoints in directory:\")\n","            for ckpt in available_checkpoints:\n","                ckpt_path = os.path.join(checkpoint_dir, ckpt)\n","                size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n","                print(f\"  - {ckpt} ({size_mb:.1f} MB)\")\n","            print(\"\\nPossible solutions:\")\n","            print(\"1. Check if training saved checkpoint with different name\")\n","            print(\"2. Re-run Cell 2 (Training) to generate checkpoint\")\n","        else:\n","            print(f\"\\nNo checkpoints found in directory!\")\n","            print(\"\\nRequired actions:\")\n","            print(\"1. Execute Cell 2 (Training Pipeline) first\")\n","            print(\"2. Wait for training to complete (~2.5-3 hours)\")\n","            print(\"3. Verify checkpoint is saved successfully\")\n","            print(\"4. Then run this Cell 3 (Evaluation)\")\n","\n","        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n","\n","    # Validate checkpoint file size\n","    checkpoint_size = os.path.getsize(checkpoint_path) / (1024 * 1024)\n","    print(f\"Checkpoint found: {checkpoint_size:.1f} MB\")\n","\n","    if checkpoint_size < 10:\n","        print(f\"WARNING: Checkpoint size is unusually small ({checkpoint_size:.1f} MB)\")\n","        print(\"This might indicate a corrupted or incomplete checkpoint\")\n","\n","    # Load checkpoint\n","    print(f\"Loading checkpoint from disk...\")\n","    try:\n","        checkpoint = torch.load(checkpoint_path, map_location=device)\n","    except Exception as e:\n","        print(f\"ERROR: Failed to load checkpoint: {e}\")\n","        print(\"\\nPossible causes:\")\n","        print(\"1. Corrupted checkpoint file\")\n","        print(\"2. Incompatible PyTorch version\")\n","        print(\"3. Disk I/O error during save\")\n","        raise\n","\n","    # Initialize model\n","    print(f\"Initializing PoolFormer model...\")\n","    model = PoolFormerCASME2Baseline(\n","        num_classes=7,\n","        dropout_rate=CASME2_POOLFORMER_CONFIG['dropout_rate']\n","    ).to(device)\n","\n","    # Load model weights\n","    try:\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","    except Exception as e:\n","        print(f\"ERROR: Failed to load model weights: {e}\")\n","        print(\"\\nPossible causes:\")\n","        print(\"1. Model architecture mismatch\")\n","        print(\"2. Checkpoint from different model variant\")\n","        raise\n","\n","    model.eval()\n","\n","    print(f\"Model loaded successfully from epoch {checkpoint['epoch']}\")\n","    print(f\"Best validation F1: {checkpoint['best_metrics']['f1']:.4f}\")\n","\n","    training_info = {\n","        'best_epoch': checkpoint['epoch'],\n","        'best_val_f1': checkpoint['best_metrics']['f1'],\n","        'best_val_loss': checkpoint['best_metrics']['loss'],\n","        'best_val_accuracy': checkpoint['best_metrics']['accuracy']\n","    }\n","\n","    return model, training_info\n","\n","# =====================================================\n","# FRAME-LEVEL INFERENCE (for v7 AF)\n","# =====================================================\n","\n","def run_frame_level_inference(model, dataloader, device):\n","    \"\"\"Run frame-level inference for AF evaluation\"\"\"\n","    model.eval()\n","\n","    all_predictions = []\n","    all_labels = []\n","    all_filenames = []\n","    all_probabilities = []\n","\n","    print(\"Running frame-level inference...\")\n","    start_time = time.time()\n","\n","    with torch.no_grad():\n","        for images, labels, filenames in tqdm(dataloader, desc=\"Frame-level inference\"):\n","            images = images.to(device, non_blocking=True)\n","\n","            outputs = model(images)\n","            probabilities = torch.softmax(outputs, dim=1)\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","            all_filenames.extend(filenames)\n","            all_probabilities.extend(probabilities.cpu().numpy())\n","\n","    inference_time = time.time() - start_time\n","\n","    print(f\"Frame-level inference completed in {inference_time:.2f}s\")\n","    print(f\"Processed {len(all_predictions)} frames\")\n","\n","    return {\n","        'predictions': np.array(all_predictions),\n","        'labels': np.array(all_labels),\n","        'filenames': all_filenames,\n","        'probabilities': np.array(all_probabilities),\n","        'inference_time': inference_time,\n","        'evaluation_mode': 'frame_level'\n","    }\n","\n","# =====================================================\n","# VIDEO-LEVEL INFERENCE WITH LATE FUSION (for v8 KFS)\n","# =====================================================\n","\n","def run_video_level_inference_late_fusion(model, dataloader, device):\n","    \"\"\"Run video-level inference with late fusion for KFS evaluation\"\"\"\n","    model.eval()\n","\n","    frame_predictions = []\n","    frame_labels = []\n","    frame_filenames = []\n","    frame_probabilities = []\n","    frame_video_ids = []\n","\n","    print(\"Running frame-level predictions for late fusion...\")\n","    start_time = time.time()\n","\n","    with torch.no_grad():\n","        for images, labels, filenames in tqdm(dataloader, desc=\"Frame predictions\"):\n","            images = images.to(device, non_blocking=True)\n","\n","            outputs = model(images)\n","            probabilities = torch.softmax(outputs, dim=1)\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            frame_predictions.extend(predicted.cpu().numpy())\n","            frame_labels.extend(labels.numpy())\n","            frame_filenames.extend(filenames)\n","            frame_probabilities.extend(probabilities.cpu().numpy())\n","\n","            for filename in filenames:\n","                video_id = extract_video_id_from_filename(filename)\n","                frame_video_ids.append(video_id)\n","\n","    print(f\"\\nAggregating frame predictions to video level...\")\n","\n","    video_data = defaultdict(lambda: {\n","        'frame_predictions': [],\n","        'frame_probabilities': [],\n","        'frame_filenames': [],\n","        'true_label': None\n","    })\n","\n","    for i, video_id in enumerate(frame_video_ids):\n","        video_data[video_id]['frame_predictions'].append(frame_predictions[i])\n","        video_data[video_id]['frame_probabilities'].append(frame_probabilities[i])\n","        video_data[video_id]['frame_filenames'].append(frame_filenames[i])\n","        if video_data[video_id]['true_label'] is None:\n","            video_data[video_id]['true_label'] = frame_labels[i]\n","\n","    video_predictions = []\n","    video_labels = []\n","    video_ids_list = []\n","\n","    for video_id, data in video_data.items():\n","        avg_probabilities = np.mean(data['frame_probabilities'], axis=0)\n","        video_prediction = np.argmax(avg_probabilities)\n","\n","        video_predictions.append(video_prediction)\n","        video_labels.append(data['true_label'])\n","        video_ids_list.append(video_id)\n","\n","    inference_time = time.time() - start_time\n","\n","    print(f\"Late fusion completed in {inference_time:.2f}s\")\n","    print(f\"Aggregated {len(frame_predictions)} frames into {len(video_predictions)} videos\")\n","\n","    return {\n","        'predictions': np.array(video_predictions),\n","        'labels': np.array(video_labels),\n","        'filenames': video_ids_list,\n","        'probabilities': None,\n","        'inference_time': inference_time,\n","        'evaluation_mode': 'video_level',\n","        'kfs_late_fusion_info': {\n","            'total_frames': len(frame_predictions),\n","            'total_videos': len(video_predictions),\n","            'aggregation_method': 'average_probabilities'\n","        }\n","    }\n","\n","# =====================================================\n","# COMPREHENSIVE METRICS CALCULATION\n","# =====================================================\n","\n","def calculate_comprehensive_metrics(inference_results):\n","    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n","    predictions = inference_results['predictions']\n","    labels = inference_results['labels']\n","    filenames = inference_results['filenames']\n","\n","    unique_labels = np.unique(labels)\n","    missing_classes = [i for i in range(len(CASME2_CLASSES)) if i not in unique_labels]\n","    available_classes = [i for i in range(len(CASME2_CLASSES)) if i in unique_labels]\n","\n","    accuracy = accuracy_score(labels, predictions)\n","\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions,\n","        labels=available_classes,\n","        average='macro',\n","        zero_division=0\n","    )\n","\n","    cm = confusion_matrix(labels, predictions, labels=list(range(len(CASME2_CLASSES))))\n","\n","    per_class_metrics = {}\n","\n","    for i, class_name in enumerate(CASME2_CLASSES):\n","        if i in available_classes:\n","            class_mask = (labels == i)\n","\n","            if np.sum(class_mask) > 0:\n","                # Create binary classification: current class vs all others\n","                binary_labels = (labels == i).astype(int)\n","                binary_predictions = (predictions == i).astype(int)\n","\n","                class_precision, class_recall, class_f1, _ = precision_recall_fscore_support(\n","                    binary_labels, binary_predictions,\n","                    average='binary',\n","                    pos_label=1,\n","                    zero_division=0\n","                )\n","\n","                labels_binary = label_binarize(labels, classes=list(range(len(CASME2_CLASSES))))\n","                if inference_results['probabilities'] is not None:\n","                    probs = inference_results['probabilities'][:, i]\n","                    try:\n","                        fpr, tpr, _ = roc_curve(labels_binary[:, i], probs)\n","                        class_auc = auc(fpr, tpr)\n","                    except:\n","                        class_auc = 0.0\n","                else:\n","                    class_auc = 0.0\n","\n","                per_class_metrics[class_name] = {\n","                    'precision': float(class_precision),\n","                    'recall': float(class_recall),\n","                    'f1_score': float(class_f1),\n","                    'auc': float(class_auc),\n","                    'support': int(np.sum(class_mask)),\n","                    'in_test_set': True\n","                }\n","            else:\n","                per_class_metrics[class_name] = {\n","                    'precision': 0.0,\n","                    'recall': 0.0,\n","                    'f1_score': 0.0,\n","                    'auc': 0.0,\n","                    'support': 0,\n","                    'in_test_set': True\n","                }\n","        else:\n","            per_class_metrics[class_name] = {\n","                'precision': 0.0,\n","                'recall': 0.0,\n","                'f1_score': 0.0,\n","                'auc': 0.0,\n","                'support': 0,\n","                'in_test_set': False\n","            }\n","\n","    macro_auc = np.mean([m['auc'] for m in per_class_metrics.values() if m['in_test_set']])\n","\n","    results = {\n","        'overall_performance': {\n","            'accuracy': float(accuracy),\n","            'macro_precision': float(precision),\n","            'macro_recall': float(recall),\n","            'macro_f1': float(f1),\n","            'macro_auc': float(macro_auc)\n","        },\n","        'per_class_performance': per_class_metrics,\n","        'confusion_matrix': cm.tolist(),\n","        'evaluation_metadata': {\n","            'dataset': 'CASME_II',\n","            'model_type': 'PoolFormerCASME2Baseline',\n","            'test_samples': len(predictions),\n","            'class_names': CASME2_CLASSES,\n","            'missing_classes': [CASME2_CLASSES[i] for i in missing_classes],\n","            'available_classes': [CASME2_CLASSES[i] for i in available_classes],\n","            'evaluation_timestamp': datetime.now().isoformat(),\n","            'evaluation_mode': inference_results['evaluation_mode']\n","        },\n","        'inference_performance': {\n","            'total_time_seconds': float(inference_results['inference_time']),\n","            'average_time_ms_per_sample': float(inference_results['inference_time'] * 1000 / len(predictions))\n","        }\n","    }\n","\n","    if 'kfs_late_fusion_info' in inference_results:\n","        results['kfs_late_fusion_info'] = inference_results['kfs_late_fusion_info']\n","\n","    return results\n","\n","# =====================================================\n","# WRONG PREDICTIONS ANALYSIS\n","# =====================================================\n","\n","def analyze_wrong_predictions(inference_results):\n","    \"\"\"Analyze wrong predictions for error pattern identification\"\"\"\n","    predictions = inference_results['predictions']\n","    labels = inference_results['labels']\n","    filenames = inference_results['filenames']\n","\n","    wrong_predictions = []\n","    wrong_by_class = defaultdict(int)\n","    confusion_patterns = defaultdict(int)\n","\n","    for i in range(len(predictions)):\n","        if predictions[i] != labels[i]:\n","            true_class = CASME2_CLASSES[labels[i]]\n","            pred_class = CASME2_CLASSES[predictions[i]]\n","\n","            wrong_predictions.append({\n","                'filename': filenames[i],\n","                'true_label': int(labels[i]),\n","                'true_class': true_class,\n","                'predicted_label': int(predictions[i]),\n","                'predicted_class': pred_class\n","            })\n","\n","            wrong_by_class[true_class] += 1\n","            confusion_patterns[f\"{true_class} -> {pred_class}\"] += 1\n","\n","    error_summary = {}\n","    for class_name in CASME2_CLASSES:\n","        total_samples = np.sum(labels == CLASS_TO_IDX[class_name])\n","        errors = wrong_by_class.get(class_name, 0)\n","        error_summary[class_name] = {\n","            'total_samples': int(total_samples),\n","            'wrong_predictions': int(errors),\n","            'error_rate': float(errors / total_samples * 100) if total_samples > 0 else 0.0\n","        }\n","\n","    results = {\n","        'analysis_metadata': {\n","            'total_samples': len(predictions),\n","            'total_wrong_predictions': len(wrong_predictions),\n","            'overall_error_rate': (len(wrong_predictions) / len(predictions) * 100) if len(predictions) > 0 else 0.0\n","        },\n","        'wrong_predictions': wrong_predictions,\n","        'wrong_predictions_by_class': dict(wrong_by_class),\n","        'error_summary': error_summary,\n","        'confusion_patterns': dict(confusion_patterns)\n","    }\n","\n","    return results\n","\n","# =====================================================\n","# SAVE EVALUATION RESULTS\n","# =====================================================\n","\n","def save_evaluation_results(evaluation_results, wrong_predictions_results, results_dir, test_version):\n","    \"\"\"Save comprehensive evaluation results\"\"\"\n","    os.makedirs(results_dir, exist_ok=True)\n","\n","    results_file = f\"{results_dir}/casme2_poolformer_evaluation_results_{test_version}.json\"\n","    with open(results_file, 'w') as f:\n","        json.dump(evaluation_results, f, indent=2, default=str)\n","\n","    wrong_predictions_file = f\"{results_dir}/casme2_poolformer_wrong_predictions_{test_version}.json\"\n","    with open(wrong_predictions_file, 'w') as f:\n","        json.dump(wrong_predictions_results, f, indent=2, default=str)\n","\n","    print(f\"Evaluation results saved:\")\n","    print(f\"  Main results: {os.path.basename(results_file)}\")\n","    print(f\"  Wrong predictions: {os.path.basename(wrong_predictions_file)}\")\n","\n","    return results_file, wrong_predictions_file\n","\n","# =====================================================\n","# MAIN EVALUATION EXECUTION\n","# =====================================================\n","\n","all_evaluation_results = {}\n","\n","for dataset_version in EVALUATE_DATASETS:\n","    print(\"\\n\" + \"=\" * 70)\n","    print(f\"EVALUATING DATASET: {dataset_version.upper()}\")\n","    print(\"=\" * 70)\n","\n","    try:\n","        # Get dataset configuration\n","        test_config = get_test_dataset_config(dataset_version, PROJECT_ROOT)\n","\n","        print(f\"\\nTest Dataset Configuration:\")\n","        print(f\"  Version: {test_config['version']}\")\n","        print(f\"  Variant: {test_config['variant']}\")\n","        print(f\"  Description: {test_config['description']}\")\n","        print(f\"  Frame strategy: {test_config['frame_strategy']}\")\n","        print(f\"  Evaluation mode: {test_config['evaluation_mode']}\")\n","        if 'aggregation' in test_config and test_config['aggregation']:\n","            print(f\"  Aggregation: {test_config['aggregation']}\")\n","        print(f\"  Dataset path: {test_config['dataset_path']}\")\n","\n","        # Create test dataset\n","        print(f\"\\nCreating CASME II test dataset from {test_config['variant']}...\")\n","        test_dataset = CASME2DatasetEvaluation(\n","            dataset_root=test_config['dataset_path'],\n","            split='test',\n","            transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","            use_ram_cache=True\n","        )\n","\n","        if len(test_dataset) == 0:\n","            raise ValueError(f\"No test samples found for {dataset_version}!\")\n","\n","        test_loader = DataLoader(\n","            test_dataset,\n","            batch_size=CASME2_POOLFORMER_CONFIG['batch_size'],\n","            shuffle=False,\n","            num_workers=CASME2_POOLFORMER_CONFIG['num_workers'],\n","            pin_memory=True\n","        )\n","\n","        # Load trained model\n","        checkpoint_path = f\"{GLOBAL_CONFIG_CASME2['checkpoint_root']}/casme2_poolformer_mfs_best_f1.pth\"\n","        model, training_info = load_trained_model_casme2(checkpoint_path, GLOBAL_CONFIG_CASME2['device'])\n","\n","        # Run inference based on evaluation mode\n","        if test_config['evaluation_mode'] == 'frame_level':\n","            print(f\"\\nRunning frame-level evaluation for {test_config['variant']}...\")\n","            inference_results = run_frame_level_inference(model, test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","        elif test_config['evaluation_mode'] == 'video_level':\n","            print(f\"\\nRunning video-level evaluation with late fusion for {test_config['variant']}...\")\n","            inference_results = run_video_level_inference_late_fusion(model, test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","        else:\n","            raise ValueError(f\"Unknown evaluation mode: {test_config['evaluation_mode']}\")\n","\n","        # Calculate comprehensive metrics\n","        evaluation_results = calculate_comprehensive_metrics(inference_results)\n","\n","        # Analyze wrong predictions\n","        wrong_predictions_results = analyze_wrong_predictions(inference_results)\n","\n","        # Add training information\n","        evaluation_results['training_information'] = training_info\n","        evaluation_results['test_configuration'] = test_config\n","\n","        # Save results\n","        results_dir = f\"{GLOBAL_CONFIG_CASME2['results_root']}/evaluation_results\"\n","        save_evaluation_results(\n","            evaluation_results, wrong_predictions_results, results_dir, test_config['version']\n","        )\n","\n","        # Store for comparison\n","        all_evaluation_results[dataset_version] = {\n","            'evaluation': evaluation_results,\n","            'wrong_predictions': wrong_predictions_results,\n","            'config': test_config\n","        }\n","\n","        # Display results\n","        print(\"\\n\" + \"=\" * 60)\n","        print(f\"EVALUATION RESULTS - {test_config['variant']} ({dataset_version})\")\n","        print(\"=\" * 60)\n","\n","        overall = evaluation_results['overall_performance']\n","        print(f\"\\nOverall Performance:\")\n","        print(f\"  Accuracy:  {overall['accuracy']:.4f}\")\n","        print(f\"  Precision: {overall['macro_precision']:.4f}\")\n","        print(f\"  Recall:    {overall['macro_recall']:.4f}\")\n","        print(f\"  F1 Score:  {overall['macro_f1']:.4f}\")\n","        print(f\"  AUC:       {overall['macro_auc']:.4f}\")\n","\n","        if 'kfs_late_fusion_info' in evaluation_results:\n","            fusion_info = evaluation_results['kfs_late_fusion_info']\n","            print(f\"\\nLate Fusion Info:\")\n","            print(f\"  Total frames processed: {fusion_info['total_frames']}\")\n","            print(f\"  Video-level predictions: {fusion_info['total_videos']}\")\n","            print(f\"  Aggregation method: {fusion_info['aggregation_method']}\")\n","\n","        print(f\"\\nPer-Class Performance:\")\n","        for class_name, metrics in evaluation_results['per_class_performance'].items():\n","            in_test = \"Present\" if metrics['in_test_set'] else \"Missing\"\n","            print(f\"  {class_name} [{in_test}]: F1={metrics['f1_score']:.4f}, \"\n","                  f\"Support={metrics['support']}\")\n","\n","        wrong_meta = wrong_predictions_results['analysis_metadata']\n","        print(f\"\\nWrong Predictions Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']} / {wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        print(f\"\\nInference Performance:\")\n","        print(f\"  Total time: {inference_results['inference_time']:.2f}s\")\n","        print(f\"  Speed: {evaluation_results['inference_performance']['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","    except Exception as e:\n","        print(f\"Evaluation failed for {dataset_version}: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        continue\n","\n","# =====================================================\n","# COMPARATIVE ANALYSIS (if both datasets evaluated)\n","# =====================================================\n","\n","if len(all_evaluation_results) == 2 and 'v7' in all_evaluation_results and 'v8' in all_evaluation_results:\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"COMPARATIVE ANALYSIS: AF (v7) vs KFS (v8)\")\n","    print(\"=\" * 70)\n","\n","    v7_results = all_evaluation_results['v7']['evaluation']\n","    v8_results = all_evaluation_results['v8']['evaluation']\n","\n","    print(\"\\nOverall Performance Comparison:\")\n","    print(f\"{'Metric':<20} {'AF (v7)':<15} {'KFS (v8)':<15} {'Difference':<15}\")\n","    print(\"-\" * 65)\n","\n","    metrics_to_compare = ['accuracy', 'macro_precision', 'macro_recall', 'macro_f1', 'macro_auc']\n","\n","    for metric in metrics_to_compare:\n","        v7_val = v7_results['overall_performance'][metric]\n","        v8_val = v8_results['overall_performance'][metric]\n","        diff = v8_val - v7_val\n","\n","        print(f\"{metric:<20} {v7_val:<15.4f} {v8_val:<15.4f} {diff:+.4f}\")\n","\n","    print(f\"\\nEvaluation Modes:\")\n","    print(f\"  AF (v7): {v7_results['evaluation_metadata']['evaluation_mode']}\")\n","    print(f\"  KFS (v8): {v8_results['evaluation_metadata']['evaluation_mode']}\")\n","\n","    if 'kfs_late_fusion_info' in v8_results:\n","        print(f\"\\nKFS Late Fusion Strategy:\")\n","        print(f\"  Frames used: {v8_results['kfs_late_fusion_info']['total_frames']}\")\n","        print(f\"  Video predictions: {v8_results['kfs_late_fusion_info']['total_videos']}\")\n","        print(f\"  Aggregation: {v8_results['kfs_late_fusion_info']['aggregation_method']}\")\n","\n","# Memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II POOLFORMER EVALUATION COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Evaluated datasets: {EVALUATE_DATASETS}\")\n","print(\"Next: Cell 4 - Generate confusion matrices and visualization\")"],"metadata":{"cellView":"form","id":"hhcj9EiVSmXu","collapsed":true,"executionInfo":{"status":"ok","timestamp":1761290946281,"user_tz":-420,"elapsed":10765,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ec8f77f5-69f8-42e6-f416-a71ba6923732"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II PoolFormer Evaluation Framework with Dual Dataset Support\n","============================================================\n","Datasets to evaluate: ['v7', 'v8']\n","============================================================\n","\n","======================================================================\n","EVALUATING DATASET: V7\n","======================================================================\n","\n","Test Dataset Configuration:\n","  Version: v7\n","  Variant: AF\n","  Description: Apex Frame with Face-Aware Preprocessing\n","  Frame strategy: apex_frame\n","  Evaluation mode: frame_level\n","  Dataset path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7\n","\n","Creating CASME II test dataset from AF...\n","Loading CASME II test dataset for evaluation...\n","Found 28 image files in directory\n","Loaded 28 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 10 samples (35.7%)\n","  disgust: 7 samples (25.0%)\n","  happiness: 4 samples (14.3%)\n","  repression: 3 samples (10.7%)\n","  surprise: 3 samples (10.7%)\n","  sadness: 1 samples (3.6%)\n","Unique video IDs: 28\n","Missing classes in test set: ['fear']\n","Preloading 28 test images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test set to RAM: 100%|██████████| 28/28 [00:00<00:00, 28.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TEST RAM caching completed: 28/28 images, ~0.02GB\n","\n","Validating checkpoint availability...\n","Expected checkpoint: casme2_poolformer_mfs_best_f1.pth\n","Full path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/07_03_poolformer_casme2_mfs_prep/casme2_poolformer_mfs_best_f1.pth\n","Checkpoint found: 837.9 MB\n","Loading checkpoint from disk...\n","Initializing PoolFormer model...\n","PoolFormer feature dimension: 768\n","Classification head: 768 -> GAP2D -> 512 -> 128 -> 7\n","Dropout rate: 0.3 (balanced for large dataset)\n","Model loaded successfully from epoch 12\n","Best validation F1: 0.1896\n","\n","Running frame-level evaluation for AF...\n","Running frame-level inference...\n"]},{"output_type":"stream","name":"stderr","text":["Frame-level inference: 100%|██████████| 2/2 [00:00<00:00,  2.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Frame-level inference completed in 0.79s\n","Processed 28 frames\n","Evaluation results saved:\n","  Main results: casme2_poolformer_evaluation_results_v7.json\n","  Wrong predictions: casme2_poolformer_wrong_predictions_v7.json\n","\n","============================================================\n","EVALUATION RESULTS - AF (v7)\n","============================================================\n","\n","Overall Performance:\n","  Accuracy:  0.4643\n","  Precision: 0.4653\n","  Recall:    0.3552\n","  F1 Score:  0.3785\n","  AUC:       0.7619\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.4545, Support=10\n","  disgust [Present]: F1=0.6667, Support=7\n","  happiness [Present]: F1=0.2500, Support=4\n","  repression [Present]: F1=0.4000, Support=3\n","  surprise [Present]: F1=0.5000, Support=3\n","  sadness [Present]: F1=0.0000, Support=1\n","  fear [Missing]: F1=0.0000, Support=0\n","\n","Wrong Predictions Analysis:\n","  Total errors: 15 / 28\n","  Error rate: 53.57%\n","\n","Inference Performance:\n","  Total time: 0.79s\n","  Speed: 28.3 ms/sample\n","\n","======================================================================\n","EVALUATING DATASET: V8\n","======================================================================\n","\n","Test Dataset Configuration:\n","  Version: v8\n","  Variant: KFS\n","  Description: Key Frame Sequence with Face-Aware Preprocessing\n","  Frame strategy: key_frame_sequence\n","  Evaluation mode: video_level\n","  Aggregation: late_fusion\n","  Dataset path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8\n","\n","Creating CASME II test dataset from KFS...\n","Loading CASME II test dataset for evaluation...\n","Found 84 image files in directory\n","Loaded 84 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 30 samples (35.7%)\n","  disgust: 21 samples (25.0%)\n","  happiness: 12 samples (14.3%)\n","  repression: 9 samples (10.7%)\n","  surprise: 9 samples (10.7%)\n","  sadness: 3 samples (3.6%)\n","Unique video IDs: 84\n","Missing classes in test set: ['fear']\n","Preloading 84 test images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test set to RAM: 100%|██████████| 84/84 [00:01<00:00, 49.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TEST RAM caching completed: 84/84 images, ~0.05GB\n","\n","Validating checkpoint availability...\n","Expected checkpoint: casme2_poolformer_mfs_best_f1.pth\n","Full path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/07_03_poolformer_casme2_mfs_prep/casme2_poolformer_mfs_best_f1.pth\n","Checkpoint found: 837.9 MB\n","Loading checkpoint from disk...\n","Initializing PoolFormer model...\n","PoolFormer feature dimension: 768\n","Classification head: 768 -> GAP2D -> 512 -> 128 -> 7\n","Dropout rate: 0.3 (balanced for large dataset)\n","Model loaded successfully from epoch 12\n","Best validation F1: 0.1896\n","\n","Running video-level evaluation with late fusion for KFS...\n","Running frame-level predictions for late fusion...\n"]},{"output_type":"stream","name":"stderr","text":["Frame predictions: 100%|██████████| 6/6 [00:00<00:00,  6.28it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Aggregating frame predictions to video level...\n","Late fusion completed in 0.96s\n","Aggregated 84 frames into 84 videos\n","Evaluation results saved:\n","  Main results: casme2_poolformer_evaluation_results_v8.json\n","  Wrong predictions: casme2_poolformer_wrong_predictions_v8.json\n","\n","============================================================\n","EVALUATION RESULTS - KFS (v8)\n","============================================================\n","\n","Overall Performance:\n","  Accuracy:  0.4405\n","  Precision: 0.3626\n","  Recall:    0.3181\n","  F1 Score:  0.3261\n","  AUC:       0.0000\n","\n","Late Fusion Info:\n","  Total frames processed: 84\n","  Video-level predictions: 84\n","  Aggregation method: average_probabilities\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.4918, Support=30\n","  disgust [Present]: F1=0.6522, Support=21\n","  happiness [Present]: F1=0.2400, Support=12\n","  repression [Present]: F1=0.1111, Support=9\n","  surprise [Present]: F1=0.4615, Support=9\n","  sadness [Present]: F1=0.0000, Support=3\n","  fear [Missing]: F1=0.0000, Support=0\n","\n","Wrong Predictions Analysis:\n","  Total errors: 47 / 84\n","  Error rate: 55.95%\n","\n","Inference Performance:\n","  Total time: 0.96s\n","  Speed: 11.4 ms/sample\n","\n","======================================================================\n","COMPARATIVE ANALYSIS: AF (v7) vs KFS (v8)\n","======================================================================\n","\n","Overall Performance Comparison:\n","Metric               AF (v7)         KFS (v8)        Difference     \n","-----------------------------------------------------------------\n","accuracy             0.4643          0.4405          -0.0238\n","macro_precision      0.4653          0.3626          -0.1027\n","macro_recall         0.3552          0.3181          -0.0370\n","macro_f1             0.3785          0.3261          -0.0524\n","macro_auc            0.7619          0.0000          -0.7619\n","\n","Evaluation Modes:\n","  AF (v7): frame_level\n","  KFS (v8): video_level\n","\n","KFS Late Fusion Strategy:\n","  Frames used: 84\n","  Video predictions: 84\n","  Aggregation: average_probabilities\n","\n","======================================================================\n","CASME II POOLFORMER EVALUATION COMPLETED\n","======================================================================\n","Evaluated datasets: ['v7', 'v8']\n","Next: Cell 4 - Generate confusion matrices and visualization\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title Cell 4: CASME II PoolFormer Confusion Matrix Generation\n","\n","# File: 07_03_PoolFormer_CASME2_MFS_Cell4.py\n","# Location: experiments/07_03_PoolFormer_CASME2-MFS-PREP.ipynb\n","# Purpose: Generate professional confusion matrix visualization for AF and KFS evaluations\n","\n","import json\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import seaborn as sns\n","from datetime import datetime\n","\n","print(\"CASME II PoolFormer Confusion Matrix Generation\")\n","print(\"=\" * 60)\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/07_03_poolformer_casme2_mfs_prep\"\n","\n","def find_evaluation_json_files(results_path):\n","    \"\"\"Find evaluation JSON files with version detection\"\"\"\n","    json_files = {}\n","    eval_dir = f\"{results_path}/evaluation_results\"\n","\n","    if os.path.exists(eval_dir):\n","        for version in ['v7', 'v8']:\n","            eval_pattern = f\"{eval_dir}/casme2_poolformer_evaluation_results_{version}.json\"\n","            eval_files = glob.glob(eval_pattern)\n","\n","            if eval_files:\n","                json_files[f'main_{version}'] = eval_files[0]\n","                print(f\"Found {version.upper()} evaluation file: {os.path.basename(eval_files[0])}\")\n","\n","            wrong_pattern = f\"{eval_dir}/casme2_poolformer_wrong_predictions_{version}.json\"\n","            wrong_files = glob.glob(wrong_pattern)\n","\n","            if wrong_files:\n","                json_files[f'wrong_{version}'] = wrong_files[0]\n","                print(f\"Found {version.upper()} wrong predictions: {os.path.basename(wrong_files[0])}\")\n","\n","        if not json_files:\n","            print(f\"WARNING: No evaluation results found in {eval_dir}\")\n","            print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","    else:\n","        print(f\"ERROR: Evaluation directory not found: {eval_dir}\")\n","\n","    return json_files\n","\n","def load_evaluation_results(json_path):\n","    \"\"\"Load and parse evaluation results JSON\"\"\"\n","    try:\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"Successfully loaded: {os.path.basename(json_path)}\")\n","        return data\n","    except Exception as e:\n","        print(f\"ERROR loading {json_path}: {str(e)}\")\n","        return None\n","\n","def calculate_weighted_f1(per_class_performance):\n","    \"\"\"Calculate weighted F1 score\"\"\"\n","    total_support = sum([class_data['support'] for class_data in per_class_performance.values()\n","                        if class_data['support'] > 0])\n","\n","    if total_support == 0:\n","        return 0.0\n","\n","    weighted_f1 = 0.0\n","\n","    for class_name, class_data in per_class_performance.items():\n","        if class_data['support'] > 0:\n","            weight = class_data['support'] / total_support\n","            weighted_f1 += class_data['f1_score'] * weight\n","\n","    return weighted_f1\n","\n","def calculate_balanced_accuracy(confusion_matrix):\n","    \"\"\"Calculate balanced accuracy handling classes with zero support\"\"\"\n","    cm = np.array(confusion_matrix)\n","    n_classes = cm.shape[0]\n","\n","    per_class_balanced_acc = []\n","    classes_with_samples = []\n","\n","    for i in range(n_classes):\n","        if cm[i, :].sum() > 0:\n","            classes_with_samples.append(i)\n","\n","    for i in classes_with_samples:\n","        tp = cm[i, i]\n","        fn = cm[i, :].sum() - tp\n","        fp = cm[:, i].sum() - tp\n","        tn = cm.sum() - tp - fn - fp\n","\n","        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n","\n","        class_balanced_acc = (sensitivity + specificity) / 2\n","        per_class_balanced_acc.append(class_balanced_acc)\n","\n","    balanced_acc = np.mean(per_class_balanced_acc) if per_class_balanced_acc else 0.0\n","\n","    return balanced_acc\n","\n","def determine_text_color(color_value, threshold=0.5):\n","    \"\"\"Determine optimal text color based on background intensity\"\"\"\n","    return 'white' if color_value > threshold else 'black'\n","\n","def create_confusion_matrix_plot(data, output_path, test_version):\n","    \"\"\"Create professional confusion matrix visualization for CASME II micro-expression recognition\"\"\"\n","\n","    meta = data['evaluation_metadata']\n","    class_names = meta['class_names']\n","    cm = np.array(data['confusion_matrix'], dtype=int)\n","    overall = data['overall_performance']\n","    per_class = data['per_class_performance']\n","\n","    test_config = data.get('test_configuration', {})\n","    variant = test_config.get('variant', test_version.upper())\n","    description = test_config.get('description', f'{test_version} preprocessing')\n","    eval_mode = meta.get('evaluation_mode', 'frame_level')\n","\n","    print(f\"Processing confusion matrix for {variant} ({test_version})\")\n","    print(f\"Dataset: {description}\")\n","    print(f\"Evaluation mode: {eval_mode}\")\n","    print(f\"Confusion matrix shape: {cm.shape}\")\n","\n","    macro_f1 = overall.get('macro_f1', 0.0)\n","    accuracy = overall.get('accuracy', 0.0)\n","    weighted_f1 = calculate_weighted_f1(per_class)\n","    balanced_acc = calculate_balanced_accuracy(cm)\n","\n","    print(f\"Calculated metrics - Macro F1: {macro_f1:.4f}, Weighted F1: {weighted_f1:.4f}, \"\n","          f\"Balanced Acc: {balanced_acc:.4f}, Accuracy: {accuracy:.4f}\")\n","\n","    row_sums = cm.sum(axis=1, keepdims=True)\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        cm_pct = np.divide(cm, row_sums, where=(row_sums!=0))\n","        cm_pct = np.nan_to_num(cm_pct)\n","\n","    fig, ax = plt.subplots(figsize=(12, 10))\n","\n","    cmap = 'Blues'\n","    im = ax.imshow(cm_pct, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=0.8)\n","\n","    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","    cbar.set_label('True Class Percentage', rotation=270, labelpad=15, fontsize=11)\n","\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            count = cm[i, j]\n","\n","            if row_sums[i, 0] > 0:\n","                percentage = cm_pct[i, j] * 100\n","                text = f\"{count}\\n{percentage:.1f}%\"\n","            else:\n","                text = f\"{count}\\n(N/A)\"\n","\n","            cell_value = cm_pct[i, j]\n","            text_color = determine_text_color(cell_value, threshold=0.4)\n","\n","            ax.text(j, i, text, ha=\"center\", va=\"center\",\n","                   color=text_color, fontsize=9, fontweight='bold')\n","\n","    ax.set_xticks(np.arange(len(class_names)))\n","    ax.set_yticks(np.arange(len(class_names)))\n","    ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=10)\n","    ax.set_yticklabels(class_names, fontsize=10)\n","    ax.set_xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n","    ax.set_ylabel(\"True Label\", fontsize=12, fontweight='bold')\n","\n","    preprocessing_note = f\"Preprocessing: {description}\\n\"\n","    preprocessing_note += f\"Dataset: {test_version}\\n\"\n","    preprocessing_note += f\"Evaluation: {eval_mode.replace('_', ' ').title()}\"\n","\n","    if 'kfs_late_fusion_info' in data:\n","        fusion_info = data['kfs_late_fusion_info']\n","        preprocessing_note += f\"\\nFrames: {fusion_info['total_frames']}, Videos: {fusion_info['total_videos']}\"\n","\n","    missing_classes = meta.get('missing_classes', [])\n","    if missing_classes:\n","        preprocessing_note += f\"\\nMissing: {', '.join(missing_classes)}\"\n","\n","    ax.text(0.02, 0.98, preprocessing_note, transform=ax.transAxes, fontsize=8,\n","            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n","\n","    title = f\"CASME II {variant} Micro-Expression Recognition - PoolFormer\\n\"\n","    title += f\"Acc: {accuracy:.4f}  |  Macro F1: {macro_f1:.4f}  |  Weighted F1: {weighted_f1:.4f}  |  Balanced Acc: {balanced_acc:.4f}\"\n","    ax.set_title(title, fontsize=12, pad=25, fontweight='bold')\n","\n","    plt.tight_layout()\n","    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n","    plt.close(fig)\n","\n","    print(f\"Confusion matrix saved to: {os.path.basename(output_path)}\")\n","\n","    return {\n","        'accuracy': accuracy,\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        'balanced_accuracy': balanced_acc,\n","        'missing_classes': missing_classes\n","    }\n","\n","def generate_performance_summary(evaluation_data, wrong_predictions_data=None):\n","    \"\"\"Generate comprehensive performance summary\"\"\"\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\")\n","    print(\"=\" * 60)\n","\n","    overall = evaluation_data['overall_performance']\n","    meta = evaluation_data['evaluation_metadata']\n","    test_config = evaluation_data.get('test_configuration', {})\n","\n","    variant = test_config.get('variant', 'N/A')\n","\n","    print(f\"Dataset: {meta['dataset']}\")\n","    print(f\"Variant: {variant}\")\n","    print(f\"Dataset version: {test_config.get('version', 'N/A')}\")\n","    print(f\"Preprocessing: {test_config.get('description', 'N/A')}\")\n","    print(f\"Test samples: {meta['test_samples']}\")\n","    print(f\"Model: {meta['model_type']}\")\n","    print(f\"Evaluation date: {meta['evaluation_timestamp']}\")\n","\n","    if 'kfs_late_fusion_info' in evaluation_data:\n","        fusion_info = evaluation_data['kfs_late_fusion_info']\n","        print(f\"\\nLate Fusion Information:\")\n","        print(f\"  Total frames: {fusion_info['total_frames']}\")\n","        print(f\"  Video predictions: {fusion_info['total_videos']}\")\n","        print(f\"  Aggregation: {fusion_info['aggregation_method']}\")\n","\n","    print(f\"\\nOverall Performance:\")\n","    print(f\"  Accuracy:         {overall['accuracy']:.4f}\")\n","    print(f\"  Macro Precision:  {overall['macro_precision']:.4f}\")\n","    print(f\"  Macro Recall:     {overall['macro_recall']:.4f}\")\n","    print(f\"  Macro F1:         {overall['macro_f1']:.4f}\")\n","    print(f\"  Macro AUC:        {overall['macro_auc']:.4f}\")\n","\n","    print(f\"\\nPer-Class Performance:\")\n","    per_class = evaluation_data['per_class_performance']\n","\n","    print(f\"{'Class':<12} {'F1':<8} {'Precision':<10} {'Recall':<8} {'AUC':<8} {'Support':<8} {'In Test'}\")\n","    print(\"-\" * 65)\n","\n","    for class_name, metrics in per_class.items():\n","        in_test = \"Yes\" if metrics['in_test_set'] else \"No\"\n","        print(f\"{class_name:<12} {metrics['f1_score']:<8.4f} {metrics['precision']:<10.4f} \"\n","              f\"{metrics['recall']:<8.4f} {metrics['auc']:<8.4f} {metrics['support']:<8} {in_test}\")\n","\n","    if 'training_information' in evaluation_data:\n","        training = evaluation_data['training_information']\n","        print(f\"\\nTraining vs Test Comparison:\")\n","        print(f\"  Training Val F1:  {training['best_val_f1']:.4f}\")\n","        print(f\"  Test F1:          {overall['macro_f1']:.4f}\")\n","        print(f\"  Performance Gap:  {training['best_val_f1'] - overall['macro_f1']:+.4f}\")\n","        print(f\"  Best Epoch:       {training['best_epoch']}\")\n","\n","    missing_classes = meta.get('missing_classes', [])\n","    available_classes = meta.get('available_classes', [])\n","\n","    print(f\"\\nClass Availability Analysis:\")\n","    print(f\"  Available classes: {len(available_classes)}/7\")\n","    print(f\"  Missing classes: {missing_classes if missing_classes else 'None'}\")\n","\n","    if wrong_predictions_data:\n","        wrong_meta = wrong_predictions_data['analysis_metadata']\n","        print(f\"\\nError Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']}/{wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        patterns = wrong_predictions_data.get('confusion_patterns', {})\n","        if patterns:\n","            print(f\"\\nTop Confusion Patterns:\")\n","            sorted_patterns = sorted(patterns.items(), key=lambda x: x[1], reverse=True)[:3]\n","            for pattern, count in sorted_patterns:\n","                print(f\"  {pattern}: {count} cases\")\n","\n","    print(f\"\\nInference Performance:\")\n","    inference = evaluation_data['inference_performance']\n","    print(f\"  Total time: {inference['total_time_seconds']:.2f}s\")\n","    print(f\"  Speed: {inference['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","json_files = find_evaluation_json_files(RESULTS_ROOT)\n","\n","if not json_files:\n","    print(f\"ERROR: No evaluation JSON files found in {RESULTS_ROOT}\")\n","    print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","else:\n","    print(f\"\\nFound {len([k for k in json_files.keys() if k.startswith('main_')])} evaluation result(s)\")\n","\n","output_dir = f\"{RESULTS_ROOT}/confusion_matrix_analysis\"\n","Path(output_dir).mkdir(parents=True, exist_ok=True)\n","\n","results_summary = {}\n","generated_files = []\n","\n","for version in ['v7', 'v8']:\n","    main_key = f'main_{version}'\n","    wrong_key = f'wrong_{version}'\n","\n","    if main_key in json_files:\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Processing {version.upper()} Evaluation Results\")\n","        print(f\"{'='*60}\")\n","\n","        eval_data = load_evaluation_results(json_files[main_key])\n","\n","        wrong_data = None\n","        if wrong_key in json_files:\n","            wrong_data = load_evaluation_results(json_files[wrong_key])\n","\n","        if eval_data is not None:\n","            try:\n","                cm_output_path = os.path.join(output_dir, f\"confusion_matrix_CASME2_PoolFormer_{version}.png\")\n","                metrics = create_confusion_matrix_plot(eval_data, cm_output_path, version)\n","                generated_files.append(cm_output_path)\n","\n","                results_summary[version] = metrics\n","\n","                print(f\"\\nSUCCESS: {version.upper()} confusion matrix generated successfully\")\n","                print(f\"Output file: {os.path.basename(cm_output_path)}\")\n","\n","                print(f\"\\nPerformance Metrics Summary:\")\n","                print(f\"  Accuracy:        {metrics['accuracy']:.4f}\")\n","                print(f\"  Macro F1:        {metrics['macro_f1']:.4f}\")\n","                print(f\"  Weighted F1:     {metrics['weighted_f1']:.4f}\")\n","                print(f\"  Balanced Acc:    {metrics['balanced_accuracy']:.4f}\")\n","\n","                if metrics['missing_classes']:\n","                    print(f\"  Missing classes: {metrics['missing_classes']}\")\n","\n","            except Exception as e:\n","                print(f\"ERROR: Failed to generate {version.upper()} confusion matrix: {str(e)}\")\n","                import traceback\n","                traceback.print_exc()\n","\n","            generate_performance_summary(eval_data, wrong_data)\n","        else:\n","            print(f\"ERROR: Could not load {version.upper()} evaluation data\")\n","\n","if generated_files:\n","    print(f\"\\n\" + \"=\" * 60)\n","    print(\"CASME II CONFUSION MATRIX GENERATION COMPLETED\")\n","    print(\"=\" * 60)\n","\n","    print(f\"\\nGenerated visualization files:\")\n","    for file_path in generated_files:\n","        filename = os.path.basename(file_path)\n","        print(f\"  {filename}\")\n","\n","    for version in ['v7', 'v8']:\n","        if version in results_summary:\n","            variant = 'AF' if version == 'v7' else 'KFS'\n","            print(f\"\\n{variant} ({version.upper()}) Performance Summary:\")\n","            metrics = results_summary[version]\n","            print(f\"  Accuracy:       {metrics['accuracy']:.4f}\")\n","            print(f\"  Macro F1:       {metrics['macro_f1']:.4f}\")\n","            print(f\"  Weighted F1:    {metrics['weighted_f1']:.4f}\")\n","            print(f\"  Balanced Acc:   {metrics['balanced_accuracy']:.4f}\")\n","\n","    print(f\"\\nFiles saved in: {output_dir}\")\n","    print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","else:\n","    print(f\"\\nERROR: No visualizations were generated\")\n","    print(\"Please check:\")\n","    print(\"1. Cell 3 evaluation results exist\")\n","    print(\"2. JSON file structure is correct\")\n","    print(\"3. No file permission issues\")\n","\n","print(\"\\nCell 4 completed - CASME II confusion matrix analysis generated\")\n","print(\"\\n\" + \"=\" * 60)\n","print(\"ALL EXPERIMENTS COMPLETED - READY FOR CONFERENCE PAPER\")\n","print(\"=\" * 60)"],"metadata":{"cellView":"form","id":"3g7zBveOSpHE","collapsed":true,"executionInfo":{"status":"ok","timestamp":1761290949468,"user_tz":-420,"elapsed":3180,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e3a4f139-deaf-47bd-95b7-d51f4d79a2fe"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II PoolFormer Confusion Matrix Generation\n","============================================================\n","Found V7 evaluation file: casme2_poolformer_evaluation_results_v7.json\n","Found V7 wrong predictions: casme2_poolformer_wrong_predictions_v7.json\n","Found V8 evaluation file: casme2_poolformer_evaluation_results_v8.json\n","Found V8 wrong predictions: casme2_poolformer_wrong_predictions_v8.json\n","\n","Found 2 evaluation result(s)\n","\n","============================================================\n","Processing V7 Evaluation Results\n","============================================================\n","Successfully loaded: casme2_poolformer_evaluation_results_v7.json\n","Successfully loaded: casme2_poolformer_wrong_predictions_v7.json\n","Processing confusion matrix for AF (v7)\n","Dataset: Apex Frame with Face-Aware Preprocessing\n","Evaluation mode: frame_level\n","Confusion matrix shape: (7, 7)\n","Calculated metrics - Macro F1: 0.3785, Weighted F1: 0.4611, Balanced Acc: 0.6164, Accuracy: 0.4643\n","Confusion matrix saved to: confusion_matrix_CASME2_PoolFormer_v7.png\n","\n","SUCCESS: V7 confusion matrix generated successfully\n","Output file: confusion_matrix_CASME2_PoolFormer_v7.png\n","\n","Performance Metrics Summary:\n","  Accuracy:        0.4643\n","  Macro F1:        0.3785\n","  Weighted F1:     0.4611\n","  Balanced Acc:    0.6164\n","  Missing classes: ['fear']\n","\n","============================================================\n","CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\n","============================================================\n","Dataset: CASME_II\n","Variant: AF\n","Dataset version: v7\n","Preprocessing: Apex Frame with Face-Aware Preprocessing\n","Test samples: 28\n","Model: PoolFormerCASME2Baseline\n","Evaluation date: 2025-10-24T07:29:00.280410\n","\n","Overall Performance:\n","  Accuracy:         0.4643\n","  Macro Precision:  0.4653\n","  Macro Recall:     0.3552\n","  Macro F1:         0.3785\n","  Macro AUC:        0.7619\n","\n","Per-Class Performance:\n","Class        F1       Precision  Recall   AUC      Support  In Test\n","-----------------------------------------------------------------\n","others       0.4545   0.4167     0.5000   0.7222   10       Yes\n","disgust      0.6667   0.6250     0.7143   0.7823   7        Yes\n","happiness    0.2500   0.2500     0.2500   0.6667   4        Yes\n","repression   0.4000   0.5000     0.3333   0.7600   3        Yes\n","surprise     0.5000   1.0000     0.3333   0.9733   3        Yes\n","sadness      0.0000   0.0000     0.0000   0.6667   1        Yes\n","fear         0.0000   0.0000     0.0000   0.0000   0        No\n","\n","Training vs Test Comparison:\n","  Training Val F1:  0.1896\n","  Test F1:          0.3785\n","  Performance Gap:  -0.1889\n","  Best Epoch:       12\n","\n","Class Availability Analysis:\n","  Available classes: 6/7\n","  Missing classes: ['fear']\n","\n","Error Analysis:\n","  Total errors: 15/28\n","  Error rate: 53.57%\n","\n","Top Confusion Patterns:\n","  others -> disgust: 2 cases\n","  repression -> others: 2 cases\n","  happiness -> others: 2 cases\n","\n","Inference Performance:\n","  Total time: 0.79s\n","  Speed: 28.3 ms/sample\n","\n","============================================================\n","Processing V8 Evaluation Results\n","============================================================\n","Successfully loaded: casme2_poolformer_evaluation_results_v8.json\n","Successfully loaded: casme2_poolformer_wrong_predictions_v8.json\n","Processing confusion matrix for KFS (v8)\n","Dataset: Key Frame Sequence with Face-Aware Preprocessing\n","Evaluation mode: video_level\n","Confusion matrix shape: (7, 7)\n","Calculated metrics - Macro F1: 0.3261, Weighted F1: 0.4343, Balanced Acc: 0.5975, Accuracy: 0.4405\n","Confusion matrix saved to: confusion_matrix_CASME2_PoolFormer_v8.png\n","\n","SUCCESS: V8 confusion matrix generated successfully\n","Output file: confusion_matrix_CASME2_PoolFormer_v8.png\n","\n","Performance Metrics Summary:\n","  Accuracy:        0.4405\n","  Macro F1:        0.3261\n","  Weighted F1:     0.4343\n","  Balanced Acc:    0.5975\n","  Missing classes: ['fear']\n","\n","============================================================\n","CASME II MICRO-EXPRESSION RECOGNITION PERFORMANCE SUMMARY\n","============================================================\n","Dataset: CASME_II\n","Variant: KFS\n","Dataset version: v8\n","Preprocessing: Key Frame Sequence with Face-Aware Preprocessing\n","Test samples: 84\n","Model: PoolFormerCASME2Baseline\n","Evaluation date: 2025-10-24T07:29:06.225170\n","\n","Late Fusion Information:\n","  Total frames: 84\n","  Video predictions: 84\n","  Aggregation: average_probabilities\n","\n","Overall Performance:\n","  Accuracy:         0.4405\n","  Macro Precision:  0.3626\n","  Macro Recall:     0.3181\n","  Macro F1:         0.3261\n","  Macro AUC:        0.0000\n","\n","Per-Class Performance:\n","Class        F1       Precision  Recall   AUC      Support  In Test\n","-----------------------------------------------------------------\n","others       0.4918   0.4839     0.5000   0.0000   30       Yes\n","disgust      0.6522   0.6000     0.7143   0.0000   21       Yes\n","happiness    0.2400   0.2308     0.2500   0.0000   12       Yes\n","repression   0.1111   0.1111     0.1111   0.0000   9        Yes\n","surprise     0.4615   0.7500     0.3333   0.0000   9        Yes\n","sadness      0.0000   0.0000     0.0000   0.0000   3        Yes\n","fear         0.0000   0.0000     0.0000   0.0000   0        No\n","\n","Training vs Test Comparison:\n","  Training Val F1:  0.1896\n","  Test F1:          0.3261\n","  Performance Gap:  -0.1365\n","  Best Epoch:       12\n","\n","Class Availability Analysis:\n","  Available classes: 6/7\n","  Missing classes: ['fear']\n","\n","Error Analysis:\n","  Total errors: 47/84\n","  Error rate: 55.95%\n","\n","Top Confusion Patterns:\n","  others -> disgust: 7 cases\n","  repression -> others: 6 cases\n","  happiness -> others: 4 cases\n","\n","Inference Performance:\n","  Total time: 0.96s\n","  Speed: 11.4 ms/sample\n","\n","============================================================\n","CASME II CONFUSION MATRIX GENERATION COMPLETED\n","============================================================\n","\n","Generated visualization files:\n","  confusion_matrix_CASME2_PoolFormer_v7.png\n","  confusion_matrix_CASME2_PoolFormer_v8.png\n","\n","AF (V7) Performance Summary:\n","  Accuracy:       0.4643\n","  Macro F1:       0.3785\n","  Weighted F1:    0.4611\n","  Balanced Acc:   0.6164\n","\n","KFS (V8) Performance Summary:\n","  Accuracy:       0.4405\n","  Macro F1:       0.3261\n","  Weighted F1:    0.4343\n","  Balanced Acc:   0.5975\n","\n","Files saved in: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/07_03_poolformer_casme2_mfs_prep/confusion_matrix_analysis\n","Analysis completed at: 2025-10-24 07:29:09\n","\n","Cell 4 completed - CASME II confusion matrix analysis generated\n","\n","============================================================\n","ALL EXPERIMENTS COMPLETED - READY FOR CONFERENCE PAPER\n","============================================================\n"]}]}]}