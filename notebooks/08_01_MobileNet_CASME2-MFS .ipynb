{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyOd+FvF+NIeIeDy4VUOUB+5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a06d928b043d4b95b4f37abeec4080d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81f90f85093749609e9f9a0d0b4c1b41","IPY_MODEL_8104dea3a4b744d6bcbf505124bc32e0","IPY_MODEL_4e953ef331234f1bbfcad3b9f848657d"],"layout":"IPY_MODEL_ae913560876c41efab189236b21a0d01"}},"81f90f85093749609e9f9a0d0b4c1b41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31a36969f3f045a79f7d4d92f3e14de1","placeholder":"​","style":"IPY_MODEL_d6818ff042834161995d82a067b6de9b","value":"model.safetensors: 100%"}},"8104dea3a4b744d6bcbf505124bc32e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dbddaf4bc9b46a58a209a53aabe1229","max":10241912,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ffae2870da8b47b9af9f80c48c56ff31","value":10241912}},"4e953ef331234f1bbfcad3b9f848657d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c71aab2977d747f4ae2caec2f3a0e75e","placeholder":"​","style":"IPY_MODEL_63fe86e77cb04360b149df5252315269","value":" 10.2M/10.2M [00:00&lt;00:00, 16.3MB/s]"}},"ae913560876c41efab189236b21a0d01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31a36969f3f045a79f7d4d92f3e14de1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6818ff042834161995d82a067b6de9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0dbddaf4bc9b46a58a209a53aabe1229":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffae2870da8b47b9af9f80c48c56ff31":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c71aab2977d747f4ae2caec2f3a0e75e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63fe86e77cb04360b149df5252315269":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a06d928b043d4b95b4f37abeec4080d5","81f90f85093749609e9f9a0d0b4c1b41","8104dea3a4b744d6bcbf505124bc32e0","4e953ef331234f1bbfcad3b9f848657d","ae913560876c41efab189236b21a0d01","31a36969f3f045a79f7d4d92f3e14de1","d6818ff042834161995d82a067b6de9b","0dbddaf4bc9b46a58a209a53aabe1229","ffae2870da8b47b9af9f80c48c56ff31","c71aab2977d747f4ae2caec2f3a0e75e","63fe86e77cb04360b149df5252315269"]},"collapsed":true,"id":"IUMRIjmQjlOu","executionInfo":{"status":"ok","timestamp":1762948128366,"user_tz":-420,"elapsed":107453,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"5309769b-7694-421f-add8-59a8714561d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","CASME II CNN BASELINE - MobileNetV3-Small M1 MFS\n","============================================================\n","\n","[1] Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully\n","\n","[2] Importing required libraries...\n","\n","CASME II MobileNetV3-Small M1 MFS Baseline - Infrastructure Configuration\n","============================================================\n","Loading CASME II Phase 3 dataset metadata...\n","Dataset: CASME2_MultiFrameSampling\n","Phase: Phase 3\n","Total images: 2774\n","Extraction strategy: {'train': 'multi_frame_windows_with_fallback', 'val': 'key_frames_only', 'test': 'key_frames_only', 'fallback_method': 'nearest_frame_duplication'}\n","\n","==================================================\n","EXPERIMENT CONFIGURATION - CNN M1 MFS\n","==================================================\n","Model: MobileNetV3-Small (TIMM)\n","Methodology: M1 (Raw Images)\n","Input Resolution: 640x480 RGB\n","Preprocessing: None (raw CASME II resolution)\n","Loss Function: Focal Loss\n","  Gamma: 2.5\n","  Alpha Weights: [0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285]\n","  Alpha Sum: 1.000\n","==================================================\n","\n","Device: cuda\n","GPU: NVIDIA L4 (23.8 GB)\n","L4: Balanced performance configuration\n","RAM preload workers: 32\n","\n","Loading class distribution...\n","Using class distribution from split_metadata (v3 format)\n","\n","Train distribution: {'others': 1027, 'disgust': 650, 'happiness': 325, 'repression': 273, 'surprise': 260, 'sadness': 65, 'fear': 13}\n","Validation distribution: {'others': 30, 'disgust': 18, 'happiness': 9, 'repression': 9, 'surprise': 6, 'sadness': 3, 'fear': 3}\n","Test distribution: {'others': 30, 'disgust': 21, 'happiness': 12, 'repression': 8, 'surprise': 9, 'sadness': 3}\n","Applied Focal Loss alpha weights: [0.14285715 0.14285715 0.14285715 0.14285715 0.14285715 0.14285715\n"," 0.14285715]\n","Alpha weights sum: 1.000\n","\n","MobileNetV3 Configuration Summary:\n","  Model: mobilenetv3_small_100\n","  Input size: 640x480 RGB\n","  Methodology: M1 (raw images)\n","  Learning rate: 5e-05\n","  Batch size: 16\n","  Dataset phase: v3\n","  Frame strategy: multi_frame_sampling\n","\n","Setting up transforms for M1 methodology (raw 640x480 RGB)...\n","M1 transforms configured: raw 640x480 RGB with ImageNet normalization\n","\n","Dataset paths:\n","Train: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v3/train\n","Validation: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v3/val\n","Test: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split_v3/test\n","\n","MobileNetV3 CASME II architecture validation...\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/10.2M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a06d928b043d4b95b4f37abeec4080d5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["MobileNetV3-Small feature dimension: 1024\n","MobileNet CASME II: 1024 -> 512 -> 128 -> 7\n","Validation failed: Expected more than 1 value per channel when training, got input size torch.Size([1, 512])\n","\n","============================================================\n","CASME II MOBILENETV3-SMALL M1 MFS BASELINE CONFIGURATION COMPLETE\n","============================================================\n","Loss Configuration:\n","  Function: Optimized Focal Loss\n","  Gamma: 2.5\n","  Per-class Alpha: [0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285]\n","  Alpha Sum: 1.000\n","\n","Model Configuration:\n","  Architecture: MobileNetV3-Small\n","  Parameters: ~2.5M\n","  Input Resolution: 640x480 RGB (raw)\n","  Methodology: M1 (No preprocessing)\n","\n","Dataset Configuration:\n","  Phase: v3\n","  Frame strategy: multi_frame_sampling\n","  Train augmentation: temporal_windows\n","  Classes: 7\n","  Train samples: 2613\n","\n","Next: Cell 2 - Dataset Loading and Training Pipeline\n"]}],"source":["# @title Cell 1: CASME II MobileNetV3-Small M1 MFS Infrastructure Configuration\n","\n","# File: 08_01_MobileNet_CASME2_M1_MFS_Cell1.py\n","# Location: experiments/08_01_MobileNet_CASME2_M1_MFS.ipynb\n","# Purpose: MobileNetV3-Small for CASME II micro-expression recognition with M1 MFS methodology\n","\n","from google.colab import drive\n","print(\"=\" * 60)\n","print(\"CASME II CNN BASELINE - MobileNetV3-Small M1 MFS\")\n","print(\"=\" * 60)\n","print(\"\\n[1] Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Google Drive mounted successfully\")\n","\n","print(\"\\n[2] Importing required libraries...\")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import timm\n","import json\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import time\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","DATASET_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/data_split_v3\"\n","CHECKPOINT_ROOT = f\"{PROJECT_ROOT}/models/08_01_mobilenet_casme2_mfs\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/08_01_mobilenet_casme2_mfs\"\n","\n","METADATA_TRAIN = f\"{DATASET_ROOT}/split_metadata_v3.json\"\n","PROCESSING_SUMMARY = f\"{DATASET_ROOT}/processing_summary_v3.json\"\n","\n","print(\"\\nCASME II MobileNetV3-Small M1 MFS Baseline - Infrastructure Configuration\")\n","print(\"=\" * 60)\n","\n","if not os.path.exists(METADATA_TRAIN):\n","    raise FileNotFoundError(f\"Phase 3 metadata not found: {METADATA_TRAIN}\")\n","if not os.path.exists(PROCESSING_SUMMARY):\n","    raise FileNotFoundError(f\"Phase 3 processing summary not found: {PROCESSING_SUMMARY}\")\n","\n","print(\"Loading CASME II Phase 3 dataset metadata...\")\n","with open(METADATA_TRAIN, 'r') as f:\n","    casme2_metadata = json.load(f)\n","\n","with open(PROCESSING_SUMMARY, 'r') as f:\n","    processing_info = json.load(f)\n","\n","print(f\"Dataset: {processing_info['dataset']}\")\n","print(f\"Phase: {processing_info['phase']}\")\n","print(f\"Total images: {processing_info['total_images_copied']}\")\n","print(f\"Extraction strategy: {processing_info.get('extraction_strategy', {})}\")\n","\n","USE_FOCAL_LOSS = True\n","FOCAL_LOSS_GAMMA = 2.5\n","\n","CROSSENTROPY_CLASS_WEIGHTS = [1.00, 1.25, 1.76, 1.91, 1.99, 3.76, 7.04]\n","FOCAL_LOSS_ALPHA_WEIGHTS = [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","\n","MOBILENET_MODEL_NAME = 'mobilenetv3_small_100'\n","\n","print(\"\\n\" + \"=\" * 50)\n","print(\"EXPERIMENT CONFIGURATION - CNN M1 MFS\")\n","print(\"=\" * 50)\n","print(f\"Model: MobileNetV3-Small (TIMM)\")\n","print(f\"Methodology: M1 (Raw Images)\")\n","print(f\"Input Resolution: 640x480 RGB\")\n","print(f\"Preprocessing: None (raw CASME II resolution)\")\n","print(f\"Loss Function: Focal Loss\")\n","print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","print(f\"  Alpha Weights: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","print(\"=\" * 50)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n","gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n","\n","print(f\"\\nDevice: {device}\")\n","print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n","\n","if 'A100' in gpu_name:\n","    BATCH_SIZE = 24\n","    NUM_WORKERS = 8\n","    torch.backends.cudnn.benchmark = True\n","    print(\"A100: Optimized batch size for 640x480 RGB input\")\n","elif 'L4' in gpu_name:\n","    BATCH_SIZE = 16\n","    NUM_WORKERS = 8\n","    torch.backends.cudnn.benchmark = True\n","    print(\"L4: Balanced performance configuration\")\n","else:\n","    BATCH_SIZE = 8\n","    NUM_WORKERS = 8\n","    print(\"Default GPU: Conservative settings for large input size\")\n","\n","RAM_PRELOAD_WORKERS = 32\n","print(f\"RAM preload workers: {RAM_PRELOAD_WORKERS}\")\n","\n","CASME2_CLASSES = ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CASME2_CLASSES)}\n","\n","print(\"\\nLoading class distribution...\")\n","try:\n","    if 'splits' in casme2_metadata:\n","        train_dist = casme2_metadata['splits']['train']['class_distribution']\n","        val_dist = casme2_metadata['splits']['val']['class_distribution']\n","        test_dist = casme2_metadata['splits']['test']['class_distribution']\n","        print(\"Using class distribution from split_metadata (v3 format)\")\n","    elif 'train' in casme2_metadata and 'class_distribution' in casme2_metadata['train']:\n","        train_dist = casme2_metadata['train']['class_distribution']\n","        val_dist = casme2_metadata['val']['class_distribution']\n","        test_dist = casme2_metadata['test']['class_distribution']\n","        print(\"Using class distribution from split_metadata (v1 format)\")\n","    else:\n","        train_dist = processing_info['class_preservation']['train']\n","        val_dist = processing_info['class_preservation']['val']\n","        test_dist = processing_info['class_preservation']['test']\n","        print(\"Using class distribution from processing_summary (v2 format)\")\n","except KeyError as e:\n","    raise KeyError(f\"Could not load class distribution from metadata. Missing key: {e}\")\n","\n","print(f\"\\nTrain distribution: {train_dist}\")\n","print(f\"Validation distribution: {val_dist}\")\n","print(f\"Test distribution: {test_dist}\")\n","\n","if USE_FOCAL_LOSS:\n","    class_weights = torch.tensor(FOCAL_LOSS_ALPHA_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied Focal Loss alpha weights: {class_weights.cpu().numpy()}\")\n","    print(f\"Alpha weights sum: {class_weights.sum().item():.3f}\")\n","else:\n","    class_weights = torch.tensor(CROSSENTROPY_CLASS_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied CrossEntropy class weights: {class_weights.cpu().numpy()}\")\n","\n","CASME2_MOBILENET_CONFIG = {\n","    'model_name': MOBILENET_MODEL_NAME,\n","    'input_size': (640, 480),\n","    'num_classes': 7,\n","    'dropout_rate': 0.3,\n","\n","    'learning_rate': 5e-5,\n","    'weight_decay': 1e-5,\n","    'gradient_clip': 1.0,\n","    'num_epochs': 50,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'device': device,\n","\n","    'scheduler_type': 'plateau',\n","    'scheduler_mode': 'max',\n","    'scheduler_factor': 0.5,\n","    'scheduler_patience': 5,\n","    'scheduler_min_lr': 1e-6,\n","    'scheduler_monitor': 'val_f1_macro',\n","\n","    'use_focal_loss': USE_FOCAL_LOSS,\n","    'focal_loss_gamma': FOCAL_LOSS_GAMMA,\n","    'focal_loss_alpha_weights': FOCAL_LOSS_ALPHA_WEIGHTS,\n","    'crossentropy_class_weights': CROSSENTROPY_CLASS_WEIGHTS,\n","    'class_weights': class_weights,\n","\n","    'use_macro_avg': True,\n","    'early_stopping': False,\n","    'save_best_f1': True,\n","    'save_strategy': 'best_only',\n","\n","    'dataset_phase': 'v3',\n","    'methodology': 'M1',\n","    'preprocessing': 'raw_images',\n","    'frame_strategy': 'multi_frame_sampling',\n","    'train_augmentation': 'temporal_windows',\n","    'frame_types': ['onset', 'apex', 'offset'],\n","    'extraction_strategy': processing_info.get('extraction_strategy', {}),\n","    'copy_statistics': processing_info.get('copy_statistics', {})\n","}\n","\n","print(f\"\\nMobileNetV3 Configuration Summary:\")\n","print(f\"  Model: {CASME2_MOBILENET_CONFIG['model_name']}\")\n","print(f\"  Input size: {CASME2_MOBILENET_CONFIG['input_size'][0]}x{CASME2_MOBILENET_CONFIG['input_size'][1]} RGB\")\n","print(f\"  Methodology: {CASME2_MOBILENET_CONFIG['methodology']} (raw images)\")\n","print(f\"  Learning rate: {CASME2_MOBILENET_CONFIG['learning_rate']}\")\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","print(f\"  Dataset phase: {CASME2_MOBILENET_CONFIG['dataset_phase']}\")\n","print(f\"  Frame strategy: {CASME2_MOBILENET_CONFIG['frame_strategy']}\")\n","\n","class OptimizedFocalLoss(nn.Module):\n","    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n","        super(OptimizedFocalLoss, self).__init__()\n","\n","        if alpha is not None:\n","            if isinstance(alpha, list):\n","                self.alpha = torch.tensor(alpha, dtype=torch.float32)\n","            else:\n","                self.alpha = alpha\n","\n","            alpha_sum = self.alpha.sum().item()\n","            if abs(alpha_sum - 1.0) > 0.01:\n","                print(f\"Warning: Alpha weights sum to {alpha_sum:.3f}, expected 1.0\")\n","        else:\n","            self.alpha = None\n","\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","\n","        if self.alpha is not None:\n","            if self.alpha.device != targets.device:\n","                self.alpha = self.alpha.to(targets.device)\n","            alpha_t = self.alpha.gather(0, targets)\n","        else:\n","            alpha_t = 1.0\n","\n","        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","\n","class MobileNetCASME2Baseline(nn.Module):\n","    def __init__(self, num_classes, dropout_rate=0.2):\n","        super(MobileNetCASME2Baseline, self).__init__()\n","\n","        self.mobilenet = timm.create_model(\n","            MOBILENET_MODEL_NAME,\n","            pretrained=True,\n","            num_classes=0,\n","            global_pool='avg'\n","        )\n","\n","        for param in self.mobilenet.parameters():\n","            param.requires_grad = True\n","\n","        with torch.no_grad():\n","            test_input = torch.randn(1, 3, 480, 640)\n","            test_output = self.mobilenet(test_input)\n","            self.mobilenet_feature_dim = test_output.shape[1]\n","\n","        print(f\"MobileNetV3-Small feature dimension: {self.mobilenet_feature_dim}\")\n","\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.mobilenet_feature_dim, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(dropout_rate),\n","\n","            nn.Linear(512, 128),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(dropout_rate),\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","        print(f\"MobileNet CASME II: {self.mobilenet_feature_dim} -> 512 -> 128 -> {num_classes}\")\n","\n","    def forward(self, x):\n","        features = self.mobilenet(x)\n","        processed_features = self.classifier_layers(features)\n","        output = self.classifier(processed_features)\n","        return output\n","\n","def create_optimizer_scheduler_casme2(model, config):\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=config['learning_rate'],\n","        weight_decay=config['weight_decay'],\n","        betas=(0.9, 0.999)\n","    )\n","\n","    if config['scheduler_type'] == 'plateau':\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer,\n","            mode=config['scheduler_mode'],\n","            factor=config['scheduler_factor'],\n","            patience=config['scheduler_patience'],\n","            min_lr=config['scheduler_min_lr']\n","        )\n","        print(f\"Scheduler: ReduceLROnPlateau monitoring {config['scheduler_monitor']}\")\n","    else:\n","        scheduler = None\n","\n","    return optimizer, scheduler\n","\n","print(\"\\nSetting up transforms for M1 methodology (raw 640x480 RGB)...\")\n","\n","mobilenet_transform_train = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","mobilenet_transform_val = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","print(\"M1 transforms configured: raw 640x480 RGB with ImageNet normalization\")\n","\n","class CASME2Dataset(Dataset):\n","    def __init__(self, split_metadata, dataset_root, transform=None, split='train'):\n","        self.metadata = split_metadata[split]['samples']\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","        self.split = split\n","\n","        print(f\"Loaded {len(self.metadata)} samples for {split} split\")\n","\n","    def __len__(self):\n","        return len(self.metadata)\n","\n","    def __getitem__(self, idx):\n","        sample = self.metadata[idx]\n","\n","        image_path = os.path.join(self.dataset_root, self.split, sample['image_filename'])\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        emotion = sample['emotion']\n","        label = CLASS_TO_IDX[emotion]\n","\n","        return image, label, sample['sample_id']\n","\n","os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/training_logs\", exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/evaluation_results\", exist_ok=True)\n","\n","TRAIN_PATH = f\"{DATASET_ROOT}/train\"\n","VAL_PATH = f\"{DATASET_ROOT}/val\"\n","TEST_PATH = f\"{DATASET_ROOT}/test\"\n","\n","print(f\"\\nDataset paths:\")\n","print(f\"Train: {TRAIN_PATH}\")\n","print(f\"Validation: {VAL_PATH}\")\n","print(f\"Test: {TEST_PATH}\")\n","\n","print(\"\\nMobileNetV3 CASME II architecture validation...\")\n","\n","try:\n","    test_model = MobileNetCASME2Baseline(num_classes=7, dropout_rate=0.2).to(device)\n","    test_input = torch.randn(1, 3, 480, 640).to(device)\n","    test_output = test_model(test_input)\n","\n","    print(f\"Validation successful: Output shape {test_output.shape}\")\n","    print(f\"Expected output shape: [1, 7] for CASME II 7 classes\")\n","\n","    del test_model, test_input, test_output\n","    torch.cuda.empty_cache()\n","\n","except Exception as e:\n","    print(f\"Validation failed: {e}\")\n","\n","def create_criterion_casme2(weights, use_focal_loss=False, alpha_weights=None, gamma=2.0):\n","    if use_focal_loss:\n","        print(f\"Using Optimized Focal Loss with gamma={gamma}\")\n","        if alpha_weights:\n","            print(f\"Per-class alpha weights: {alpha_weights}\")\n","            print(f\"Alpha sum: {sum(alpha_weights):.3f}\")\n","        return OptimizedFocalLoss(alpha=alpha_weights, gamma=gamma)\n","    else:\n","        print(f\"Using CrossEntropy Loss with optimized class weights\")\n","        print(f\"Class weights: {weights.cpu().numpy()}\")\n","        return nn.CrossEntropyLoss(weight=weights)\n","\n","GLOBAL_CONFIG_CASME2 = {\n","    'device': device,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'num_classes': 7,\n","    'class_weights': class_weights,\n","    'class_names': CASME2_CLASSES,\n","    'class_to_idx': CLASS_TO_IDX,\n","    'transform_train': mobilenet_transform_train,\n","    'transform_val': mobilenet_transform_val,\n","    'mobilenet_config': CASME2_MOBILENET_CONFIG,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'results_root': RESULTS_ROOT,\n","    'train_path': TRAIN_PATH,\n","    'val_path': VAL_PATH,\n","    'test_path': TEST_PATH,\n","    'metadata': casme2_metadata,\n","    'optimizer_scheduler_factory': create_optimizer_scheduler_casme2,\n","    'criterion_factory': create_criterion_casme2\n","}\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"CASME II MOBILENETV3-SMALL M1 MFS BASELINE CONFIGURATION COMPLETE\")\n","print(\"=\" * 60)\n","\n","print(f\"Loss Configuration:\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Function: Optimized Focal Loss\")\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Per-class Alpha: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Function: CrossEntropy with Optimized Weights\")\n","    print(f\"  Class Weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","\n","print(f\"\\nModel Configuration:\")\n","print(f\"  Architecture: MobileNetV3-Small\")\n","print(f\"  Parameters: ~2.5M\")\n","print(f\"  Input Resolution: 640x480 RGB (raw)\")\n","print(f\"  Methodology: M1 (No preprocessing)\")\n","\n","print(f\"\\nDataset Configuration:\")\n","print(f\"  Phase: {CASME2_MOBILENET_CONFIG['dataset_phase']}\")\n","print(f\"  Frame strategy: {CASME2_MOBILENET_CONFIG['frame_strategy']}\")\n","print(f\"  Train augmentation: {CASME2_MOBILENET_CONFIG['train_augmentation']}\")\n","print(f\"  Classes: {len(CASME2_CLASSES)}\")\n","print(f\"  Train samples: {processing_info.get('copy_statistics', {}).get('train', {}).get('total_images', 2061)}\")\n","\n","print(\"\\nNext: Cell 2 - Dataset Loading and Training Pipeline\")"]},{"cell_type":"code","source":["# @title Cell 2: CASME II MobileNetV3-Small Training Pipeline\n","\n","# File: 08_01_MobileNet_CASME2_MFS_Cell2.py\n","# Location: experiments/08_01_MobileNet_CASME2-MFS.ipynb\n","# Purpose: Enhanced training pipeline for CASME II MobileNetV3-Small with temporal window augmentation\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing as mp\n","import shutil\n","import tempfile\n","\n","print(\"CASME II MobileNetV3-Small Training Pipeline\")\n","print(\"=\" * 70)\n","print(f\"Model: MobileNetV3-Small\")\n","print(f\"Methodology: M1 (Raw 640x480 RGB)\")\n","print(f\"Loss Function: Focal Loss\")\n","print(f\"  Gamma: {CASME2_MOBILENET_CONFIG['focal_loss_gamma']}\")\n","print(f\"  Per-class Alpha: {CASME2_MOBILENET_CONFIG['focal_loss_alpha_weights']}\")\n","print(f\"  Alpha Sum: {sum(CASME2_MOBILENET_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","print(f\"Dataset Phase: {CASME2_MOBILENET_CONFIG['dataset_phase']}\")\n","print(f\"Frame Strategy: {CASME2_MOBILENET_CONFIG['frame_strategy']}\")\n","print(f\"Training epochs: {CASME2_MOBILENET_CONFIG['num_epochs']}\")\n","print(f\"Scheduler patience: {CASME2_MOBILENET_CONFIG['scheduler_patience']}\")\n","\n","def normalize_metadata_structure(metadata):\n","    if 'splits' in metadata:\n","        print(\"Detected v2/v3 metadata format (with 'splits' key)\")\n","        return metadata['splits']\n","    elif 'train' in metadata:\n","        print(\"Detected v1 metadata format (direct split keys)\")\n","        return metadata\n","    else:\n","        raise ValueError(\"Unknown metadata format: missing both 'splits' and 'train' keys\")\n","\n","class CASME2DatasetTraining(Dataset):\n","    def __init__(self, split_metadata, dataset_root, transform=None, split='train', use_ram_cache=True):\n","        self.metadata = split_metadata[split]['samples']\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","        self.split = split\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.sample_ids = []\n","        self.cached_images = []\n","\n","        print(f\"Loading CASME II {split} dataset for training...\")\n","\n","        for sample in self.metadata:\n","            image_path = os.path.join(dataset_root, split, sample['image_filename'])\n","            self.images.append(image_path)\n","            self.labels.append(CLASS_TO_IDX[sample['emotion']])\n","            self.sample_ids.append(sample['sample_id'])\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples\")\n","        self._print_distribution()\n","\n","        if self.use_ram_cache:\n","            self._preload_to_ram()\n","\n","    def _print_distribution(self):\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def _preload_to_ram(self):\n","        print(f\"Preloading {len(self.images)} {self.split} images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (640, 480):\n","                    image = image.resize((640, 480), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (640, 480), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=f\"Loading {self.split} to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * 640 * 480 * 3 * 4 / 1e9\n","        print(f\"{self.split.upper()} RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (640, 480):\n","                    image = image.resize((640, 480), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (640, 480), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.sample_ids[idx]\n","\n","def calculate_metrics_safe_robust(outputs, labels, class_names, average='macro'):\n","    try:\n","        if outputs.size(0) != labels.size(0):\n","            raise ValueError(f\"Batch size mismatch: outputs {outputs.size(0)} vs labels {labels.size(0)}\")\n","\n","        if isinstance(outputs, torch.Tensor):\n","            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n","        else:\n","            predictions = np.array(outputs)\n","\n","        if isinstance(labels, torch.Tensor):\n","            labels = labels.cpu().numpy()\n","        else:\n","            labels = np.array(labels)\n","\n","        accuracy = accuracy_score(labels, predictions)\n","        precision, recall, f1, _ = precision_recall_fscore_support(\n","            labels, predictions,\n","            average=average,\n","            zero_division=0,\n","            labels=list(range(len(class_names)))\n","        )\n","\n","        return {\n","            'accuracy': float(accuracy),\n","            'precision': float(precision),\n","            'recall': float(recall),\n","            'f1_score': float(f1)\n","        }\n","    except Exception as e:\n","        print(f\"Warning: Enhanced metrics calculation error: {e}\")\n","        return {\n","            'accuracy': 0.0,\n","            'precision': 0.0,\n","            'recall': 0.0,\n","            'f1_score': 0.0\n","        }\n","\n","def train_epoch(model, dataloader, criterion, optimizer, device, epoch, total_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    all_outputs = []\n","    all_labels = []\n","\n","    progress_bar = tqdm(dataloader, desc=f\"CASME II Training Epoch {epoch+1}/{total_epochs}\")\n","\n","    for batch_idx, (images, labels, sample_ids) in enumerate(progress_bar):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        model_output = model(images)\n","\n","        if isinstance(model_output, (tuple, list)):\n","            outputs = model_output[0]\n","        elif isinstance(model_output, dict):\n","            outputs = model_output.get('logits', model_output.get('prediction', model_output))\n","        else:\n","            outputs = model_output\n","\n","        if outputs.dim() != 2 or outputs.size(1) != 7:\n","            raise ValueError(f\"Invalid CASME II output shape: {outputs.shape}, expected [batch_size, 7]\")\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), CASME2_MOBILENET_CONFIG['gradient_clip'])\n","\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        all_outputs.append(outputs.detach().cpu())\n","        all_labels.append(labels.detach().cpu())\n","\n","        if batch_idx % 5 == 0:\n","            avg_loss = running_loss / (batch_idx + 1)\n","            current_lr = optimizer.param_groups[0]['lr']\n","            progress_bar.set_postfix({\n","                'Loss': f'{avg_loss:.4f}',\n","                'LR': f'{current_lr:.2e}'\n","            })\n","\n","    try:\n","        epoch_outputs = torch.cat(all_outputs, dim=0)\n","        epoch_labels = torch.cat(all_labels, dim=0)\n","        metrics = calculate_metrics_safe_robust(epoch_outputs, epoch_labels, CASME2_CLASSES, average='macro')\n","    except Exception as e:\n","        print(f\"Warning: Training metrics calculation failed: {e}\")\n","        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n","\n","    avg_loss = running_loss / len(dataloader)\n","    return avg_loss, metrics\n","\n","def validate_epoch(model, dataloader, criterion, device, epoch, total_epochs):\n","    model.eval()\n","    running_loss = 0.0\n","    all_outputs = []\n","    all_labels = []\n","    all_sample_ids = []\n","\n","    with torch.no_grad():\n","        progress_bar = tqdm(dataloader, desc=f\"CASME II Validation Epoch {epoch+1}/{total_epochs}\")\n","\n","        for batch_idx, (images, labels, sample_ids) in enumerate(progress_bar):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            model_output = model(images)\n","\n","            if isinstance(model_output, (tuple, list)):\n","                outputs = model_output[0]\n","            elif isinstance(model_output, dict):\n","                outputs = model_output.get('logits', model_output.get('prediction', model_output))\n","            else:\n","                outputs = model_output\n","\n","            loss = criterion(outputs, labels)\n","            running_loss += loss.item()\n","\n","            all_outputs.append(outputs.detach().cpu())\n","            all_labels.append(labels.detach().cpu())\n","            all_sample_ids.extend(sample_ids)\n","\n","            if batch_idx % 3 == 0:\n","                avg_loss = running_loss / (batch_idx + 1)\n","                progress_bar.set_postfix({'Val Loss': f'{avg_loss:.4f}'})\n","\n","    try:\n","        epoch_outputs = torch.cat(all_outputs, dim=0)\n","        epoch_labels = torch.cat(all_labels, dim=0)\n","        metrics = calculate_metrics_safe_robust(epoch_outputs, epoch_labels, CASME2_CLASSES, average='macro')\n","    except Exception as e:\n","        print(f\"Warning: Validation metrics calculation failed: {e}\")\n","        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n","\n","    avg_loss = running_loss / len(dataloader)\n","    return avg_loss, metrics, all_sample_ids\n","\n","def save_checkpoint_robust(model, optimizer, scheduler, epoch, train_metrics, val_metrics,\n","                         checkpoint_dir, best_metrics, config, max_retries=3):\n","    def make_serializable_cpu(obj):\n","        if isinstance(obj, torch.Tensor):\n","            cpu_obj = obj.detach().cpu()\n","            return cpu_obj.item() if cpu_obj.numel() == 1 else cpu_obj.tolist()\n","        elif isinstance(obj, np.ndarray):\n","            return obj.tolist()\n","        elif isinstance(obj, np.integer):\n","            return int(obj)\n","        elif isinstance(obj, np.floating):\n","            return float(obj)\n","        elif isinstance(obj, dict):\n","            return {k: make_serializable_cpu(v) for k, v in obj.items()}\n","        elif isinstance(obj, (list, tuple)):\n","            return [make_serializable_cpu(item) for item in obj]\n","        else:\n","            return obj\n","\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","        'train_metrics': make_serializable_cpu(train_metrics),\n","        'val_metrics': make_serializable_cpu(val_metrics),\n","        'casme2_config': make_serializable_cpu(config),\n","        'best_f1': float(best_metrics['f1']),\n","        'best_loss': float(best_metrics['loss']),\n","        'best_acc': float(best_metrics['accuracy']),\n","        'class_names': CASME2_CLASSES,\n","        'num_classes': 7\n","    }\n","\n","    final_path = f\"{checkpoint_dir}/casme2_mobilenet_mfs_best_f1.pth\"\n","\n","    for attempt in range(max_retries):\n","        try:\n","            temp_fd, temp_path = tempfile.mkstemp(dir=checkpoint_dir, suffix='.pth.tmp')\n","            os.close(temp_fd)\n","\n","            print(f\"Attempt {attempt + 1}: Saving checkpoint to temporary file...\")\n","            torch.save(checkpoint, temp_path)\n","\n","            print(\"Validating checkpoint integrity...\")\n","            validation_checkpoint = torch.load(temp_path, map_location='cpu')\n","\n","            required_keys = ['model_state_dict', 'epoch', 'best_f1', 'num_classes']\n","            for key in required_keys:\n","                if key not in validation_checkpoint:\n","                    raise ValueError(f\"Checkpoint validation failed: missing key '{key}'\")\n","\n","            if validation_checkpoint['epoch'] != epoch:\n","                raise ValueError(f\"Checkpoint epoch mismatch: saved {epoch}, loaded {validation_checkpoint['epoch']}\")\n","\n","            print(\"Checkpoint validation passed\")\n","\n","            print(f\"Moving validated checkpoint to final location...\")\n","            shutil.move(temp_path, final_path)\n","\n","            print(f\"Checkpoint saved and validated successfully: {os.path.basename(final_path)}\")\n","            print(f\"  Epoch: {epoch + 1}\")\n","            print(f\"  Val F1: {best_metrics['f1']:.4f}\")\n","            print(f\"  Val Loss: {best_metrics['loss']:.4f}\")\n","            print(f\"  Val Acc: {best_metrics['accuracy']:.4f}\")\n","\n","            return final_path\n","\n","        except Exception as e:\n","            print(f\"Checkpoint save attempt {attempt + 1}/{max_retries} failed: {e}\")\n","\n","            if os.path.exists(temp_path):\n","                try:\n","                    os.remove(temp_path)\n","                except:\n","                    pass\n","\n","            if attempt < max_retries - 1:\n","                wait_time = 2 ** attempt\n","                print(f\"Retrying in {wait_time} seconds...\")\n","                time.sleep(wait_time)\n","            else:\n","                print(f\"All {max_retries} checkpoint save attempts failed\")\n","                return None\n","\n","    return None\n","\n","def safe_json_serialize(obj):\n","    if isinstance(obj, torch.Tensor):\n","        return obj.cpu().item() if obj.numel() == 1 else obj.cpu().numpy().tolist()\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    elif isinstance(obj, (np.integer, np.int32, np.int64)):\n","        return int(obj)\n","    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n","        return float(obj)\n","    elif isinstance(obj, dict):\n","        return {k: safe_json_serialize(v) for k, v in obj.items()}\n","    elif isinstance(obj, (list, tuple)):\n","        return [safe_json_serialize(item) for item in obj]\n","    elif hasattr(obj, '__dict__'):\n","        return safe_json_serialize(obj.__dict__)\n","    else:\n","        try:\n","            return float(obj) if isinstance(obj, (int, float)) else str(obj)\n","        except:\n","            return str(obj)\n","\n","print(\"\\nCreating CASME II MobileNetV3-Small training datasets...\")\n","\n","normalized_metadata = normalize_metadata_structure(GLOBAL_CONFIG_CASME2['metadata'])\n","\n","train_dataset = CASME2DatasetTraining(\n","    split_metadata=normalized_metadata,\n","    dataset_root=GLOBAL_CONFIG_CASME2['train_path'].replace('/train', ''),\n","    transform=GLOBAL_CONFIG_CASME2['transform_train'],\n","    split='train',\n","    use_ram_cache=True\n",")\n","\n","val_dataset = CASME2DatasetTraining(\n","    split_metadata=normalized_metadata,\n","    dataset_root=GLOBAL_CONFIG_CASME2['val_path'].replace('/val', ''),\n","    transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","    split='val',\n","    use_ram_cache=True\n",")\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=CASME2_MOBILENET_CONFIG['batch_size'],\n","    shuffle=True,\n","    num_workers=CASME2_MOBILENET_CONFIG['num_workers'],\n","    pin_memory=True,\n","    prefetch_factor=2\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=CASME2_MOBILENET_CONFIG['batch_size'],\n","    shuffle=False,\n","    num_workers=CASME2_MOBILENET_CONFIG['num_workers'],\n","    pin_memory=True,\n","    prefetch_factor=2\n",")\n","\n","print(f\"Training batches: {len(train_loader)} (samples: {len(train_dataset)})\")\n","print(f\"Validation batches: {len(val_loader)} (samples: {len(val_dataset)})\")\n","\n","print(\"\\nInitializing CASME II MobileNetV3-Small model...\")\n","model = MobileNetCASME2Baseline(\n","    num_classes=GLOBAL_CONFIG_CASME2['num_classes'],\n","    dropout_rate=CASME2_MOBILENET_CONFIG['dropout_rate']\n",").to(GLOBAL_CONFIG_CASME2['device'])\n","\n","if CASME2_MOBILENET_CONFIG['use_focal_loss']:\n","    criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","        weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","        use_focal_loss=True,\n","        alpha_weights=CASME2_MOBILENET_CONFIG['focal_loss_alpha_weights'],\n","        gamma=CASME2_MOBILENET_CONFIG['focal_loss_gamma']\n","    )\n","else:\n","    criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","        weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","        use_focal_loss=False,\n","        alpha_weights=None,\n","        gamma=2.0\n","    )\n","\n","optimizer, scheduler = GLOBAL_CONFIG_CASME2['optimizer_scheduler_factory'](\n","    model, CASME2_MOBILENET_CONFIG\n",")\n","\n","print(f\"Optimizer: AdamW (LR={CASME2_MOBILENET_CONFIG['learning_rate']})\")\n","print(f\"Scheduler: ReduceLROnPlateau (patience={CASME2_MOBILENET_CONFIG['scheduler_patience']})\")\n","print(f\"Criterion: Optimized Focal Loss\")\n","\n","training_history = {\n","    'train_loss': [],\n","    'val_loss': [],\n","    'train_f1': [],\n","    'val_f1': [],\n","    'train_acc': [],\n","    'val_acc': [],\n","    'learning_rate': [],\n","    'epoch_time': []\n","}\n","\n","best_metrics = {\n","    'f1': 0.0,\n","    'loss': float('inf'),\n","    'accuracy': 0.0,\n","    'epoch': 0\n","}\n","\n","print(\"\\nStarting CASME II MobileNetV3-Small training...\")\n","print(f\"Training configuration: {CASME2_MOBILENET_CONFIG['num_epochs']} epochs\")\n","print(f\"Input resolution: 640x480 RGB (M1 methodology)\")\n","print(\"=\" * 70)\n","\n","start_time = time.time()\n","\n","for epoch in range(CASME2_MOBILENET_CONFIG['num_epochs']):\n","    epoch_start_time = time.time()\n","    print(f\"\\nEpoch {epoch+1}/{CASME2_MOBILENET_CONFIG['num_epochs']}\")\n","\n","    train_loss, train_metrics = train_epoch(\n","        model, train_loader, criterion, optimizer,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_MOBILENET_CONFIG['num_epochs']\n","    )\n","\n","    val_loss, val_metrics, val_sample_ids = validate_epoch(\n","        model, val_loader, criterion,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_MOBILENET_CONFIG['num_epochs']\n","    )\n","\n","    if scheduler:\n","        scheduler.step(val_metrics['f1_score'])\n","\n","    epoch_time = time.time() - epoch_start_time\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    training_history['train_loss'].append(float(train_loss))\n","    training_history['val_loss'].append(float(val_loss))\n","    training_history['train_f1'].append(float(train_metrics['f1_score']))\n","    training_history['val_f1'].append(float(val_metrics['f1_score']))\n","    training_history['train_acc'].append(float(train_metrics['accuracy']))\n","    training_history['val_acc'].append(float(val_metrics['accuracy']))\n","    training_history['learning_rate'].append(float(current_lr))\n","    training_history['epoch_time'].append(float(epoch_time))\n","\n","    print(f\"Train - Loss: {train_loss:.4f}, F1: {train_metrics['f1_score']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n","    print(f\"Val   - Loss: {val_loss:.4f}, F1: {val_metrics['f1_score']:.4f}, Acc: {val_metrics['accuracy']:.4f}\")\n","    print(f\"Time  - Epoch: {epoch_time:.1f}s, LR: {current_lr:.2e}\")\n","\n","    save_model = False\n","    improvement_reason = \"\"\n","\n","    if val_metrics['f1_score'] > best_metrics['f1']:\n","        save_model = True\n","        improvement_reason = \"Higher F1\"\n","    elif val_metrics['f1_score'] == best_metrics['f1']:\n","        if val_loss < best_metrics['loss']:\n","            save_model = True\n","            improvement_reason = \"Same F1, Lower Loss\"\n","        elif val_loss == best_metrics['loss'] and val_metrics['accuracy'] > best_metrics['accuracy']:\n","            save_model = True\n","            improvement_reason = \"Same F1&Loss, Higher Accuracy\"\n","\n","    if save_model:\n","        best_metrics['f1'] = val_metrics['f1_score']\n","        best_metrics['loss'] = val_loss\n","        best_metrics['accuracy'] = val_metrics['accuracy']\n","        best_metrics['epoch'] = epoch + 1\n","\n","        best_model_path = save_checkpoint_robust(\n","            model, optimizer, scheduler, epoch,\n","            train_metrics, val_metrics, GLOBAL_CONFIG_CASME2['checkpoint_root'],\n","            best_metrics, CASME2_MOBILENET_CONFIG\n","        )\n","\n","        if best_model_path:\n","            print(f\"New best model: {improvement_reason} - F1: {best_metrics['f1']:.4f}\")\n","        else:\n","            print(f\"Warning: Failed to save checkpoint despite improvement\")\n","\n","    elapsed_time = time.time() - start_time\n","    estimated_total = (elapsed_time / (epoch + 1)) * CASME2_MOBILENET_CONFIG['num_epochs']\n","    remaining_time = estimated_total - elapsed_time\n","    progress_pct = ((epoch + 1) / CASME2_MOBILENET_CONFIG['num_epochs']) * 100\n","\n","    print(f\"Progress: {progress_pct:.1f}% | Best F1: {best_metrics['f1']:.4f} | ETA: {remaining_time/60:.1f}min\")\n","\n","total_time = time.time() - start_time\n","actual_epochs = CASME2_MOBILENET_CONFIG['num_epochs']\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II MOBILENETV3-SMALL BASELINE TRAINING COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Training time: {total_time/60:.1f} minutes\")\n","print(f\"Epochs completed: {actual_epochs}\")\n","print(f\"Best validation F1: {best_metrics['f1']:.4f} (epoch {best_metrics['epoch']})\")\n","print(f\"Final train F1: {training_history['train_f1'][-1]:.4f}\")\n","print(f\"Final validation F1: {training_history['val_f1'][-1]:.4f}\")\n","\n","results_dir = GLOBAL_CONFIG_CASME2['results_root']\n","os.makedirs(f\"{results_dir}/training_logs\", exist_ok=True)\n","\n","training_history_path = f\"{results_dir}/training_logs/casme2_mobilenet_mfs_training_history.json\"\n","\n","print(\"\\nExporting enhanced training documentation...\")\n","\n","try:\n","    training_summary = {\n","        'experiment_type': 'CASME2_MobileNetV3Small_MFS_Baseline',\n","        'experiment_configuration': {\n","            'model_architecture': 'MobileNetV3-Small',\n","            'model_parameters': '2.5M',\n","            'dataset_phase': CASME2_MOBILENET_CONFIG['dataset_phase'],\n","            'methodology': CASME2_MOBILENET_CONFIG['methodology'],\n","            'preprocessing': CASME2_MOBILENET_CONFIG['preprocessing'],\n","            'input_resolution': '640x480 RGB',\n","            'frame_strategy': CASME2_MOBILENET_CONFIG['frame_strategy'],\n","            'train_augmentation': CASME2_MOBILENET_CONFIG['train_augmentation'],\n","            'frame_types': CASME2_MOBILENET_CONFIG['frame_types'],\n","            'loss_function': 'Optimized Focal Loss',\n","            'focal_loss_gamma': CASME2_MOBILENET_CONFIG['focal_loss_gamma'],\n","            'focal_loss_alpha_weights': CASME2_MOBILENET_CONFIG['focal_loss_alpha_weights'],\n","            'model_name': CASME2_MOBILENET_CONFIG['model_name']\n","        },\n","        'training_history': safe_json_serialize(training_history),\n","        'best_val_f1': float(best_metrics['f1']),\n","        'best_val_loss': float(best_metrics['loss']),\n","        'best_val_accuracy': float(best_metrics['accuracy']),\n","        'best_epoch': int(best_metrics['epoch']),\n","        'total_epochs': int(actual_epochs),\n","        'total_time_minutes': float(total_time / 60),\n","        'average_epoch_time_seconds': float(np.mean(training_history['epoch_time'])),\n","        'config': safe_json_serialize(CASME2_MOBILENET_CONFIG),\n","        'final_train_f1': float(training_history['train_f1'][-1]),\n","        'final_val_f1': float(training_history['val_f1'][-1]),\n","        'model_checkpoint': 'casme2_mobilenet_mfs_best_f1.pth',\n","        'dataset_info': {\n","            'name': 'CASME_II',\n","            'phase': CASME2_MOBILENET_CONFIG['dataset_phase'],\n","            'methodology': CASME2_MOBILENET_CONFIG['methodology'],\n","            'input_resolution': '640x480 RGB',\n","            'frame_strategy': CASME2_MOBILENET_CONFIG['frame_strategy'],\n","            'train_augmentation': CASME2_MOBILENET_CONFIG['train_augmentation'],\n","            'frame_types': CASME2_MOBILENET_CONFIG['frame_types'],\n","            'train_samples': len(train_dataset),\n","            'val_samples': len(val_dataset),\n","            'num_classes': 7,\n","            'class_names': CASME2_CLASSES\n","        },\n","        'architecture_info': {\n","            'model_type': 'MobileNetCASME2Baseline',\n","            'backbone': CASME2_MOBILENET_CONFIG['model_name'],\n","            'input_size': '640x480 RGB',\n","            'classification_head': '576->512->128->7'\n","        },\n","        'enhanced_features': {\n","            'hardened_checkpoint_system': True,\n","            'atomic_checkpoint_save': True,\n","            'checkpoint_validation': True,\n","            'model_output_validation': True,\n","            'enhanced_error_handling': True,\n","            'multi_criteria_checkpoint_logic': True,\n","            'memory_optimized_training': True,\n","            'retry_with_backoff': True,\n","            'multi_frame_temporal_windows': True\n","        }\n","    }\n","\n","    with open(training_history_path, 'w') as f:\n","        json.dump(training_summary, f, indent=2)\n","\n","    print(f\"Enhanced training documentation saved successfully: {training_history_path}\")\n","    print(f\"Model: {training_summary['experiment_configuration']['model_architecture']}\")\n","    print(f\"Methodology: {training_summary['experiment_configuration']['methodology']}\")\n","    print(f\"Input resolution: {training_summary['experiment_configuration']['input_resolution']}\")\n","    print(f\"Loss function: {training_summary['experiment_configuration']['loss_function']}\")\n","\n","except Exception as e:\n","    print(f\"Warning: Could not save training documentation: {e}\")\n","    print(\"Training completed successfully, but documentation export failed\")\n","\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\nNext: Cell 3 - CASME II MobileNetV3-Small Evaluation\")\n","print(\"Enhanced training pipeline with multi-frame temporal windows completed successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"cellView":"form","id":"EpQzR9XUkSgb","executionInfo":{"status":"ok","timestamp":1762948702635,"user_tz":-420,"elapsed":574265,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"5a9667b3-4b80-417b-e98d-cb4f5b659386"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II MobileNetV3-Small Training Pipeline\n","======================================================================\n","Model: MobileNetV3-Small\n","Methodology: M1 (Raw 640x480 RGB)\n","Loss Function: Focal Loss\n","  Gamma: 2.5\n","  Per-class Alpha: [0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285]\n","  Alpha Sum: 1.000\n","Dataset Phase: v3\n","Frame Strategy: multi_frame_sampling\n","Training epochs: 50\n","Scheduler patience: 5\n","\n","Creating CASME II MobileNetV3-Small training datasets...\n","Detected v2/v3 metadata format (with 'splits' key)\n","Loading CASME II train dataset for training...\n","Loaded 2613 CASME II train samples\n","  others: 1027 samples (39.3%)\n","  disgust: 650 samples (24.9%)\n","  happiness: 325 samples (12.4%)\n","  repression: 273 samples (10.4%)\n","  surprise: 260 samples (10.0%)\n","  sadness: 65 samples (2.5%)\n","  fear: 13 samples (0.5%)\n","Preloading 2613 train images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading train to RAM: 100%|██████████| 2613/2613 [00:29<00:00, 87.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TRAIN RAM caching completed: 2613/2613 images, ~9.63GB\n","Loading CASME II val dataset for training...\n","Loaded 78 CASME II val samples\n","  others: 30 samples (38.5%)\n","  disgust: 18 samples (23.1%)\n","  happiness: 9 samples (11.5%)\n","  repression: 9 samples (11.5%)\n","  surprise: 6 samples (7.7%)\n","  sadness: 3 samples (3.8%)\n","  fear: 3 samples (3.8%)\n","Preloading 78 val images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading val to RAM: 100%|██████████| 78/78 [00:01<00:00, 73.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["VAL RAM caching completed: 78/78 images, ~0.29GB\n","Training batches: 164 (samples: 2613)\n","Validation batches: 5 (samples: 78)\n","\n","Initializing CASME II MobileNetV3-Small model...\n","MobileNetV3-Small feature dimension: 1024\n","MobileNet CASME II: 1024 -> 512 -> 128 -> 7\n","Using Optimized Focal Loss with gamma=2.5\n","Per-class alpha weights: [0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285]\n","Alpha sum: 1.000\n","Scheduler: ReduceLROnPlateau monitoring val_f1_macro\n","Optimizer: AdamW (LR=5e-05)\n","Scheduler: ReduceLROnPlateau (patience=5)\n","Criterion: Optimized Focal Loss\n","\n","Starting CASME II MobileNetV3-Small training...\n","Training configuration: 50 epochs\n","Input resolution: 640x480 RGB (M1 methodology)\n","======================================================================\n","\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 1/50: 100%|██████████| 164/164 [00:37<00:00,  4.34it/s, Loss=0.1406, LR=5.00e-05]\n","CASME II Validation Epoch 1/50: 100%|██████████| 5/5 [00:14<00:00,  2.84s/it, Val Loss=0.1610]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.1404, F1: 0.3477, Acc: 0.4259\n","Val   - Loss: 0.1741, F1: 0.1649, Acc: 0.2949\n","Time  - Epoch: 52.0s, LR: 5.00e-05\n","Attempt 1: Saving checkpoint to temporary file...\n","Validating checkpoint integrity...\n","Checkpoint validation passed\n","Moving validated checkpoint to final location...\n","Checkpoint saved and validated successfully: casme2_mobilenet_mfs_best_f1.pth\n","  Epoch: 1\n","  Val F1: 0.1649\n","  Val Loss: 0.1741\n","  Val Acc: 0.2949\n","New best model: Higher F1 - F1: 0.1649\n","Progress: 2.0% | Best F1: 0.1649 | ETA: 42.7min\n","\n","Epoch 2/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 2/50: 100%|██████████| 164/164 [00:09<00:00, 17.95it/s, Loss=0.0645, LR=5.00e-05]\n","CASME II Validation Epoch 2/50: 100%|██████████| 5/5 [00:00<00:00,  6.27it/s, Val Loss=0.1316]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0640, F1: 0.6330, Acc: 0.7298\n","Val   - Loss: 0.1453, F1: 0.2466, Acc: 0.3974\n","Time  - Epoch: 9.9s, LR: 5.00e-05\n","Attempt 1: Saving checkpoint to temporary file...\n","Validating checkpoint integrity...\n","Checkpoint validation passed\n","Moving validated checkpoint to final location...\n","Checkpoint saved and validated successfully: casme2_mobilenet_mfs_best_f1.pth\n","  Epoch: 2\n","  Val F1: 0.2466\n","  Val Loss: 0.1453\n","  Val Acc: 0.3974\n","New best model: Higher F1 - F1: 0.2466\n","Progress: 4.0% | Best F1: 0.2466 | ETA: 25.0min\n","\n","Epoch 3/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 3/50: 100%|██████████| 164/164 [00:09<00:00, 17.67it/s, Loss=0.0253, LR=5.00e-05]\n","CASME II Validation Epoch 3/50: 100%|██████████| 5/5 [00:00<00:00,  6.23it/s, Val Loss=0.1705]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0252, F1: 0.8007, Acc: 0.9112\n","Val   - Loss: 0.1951, F1: 0.1744, Acc: 0.2949\n","Time  - Epoch: 10.1s, LR: 5.00e-05\n","Progress: 6.0% | Best F1: 0.2466 | ETA: 18.9min\n","\n","Epoch 4/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 4/50: 100%|██████████| 164/164 [00:09<00:00, 17.79it/s, Loss=0.0103, LR=5.00e-05]\n","CASME II Validation Epoch 4/50: 100%|██████████| 5/5 [00:00<00:00,  6.20it/s, Val Loss=0.1643]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0104, F1: 0.9007, Acc: 0.9709\n","Val   - Loss: 0.1872, F1: 0.1944, Acc: 0.3462\n","Time  - Epoch: 10.0s, LR: 5.00e-05\n","Progress: 8.0% | Best F1: 0.2466 | ETA: 15.8min\n","\n","Epoch 5/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 5/50: 100%|██████████| 164/164 [00:09<00:00, 17.98it/s, Loss=0.0060, LR=5.00e-05]\n","CASME II Validation Epoch 5/50: 100%|██████████| 5/5 [00:00<00:00,  6.53it/s, Val Loss=0.1365]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0064, F1: 0.9765, Acc: 0.9820\n","Val   - Loss: 0.1817, F1: 0.1412, Acc: 0.3718\n","Time  - Epoch: 9.9s, LR: 5.00e-05\n","Progress: 10.0% | Best F1: 0.2466 | ETA: 13.9min\n","\n","Epoch 6/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 6/50: 100%|██████████| 164/164 [00:09<00:00, 17.44it/s, Loss=0.0031, LR=5.00e-05]\n","CASME II Validation Epoch 6/50: 100%|██████████| 5/5 [00:00<00:00,  6.55it/s, Val Loss=0.1676]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0032, F1: 0.9902, Acc: 0.9904\n","Val   - Loss: 0.1942, F1: 0.2892, Acc: 0.4231\n","Time  - Epoch: 10.2s, LR: 5.00e-05\n","Attempt 1: Saving checkpoint to temporary file...\n","Validating checkpoint integrity...\n","Checkpoint validation passed\n","Moving validated checkpoint to final location...\n","Checkpoint saved and validated successfully: casme2_mobilenet_mfs_best_f1.pth\n","  Epoch: 6\n","  Val F1: 0.2892\n","  Val Loss: 0.1942\n","  Val Acc: 0.4231\n","New best model: Higher F1 - F1: 0.2892\n","Progress: 12.0% | Best F1: 0.2892 | ETA: 12.6min\n","\n","Epoch 7/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 7/50: 100%|██████████| 164/164 [00:09<00:00, 17.80it/s, Loss=0.0020, LR=5.00e-05]\n","CASME II Validation Epoch 7/50: 100%|██████████| 5/5 [00:00<00:00,  6.49it/s, Val Loss=0.1507]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0020, F1: 0.9963, Acc: 0.9958\n","Val   - Loss: 0.1977, F1: 0.2694, Acc: 0.4487\n","Time  - Epoch: 10.0s, LR: 5.00e-05\n","Progress: 14.0% | Best F1: 0.2892 | ETA: 11.6min\n","\n","Epoch 8/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 8/50: 100%|██████████| 164/164 [00:09<00:00, 17.90it/s, Loss=0.0018, LR=5.00e-05]\n","CASME II Validation Epoch 8/50: 100%|██████████| 5/5 [00:00<00:00,  6.35it/s, Val Loss=0.1892]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0019, F1: 0.9947, Acc: 0.9954\n","Val   - Loss: 0.2298, F1: 0.1589, Acc: 0.3718\n","Time  - Epoch: 10.0s, LR: 5.00e-05\n","Progress: 16.0% | Best F1: 0.2892 | ETA: 10.8min\n","\n","Epoch 9/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 9/50: 100%|██████████| 164/164 [00:09<00:00, 17.77it/s, Loss=0.0015, LR=5.00e-05]\n","CASME II Validation Epoch 9/50: 100%|██████████| 5/5 [00:00<00:00,  6.25it/s, Val Loss=0.1870]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0015, F1: 0.9943, Acc: 0.9943\n","Val   - Loss: 0.2300, F1: 0.2199, Acc: 0.3718\n","Time  - Epoch: 10.0s, LR: 5.00e-05\n","Progress: 18.0% | Best F1: 0.2892 | ETA: 10.1min\n","\n","Epoch 10/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 10/50: 100%|██████████| 164/164 [00:09<00:00, 17.47it/s, Loss=0.0013, LR=5.00e-05]\n","CASME II Validation Epoch 10/50: 100%|██████████| 5/5 [00:00<00:00,  6.79it/s, Val Loss=0.1980]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0013, F1: 0.9972, Acc: 0.9962\n","Val   - Loss: 0.2568, F1: 0.2022, Acc: 0.3462\n","Time  - Epoch: 10.1s, LR: 5.00e-05\n","Progress: 20.0% | Best F1: 0.2892 | ETA: 9.5min\n","\n","Epoch 11/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 11/50: 100%|██████████| 164/164 [00:09<00:00, 17.73it/s, Loss=0.0007, LR=5.00e-05]\n","CASME II Validation Epoch 11/50: 100%|██████████| 5/5 [00:00<00:00,  6.33it/s, Val Loss=0.1999]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0012, F1: 0.9952, Acc: 0.9958\n","Val   - Loss: 0.2441, F1: 0.1664, Acc: 0.3462\n","Time  - Epoch: 10.1s, LR: 5.00e-05\n","Progress: 22.0% | Best F1: 0.2892 | ETA: 9.0min\n","\n","Epoch 12/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 12/50: 100%|██████████| 164/164 [00:09<00:00, 17.48it/s, Loss=0.0005, LR=5.00e-05]\n","CASME II Validation Epoch 12/50: 100%|██████████| 5/5 [00:00<00:00,  6.18it/s, Val Loss=0.1476]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 0.9990, Acc: 0.9985\n","Val   - Loss: 0.2210, F1: 0.2338, Acc: 0.4103\n","Time  - Epoch: 10.2s, LR: 2.50e-05\n","Progress: 24.0% | Best F1: 0.2892 | ETA: 8.6min\n","\n","Epoch 13/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 13/50: 100%|██████████| 164/164 [00:09<00:00, 17.78it/s, Loss=0.0003, LR=2.50e-05]\n","CASME II Validation Epoch 13/50: 100%|██████████| 5/5 [00:00<00:00,  6.48it/s, Val Loss=0.1832]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 0.9995, Acc: 0.9996\n","Val   - Loss: 0.2419, F1: 0.2017, Acc: 0.3974\n","Time  - Epoch: 10.0s, LR: 2.50e-05\n","Progress: 26.0% | Best F1: 0.2892 | ETA: 8.2min\n","\n","Epoch 14/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 14/50: 100%|██████████| 164/164 [00:09<00:00, 18.01it/s, Loss=0.0004, LR=2.50e-05]\n","CASME II Validation Epoch 14/50: 100%|██████████| 5/5 [00:00<00:00,  6.59it/s, Val Loss=0.1794]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 0.9993, Acc: 0.9992\n","Val   - Loss: 0.2438, F1: 0.1813, Acc: 0.3718\n","Time  - Epoch: 9.9s, LR: 2.50e-05\n","Progress: 28.0% | Best F1: 0.2892 | ETA: 7.9min\n","\n","Epoch 15/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 15/50: 100%|██████████| 164/164 [00:09<00:00, 17.92it/s, Loss=0.0002, LR=2.50e-05]\n","CASME II Validation Epoch 15/50: 100%|██████████| 5/5 [00:00<00:00,  6.33it/s, Val Loss=0.1974]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2577, F1: 0.1866, Acc: 0.3718\n","Time  - Epoch: 10.0s, LR: 2.50e-05\n","Progress: 30.0% | Best F1: 0.2892 | ETA: 7.5min\n","\n","Epoch 16/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 16/50: 100%|██████████| 164/164 [00:09<00:00, 17.94it/s, Loss=0.0002, LR=2.50e-05]\n","CASME II Validation Epoch 16/50: 100%|██████████| 5/5 [00:00<00:00,  6.33it/s, Val Loss=0.1983]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2519, F1: 0.1992, Acc: 0.3718\n","Time  - Epoch: 9.9s, LR: 2.50e-05\n","Progress: 32.0% | Best F1: 0.2892 | ETA: 7.2min\n","\n","Epoch 17/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 17/50: 100%|██████████| 164/164 [00:09<00:00, 18.15it/s, Loss=0.0002, LR=2.50e-05]\n","CASME II Validation Epoch 17/50: 100%|██████████| 5/5 [00:00<00:00,  6.65it/s, Val Loss=0.1875]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2564, F1: 0.1774, Acc: 0.3590\n","Time  - Epoch: 9.8s, LR: 2.50e-05\n","Progress: 34.0% | Best F1: 0.2892 | ETA: 6.9min\n","\n","Epoch 18/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 18/50: 100%|██████████| 164/164 [00:09<00:00, 17.72it/s, Loss=0.0002, LR=2.50e-05]\n","CASME II Validation Epoch 18/50: 100%|██████████| 5/5 [00:00<00:00,  6.52it/s, Val Loss=0.1885]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 0.9998, Acc: 0.9996\n","Val   - Loss: 0.2602, F1: 0.1794, Acc: 0.3590\n","Time  - Epoch: 10.0s, LR: 1.25e-05\n","Progress: 36.0% | Best F1: 0.2892 | ETA: 6.6min\n","\n","Epoch 19/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 19/50: 100%|██████████| 164/164 [00:09<00:00, 17.96it/s, Loss=0.0005, LR=1.25e-05]\n","CASME II Validation Epoch 19/50: 100%|██████████| 5/5 [00:00<00:00,  6.71it/s, Val Loss=0.2019]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0004, F1: 0.9994, Acc: 0.9989\n","Val   - Loss: 0.2656, F1: 0.1894, Acc: 0.3846\n","Time  - Epoch: 9.9s, LR: 1.25e-05\n","Progress: 38.0% | Best F1: 0.2892 | ETA: 6.3min\n","\n","Epoch 20/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 20/50: 100%|██████████| 164/164 [00:09<00:00, 17.54it/s, Loss=0.0001, LR=1.25e-05]\n","CASME II Validation Epoch 20/50: 100%|██████████| 5/5 [00:00<00:00,  6.47it/s, Val Loss=0.2042]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2731, F1: 0.1765, Acc: 0.3590\n","Time  - Epoch: 10.1s, LR: 1.25e-05\n","Progress: 40.0% | Best F1: 0.2892 | ETA: 6.1min\n","\n","Epoch 21/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 21/50: 100%|██████████| 164/164 [00:09<00:00, 17.75it/s, Loss=0.0002, LR=1.25e-05]\n","CASME II Validation Epoch 21/50: 100%|██████████| 5/5 [00:00<00:00,  6.49it/s, Val Loss=0.1958]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2643, F1: 0.1902, Acc: 0.3718\n","Time  - Epoch: 10.0s, LR: 1.25e-05\n","Progress: 42.0% | Best F1: 0.2892 | ETA: 5.8min\n","\n","Epoch 22/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 22/50: 100%|██████████| 164/164 [00:09<00:00, 17.64it/s, Loss=0.0001, LR=1.25e-05]\n","CASME II Validation Epoch 22/50: 100%|██████████| 5/5 [00:00<00:00,  6.29it/s, Val Loss=0.2296]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0005, F1: 0.9994, Acc: 0.9992\n","Val   - Loss: 0.2933, F1: 0.1211, Acc: 0.3205\n","Time  - Epoch: 10.1s, LR: 1.25e-05\n","Progress: 44.0% | Best F1: 0.2892 | ETA: 5.6min\n","\n","Epoch 23/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 23/50: 100%|██████████| 164/164 [00:09<00:00, 17.88it/s, Loss=0.0001, LR=1.25e-05]\n","CASME II Validation Epoch 23/50: 100%|██████████| 5/5 [00:00<00:00,  6.55it/s, Val Loss=0.2065]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2748, F1: 0.1605, Acc: 0.3718\n","Time  - Epoch: 10.0s, LR: 1.25e-05\n","Progress: 46.0% | Best F1: 0.2892 | ETA: 5.3min\n","\n","Epoch 24/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 24/50: 100%|██████████| 164/164 [00:09<00:00, 17.96it/s, Loss=0.0001, LR=1.25e-05]\n","CASME II Validation Epoch 24/50: 100%|██████████| 5/5 [00:00<00:00,  6.38it/s, Val Loss=0.1973]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2643, F1: 0.1707, Acc: 0.3333\n","Time  - Epoch: 9.9s, LR: 6.25e-06\n","Progress: 48.0% | Best F1: 0.2892 | ETA: 5.1min\n","\n","Epoch 25/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 25/50: 100%|██████████| 164/164 [00:09<00:00, 17.89it/s, Loss=0.0001, LR=6.25e-06]\n","CASME II Validation Epoch 25/50: 100%|██████████| 5/5 [00:00<00:00,  6.41it/s, Val Loss=0.2077]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2760, F1: 0.1783, Acc: 0.3462\n","Time  - Epoch: 10.0s, LR: 6.25e-06\n","Progress: 50.0% | Best F1: 0.2892 | ETA: 4.9min\n","\n","Epoch 26/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 26/50: 100%|██████████| 164/164 [00:09<00:00, 17.66it/s, Loss=0.0001, LR=6.25e-06]\n","CASME II Validation Epoch 26/50: 100%|██████████| 5/5 [00:00<00:00,  6.34it/s, Val Loss=0.2015]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 0.9988, Acc: 0.9996\n","Val   - Loss: 0.2649, F1: 0.1963, Acc: 0.3974\n","Time  - Epoch: 10.1s, LR: 6.25e-06\n","Progress: 52.0% | Best F1: 0.2892 | ETA: 4.7min\n","\n","Epoch 27/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 27/50: 100%|██████████| 164/164 [00:09<00:00, 17.80it/s, Loss=0.0003, LR=6.25e-06]\n","CASME II Validation Epoch 27/50: 100%|██████████| 5/5 [00:00<00:00,  6.06it/s, Val Loss=0.2114]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 0.9985, Acc: 0.9989\n","Val   - Loss: 0.2800, F1: 0.1761, Acc: 0.3590\n","Time  - Epoch: 10.1s, LR: 6.25e-06\n","Progress: 54.0% | Best F1: 0.2892 | ETA: 4.4min\n","\n","Epoch 28/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 28/50: 100%|██████████| 164/164 [00:09<00:00, 17.61it/s, Loss=0.0001, LR=6.25e-06]\n","CASME II Validation Epoch 28/50: 100%|██████████| 5/5 [00:00<00:00,  6.34it/s, Val Loss=0.2180]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2795, F1: 0.1824, Acc: 0.3590\n","Time  - Epoch: 10.1s, LR: 6.25e-06\n","Progress: 56.0% | Best F1: 0.2892 | ETA: 4.2min\n","\n","Epoch 29/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 29/50: 100%|██████████| 164/164 [00:09<00:00, 17.59it/s, Loss=0.0001, LR=6.25e-06]\n","CASME II Validation Epoch 29/50: 100%|██████████| 5/5 [00:00<00:00,  6.27it/s, Val Loss=0.1985]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2662, F1: 0.1904, Acc: 0.3846\n","Time  - Epoch: 10.1s, LR: 6.25e-06\n","Progress: 58.0% | Best F1: 0.2892 | ETA: 4.0min\n","\n","Epoch 30/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 30/50: 100%|██████████| 164/164 [00:09<00:00, 17.77it/s, Loss=0.0001, LR=6.25e-06]\n","CASME II Validation Epoch 30/50: 100%|██████████| 5/5 [00:00<00:00,  6.39it/s, Val Loss=0.2065]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2660, F1: 0.1956, Acc: 0.3718\n","Time  - Epoch: 10.0s, LR: 3.13e-06\n","Progress: 60.0% | Best F1: 0.2892 | ETA: 3.8min\n","\n","Epoch 31/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 31/50: 100%|██████████| 164/164 [00:09<00:00, 17.77it/s, Loss=0.0001, LR=3.13e-06]\n","CASME II Validation Epoch 31/50: 100%|██████████| 5/5 [00:00<00:00,  6.41it/s, Val Loss=0.1992]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 0.9998, Acc: 0.9996\n","Val   - Loss: 0.2664, F1: 0.1839, Acc: 0.3718\n","Time  - Epoch: 10.0s, LR: 3.13e-06\n","Progress: 62.0% | Best F1: 0.2892 | ETA: 3.6min\n","\n","Epoch 32/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 32/50: 100%|██████████| 164/164 [00:09<00:00, 18.04it/s, Loss=0.0001, LR=3.13e-06]\n","CASME II Validation Epoch 32/50: 100%|██████████| 5/5 [00:00<00:00,  6.32it/s, Val Loss=0.2067]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2765, F1: 0.1918, Acc: 0.3718\n","Time  - Epoch: 9.9s, LR: 3.13e-06\n","Progress: 64.0% | Best F1: 0.2892 | ETA: 3.4min\n","\n","Epoch 33/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 33/50: 100%|██████████| 164/164 [00:09<00:00, 17.92it/s, Loss=0.0001, LR=3.13e-06]\n","CASME II Validation Epoch 33/50: 100%|██████████| 5/5 [00:00<00:00,  6.46it/s, Val Loss=0.2017]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2662, F1: 0.1920, Acc: 0.3718\n","Time  - Epoch: 9.9s, LR: 3.13e-06\n","Progress: 66.0% | Best F1: 0.2892 | ETA: 3.2min\n","\n","Epoch 34/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 34/50: 100%|██████████| 164/164 [00:09<00:00, 17.78it/s, Loss=0.0001, LR=3.13e-06]\n","CASME II Validation Epoch 34/50: 100%|██████████| 5/5 [00:00<00:00,  6.40it/s, Val Loss=0.2100]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2821, F1: 0.1891, Acc: 0.3718\n","Time  - Epoch: 10.0s, LR: 3.13e-06\n","Progress: 68.0% | Best F1: 0.2892 | ETA: 3.0min\n","\n","Epoch 35/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 35/50: 100%|██████████| 164/164 [00:09<00:00, 17.75it/s, Loss=0.0001, LR=3.13e-06]\n","CASME II Validation Epoch 35/50: 100%|██████████| 5/5 [00:00<00:00,  6.35it/s, Val Loss=0.2170]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2850, F1: 0.1880, Acc: 0.3590\n","Time  - Epoch: 10.0s, LR: 3.13e-06\n","Progress: 70.0% | Best F1: 0.2892 | ETA: 2.8min\n","\n","Epoch 36/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 36/50: 100%|██████████| 164/164 [00:09<00:00, 17.85it/s, Loss=0.0001, LR=3.13e-06]\n","CASME II Validation Epoch 36/50: 100%|██████████| 5/5 [00:00<00:00,  6.26it/s, Val Loss=0.2064]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2767, F1: 0.1945, Acc: 0.3846\n","Time  - Epoch: 10.0s, LR: 1.56e-06\n","Progress: 72.0% | Best F1: 0.2892 | ETA: 2.6min\n","\n","Epoch 37/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 37/50: 100%|██████████| 164/164 [00:09<00:00, 17.75it/s, Loss=0.0001, LR=1.56e-06]\n","CASME II Validation Epoch 37/50: 100%|██████████| 5/5 [00:00<00:00,  6.20it/s, Val Loss=0.2002]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2729, F1: 0.1958, Acc: 0.3974\n","Time  - Epoch: 10.1s, LR: 1.56e-06\n","Progress: 74.0% | Best F1: 0.2892 | ETA: 2.4min\n","\n","Epoch 38/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 38/50: 100%|██████████| 164/164 [00:09<00:00, 17.95it/s, Loss=0.0001, LR=1.56e-06]\n","CASME II Validation Epoch 38/50: 100%|██████████| 5/5 [00:00<00:00,  6.20it/s, Val Loss=0.2147]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2908, F1: 0.1857, Acc: 0.3974\n","Time  - Epoch: 10.0s, LR: 1.56e-06\n","Progress: 76.0% | Best F1: 0.2892 | ETA: 2.2min\n","\n","Epoch 39/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 39/50: 100%|██████████| 164/164 [00:09<00:00, 17.55it/s, Loss=0.0001, LR=1.56e-06]\n","CASME II Validation Epoch 39/50: 100%|██████████| 5/5 [00:00<00:00,  6.57it/s, Val Loss=0.2176]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 0.9994, Acc: 0.9992\n","Val   - Loss: 0.2841, F1: 0.1853, Acc: 0.3590\n","Time  - Epoch: 10.1s, LR: 1.56e-06\n","Progress: 78.0% | Best F1: 0.2892 | ETA: 2.0min\n","\n","Epoch 40/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 40/50: 100%|██████████| 164/164 [00:09<00:00, 17.56it/s, Loss=0.0001, LR=1.56e-06]\n","CASME II Validation Epoch 40/50: 100%|██████████| 5/5 [00:00<00:00,  6.47it/s, Val Loss=0.2127]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2898, F1: 0.1921, Acc: 0.3846\n","Time  - Epoch: 10.1s, LR: 1.56e-06\n","Progress: 80.0% | Best F1: 0.2892 | ETA: 1.8min\n","\n","Epoch 41/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 41/50: 100%|██████████| 164/164 [00:09<00:00, 18.14it/s, Loss=0.0001, LR=1.56e-06]\n","CASME II Validation Epoch 41/50: 100%|██████████| 5/5 [00:00<00:00,  6.13it/s, Val Loss=0.2085]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2869, F1: 0.1786, Acc: 0.3718\n","Time  - Epoch: 9.9s, LR: 1.56e-06\n","Progress: 82.0% | Best F1: 0.2892 | ETA: 1.7min\n","\n","Epoch 42/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 42/50: 100%|██████████| 164/164 [00:09<00:00, 17.83it/s, Loss=0.0002, LR=1.56e-06]\n","CASME II Validation Epoch 42/50: 100%|██████████| 5/5 [00:00<00:00,  6.34it/s, Val Loss=0.2163]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0002, F1: 0.9997, Acc: 0.9996\n","Val   - Loss: 0.2893, F1: 0.1828, Acc: 0.3718\n","Time  - Epoch: 10.0s, LR: 1.00e-06\n","Progress: 84.0% | Best F1: 0.2892 | ETA: 1.5min\n","\n","Epoch 43/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 43/50: 100%|██████████| 164/164 [00:09<00:00, 17.98it/s, Loss=0.0001, LR=1.00e-06]\n","CASME II Validation Epoch 43/50: 100%|██████████| 5/5 [00:00<00:00,  6.56it/s, Val Loss=0.2056]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 0.9997, Acc: 0.9996\n","Val   - Loss: 0.2796, F1: 0.1913, Acc: 0.3846\n","Time  - Epoch: 9.9s, LR: 1.00e-06\n","Progress: 86.0% | Best F1: 0.2892 | ETA: 1.3min\n","\n","Epoch 44/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 44/50: 100%|██████████| 164/164 [00:09<00:00, 17.79it/s, Loss=0.0001, LR=1.00e-06]\n","CASME II Validation Epoch 44/50: 100%|██████████| 5/5 [00:00<00:00,  6.38it/s, Val Loss=0.2073]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2774, F1: 0.1828, Acc: 0.3718\n","Time  - Epoch: 10.0s, LR: 1.00e-06\n","Progress: 88.0% | Best F1: 0.2892 | ETA: 1.1min\n","\n","Epoch 45/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 45/50: 100%|██████████| 164/164 [00:09<00:00, 17.86it/s, Loss=0.0001, LR=1.00e-06]\n","CASME II Validation Epoch 45/50: 100%|██████████| 5/5 [00:00<00:00,  6.43it/s, Val Loss=0.2116]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2826, F1: 0.1771, Acc: 0.3718\n","Time  - Epoch: 10.0s, LR: 1.00e-06\n","Progress: 90.0% | Best F1: 0.2892 | ETA: 0.9min\n","\n","Epoch 46/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 46/50: 100%|██████████| 164/164 [00:09<00:00, 17.79it/s, Loss=0.0001, LR=1.00e-06]\n","CASME II Validation Epoch 46/50: 100%|██████████| 5/5 [00:00<00:00,  6.52it/s, Val Loss=0.2192]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2888, F1: 0.1866, Acc: 0.3718\n","Time  - Epoch: 10.0s, LR: 1.00e-06\n","Progress: 92.0% | Best F1: 0.2892 | ETA: 0.7min\n","\n","Epoch 47/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 47/50: 100%|██████████| 164/164 [00:09<00:00, 17.86it/s, Loss=0.0001, LR=1.00e-06]\n","CASME II Validation Epoch 47/50: 100%|██████████| 5/5 [00:00<00:00,  6.07it/s, Val Loss=0.2234]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0003, F1: 0.9997, Acc: 0.9996\n","Val   - Loss: 0.2909, F1: 0.1976, Acc: 0.3846\n","Time  - Epoch: 10.0s, LR: 1.00e-06\n","Progress: 94.0% | Best F1: 0.2892 | ETA: 0.5min\n","\n","Epoch 48/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 48/50: 100%|██████████| 164/164 [00:09<00:00, 17.65it/s, Loss=0.0001, LR=1.00e-06]\n","CASME II Validation Epoch 48/50: 100%|██████████| 5/5 [00:00<00:00,  6.17it/s, Val Loss=0.2077]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2796, F1: 0.1803, Acc: 0.3718\n","Time  - Epoch: 10.1s, LR: 1.00e-06\n","Progress: 96.0% | Best F1: 0.2892 | ETA: 0.4min\n","\n","Epoch 49/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 49/50: 100%|██████████| 164/164 [00:09<00:00, 17.54it/s, Loss=0.0000, LR=1.00e-06]\n","CASME II Validation Epoch 49/50: 100%|██████████| 5/5 [00:00<00:00,  6.36it/s, Val Loss=0.2186]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 0.9994, Acc: 0.9992\n","Val   - Loss: 0.2904, F1: 0.1931, Acc: 0.3846\n","Time  - Epoch: 10.2s, LR: 1.00e-06\n","Progress: 98.0% | Best F1: 0.2892 | ETA: 0.2min\n","\n","Epoch 50/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 50/50: 100%|██████████| 164/164 [00:09<00:00, 17.65it/s, Loss=0.0001, LR=1.00e-06]\n","CASME II Validation Epoch 50/50: 100%|██████████| 5/5 [00:00<00:00,  6.19it/s, Val Loss=0.2169]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0001, F1: 1.0000, Acc: 1.0000\n","Val   - Loss: 0.2913, F1: 0.1924, Acc: 0.3846\n","Time  - Epoch: 10.1s, LR: 1.00e-06\n","Progress: 100.0% | Best F1: 0.2892 | ETA: 0.0min\n","\n","======================================================================\n","CASME II MOBILENETV3-SMALL BASELINE TRAINING COMPLETED\n","======================================================================\n","Training time: 9.1 minutes\n","Epochs completed: 50\n","Best validation F1: 0.2892 (epoch 6)\n","Final train F1: 1.0000\n","Final validation F1: 0.1924\n","\n","Exporting enhanced training documentation...\n","Enhanced training documentation saved successfully: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/08_01_mobilenet_casme2_mfs/training_logs/casme2_mobilenet_mfs_training_history.json\n","Model: MobileNetV3-Small\n","Methodology: M1\n","Input resolution: 640x480 RGB\n","Loss function: Optimized Focal Loss\n","\n","Next: Cell 3 - CASME II MobileNetV3-Small Evaluation\n","Enhanced training pipeline with multi-frame temporal windows completed successfully!\n"]}]},{"cell_type":"code","source":["# @title Cell 3: CASME II MobileNetV3-Small Evaluation (Configurable)\n","\n","# File: 08_01_MobileNet_CASME2_MFS_Cell3.py\n","# Location: experiments/08_01_MobileNet_CASME2-MFS.ipynb\n","# Purpose: Configurable evaluation framework for KFS and AF test sets\n","\n","# CONFIGURATION: Choose which test version to evaluate\n","# Options: 'kfs', 'af', or 'both'\n","# - 'kfs': Evaluate Key-Frame Sampling test set only (84 samples)\n","# - 'af': Evaluate Apex-Frame test set only (28 samples)\n","# - 'both': Evaluate both KFS and AF sequentially\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from datetime import datetime\n","\n","from sklearn.metrics import (\n","    accuracy_score, precision_recall_fscore_support,\n","    classification_report, confusion_matrix,\n","    roc_curve, auc\n",")\n","from sklearn.preprocessing import label_binarize\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing as mp\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"CASME II MobileNetV3-Small Evaluation Framework\")\n","print(\"=\" * 60)\n","\n","TEST_VERSION_TO_EVALUATE = 'both'\n","\n","print(f\"Evaluation Configuration: {TEST_VERSION_TO_EVALUATE.upper()}\")\n","if TEST_VERSION_TO_EVALUATE == 'both':\n","    print(\"  Strategy: Sequential KFS → AF\")\n","    print(\"  Expected: ~84 samples (KFS) + ~28 samples (AF)\")\n","elif TEST_VERSION_TO_EVALUATE == 'kfs':\n","    print(\"  Strategy: KFS only\")\n","    print(\"  Expected: ~84 samples (Key-Frame Sampling)\")\n","elif TEST_VERSION_TO_EVALUATE == 'af':\n","    print(\"  Strategy: AF only\")\n","    print(\"  Expected: ~28 samples (Apex-Frame)\")\n","else:\n","    raise ValueError(f\"Invalid TEST_VERSION_TO_EVALUATE: {TEST_VERSION_TO_EVALUATE}. Must be 'kfs', 'af', or 'both'\")\n","print(\"=\" * 60)\n","\n","def get_test_dataset_config(version, project_root):\n","    if version == 'kfs':\n","        config = {\n","            'version': 'kfs',\n","            'phase': 'Phase 2',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/data_split_v2\",\n","            'metadata_file': 'split_metadata_v2.json',\n","            'processing_summary': 'processing_summary_v2.json',\n","            'description': 'Key-frames (onset, apex, offset)',\n","            'expected_samples': 84,\n","            'frame_types': ['onset', 'apex', 'offset']\n","        }\n","    elif version == 'af':\n","        config = {\n","            'version': 'af',\n","            'phase': 'Phase 1',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/data_split_v1\",\n","            'metadata_file': 'split_metadata.json',\n","            'processing_summary': 'processing_summary.json',\n","            'description': 'Apex-only frames',\n","            'expected_samples': 28,\n","            'frame_types': ['apex']\n","        }\n","    else:\n","        raise ValueError(f\"Invalid version: {version}. Must be 'kfs' or 'af'\")\n","\n","    return config\n","\n","class CASME2DatasetEvaluation(Dataset):\n","    def __init__(self, split_metadata, dataset_root, transform=None, split='test', use_ram_cache=True):\n","        self.metadata = split_metadata[split]['samples']\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","        self.split = split\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.sample_ids = []\n","        self.emotions = []\n","        self.subjects = []\n","        self.cached_images = []\n","\n","        print(f\"Loading CASME II {split} dataset for evaluation...\")\n","\n","        for sample in self.metadata:\n","            image_path = os.path.join(dataset_root, split, sample['image_filename'])\n","            self.images.append(image_path)\n","            self.labels.append(CLASS_TO_IDX[sample['emotion']])\n","            self.sample_ids.append(sample['sample_id'])\n","            self.emotions.append(sample['emotion'])\n","            self.subjects.append(sample['subject'])\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples for evaluation\")\n","        self._print_evaluation_distribution()\n","\n","        if self.use_ram_cache:\n","            self._preload_to_ram_evaluation()\n","\n","    def _print_evaluation_distribution(self):\n","        if len(self.labels) == 0:\n","            print(\"No test samples found!\")\n","            return\n","\n","        label_counts = {}\n","        subject_counts = {}\n","\n","        for label, subject in zip(self.labels, self.subjects):\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","            subject_counts[subject] = subject_counts.get(subject, 0) + 1\n","\n","        print(\"Test set class distribution:\")\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","        print(f\"Test set covers {len(subject_counts)} subjects\")\n","\n","        missing_classes = []\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i not in label_counts:\n","                missing_classes.append(class_name)\n","\n","        if missing_classes:\n","            print(f\"Missing classes in test set: {missing_classes}\")\n","\n","    def _preload_to_ram_evaluation(self):\n","        print(f\"Preloading {len(self.images)} test images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (640, 480):\n","                    image = image.resize((640, 480), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (640, 480), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=\"Loading test images to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * 640 * 480 * 3 * 4 / 1e9\n","        print(f\"Test RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (640, 480):\n","                    image = image.resize((640, 480), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (640, 480), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return (image, self.labels[idx], self.sample_ids[idx],\n","                self.emotions[idx], self.subjects[idx], os.path.basename(self.images[idx]))\n","\n","EVALUATION_CONFIG_CASME2 = {\n","    'model_type': 'MobileNetV3Small_CASME2_MFS_Baseline',\n","    'task_type': 'micro_expression_recognition',\n","    'num_classes': 7,\n","    'class_names': CASME2_CLASSES,\n","    'checkpoint_file': 'casme2_mobilenet_mfs_best_f1.pth',\n","    'dataset_name': 'CASME_II',\n","    'methodology': 'M1',\n","    'input_resolution': '640x480 RGB',\n","    'evaluation_protocol': 'stratified_split'\n","}\n","\n","print(f\"\\nCASME II MobileNetV3 Evaluation Configuration:\")\n","print(f\"  Model: {EVALUATION_CONFIG_CASME2['model_type']}\")\n","print(f\"  Methodology: {EVALUATION_CONFIG_CASME2['methodology']}\")\n","print(f\"  Input resolution: {EVALUATION_CONFIG_CASME2['input_resolution']}\")\n","print(f\"  Classes: {EVALUATION_CONFIG_CASME2['class_names']}\")\n","\n","def extract_logits_safe_casme2(outputs_all):\n","    if isinstance(outputs_all, torch.Tensor):\n","        return outputs_all\n","    if isinstance(outputs_all, (tuple, list)):\n","        for item in outputs_all:\n","            if isinstance(item, torch.Tensor):\n","                return item\n","    if isinstance(outputs_all, dict):\n","        for key in ('logits', 'logit', 'predictions', 'outputs', 'scores'):\n","            value = outputs_all.get(key)\n","            if isinstance(value, torch.Tensor):\n","                return value\n","        for value in outputs_all.values():\n","            if isinstance(value, torch.Tensor):\n","                return value\n","    raise RuntimeError(\"Unable to extract tensor logits from model output\")\n","\n","def load_trained_model_casme2(checkpoint_path, device):\n","    print(f\"Loading trained CASME II MobileNetV3-Small model from: {checkpoint_path}\")\n","\n","    if not os.path.exists(checkpoint_path):\n","        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n","\n","    checkpoint = None\n","    loading_method = \"unknown\"\n","\n","    try:\n","        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","        loading_method = \"standard\"\n","    except Exception as e1:\n","        try:\n","            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n","            loading_method = \"weights_only_false\"\n","        except Exception as e2:\n","            try:\n","                import pickle\n","                with open(checkpoint_path, 'rb') as f:\n","                    checkpoint = pickle.load(f)\n","                loading_method = \"pickle\"\n","            except Exception as e3:\n","                raise RuntimeError(f\"All loading methods failed: {e1}, {e2}, {e3}\")\n","\n","    print(f\"Checkpoint loaded using: {loading_method}\")\n","\n","    model = MobileNetCASME2Baseline(\n","        num_classes=EVALUATION_CONFIG_CASME2['num_classes'],\n","        dropout_rate=CASME2_MOBILENET_CONFIG['dropout_rate']\n","    ).to(device)\n","\n","    state_dict = checkpoint.get('model_state_dict', checkpoint)\n","\n","    try:\n","        model.load_state_dict(state_dict, strict=True)\n","        print(\"Model state loaded with strict=True\")\n","    except Exception as e:\n","        print(f\"Strict loading failed, trying non-strict: {str(e)[:100]}...\")\n","        try:\n","            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n","            if missing_keys or unexpected_keys:\n","                print(f\"Non-strict loading: Missing {len(missing_keys)}, Unexpected {len(unexpected_keys)}\")\n","            else:\n","                print(\"Model state loaded with strict=False (no key mismatches)\")\n","        except Exception as e2:\n","            raise RuntimeError(f\"Both loading approaches failed: {e2}\")\n","\n","    model.eval()\n","\n","    training_info = {\n","        'best_val_f1': float(checkpoint.get('best_f1', 0.0)),\n","        'best_val_loss': float(checkpoint.get('best_loss', float('inf'))),\n","        'best_val_accuracy': float(checkpoint.get('best_acc', 0.0)),\n","        'best_epoch': int(checkpoint.get('epoch', 0)) + 1,\n","        'model_checkpoint': EVALUATION_CONFIG_CASME2['checkpoint_file'],\n","        'num_classes': EVALUATION_CONFIG_CASME2['num_classes'],\n","        'config': checkpoint.get('casme2_config', {})\n","    }\n","\n","    print(f\"Model loaded successfully:\")\n","    print(f\"  Best validation F1: {training_info['best_val_f1']:.4f}\")\n","    print(f\"  Best validation accuracy: {training_info['best_val_accuracy']:.4f}\")\n","    print(f\"  Best epoch: {training_info['best_epoch']}\")\n","\n","    return model, training_info\n","\n","def run_model_inference_casme2(model, test_loader, device, test_version):\n","    print(f\"Running CASME II MobileNetV3 inference on {test_version.upper()} test set...\")\n","\n","    model.eval()\n","    all_predictions = []\n","    all_probabilities = []\n","    all_labels = []\n","    all_sample_ids = []\n","    all_emotions = []\n","    all_subjects = []\n","    all_filenames = []\n","\n","    inference_start = time.time()\n","\n","    with torch.no_grad():\n","        for batch_idx, (images, labels, sample_ids, emotions, subjects, filenames) in enumerate(\n","            tqdm(test_loader, desc=f\"CASME II Inference ({test_version.upper()})\")):\n","\n","            images = images.to(device)\n","\n","            try:\n","                outputs_raw = model(images)\n","                outputs = extract_logits_safe_casme2(outputs_raw)\n","            except Exception as e:\n","                print(f\"Error in model forward pass: {e}\")\n","                outputs = model(images)\n","                if not isinstance(outputs, torch.Tensor):\n","                    outputs = outputs[0] if isinstance(outputs, (list, tuple)) else outputs\n","\n","            if outputs.shape[1] != 7:\n","                print(f\"Warning: Expected 7 classes output, got {outputs.shape[1]}\")\n","\n","            probabilities = torch.softmax(outputs, dim=1)\n","            predictions = torch.argmax(probabilities, dim=1)\n","\n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_probabilities.extend(probabilities.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","            all_sample_ids.extend(sample_ids)\n","            all_emotions.extend(emotions)\n","            all_subjects.extend(subjects)\n","            all_filenames.extend(filenames)\n","\n","    inference_time = time.time() - inference_start\n","\n","    print(f\"CASME II inference completed: {len(all_predictions)} samples in {inference_time:.2f}s\")\n","\n","    predictions_array = np.array(all_predictions)\n","    labels_array = np.array(all_labels)\n","\n","    unique_predictions, pred_counts = np.unique(predictions_array, return_counts=True)\n","    print(f\"Predicted classes: {[CASME2_CLASSES[i] for i in unique_predictions]}\")\n","\n","    unique_labels, label_counts = np.unique(labels_array, return_counts=True)\n","    print(f\"True classes in test: {[CASME2_CLASSES[i] for i in unique_labels]}\")\n","\n","    return {\n","        'predictions': predictions_array,\n","        'probabilities': np.array(all_probabilities),\n","        'labels': labels_array,\n","        'sample_ids': all_sample_ids,\n","        'emotions': all_emotions,\n","        'subjects': all_subjects,\n","        'filenames': all_filenames,\n","        'inference_time': inference_time,\n","        'samples_count': len(predictions_array)\n","    }\n","\n","def analyze_wrong_predictions_casme2(inference_results, test_version):\n","    print(f\"Analyzing wrong predictions for {test_version.upper()}...\")\n","\n","    predictions = inference_results['predictions']\n","    labels = inference_results['labels']\n","    sample_ids = inference_results['sample_ids']\n","    emotions = inference_results['emotions']\n","    subjects = inference_results['subjects']\n","    filenames = inference_results['filenames']\n","\n","    wrong_mask = predictions != labels\n","    wrong_indices = np.where(wrong_mask)[0]\n","\n","    wrong_predictions_by_class = {}\n","    subject_error_analysis = {}\n","\n","    for class_name in CASME2_CLASSES:\n","        wrong_predictions_by_class[class_name] = []\n","\n","    for idx in wrong_indices:\n","        true_label = labels[idx]\n","        pred_label = predictions[idx]\n","        sample_id = sample_ids[idx]\n","        emotion = emotions[idx]\n","        subject = subjects[idx]\n","        filename = filenames[idx]\n","\n","        true_class = CASME2_CLASSES[true_label]\n","        pred_class = CASME2_CLASSES[pred_label]\n","\n","        wrong_info = {\n","            'sample_id': sample_id,\n","            'filename': filename,\n","            'subject': subject,\n","            'true_label': int(true_label),\n","            'true_class': true_class,\n","            'predicted_label': int(pred_label),\n","            'predicted_class': pred_class,\n","            'emotion': emotion\n","        }\n","\n","        wrong_predictions_by_class[true_class].append(wrong_info)\n","\n","        if subject not in subject_error_analysis:\n","            subject_error_analysis[subject] = {'total': 0, 'wrong': 0, 'errors': []}\n","        subject_error_analysis[subject]['wrong'] += 1\n","        subject_error_analysis[subject]['errors'].append(wrong_info)\n","\n","    for subject in subjects:\n","        if subject in subject_error_analysis:\n","            subject_error_analysis[subject]['total'] += 1\n","        else:\n","            subject_error_analysis[subject] = {'total': 1, 'wrong': 0, 'errors': []}\n","\n","    for subject in subject_error_analysis:\n","        total = subject_error_analysis[subject]['total']\n","        wrong = subject_error_analysis[subject]['wrong']\n","        subject_error_analysis[subject]['error_rate'] = wrong / total if total > 0 else 0.0\n","\n","    total_wrong = len(wrong_indices)\n","    total_samples = len(predictions)\n","    error_rate = (total_wrong / total_samples) * 100\n","\n","    confusion_patterns = {}\n","    for idx in wrong_indices:\n","        true_label = labels[idx]\n","        pred_label = predictions[idx]\n","        pattern = f\"{CASME2_CLASSES[true_label]}_to_{CASME2_CLASSES[pred_label]}\"\n","        confusion_patterns[pattern] = confusion_patterns.get(pattern, 0) + 1\n","\n","    analysis_results = {\n","        'analysis_metadata': {\n","            'evaluation_timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n","            'model_type': EVALUATION_CONFIG_CASME2['model_type'],\n","            'dataset': EVALUATION_CONFIG_CASME2['dataset_name'],\n","            'test_version': test_version,\n","            'methodology': EVALUATION_CONFIG_CASME2['methodology'],\n","            'total_samples': int(total_samples),\n","            'total_wrong_predictions': int(total_wrong),\n","            'overall_error_rate': float(error_rate)\n","        },\n","        'wrong_predictions_by_class': wrong_predictions_by_class,\n","        'subject_error_analysis': subject_error_analysis,\n","        'confusion_patterns': confusion_patterns,\n","        'error_summary': {\n","            class_name: len(wrong_predictions_by_class[class_name])\n","            for class_name in CASME2_CLASSES\n","        }\n","    }\n","\n","    return analysis_results\n","\n","def calculate_comprehensive_metrics_casme2(inference_results, test_version, test_config):\n","    print(f\"Calculating comprehensive metrics for {test_version.upper()}...\")\n","\n","    predictions = inference_results['predictions']\n","    probabilities = inference_results['probabilities']\n","    labels = inference_results['labels']\n","\n","    if len(predictions) == 0:\n","        raise ValueError(\"No predictions to evaluate!\")\n","\n","    unique_test_labels = sorted(np.unique(labels))\n","    unique_predictions = sorted(np.unique(predictions))\n","\n","    print(f\"Test set contains labels: {[CASME2_CLASSES[i] for i in unique_test_labels]}\")\n","    print(f\"Model predicted classes: {[CASME2_CLASSES[i] for i in unique_predictions]}\")\n","\n","    accuracy = accuracy_score(labels, predictions)\n","\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions, labels=unique_test_labels, average='macro', zero_division=0\n","    )\n","\n","    print(f\"Macro F1 (available classes): {f1:.4f}\")\n","\n","    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n","        labels, predictions, labels=range(7), average=None, zero_division=0\n","    )\n","\n","    cm = confusion_matrix(labels, predictions, labels=range(7))\n","\n","    auc_scores = {}\n","    fpr_dict = {}\n","    tpr_dict = {}\n","\n","    try:\n","        labels_binarized = label_binarize(labels, classes=range(7))\n","\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i in unique_test_labels and len(np.unique(labels_binarized[:, i])) > 1:\n","                fpr, tpr, _ = roc_curve(labels_binarized[:, i], probabilities[:, i])\n","                auc_score = auc(fpr, tpr)\n","                auc_scores[class_name] = float(auc_score)\n","                fpr_dict[class_name] = fpr.tolist()\n","                tpr_dict[class_name] = tpr.tolist()\n","            else:\n","                auc_scores[class_name] = 0.0\n","                fpr_dict[class_name] = [0.0, 1.0]\n","                tpr_dict[class_name] = [0.0, 0.0]\n","\n","        available_auc_scores = [auc_scores[CASME2_CLASSES[i]] for i in unique_test_labels]\n","        macro_auc = float(np.mean(available_auc_scores)) if available_auc_scores else 0.0\n","\n","    except Exception as e:\n","        print(f\"Warning: AUC calculation failed: {e}\")\n","        auc_scores = {class_name: 0.0 for class_name in CASME2_CLASSES}\n","        macro_auc = 0.0\n","\n","    subjects = inference_results['subjects']\n","    subject_performance = {}\n","\n","    for subject in set(subjects):\n","        subject_mask = [s == subject for s in subjects]\n","        subject_predictions = predictions[subject_mask]\n","        subject_labels = labels[subject_mask]\n","\n","        if len(subject_predictions) > 0:\n","            subject_acc = accuracy_score(subject_labels, subject_predictions)\n","            subject_performance[subject] = {\n","                'accuracy': float(subject_acc),\n","                'samples': int(len(subject_predictions)),\n","                'correct': int(np.sum(subject_predictions == subject_labels))\n","            }\n","\n","    comprehensive_results = {\n","        'evaluation_metadata': {\n","            'model_type': EVALUATION_CONFIG_CASME2['model_type'],\n","            'dataset': EVALUATION_CONFIG_CASME2['dataset_name'],\n","            'methodology': EVALUATION_CONFIG_CASME2['methodology'],\n","            'input_resolution': EVALUATION_CONFIG_CASME2['input_resolution'],\n","            'test_version': test_version,\n","            'test_phase': test_config['phase'],\n","            'test_description': test_config['description'],\n","            'test_frame_types': test_config['frame_types'],\n","            'evaluation_timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n","            'num_classes': EVALUATION_CONFIG_CASME2['num_classes'],\n","            'class_names': EVALUATION_CONFIG_CASME2['class_names'],\n","            'test_samples': int(len(labels)),\n","            'available_classes': [CASME2_CLASSES[i] for i in unique_test_labels],\n","            'missing_classes': [CASME2_CLASSES[i] for i in range(7) if i not in unique_test_labels]\n","        },\n","\n","        'overall_performance': {\n","            'accuracy': float(accuracy),\n","            'macro_precision': float(precision),\n","            'macro_recall': float(recall),\n","            'macro_f1': float(f1),\n","            'macro_auc': macro_auc\n","        },\n","\n","        'per_class_performance': {},\n","\n","        'confusion_matrix': cm.tolist(),\n","\n","        'subject_level_performance': subject_performance,\n","\n","        'roc_analysis': {\n","            'auc_scores': auc_scores,\n","            'fpr_curves': fpr_dict,\n","            'tpr_curves': tpr_dict\n","        },\n","\n","        'inference_performance': {\n","            'total_time_seconds': float(inference_results['inference_time']),\n","            'average_time_ms_per_sample': float(inference_results['inference_time'] * 1000 / len(labels))\n","        }\n","    }\n","\n","    for i, class_name in enumerate(CASME2_CLASSES):\n","        comprehensive_results['per_class_performance'][class_name] = {\n","            'precision': float(precision_per_class[i]),\n","            'recall': float(recall_per_class[i]),\n","            'f1_score': float(f1_per_class[i]),\n","            'support': int(support_per_class[i]),\n","            'auc': auc_scores[class_name],\n","            'in_test_set': i in unique_test_labels\n","        }\n","\n","    return comprehensive_results\n","\n","def save_evaluation_results_casme2(evaluation_results, wrong_predictions_results, results_dir, test_version):\n","    os.makedirs(results_dir, exist_ok=True)\n","\n","    results_file = f\"{results_dir}/casme2_mobilenet_mfs_evaluation_results_{test_version}.json\"\n","    with open(results_file, 'w') as f:\n","        json.dump(evaluation_results, f, indent=2, default=str)\n","\n","    wrong_predictions_file = f\"{results_dir}/casme2_mobilenet_mfs_wrong_predictions_{test_version}.json\"\n","    with open(wrong_predictions_file, 'w') as f:\n","        json.dump(wrong_predictions_results, f, indent=2, default=str)\n","\n","    print(f\"Evaluation results saved:\")\n","    print(f\"  Main results: {os.path.basename(results_file)}\")\n","    print(f\"  Wrong predictions: {os.path.basename(wrong_predictions_file)}\")\n","\n","    return results_file, wrong_predictions_file\n","\n","def normalize_metadata_structure(metadata):\n","    if 'splits' in metadata:\n","        return metadata['splits']\n","    elif 'train' in metadata or 'test' in metadata:\n","        return metadata\n","    else:\n","        raise ValueError(\"Unknown metadata format\")\n","\n","def display_evaluation_summary(evaluation_results, wrong_predictions_results, training_info, test_version):\n","    print(\"\\n\" + \"=\" * 60)\n","    print(f\"CASME II MOBILENETV3-SMALL {test_version.upper()} EVALUATION RESULTS\")\n","    print(\"=\" * 60)\n","\n","    overall = evaluation_results['overall_performance']\n","    meta = evaluation_results['evaluation_metadata']\n","\n","    print(f\"Test Dataset: {meta['test_description']} ({test_version.upper()})\")\n","    print(f\"Methodology: {meta['methodology']}\")\n","    print(f\"Input Resolution: {meta['input_resolution']}\")\n","\n","    print(f\"\\nOverall Performance (Macro - Available Classes):\")\n","    print(f\"  Accuracy:  {overall['accuracy']:.4f}\")\n","    print(f\"  Precision: {overall['macro_precision']:.4f}\")\n","    print(f\"  Recall:    {overall['macro_recall']:.4f}\")\n","    print(f\"  F1 Score:  {overall['macro_f1']:.4f}\")\n","    print(f\"  AUC:       {overall['macro_auc']:.4f}\")\n","\n","    print(f\"\\nPer-Class Performance:\")\n","    for class_name, metrics in evaluation_results['per_class_performance'].items():\n","        in_test = \"Present\" if metrics['in_test_set'] else \"Missing\"\n","        print(f\"  {class_name} [{in_test}]: F1={metrics['f1_score']:.4f}, \"\n","              f\"AUC={metrics['auc']:.4f}, Support={metrics['support']}\")\n","\n","    print(f\"\\nTraining vs Test Performance:\")\n","    training_f1 = training_info['best_val_f1']\n","    training_acc = training_info['best_val_accuracy']\n","    test_f1 = overall['macro_f1']\n","    test_acc = overall['accuracy']\n","\n","    print(f\"  Training Val F1:  {training_f1:.4f}\")\n","    print(f\"  Test F1:          {test_f1:.4f}\")\n","    print(f\"  F1 Difference:    {training_f1 - test_f1:+.4f}\")\n","    print(f\"  Training Val Acc: {training_acc:.4f}\")\n","    print(f\"  Test Accuracy:    {test_acc:.4f}\")\n","    print(f\"  Acc Difference:   {training_acc - test_acc:+.4f}\")\n","\n","    if wrong_predictions_results:\n","        wrong_meta = wrong_predictions_results['analysis_metadata']\n","        print(f\"\\nWrong Predictions Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']} / {wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        print(f\"\\n  Errors by True Class:\")\n","        for class_name, error_count in wrong_predictions_results['error_summary'].items():\n","            if error_count > 0:\n","                print(f\"    {class_name}: {error_count} errors\")\n","\n","        patterns = sorted(wrong_predictions_results['confusion_patterns'].items(),\n","                         key=lambda x: x[1], reverse=True)\n","        if patterns:\n","            print(f\"\\n  Top Confusion Patterns:\")\n","            for pattern, count in patterns[:3]:\n","                print(f\"    {pattern}: {count} cases\")\n","\n","    print(f\"\\nInference Performance:\")\n","    print(f\"  Total time: {evaluation_results['inference_performance']['total_time_seconds']:.2f}s\")\n","    print(f\"  Speed: {evaluation_results['inference_performance']['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","    print(f\"\\nTest Dataset Info:\")\n","    print(f\"  Missing classes: {meta['missing_classes']}\")\n","\n","print(\"\\n\" + \"=\" * 60)\n","if TEST_VERSION_TO_EVALUATE == 'both':\n","    print(\"SEQUENTIAL EVALUATION: KFS → AF\")\n","else:\n","    print(f\"SINGLE EVALUATION: {TEST_VERSION_TO_EVALUATE.upper()}\")\n","print(\"=\" * 60)\n","\n","checkpoint_path = f\"{GLOBAL_CONFIG_CASME2['checkpoint_root']}/{EVALUATION_CONFIG_CASME2['checkpoint_file']}\"\n","casme2_model, training_info = load_trained_model_casme2(checkpoint_path, GLOBAL_CONFIG_CASME2['device'])\n","\n","results_dir = f\"{GLOBAL_CONFIG_CASME2['results_root']}/evaluation_results\"\n","all_results = {}\n","\n","if TEST_VERSION_TO_EVALUATE == 'both':\n","    versions_to_evaluate = ['kfs', 'af']\n","elif TEST_VERSION_TO_EVALUATE in ['kfs', 'af']:\n","    versions_to_evaluate = [TEST_VERSION_TO_EVALUATE]\n","else:\n","    raise ValueError(f\"Invalid TEST_VERSION_TO_EVALUATE: {TEST_VERSION_TO_EVALUATE}\")\n","\n","print(f\"\\nVersions to evaluate: {[v.upper() for v in versions_to_evaluate]}\")\n","\n","for test_version in versions_to_evaluate:\n","    print(\"\\n\" + \"=\" * 60)\n","    print(f\"EVALUATING: {test_version.upper()}\")\n","    print(\"=\" * 60)\n","\n","    try:\n","        test_config = get_test_dataset_config(test_version, PROJECT_ROOT)\n","\n","        print(f\"\\nTest Dataset Configuration:\")\n","        print(f\"  Version: {test_config['version']}\")\n","        print(f\"  Phase: {test_config['phase']}\")\n","        print(f\"  Description: {test_config['description']}\")\n","        print(f\"  Expected samples: {test_config['expected_samples']}\")\n","        print(f\"  Frame types: {test_config['frame_types']}\")\n","\n","        test_metadata_path = f\"{test_config['dataset_path']}/{test_config['metadata_file']}\"\n","\n","        if not os.path.exists(test_metadata_path):\n","            print(f\"WARNING: Test metadata not found: {test_metadata_path}\")\n","            print(f\"Skipping {test_version.upper()} evaluation\")\n","            continue\n","\n","        with open(test_metadata_path, 'r') as f:\n","            test_metadata = json.load(f)\n","\n","        normalized_test_metadata = normalize_metadata_structure(test_metadata)\n","\n","        if 'test' not in normalized_test_metadata:\n","            print(f\"WARNING: Test split not found in metadata for {test_version.upper()}\")\n","            print(f\"Skipping {test_version.upper()} evaluation\")\n","            continue\n","\n","        actual_test_samples = len(normalized_test_metadata['test']['samples'])\n","        print(f\"Loaded {actual_test_samples} test samples (expected: {test_config['expected_samples']})\")\n","\n","        casme2_test_dataset = CASME2DatasetEvaluation(\n","            split_metadata=normalized_test_metadata,\n","            dataset_root=test_config['dataset_path'],\n","            transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","            split='test',\n","            use_ram_cache=True\n","        )\n","\n","        if len(casme2_test_dataset) == 0:\n","            print(f\"WARNING: No test samples found for {test_version.upper()}\")\n","            print(f\"Skipping {test_version.upper()} evaluation\")\n","            continue\n","\n","        casme2_test_loader = DataLoader(\n","            casme2_test_dataset,\n","            batch_size=CASME2_MOBILENET_CONFIG['batch_size'],\n","            shuffle=False,\n","            num_workers=CASME2_MOBILENET_CONFIG['num_workers'],\n","            pin_memory=True\n","        )\n","\n","        inference_results = run_model_inference_casme2(\n","            casme2_model, casme2_test_loader, GLOBAL_CONFIG_CASME2['device'], test_version\n","        )\n","\n","        evaluation_results = calculate_comprehensive_metrics_casme2(\n","            inference_results, test_version, test_config\n","        )\n","\n","        wrong_predictions_results = analyze_wrong_predictions_casme2(\n","            inference_results, test_version\n","        )\n","\n","        evaluation_results['training_information'] = training_info\n","\n","        results_file, wrong_file = save_evaluation_results_casme2(\n","            evaluation_results, wrong_predictions_results, results_dir, test_version\n","        )\n","\n","        display_evaluation_summary(\n","            evaluation_results, wrong_predictions_results, training_info, test_version\n","        )\n","\n","        all_results[test_version] = {\n","            'evaluation': evaluation_results,\n","            'wrong_predictions': wrong_predictions_results,\n","            'files': {'main': results_file, 'wrong': wrong_file}\n","        }\n","\n","        print(f\"\\n{test_version.upper()} evaluation completed successfully!\")\n","\n","    except Exception as e:\n","        print(f\"ERROR in {test_version.upper()} evaluation: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        print(f\"Continuing to next evaluation...\")\n","\n","print(\"\\n\" + \"=\" * 60)\n","if TEST_VERSION_TO_EVALUATE == 'both':\n","    print(\"SEQUENTIAL EVALUATION COMPLETED\")\n","else:\n","    print(f\"{TEST_VERSION_TO_EVALUATE.upper()} EVALUATION COMPLETED\")\n","print(\"=\" * 60)\n","\n","if all_results:\n","    print(f\"\\nEvaluated datasets: {[v.upper() for v in all_results.keys()]}\")\n","\n","    print(f\"\\nPerformance Summary:\")\n","    for version, results in all_results.items():\n","        overall = results['evaluation']['overall_performance']\n","        print(f\"\\n{version.upper()}:\")\n","        print(f\"  Accuracy:  {overall['accuracy']:.4f}\")\n","        print(f\"  Macro F1:  {overall['macro_f1']:.4f}\")\n","        print(f\"  Macro AUC: {overall['macro_auc']:.4f}\")\n","\n","    if len(all_results) == 2:\n","        print(f\"\\nComparative Analysis:\")\n","        kfs_f1 = all_results['kfs']['evaluation']['overall_performance']['macro_f1']\n","        af_f1 = all_results['af']['evaluation']['overall_performance']['macro_f1']\n","        delta_f1 = kfs_f1 - af_f1\n","\n","        print(f\"  KFS Macro F1: {kfs_f1:.4f}\")\n","        print(f\"  AF Macro F1:  {af_f1:.4f}\")\n","        print(f\"  Delta (KFS - AF): {delta_f1:+.4f}\")\n","\n","        if delta_f1 > 0:\n","            improvement_pct = (delta_f1 / af_f1) * 100\n","            print(f\"  KFS improves by {improvement_pct:.1f}% over AF\")\n","        else:\n","            degradation_pct = (abs(delta_f1) / kfs_f1) * 100\n","            print(f\"  KFS degrades by {degradation_pct:.1f}% from AF\")\n","\n","    print(f\"\\nAll evaluation files saved in: {results_dir}\")\n","else:\n","    print(\"\\nWARNING: No evaluations completed successfully\")\n","    print(\"Please check:\")\n","    print(\"  1. Model checkpoint exists\")\n","    print(\"  2. Test dataset paths are correct\")\n","    print(\"  3. Metadata files are present\")\n","\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(f\"\\nEvaluation strategy used: {TEST_VERSION_TO_EVALUATE.upper()}\")\n","print(\"Next: Cell 4 - Generate confusion matrices\")\n","print(\"Evaluation completed successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"cellView":"form","id":"yuqDBVcPtWMu","executionInfo":{"status":"ok","timestamp":1762948735276,"user_tz":-420,"elapsed":32638,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"3501c5b2-7d17-45ac-f246-2ac4d14e8171"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II MobileNetV3-Small Evaluation Framework\n","============================================================\n","Evaluation Configuration: BOTH\n","  Strategy: Sequential KFS → AF\n","  Expected: ~84 samples (KFS) + ~28 samples (AF)\n","============================================================\n","\n","CASME II MobileNetV3 Evaluation Configuration:\n","  Model: MobileNetV3Small_CASME2_MFS_Baseline\n","  Methodology: M1\n","  Input resolution: 640x480 RGB\n","  Classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","\n","============================================================\n","SEQUENTIAL EVALUATION: KFS → AF\n","============================================================\n","Loading trained CASME II MobileNetV3-Small model from: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/08_01_mobilenet_casme2_mfs/casme2_mobilenet_mfs_best_f1.pth\n","Checkpoint loaded using: standard\n","MobileNetV3-Small feature dimension: 1024\n","MobileNet CASME II: 1024 -> 512 -> 128 -> 7\n","Model state loaded with strict=True\n","Model loaded successfully:\n","  Best validation F1: 0.2892\n","  Best validation accuracy: 0.4231\n","  Best epoch: 6\n","\n","Versions to evaluate: ['KFS', 'AF']\n","\n","============================================================\n","EVALUATING: KFS\n","============================================================\n","\n","Test Dataset Configuration:\n","  Version: kfs\n","  Phase: Phase 2\n","  Description: Key-frames (onset, apex, offset)\n","  Expected samples: 84\n","  Frame types: ['onset', 'apex', 'offset']\n","Loaded 84 test samples (expected: 84)\n","Loading CASME II test dataset for evaluation...\n","Loaded 84 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 30 samples (35.7%)\n","  disgust: 21 samples (25.0%)\n","  happiness: 12 samples (14.3%)\n","  repression: 9 samples (10.7%)\n","  surprise: 9 samples (10.7%)\n","  sadness: 3 samples (3.6%)\n","Test set covers 16 subjects\n","Missing classes in test set: ['fear']\n","Preloading 84 test images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test images to RAM: 100%|██████████| 84/84 [00:01<00:00, 53.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test RAM caching completed: 84/84 images, ~0.31GB\n","Running CASME II MobileNetV3 inference on KFS test set...\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Inference (KFS): 100%|██████████| 6/6 [00:14<00:00,  2.38s/it]\n"]},{"output_type":"stream","name":"stdout","text":["CASME II inference completed: 84 samples in 14.30s\n","Predicted classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness']\n","True classes in test: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness']\n","Calculating comprehensive metrics for KFS...\n","Test set contains labels: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness']\n","Model predicted classes: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness']\n","Macro F1 (available classes): 0.3504\n","Analyzing wrong predictions for KFS...\n","Evaluation results saved:\n","  Main results: casme2_mobilenet_mfs_evaluation_results_kfs.json\n","  Wrong predictions: casme2_mobilenet_mfs_wrong_predictions_kfs.json\n","\n","============================================================\n","CASME II MOBILENETV3-SMALL KFS EVALUATION RESULTS\n","============================================================\n","Test Dataset: Key-frames (onset, apex, offset) (KFS)\n","Methodology: M1\n","Input Resolution: 640x480 RGB\n","\n","Overall Performance (Macro - Available Classes):\n","  Accuracy:  0.4881\n","  Precision: 0.3586\n","  Recall:    0.3533\n","  F1 Score:  0.3504\n","  AUC:       0.6372\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.6000, AUC=0.6938, Support=30\n","  disgust [Present]: F1=0.6000, AUC=0.7695, Support=21\n","  happiness [Present]: F1=0.2609, AUC=0.4062, Support=12\n","  repression [Present]: F1=0.2667, AUC=0.6000, Support=9\n","  surprise [Present]: F1=0.3750, AUC=0.7778, Support=9\n","  sadness [Present]: F1=0.0000, AUC=0.5761, Support=3\n","  fear [Missing]: F1=0.0000, AUC=0.0000, Support=0\n","\n","Training vs Test Performance:\n","  Training Val F1:  0.2892\n","  Test F1:          0.3504\n","  F1 Difference:    -0.0612\n","  Training Val Acc: 0.4231\n","  Test Accuracy:    0.4881\n","  Acc Difference:   -0.0650\n","\n","Wrong Predictions Analysis:\n","  Total errors: 43 / 84\n","  Error rate: 51.19%\n","\n","  Errors by True Class:\n","    others: 12 errors\n","    disgust: 6 errors\n","    happiness: 9 errors\n","    repression: 7 errors\n","    surprise: 6 errors\n","    sadness: 3 errors\n","\n","  Top Confusion Patterns:\n","    others_to_disgust: 8 cases\n","    disgust_to_surprise: 4 cases\n","    happiness_to_repression: 4 cases\n","\n","Inference Performance:\n","  Total time: 14.30s\n","  Speed: 170.3 ms/sample\n","\n","Test Dataset Info:\n","  Missing classes: ['fear']\n","\n","KFS evaluation completed successfully!\n","\n","============================================================\n","EVALUATING: AF\n","============================================================\n","\n","Test Dataset Configuration:\n","  Version: af\n","  Phase: Phase 1\n","  Description: Apex-only frames\n","  Expected samples: 28\n","  Frame types: ['apex']\n","Loaded 28 test samples (expected: 28)\n","Loading CASME II test dataset for evaluation...\n","Loaded 28 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 10 samples (35.7%)\n","  disgust: 7 samples (25.0%)\n","  happiness: 4 samples (14.3%)\n","  repression: 3 samples (10.7%)\n","  surprise: 3 samples (10.7%)\n","  sadness: 1 samples (3.6%)\n","Test set covers 16 subjects\n","Missing classes in test set: ['fear']\n","Preloading 28 test images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test images to RAM: 100%|██████████| 28/28 [00:00<00:00, 47.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test RAM caching completed: 28/28 images, ~0.10GB\n","Running CASME II MobileNetV3 inference on AF test set...\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Inference (AF): 100%|██████████| 2/2 [00:14<00:00,  7.05s/it]"]},{"output_type":"stream","name":"stdout","text":["CASME II inference completed: 28 samples in 14.09s\n","Predicted classes: ['others', 'disgust', 'happiness', 'repression', 'surprise']\n","True classes in test: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness']\n","Calculating comprehensive metrics for AF...\n","Test set contains labels: ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness']\n","Model predicted classes: ['others', 'disgust', 'happiness', 'repression', 'surprise']\n","Macro F1 (available classes): 0.3016\n","Analyzing wrong predictions for AF...\n","Evaluation results saved:\n","  Main results: casme2_mobilenet_mfs_evaluation_results_af.json\n","  Wrong predictions: casme2_mobilenet_mfs_wrong_predictions_af.json\n","\n","============================================================\n","CASME II MOBILENETV3-SMALL AF EVALUATION RESULTS\n","============================================================\n","Test Dataset: Apex-only frames (AF)\n","Methodology: M1\n","Input Resolution: 640x480 RGB\n","\n","Overall Performance (Macro - Available Classes):\n","  Accuracy:  0.4643\n","  Precision: 0.2992\n","  Recall:    0.3163\n","  F1 Score:  0.3016\n","  AUC:       0.6492\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.5714, AUC=0.6722, Support=10\n","  disgust [Present]: F1=0.5882, AUC=0.7619, Support=7\n","  happiness [Present]: F1=0.2500, AUC=0.3854, Support=4\n","  repression [Present]: F1=0.0000, AUC=0.6000, Support=3\n","  surprise [Present]: F1=0.4000, AUC=0.9200, Support=3\n","  sadness [Present]: F1=0.0000, AUC=0.5556, Support=1\n","  fear [Missing]: F1=0.0000, AUC=0.0000, Support=0\n","\n","Training vs Test Performance:\n","  Training Val F1:  0.2892\n","  Test F1:          0.3016\n","  F1 Difference:    -0.0124\n","  Training Val Acc: 0.4231\n","  Test Accuracy:    0.4643\n","  Acc Difference:   -0.0412\n","\n","Wrong Predictions Analysis:\n","  Total errors: 15 / 28\n","  Error rate: 53.57%\n","\n","  Errors by True Class:\n","    others: 4 errors\n","    disgust: 2 errors\n","    happiness: 3 errors\n","    repression: 3 errors\n","    surprise: 2 errors\n","    sadness: 1 errors\n","\n","  Top Confusion Patterns:\n","    others_to_disgust: 3 cases\n","    others_to_happiness: 1 cases\n","    disgust_to_surprise: 1 cases\n","\n","Inference Performance:\n","  Total time: 14.09s\n","  Speed: 503.3 ms/sample\n","\n","Test Dataset Info:\n","  Missing classes: ['fear']\n","\n","AF evaluation completed successfully!\n","\n","============================================================\n","SEQUENTIAL EVALUATION COMPLETED\n","============================================================\n","\n","Evaluated datasets: ['KFS', 'AF']\n","\n","Performance Summary:\n","\n","KFS:\n","  Accuracy:  0.4881\n","  Macro F1:  0.3504\n","  Macro AUC: 0.6372\n","\n","AF:\n","  Accuracy:  0.4643\n","  Macro F1:  0.3016\n","  Macro AUC: 0.6492\n","\n","Comparative Analysis:\n","  KFS Macro F1: 0.3504\n","  AF Macro F1:  0.3016\n","  Delta (KFS - AF): +0.0488\n","  KFS improves by 16.2% over AF\n","\n","All evaluation files saved in: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/08_01_mobilenet_casme2_mfs/evaluation_results\n","\n","Evaluation strategy used: BOTH\n","Next: Cell 4 - Generate confusion matrices\n","Evaluation completed successfully!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title Cell 4: CASME II MobileNetV3-Small Confusion Matrix Generation\n","\n","# File: 08_01_MobileNet_CASME2_MFS_Cell4.py\n","# Location: experiments/08_01_MobileNet_CASME2-MFS.ipynb\n","# Purpose: Generate professional confusion matrix visualizations for KFS and AF test sets\n","\n","import json\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import seaborn as sns\n","from datetime import datetime\n","\n","print(\"CASME II MobileNetV3-Small Confusion Matrix Generation\")\n","print(\"=\" * 60)\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/08_01_mobilenet_casme2_mfs\"\n","\n","def find_evaluation_json_files_casme2(results_path):\n","    json_files = {}\n","\n","    eval_dir = f\"{results_path}/evaluation_results\"\n","\n","    if os.path.exists(eval_dir):\n","        for version in ['kfs', 'af']:\n","            eval_pattern = f\"{eval_dir}/casme2_mobilenet_mfs_evaluation_results_{version}.json\"\n","            eval_files = glob.glob(eval_pattern)\n","\n","            if eval_files:\n","                json_files[f'main_{version}'] = eval_files[0]\n","                print(f\"Found {version.upper()} evaluation file: {os.path.basename(eval_files[0])}\")\n","\n","        if not json_files:\n","            print(f\"WARNING: No evaluation results found in {eval_dir}\")\n","            print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","    else:\n","        print(f\"ERROR: Evaluation directory not found: {eval_dir}\")\n","\n","    return json_files\n","\n","def load_evaluation_results_casme2(json_path):\n","    try:\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"Successfully loaded: {os.path.basename(json_path)}\")\n","        return data\n","    except Exception as e:\n","        print(f\"ERROR loading {json_path}: {str(e)}\")\n","        return None\n","\n","def calculate_weighted_f1_casme2(per_class_performance):\n","    total_support = sum([class_data['support'] for class_data in per_class_performance.values()\n","                        if class_data['support'] > 0])\n","\n","    if total_support == 0:\n","        return 0.0\n","\n","    weighted_f1 = 0.0\n","\n","    for class_name, class_data in per_class_performance.items():\n","        if class_data['support'] > 0:\n","            weight = class_data['support'] / total_support\n","            weighted_f1 += class_data['f1_score'] * weight\n","\n","    return weighted_f1\n","\n","def calculate_balanced_accuracy_casme2(confusion_matrix):\n","    cm = np.array(confusion_matrix)\n","    n_classes = cm.shape[0]\n","\n","    per_class_balanced_acc = []\n","\n","    classes_with_samples = []\n","    for i in range(n_classes):\n","        if cm[i, :].sum() > 0:\n","            classes_with_samples.append(i)\n","\n","    for i in classes_with_samples:\n","        tp = cm[i, i]\n","        fn = cm[i, :].sum() - tp\n","        fp = cm[:, i].sum() - tp\n","        tn = cm.sum() - tp - fn - fp\n","\n","        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n","\n","        class_balanced_acc = (sensitivity + specificity) / 2\n","        per_class_balanced_acc.append(class_balanced_acc)\n","\n","    balanced_acc = np.mean(per_class_balanced_acc) if per_class_balanced_acc else 0.0\n","\n","    return balanced_acc\n","\n","def determine_text_color_casme2(color_value, threshold=0.5):\n","    return 'white' if color_value > threshold else 'black'\n","\n","def create_confusion_matrix_plot_casme2(data, output_path, test_version):\n","    meta = data['evaluation_metadata']\n","    class_names = meta['class_names']\n","    cm = np.array(data['confusion_matrix'], dtype=int)\n","    overall = data['overall_performance']\n","    per_class = data['per_class_performance']\n","    test_desc = meta.get('test_description', test_version)\n","    methodology = meta.get('methodology', 'M1')\n","    input_res = meta.get('input_resolution', '640x480 RGB')\n","\n","    print(f\"Processing confusion matrix for {test_version.upper()}\")\n","    print(f\"Confusion matrix shape: {cm.shape}\")\n","\n","    macro_f1 = overall.get('macro_f1', 0.0)\n","    accuracy = overall.get('accuracy', 0.0)\n","    weighted_f1 = calculate_weighted_f1_casme2(per_class)\n","    balanced_acc = calculate_balanced_accuracy_casme2(cm)\n","\n","    print(f\"Metrics - Macro F1: {macro_f1:.4f}, Weighted F1: {weighted_f1:.4f}, \"\n","          f\"Acc: {accuracy:.4f}, Balanced Acc: {balanced_acc:.4f}\")\n","\n","    row_sums = cm.sum(axis=1, keepdims=True)\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        cm_pct = np.divide(cm, row_sums, where=(row_sums!=0))\n","        cm_pct = np.nan_to_num(cm_pct)\n","\n","    fig, ax = plt.subplots(figsize=(12, 10))\n","\n","    cmap = 'Blues'\n","\n","    im = ax.imshow(cm_pct, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=0.8)\n","\n","    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","    cbar.set_label('True Class Percentage', rotation=270, labelpad=15, fontsize=11)\n","\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            count = cm[i, j]\n","\n","            if row_sums[i, 0] > 0:\n","                percentage = cm_pct[i, j] * 100\n","                text = f\"{count}\\n{percentage:.1f}%\"\n","            else:\n","                text = f\"{count}\\n(N/A)\"\n","\n","            cell_value = cm_pct[i, j]\n","            text_color = determine_text_color_casme2(cell_value, threshold=0.4)\n","\n","            ax.text(j, i, text, ha=\"center\", va=\"center\",\n","                   color=text_color, fontsize=9, fontweight='bold')\n","\n","    ax.set_xticks(np.arange(len(class_names)))\n","    ax.set_yticks(np.arange(len(class_names)))\n","    ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=10)\n","    ax.set_yticklabels(class_names, fontsize=10)\n","    ax.set_xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n","    ax.set_ylabel(\"True Label\", fontsize=12, fontweight='bold')\n","\n","    missing_classes = meta.get('missing_classes', [])\n","    note_text = f\"Test: {test_desc} ({test_version.upper()})\\n{methodology} | {input_res}\"\n","    if missing_classes:\n","        note_text += f\"\\nMissing: {', '.join(missing_classes)}\"\n","\n","    ax.text(0.02, 0.98, note_text, transform=ax.transAxes, fontsize=9,\n","            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n","\n","    title = f\"CASME II MobileNetV3-Small Micro-Expression Recognition - {test_version.upper()}\\n\"\n","    title += f\"Macro F1: {macro_f1:.4f}  |  Weighted F1: {weighted_f1:.4f}  |  Acc: {accuracy:.4f}  |  Balanced Acc: {balanced_acc:.4f}\"\n","    ax.set_title(title, fontsize=12, pad=25, fontweight='bold')\n","\n","    plt.tight_layout()\n","    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n","    plt.close(fig)\n","\n","    print(f\"Confusion matrix saved to: {os.path.basename(output_path)}\")\n","\n","    return {\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        'accuracy': accuracy,\n","        'balanced_accuracy': balanced_acc,\n","        'missing_classes': missing_classes,\n","        'test_version': test_version\n","    }\n","\n","json_files = find_evaluation_json_files_casme2(RESULTS_ROOT)\n","\n","if not json_files:\n","    print(f\"ERROR: No evaluation JSON files found in {RESULTS_ROOT}\")\n","    print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","else:\n","    print(f\"\\nFound {len([k for k in json_files.keys() if k.startswith('main_')])} evaluation result(s)\")\n","\n","output_dir = f\"{RESULTS_ROOT}/confusion_matrix_analysis\"\n","Path(output_dir).mkdir(parents=True, exist_ok=True)\n","\n","results_summary = {}\n","generated_files = []\n","\n","for version in ['kfs', 'af']:\n","    main_key = f'main_{version}'\n","\n","    if main_key in json_files:\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Processing {version.upper()} Confusion Matrix\")\n","        print(f\"{'='*60}\")\n","\n","        eval_data = load_evaluation_results_casme2(json_files[main_key])\n","\n","        if eval_data is not None:\n","            try:\n","                cm_output_path = os.path.join(output_dir, f\"confusion_matrix_CASME2_MobileNet_MFS_{version.upper()}.png\")\n","                metrics = create_confusion_matrix_plot_casme2(eval_data, cm_output_path, version)\n","                generated_files.append(cm_output_path)\n","\n","                results_summary[version] = metrics\n","\n","                print(f\"SUCCESS: {version.upper()} confusion matrix generated\")\n","\n","            except Exception as e:\n","                print(f\"ERROR: Failed to generate {version.upper()} confusion matrix: {str(e)}\")\n","                import traceback\n","                traceback.print_exc()\n","        else:\n","            print(f\"ERROR: Could not load {version.upper()} evaluation data\")\n","\n","if generated_files:\n","    print(f\"\\n\" + \"=\" * 60)\n","    print(\"CASME II MOBILENETV3-SMALL CONFUSION MATRIX GENERATION COMPLETED\")\n","    print(\"=\" * 60)\n","\n","    print(f\"\\nGenerated confusion matrix files:\")\n","    for file_path in generated_files:\n","        filename = os.path.basename(file_path)\n","        print(f\"  {filename}\")\n","\n","    print(f\"\\nPerformance Summary:\")\n","    for version in ['kfs', 'af']:\n","        if version in results_summary:\n","            metrics = results_summary[version]\n","            print(f\"\\n{version.upper()}:\")\n","            print(f\"  Macro F1:       {metrics['macro_f1']:.4f}\")\n","            print(f\"  Weighted F1:    {metrics['weighted_f1']:.4f}\")\n","            print(f\"  Accuracy:       {metrics['accuracy']:.4f}\")\n","            print(f\"  Balanced Acc:   {metrics['balanced_accuracy']:.4f}\")\n","\n","            if metrics['missing_classes']:\n","                print(f\"  Missing classes: {len(metrics['missing_classes'])}\")\n","\n","    if len(results_summary) == 2:\n","        print(f\"\\nComparative Analysis:\")\n","        kfs_f1 = results_summary['kfs']['macro_f1']\n","        af_f1 = results_summary['af']['macro_f1']\n","        delta_f1 = kfs_f1 - af_f1\n","\n","        print(f\"  KFS vs AF (Macro F1): {kfs_f1:.4f} vs {af_f1:.4f}\")\n","        print(f\"  Delta (KFS - AF): {delta_f1:+.4f}\")\n","\n","        if delta_f1 > 0:\n","            improvement_pct = (delta_f1 / af_f1) * 100\n","            print(f\"  KFS improves by {improvement_pct:.1f}% over AF\")\n","        else:\n","            degradation_pct = (abs(delta_f1) / kfs_f1) * 100\n","            print(f\"  KFS degrades by {degradation_pct:.1f}% from AF\")\n","\n","    print(f\"\\nFiles saved in: {output_dir}\")\n","    print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","else:\n","    print(f\"\\nERROR: No confusion matrices were generated\")\n","    print(\"Please check:\")\n","    print(\"1. Cell 3 evaluation results exist\")\n","    print(\"2. JSON file structure is correct\")\n","    print(\"3. No file permission issues\")\n","\n","print(\"\\nCell 4 completed - CASME II MobileNetV3-Small confusion matrix analysis generated\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","collapsed":true,"id":"1e8Ctut6uPZa","executionInfo":{"status":"ok","timestamp":1762948738587,"user_tz":-420,"elapsed":3298,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"89fd7dac-94f5-4023-d7cd-4705dd733374"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II MobileNetV3-Small Confusion Matrix Generation\n","============================================================\n","Found KFS evaluation file: casme2_mobilenet_mfs_evaluation_results_kfs.json\n","Found AF evaluation file: casme2_mobilenet_mfs_evaluation_results_af.json\n","\n","Found 2 evaluation result(s)\n","\n","============================================================\n","Processing KFS Confusion Matrix\n","============================================================\n","Successfully loaded: casme2_mobilenet_mfs_evaluation_results_kfs.json\n","Processing confusion matrix for KFS\n","Confusion matrix shape: (7, 7)\n","Metrics - Macro F1: 0.3504, Weighted F1: 0.4703, Acc: 0.4881, Balanced Acc: 0.6204\n","Confusion matrix saved to: confusion_matrix_CASME2_MobileNet_MFS_KFS.png\n","SUCCESS: KFS confusion matrix generated\n","\n","============================================================\n","Processing AF Confusion Matrix\n","============================================================\n","Successfully loaded: casme2_mobilenet_mfs_evaluation_results_af.json\n","Processing confusion matrix for AF\n","Confusion matrix shape: (7, 7)\n","Metrics - Macro F1: 0.3016, Weighted F1: 0.4297, Acc: 0.4643, Balanced Acc: 0.5981\n","Confusion matrix saved to: confusion_matrix_CASME2_MobileNet_MFS_AF.png\n","SUCCESS: AF confusion matrix generated\n","\n","============================================================\n","CASME II MOBILENETV3-SMALL CONFUSION MATRIX GENERATION COMPLETED\n","============================================================\n","\n","Generated confusion matrix files:\n","  confusion_matrix_CASME2_MobileNet_MFS_KFS.png\n","  confusion_matrix_CASME2_MobileNet_MFS_AF.png\n","\n","Performance Summary:\n","\n","KFS:\n","  Macro F1:       0.3504\n","  Weighted F1:    0.4703\n","  Accuracy:       0.4881\n","  Balanced Acc:   0.6204\n","  Missing classes: 1\n","\n","AF:\n","  Macro F1:       0.3016\n","  Weighted F1:    0.4297\n","  Accuracy:       0.4643\n","  Balanced Acc:   0.5981\n","  Missing classes: 1\n","\n","Comparative Analysis:\n","  KFS vs AF (Macro F1): 0.3504 vs 0.3016\n","  Delta (KFS - AF): +0.0488\n","  KFS improves by 16.2% over AF\n","\n","Files saved in: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/08_01_mobilenet_casme2_mfs/confusion_matrix_analysis\n","Analysis completed at: 2025-11-12 11:58:58\n","\n","Cell 4 completed - CASME II MobileNetV3-Small confusion matrix analysis generated\n"]}]}]}