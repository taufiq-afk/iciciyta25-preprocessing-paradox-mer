{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyOEkeln9L5QT9n+qqZWg315"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"XCIGVdXmT9LX","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","collapsed":true,"executionInfo":{"status":"ok","timestamp":1764310893351,"user_tz":-420,"elapsed":52207,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"81c1a2ec-530e-425c-b1cf-2081f481288b"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","CASME II CNN BASELINE - MobileNetV3-Small M2 MFS-PREP\n","============================================================\n","\n","[1] Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully\n","\n","[2] Importing required libraries...\n","CASME II MobileNetV3-Small M2 MFS-PREP - Infrastructure Configuration\n","============================================================\n","Loading CASME II v9 preprocessing metadata...\n","Dataset variant: MFS\n","Processing date: 2025-10-19T08:20:12.098301\n","Preprocessing method: face_bbox_expansion_all_directions\n","Total images processed: 2774\n","Face detection rate: 100.00%\n","Target size: 224x224px\n","BBox expansion: 20px\n","Image format: Grayscale (1 channel)\n","\n","Dataset split information:\n","  Train: 2613 frames\n","  Validation: 78 frames\n","  Test: 83 frames\n","\n","==================================================\n","EXPERIMENT CONFIGURATION - CNN M2 MFS-PREP\n","==================================================\n","Model: MobileNetV3-Small (TIMM)\n","Methodology: M2 (Face-Aware Preprocessing)\n","Input Resolution: 224x224 Pure Grayscale (1 channel)\n","Training Strategy: From Scratch (No Pretrained Weights)\n","Preprocessing: Face detection + crop + grayscale\n","Loss Function: Focal Loss\n","  Gamma: 2.5\n","  Alpha Weights: [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","  Alpha Sum: 0.999\n","==================================================\n","\n","Device: cuda\n","GPU: NVIDIA L4 (23.8 GB)\n","L4: Balanced performance configuration\n","RAM preload workers: 32\n","\n","Loading v9 class distribution...\n","\n","v9 Train distribution: [1027, 650, 325, 273, 260, 65, 13]\n","v9 Val distribution: [30, 18, 9, 9, 6, 3, 3]\n","v9 Test distribution: [30, 21, 12, 8, 9, 3, 0]\n","Applied Focal Loss alpha weights: [0.053 0.067 0.094 0.102 0.106 0.201 0.376]\n","Alpha weights sum: 0.999\n","\n","MobileNetV3 Configuration Summary:\n","  Model: mobilenetv3_small_100\n","  Input size: 224x224 Pure Grayscale (1ch)\n","  Methodology: M2 (face-aware preprocessing)\n","  Training: From Scratch (No Pretrained Weights)\n","  Learning rate: 5e-05\n","  Batch size: 16\n","  Dataset version: v9\n","  Frame strategy: multi_frame_sampling\n","\n","Setting up transforms for M2 methodology (224x224 pure grayscale)...\n","M2 transforms configured: 224x224 pure grayscale (1 channel) with standardization\n","\n","Dataset paths:\n","Root: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9\n","  Train: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/train\n","  Validation: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/val\n","  Test: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/test\n","\n","MobileNetV3 CASME II architecture validation...\n","MobileNetV3-Small feature dimension: 1024\n","Training from scratch with 1-channel input\n","MobileNet CASME II: 1024 -> 512 -> 128 -> 7\n","Architecture: Pure grayscale (1ch) from scratch\n","Validation failed: Expected more than 1 value per channel when training, got input size torch.Size([1, 512])\n","\n","============================================================\n","CASME II MOBILENETV3-SMALL M2 MFS-PREP CONFIGURATION COMPLETE\n","============================================================\n","Loss Configuration:\n","  Function: Optimized Focal Loss\n","  Gamma: 2.5\n","  Per-class Alpha: [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","  Alpha Sum: 0.999\n","\n","Model Configuration:\n","  Architecture: MobileNetV3-Small\n","  Parameters: ~2.5M\n","  Input Resolution: 224x224 Pure Grayscale (1 channel)\n","  Methodology: M2 (Face-aware preprocessing)\n","  Training Strategy: From Scratch (No Pretrained Weights)\n","  First Conv Layer: Modified to 1-channel input\n","\n","Dataset Configuration:\n","  Version: v9\n","  Frame strategy: multi_frame_sampling\n","  Train augmentation: frame_level_independent\n","  Classes: 7\n","  Train samples: 2613 frames\n","\n","Next: Cell 2 - Dataset Loading and Training Pipeline\n"]}],"source":["# @title Cell 1: CASME II MobileNetV3-Small M2 MFS-PREP Infrastructure Configuration\n","\n","# File: 09_01_MobileNet_CASME2_MFS_PREP_Cell1.py\n","# Location: experiments/09_01_MobileNet_CASME2-MFS-PREP.ipynb\n","# Purpose: MobileNetV3-Small for CASME II micro-expression recognition with M2 preprocessed methodology\n","\n","from google.colab import drive\n","print(\"=\" * 60)\n","print(\"CASME II CNN BASELINE - MobileNetV3-Small M2 MFS-PREP\")\n","print(\"=\" * 60)\n","print(\"\\n[1] Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Google Drive mounted successfully\")\n","\n","print(\"\\n[2] Importing required libraries...\")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import timm\n","import json\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import time\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","DATASET_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/preprocessed_v9\"\n","CHECKPOINT_ROOT = f\"{PROJECT_ROOT}/models/09_01_mobilenet_casme2_mfs_prep\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/09_01_mobilenet_casme2_mfs_prep\"\n","\n","PREPROCESSING_SUMMARY = f\"{DATASET_ROOT}/preprocessing_summary.json\"\n","\n","print(\"CASME II MobileNetV3-Small M2 MFS-PREP - Infrastructure Configuration\")\n","print(\"=\" * 60)\n","\n","if not os.path.exists(PREPROCESSING_SUMMARY):\n","    raise FileNotFoundError(f\"v9 preprocessing summary not found: {PREPROCESSING_SUMMARY}\")\n","\n","print(\"Loading CASME II v9 preprocessing metadata...\")\n","with open(PREPROCESSING_SUMMARY, 'r') as f:\n","    preprocessing_info = json.load(f)\n","\n","print(f\"Dataset variant: {preprocessing_info['variant']}\")\n","print(f\"Processing date: {preprocessing_info['processing_date']}\")\n","print(f\"Preprocessing method: {preprocessing_info['preprocessing_method']}\")\n","print(f\"Total images processed: {preprocessing_info['total_processed']}\")\n","print(f\"Face detection rate: {preprocessing_info['face_detection_stats']['detection_rate']:.2%}\")\n","\n","preproc_params = preprocessing_info['preprocessing_parameters']\n","print(f\"Target size: {preproc_params['target_size']}x{preproc_params['target_size']}px\")\n","print(f\"BBox expansion: {preproc_params['bbox_expansion']}px\")\n","print(f\"Image format: Grayscale (1 channel)\")\n","\n","print(f\"\\nDataset split information:\")\n","print(f\"  Train: {preprocessing_info['splits']['train']['total_images']} frames\")\n","print(f\"  Validation: {preprocessing_info['splits']['val']['total_images']} frames\")\n","print(f\"  Test: {preprocessing_info['splits']['test']['total_images']} frames\")\n","\n","USE_FOCAL_LOSS = True\n","FOCAL_LOSS_GAMMA = 2.5\n","\n","CROSSENTROPY_CLASS_WEIGHTS = [1.00, 1.25, 1.76, 1.91, 1.99, 3.76, 7.04]\n","FOCAL_LOSS_ALPHA_WEIGHTS = [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","\n","MOBILENET_MODEL_NAME = 'mobilenetv3_small_100'\n","USE_PURE_GRAYSCALE = True\n","\n","print(\"\\n\" + \"=\" * 50)\n","print(\"EXPERIMENT CONFIGURATION - CNN M2 MFS-PREP\")\n","print(\"=\" * 50)\n","print(f\"Model: MobileNetV3-Small (TIMM)\")\n","print(f\"Methodology: M2 (Face-Aware Preprocessing)\")\n","print(f\"Input Resolution: 224x224 Pure Grayscale (1 channel)\")\n","print(f\"Training Strategy: From Scratch (No Pretrained Weights)\")\n","print(f\"Preprocessing: Face detection + crop + grayscale\")\n","print(f\"Loss Function: Focal Loss\")\n","print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","print(f\"  Alpha Weights: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","print(\"=\" * 50)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n","gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n","\n","print(f\"\\nDevice: {device}\")\n","print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n","\n","if 'A100' in gpu_name:\n","    BATCH_SIZE = 24\n","    NUM_WORKERS = 8\n","    torch.backends.cudnn.benchmark = True\n","    print(\"A100: Optimized batch size for 224x224 input\")\n","elif 'L4' in gpu_name:\n","    BATCH_SIZE = 16\n","    NUM_WORKERS = 8\n","    torch.backends.cudnn.benchmark = True\n","    print(\"L4: Balanced performance configuration\")\n","else:\n","    BATCH_SIZE = 8\n","    NUM_WORKERS = 8\n","    print(\"Default GPU: Conservative settings\")\n","\n","RAM_PRELOAD_WORKERS = 32\n","print(f\"RAM preload workers: {RAM_PRELOAD_WORKERS}\")\n","\n","CASME2_CLASSES = ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CASME2_CLASSES)}\n","\n","print(\"\\nLoading v9 class distribution...\")\n","train_dist = preprocessing_info['splits']['train']['emotion_distribution']\n","val_dist = preprocessing_info['splits']['val']['emotion_distribution']\n","test_dist = preprocessing_info['splits']['test']['emotion_distribution']\n","\n","def emotion_dist_to_list(emotion_dict, class_names):\n","    return [emotion_dict.get(cls, 0) for cls in class_names]\n","\n","train_dist_list = emotion_dist_to_list(train_dist, CASME2_CLASSES)\n","val_dist_list = emotion_dist_to_list(val_dist, CASME2_CLASSES)\n","test_dist_list = emotion_dist_to_list(test_dist, CASME2_CLASSES)\n","\n","print(f\"\\nv9 Train distribution: {train_dist_list}\")\n","print(f\"v9 Val distribution: {val_dist_list}\")\n","print(f\"v9 Test distribution: {test_dist_list}\")\n","\n","if USE_FOCAL_LOSS:\n","    class_weights = torch.tensor(FOCAL_LOSS_ALPHA_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied Focal Loss alpha weights: {class_weights.cpu().numpy()}\")\n","    print(f\"Alpha weights sum: {class_weights.sum().item():.3f}\")\n","else:\n","    class_weights = torch.tensor(CROSSENTROPY_CLASS_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied CrossEntropy class weights: {class_weights.cpu().numpy()}\")\n","\n","CASME2_MOBILENET_CONFIG = {\n","    'model_name': MOBILENET_MODEL_NAME,\n","    'input_size': (224, 224),\n","    'num_classes': 7,\n","    'dropout_rate': 0.3,\n","\n","    'learning_rate': 5e-5,\n","    'weight_decay': 1e-5,\n","    'gradient_clip': 1.0,\n","    'num_epochs': 50,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'device': device,\n","\n","    'scheduler_type': 'plateau',\n","    'scheduler_mode': 'max',\n","    'scheduler_factor': 0.5,\n","    'scheduler_patience': 5,\n","    'scheduler_min_lr': 1e-6,\n","    'scheduler_monitor': 'val_f1_macro',\n","\n","    'use_focal_loss': USE_FOCAL_LOSS,\n","    'focal_loss_gamma': FOCAL_LOSS_GAMMA,\n","    'focal_loss_alpha_weights': FOCAL_LOSS_ALPHA_WEIGHTS,\n","    'crossentropy_class_weights': CROSSENTROPY_CLASS_WEIGHTS,\n","    'class_weights': class_weights,\n","\n","    'use_macro_avg': True,\n","    'early_stopping': False,\n","    'save_best_f1': True,\n","    'save_strategy': 'best_only',\n","\n","    'dataset_version': 'v9',\n","    'methodology': 'M2',\n","    'preprocessing': 'face_aware_preprocessing',\n","    'frame_strategy': 'multi_frame_sampling',\n","    'train_augmentation': 'frame_level_independent',\n","    'image_format': 'pure_grayscale_1channel',\n","    'use_pure_grayscale': USE_PURE_GRAYSCALE,\n","    'use_pretrained_weights': False,\n","    'training_strategy': 'from_scratch',\n","    'preprocessing_details': {\n","        'face_detection': True,\n","        'crop_method': 'face_bbox_expansion',\n","        'target_size': preproc_params['target_size'],\n","        'bbox_expansion': preproc_params['bbox_expansion'],\n","        'grayscale_conversion': True,\n","        'input_channels': 1\n","    }\n","}\n","\n","print(f\"\\nMobileNetV3 Configuration Summary:\")\n","print(f\"  Model: {CASME2_MOBILENET_CONFIG['model_name']}\")\n","print(f\"  Input size: {CASME2_MOBILENET_CONFIG['input_size'][0]}x{CASME2_MOBILENET_CONFIG['input_size'][1]} Pure Grayscale (1ch)\")\n","print(f\"  Methodology: {CASME2_MOBILENET_CONFIG['methodology']} (face-aware preprocessing)\")\n","print(f\"  Training: From Scratch (No Pretrained Weights)\")\n","print(f\"  Learning rate: {CASME2_MOBILENET_CONFIG['learning_rate']}\")\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","print(f\"  Dataset version: {CASME2_MOBILENET_CONFIG['dataset_version']}\")\n","print(f\"  Frame strategy: {CASME2_MOBILENET_CONFIG['frame_strategy']}\")\n","\n","class OptimizedFocalLoss(nn.Module):\n","    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n","        super(OptimizedFocalLoss, self).__init__()\n","\n","        if alpha is not None:\n","            if isinstance(alpha, list):\n","                self.alpha = torch.tensor(alpha, dtype=torch.float32)\n","            else:\n","                self.alpha = alpha\n","\n","            alpha_sum = self.alpha.sum().item()\n","            if abs(alpha_sum - 1.0) > 0.01:\n","                print(f\"Warning: Alpha weights sum to {alpha_sum:.3f}, expected 1.0\")\n","        else:\n","            self.alpha = None\n","\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","\n","        if self.alpha is not None:\n","            if self.alpha.device != targets.device:\n","                self.alpha = self.alpha.to(targets.device)\n","            alpha_t = self.alpha.gather(0, targets)\n","        else:\n","            alpha_t = 1.0\n","\n","        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","\n","class MobileNetCASME2Baseline(nn.Module):\n","    def __init__(self, num_classes, dropout_rate=0.2, in_channels=1):\n","        super(MobileNetCASME2Baseline, self).__init__()\n","\n","        self.mobilenet = timm.create_model(\n","            MOBILENET_MODEL_NAME,\n","            pretrained=False,\n","            num_classes=0,\n","            global_pool='avg',\n","            in_chans=in_channels\n","        )\n","\n","        for param in self.mobilenet.parameters():\n","            param.requires_grad = True\n","\n","        with torch.no_grad():\n","            test_input = torch.randn(1, in_channels, 224, 224)\n","            test_output = self.mobilenet(test_input)\n","            self.mobilenet_feature_dim = test_output.shape[1]\n","\n","        print(f\"MobileNetV3-Small feature dimension: {self.mobilenet_feature_dim}\")\n","        print(f\"Training from scratch with {in_channels}-channel input\")\n","\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.mobilenet_feature_dim, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(dropout_rate),\n","\n","            nn.Linear(512, 128),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(dropout_rate),\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","        print(f\"MobileNet CASME II: {self.mobilenet_feature_dim} -> 512 -> 128 -> {num_classes}\")\n","        print(f\"Architecture: Pure grayscale (1ch) from scratch\")\n","\n","    def forward(self, x):\n","        features = self.mobilenet(x)\n","        processed_features = self.classifier_layers(features)\n","        output = self.classifier(processed_features)\n","        return output\n","\n","def create_optimizer_scheduler_casme2(model, config):\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=config['learning_rate'],\n","        weight_decay=config['weight_decay'],\n","        betas=(0.9, 0.999)\n","    )\n","\n","    if config['scheduler_type'] == 'plateau':\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer,\n","            mode=config['scheduler_mode'],\n","            factor=config['scheduler_factor'],\n","            patience=config['scheduler_patience'],\n","            min_lr=config['scheduler_min_lr']\n","        )\n","        print(f\"Scheduler: ReduceLROnPlateau monitoring {config['scheduler_monitor']}\")\n","    else:\n","        scheduler = None\n","\n","    return optimizer, scheduler\n","\n","print(\"\\nSetting up transforms for M2 methodology (224x224 pure grayscale)...\")\n","\n","mobilenet_transform_train = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=1),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])\n","])\n","\n","mobilenet_transform_val = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=1),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])\n","])\n","\n","print(\"M2 transforms configured: 224x224 pure grayscale (1 channel) with standardization\")\n","\n","class CASME2Dataset(Dataset):\n","    def __init__(self, dataset_root, split, transform=None):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading {split} dataset from {split_path}...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        loaded_count = 0\n","\n","        for filename in sorted(all_files):\n","            emotion_found = None\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                loaded_count += 1\n","\n","        print(f\"Loaded {len(self.images)} samples for {split} split\")\n","        self._print_distribution()\n","\n","    def _print_distribution(self):\n","        if len(self.labels) == 0:\n","            print(\"No samples to display distribution\")\n","            return\n","\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.images[idx]\n","        image = Image.open(image_path)\n","\n","        if image.mode != 'L':\n","            image = image.convert('L')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/training_logs\", exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/evaluation_results\", exist_ok=True)\n","\n","TRAIN_PATH = DATASET_ROOT\n","VAL_PATH = DATASET_ROOT\n","TEST_PATH = DATASET_ROOT\n","\n","print(f\"\\nDataset paths:\")\n","print(f\"Root: {DATASET_ROOT}\")\n","print(f\"  Train: {TRAIN_PATH}/train\")\n","print(f\"  Validation: {VAL_PATH}/val\")\n","print(f\"  Test: {TEST_PATH}/test\")\n","\n","print(\"\\nMobileNetV3 CASME II architecture validation...\")\n","\n","try:\n","    test_model = MobileNetCASME2Baseline(num_classes=7, dropout_rate=0.2, in_channels=1).to(device)\n","    test_input = torch.randn(1, 1, 224, 224).to(device)\n","    test_output = test_model(test_input)\n","\n","    print(f\"Validation successful: Output shape {test_output.shape}\")\n","    print(f\"Expected output shape: [1, 7] for CASME II 7 classes\")\n","    print(f\"Pure grayscale (1 channel) architecture validated\")\n","\n","    del test_model, test_input, test_output\n","    torch.cuda.empty_cache()\n","\n","except Exception as e:\n","    print(f\"Validation failed: {e}\")\n","\n","def create_criterion_casme2(weights, use_focal_loss=False, alpha_weights=None, gamma=2.0):\n","    if use_focal_loss:\n","        print(f\"Using Optimized Focal Loss with gamma={gamma}\")\n","        if alpha_weights:\n","            print(f\"Per-class alpha weights: {alpha_weights}\")\n","            print(f\"Alpha sum: {sum(alpha_weights):.3f}\")\n","        return OptimizedFocalLoss(alpha=alpha_weights, gamma=gamma)\n","    else:\n","        print(f\"Using CrossEntropy Loss with optimized class weights\")\n","        print(f\"Class weights: {weights.cpu().numpy()}\")\n","        return nn.CrossEntropyLoss(weight=weights)\n","\n","GLOBAL_CONFIG_CASME2 = {\n","    'device': device,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'num_classes': 7,\n","    'class_weights': class_weights,\n","    'class_names': CASME2_CLASSES,\n","    'class_to_idx': CLASS_TO_IDX,\n","    'transform_train': mobilenet_transform_train,\n","    'transform_val': mobilenet_transform_val,\n","    'mobilenet_config': CASME2_MOBILENET_CONFIG,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'results_root': RESULTS_ROOT,\n","    'dataset_root': DATASET_ROOT,\n","    'train_path': TRAIN_PATH,\n","    'val_path': VAL_PATH,\n","    'test_path': TEST_PATH,\n","    'optimizer_scheduler_factory': create_optimizer_scheduler_casme2,\n","    'criterion_factory': create_criterion_casme2\n","}\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"CASME II MOBILENETV3-SMALL M2 MFS-PREP CONFIGURATION COMPLETE\")\n","print(\"=\" * 60)\n","\n","print(f\"Loss Configuration:\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Function: Optimized Focal Loss\")\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Per-class Alpha: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Function: CrossEntropy with Optimized Weights\")\n","    print(f\"  Class Weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","\n","print(f\"\\nModel Configuration:\")\n","print(f\"  Architecture: MobileNetV3-Small\")\n","print(f\"  Parameters: ~2.5M\")\n","print(f\"  Input Resolution: 224x224 Pure Grayscale (1 channel)\")\n","print(f\"  Methodology: M2 (Face-aware preprocessing)\")\n","print(f\"  Training Strategy: From Scratch (No Pretrained Weights)\")\n","print(f\"  First Conv Layer: Modified to 1-channel input\")\n","\n","print(f\"\\nDataset Configuration:\")\n","print(f\"  Version: {CASME2_MOBILENET_CONFIG['dataset_version']}\")\n","print(f\"  Frame strategy: {CASME2_MOBILENET_CONFIG['frame_strategy']}\")\n","print(f\"  Train augmentation: {CASME2_MOBILENET_CONFIG['train_augmentation']}\")\n","print(f\"  Classes: {len(CASME2_CLASSES)}\")\n","print(f\"  Train samples: {preprocessing_info['splits']['train']['total_images']} frames\")\n","\n","print(\"\\nNext: Cell 2 - Dataset Loading and Training Pipeline\")"]},{"cell_type":"code","source":["# @title Cell 2: CASME II MobileNetV3-Small M2 MFS-PREP Training Pipeline\n","\n","# File: 09_01_MobileNet_CASME2_MFS_PREP_Cell2.py\n","# Location: experiments/09_01_MobileNet_CASME2-MFS-PREP.ipynb\n","# Purpose: Training pipeline for pure grayscale MobileNetV3-Small from scratch\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing as mp\n","import shutil\n","import tempfile\n","\n","print(\"CASME II MobileNetV3-Small M2 MFS-PREP Training Pipeline\")\n","print(\"=\" * 70)\n","print(f\"Model: MobileNetV3-Small\")\n","print(f\"Methodology: M2 (Face-aware preprocessing)\")\n","print(f\"Input: 224x224 Pure Grayscale (1 channel)\")\n","print(f\"Training: From Scratch (No Pretrained Weights)\")\n","print(f\"Loss Function: Focal Loss\")\n","print(f\"  Gamma: {CASME2_MOBILENET_CONFIG['focal_loss_gamma']}\")\n","print(f\"  Per-class Alpha: {CASME2_MOBILENET_CONFIG['focal_loss_alpha_weights']}\")\n","print(f\"  Alpha Sum: {sum(CASME2_MOBILENET_CONFIG['focal_loss_alpha_weights']):.3f}\")\n","print(f\"Dataset Version: {CASME2_MOBILENET_CONFIG['dataset_version']}\")\n","print(f\"Frame Strategy: {CASME2_MOBILENET_CONFIG['frame_strategy']}\")\n","print(f\"Training epochs: {CASME2_MOBILENET_CONFIG['num_epochs']}\")\n","print(f\"Scheduler patience: {CASME2_MOBILENET_CONFIG['scheduler_patience']}\")\n","\n","class CASME2DatasetTraining(Dataset):\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","        self.cached_images = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading CASME II {split} dataset for training...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","\n","        for filename in sorted(all_files):\n","            emotion_found = None\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples\")\n","        self._print_distribution()\n","\n","        if self.use_ram_cache:\n","            self._preload_to_ram()\n","\n","    def _print_distribution(self):\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def _preload_to_ram(self):\n","        print(f\"Preloading {len(self.images)} {self.split} images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            try:\n","                image = Image.open(img_path)\n","                if image.mode != 'L':\n","                    image = image.convert('L')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('L', (224, 224), 128), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=f\"Loading {self.split} to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * 224 * 224 * 1 * 4 / 1e9\n","        print(f\"{self.split.upper()} RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx])\n","                if image.mode != 'L':\n","                    image = image.convert('L')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('L', (224, 224), 128)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","def calculate_metrics_safe_robust(outputs, labels, class_names, average='macro'):\n","    try:\n","        if outputs.size(0) != labels.size(0):\n","            raise ValueError(f\"Batch size mismatch: outputs {outputs.size(0)} vs labels {labels.size(0)}\")\n","\n","        if isinstance(outputs, torch.Tensor):\n","            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n","        else:\n","            predictions = np.array(outputs)\n","\n","        if isinstance(labels, torch.Tensor):\n","            labels = labels.cpu().numpy()\n","        else:\n","            labels = np.array(labels)\n","\n","        accuracy = accuracy_score(labels, predictions)\n","        precision, recall, f1, _ = precision_recall_fscore_support(\n","            labels, predictions,\n","            average=average,\n","            zero_division=0,\n","            labels=list(range(len(class_names)))\n","        )\n","\n","        return {\n","            'accuracy': float(accuracy),\n","            'precision': float(precision),\n","            'recall': float(recall),\n","            'f1_score': float(f1)\n","        }\n","    except Exception as e:\n","        print(f\"Warning: Metrics calculation error: {e}\")\n","        return {\n","            'accuracy': 0.0,\n","            'precision': 0.0,\n","            'recall': 0.0,\n","            'f1_score': 0.0\n","        }\n","\n","def train_epoch(model, dataloader, criterion, optimizer, device, epoch, total_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    all_outputs = []\n","    all_labels = []\n","\n","    progress_bar = tqdm(dataloader, desc=f\"CASME II Training Epoch {epoch+1}/{total_epochs}\")\n","\n","    for batch_idx, (images, labels, filenames) in enumerate(progress_bar):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        model_output = model(images)\n","\n","        if isinstance(model_output, (tuple, list)):\n","            outputs = model_output[0]\n","        elif isinstance(model_output, dict):\n","            outputs = model_output.get('logits', model_output.get('prediction', model_output))\n","        else:\n","            outputs = model_output\n","\n","        if outputs.dim() != 2 or outputs.size(1) != 7:\n","            raise ValueError(f\"Invalid CASME II output shape: {outputs.shape}, expected [batch_size, 7]\")\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), CASME2_MOBILENET_CONFIG['gradient_clip'])\n","\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        all_outputs.append(outputs.detach().cpu())\n","        all_labels.append(labels.detach().cpu())\n","\n","        if batch_idx % 5 == 0:\n","            avg_loss = running_loss / (batch_idx + 1)\n","            current_lr = optimizer.param_groups[0]['lr']\n","            progress_bar.set_postfix({\n","                'Loss': f'{avg_loss:.4f}',\n","                'LR': f'{current_lr:.2e}'\n","            })\n","\n","    try:\n","        epoch_outputs = torch.cat(all_outputs, dim=0)\n","        epoch_labels = torch.cat(all_labels, dim=0)\n","        metrics = calculate_metrics_safe_robust(epoch_outputs, epoch_labels, CASME2_CLASSES, average='macro')\n","    except Exception as e:\n","        print(f\"Warning: Training metrics calculation failed: {e}\")\n","        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n","\n","    avg_loss = running_loss / len(dataloader)\n","    return avg_loss, metrics\n","\n","def validate_epoch(model, dataloader, criterion, device, epoch, total_epochs):\n","    model.eval()\n","    running_loss = 0.0\n","    all_outputs = []\n","    all_labels = []\n","    all_filenames = []\n","\n","    with torch.no_grad():\n","        progress_bar = tqdm(dataloader, desc=f\"CASME II Validation Epoch {epoch+1}/{total_epochs}\")\n","\n","        for batch_idx, (images, labels, filenames) in enumerate(progress_bar):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            model_output = model(images)\n","\n","            if isinstance(model_output, (tuple, list)):\n","                outputs = model_output[0]\n","            elif isinstance(model_output, dict):\n","                outputs = model_output.get('logits', model_output.get('prediction', model_output))\n","            else:\n","                outputs = model_output\n","\n","            loss = criterion(outputs, labels)\n","            running_loss += loss.item()\n","\n","            all_outputs.append(outputs.detach().cpu())\n","            all_labels.append(labels.detach().cpu())\n","            all_filenames.extend(filenames)\n","\n","            if batch_idx % 3 == 0:\n","                avg_loss = running_loss / (batch_idx + 1)\n","                progress_bar.set_postfix({'Val Loss': f'{avg_loss:.4f}'})\n","\n","    try:\n","        epoch_outputs = torch.cat(all_outputs, dim=0)\n","        epoch_labels = torch.cat(all_labels, dim=0)\n","        metrics = calculate_metrics_safe_robust(epoch_outputs, epoch_labels, CASME2_CLASSES, average='macro')\n","    except Exception as e:\n","        print(f\"Warning: Validation metrics calculation failed: {e}\")\n","        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n","\n","    avg_loss = running_loss / len(dataloader)\n","    return avg_loss, metrics, all_filenames\n","\n","def save_checkpoint_robust(model, optimizer, scheduler, epoch, train_metrics, val_metrics,\n","                         checkpoint_dir, best_metrics, config, max_retries=3):\n","    def make_serializable_cpu(obj):\n","        if isinstance(obj, torch.Tensor):\n","            cpu_obj = obj.detach().cpu()\n","            return cpu_obj.item() if cpu_obj.numel() == 1 else cpu_obj.tolist()\n","        elif isinstance(obj, np.ndarray):\n","            return obj.tolist()\n","        elif isinstance(obj, np.integer):\n","            return int(obj)\n","        elif isinstance(obj, np.floating):\n","            return float(obj)\n","        elif isinstance(obj, dict):\n","            return {k: make_serializable_cpu(v) for k, v in obj.items()}\n","        elif isinstance(obj, (list, tuple)):\n","            return [make_serializable_cpu(item) for item in obj]\n","        else:\n","            return obj\n","\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","        'train_metrics': make_serializable_cpu(train_metrics),\n","        'val_metrics': make_serializable_cpu(val_metrics),\n","        'casme2_config': make_serializable_cpu(config),\n","        'best_f1': float(best_metrics['f1']),\n","        'best_loss': float(best_metrics['loss']),\n","        'best_acc': float(best_metrics['accuracy']),\n","        'class_names': CASME2_CLASSES,\n","        'num_classes': 7\n","    }\n","\n","    final_path = f\"{checkpoint_dir}/casme2_mobilenet_mfs_prep_best_f1.pth\"\n","\n","    for attempt in range(max_retries):\n","        try:\n","            temp_fd, temp_path = tempfile.mkstemp(dir=checkpoint_dir, suffix='.pth.tmp')\n","            os.close(temp_fd)\n","\n","            print(f\"Attempt {attempt + 1}: Saving checkpoint to temporary file...\")\n","            torch.save(checkpoint, temp_path)\n","\n","            print(\"Validating checkpoint integrity...\")\n","            validation_checkpoint = torch.load(temp_path, map_location='cpu')\n","\n","            required_keys = ['model_state_dict', 'epoch', 'best_f1', 'num_classes']\n","            for key in required_keys:\n","                if key not in validation_checkpoint:\n","                    raise ValueError(f\"Checkpoint validation failed: missing key '{key}'\")\n","\n","            if validation_checkpoint['epoch'] != epoch:\n","                raise ValueError(f\"Checkpoint epoch mismatch: saved {epoch}, loaded {validation_checkpoint['epoch']}\")\n","\n","            print(\"Checkpoint validation passed\")\n","\n","            print(f\"Moving validated checkpoint to final location...\")\n","            shutil.move(temp_path, final_path)\n","\n","            print(f\"Checkpoint saved and validated successfully: {os.path.basename(final_path)}\")\n","            print(f\"  Epoch: {epoch + 1}\")\n","            print(f\"  Val F1: {best_metrics['f1']:.4f}\")\n","            print(f\"  Val Loss: {best_metrics['loss']:.4f}\")\n","            print(f\"  Val Acc: {best_metrics['accuracy']:.4f}\")\n","\n","            return final_path\n","\n","        except Exception as e:\n","            print(f\"Checkpoint save attempt {attempt + 1}/{max_retries} failed: {e}\")\n","\n","            if os.path.exists(temp_path):\n","                try:\n","                    os.remove(temp_path)\n","                except:\n","                    pass\n","\n","            if attempt < max_retries - 1:\n","                wait_time = 2 ** attempt\n","                print(f\"Retrying in {wait_time} seconds...\")\n","                time.sleep(wait_time)\n","            else:\n","                print(f\"All {max_retries} checkpoint save attempts failed\")\n","                return None\n","\n","    return None\n","\n","def safe_json_serialize(obj):\n","    if isinstance(obj, torch.Tensor):\n","        return obj.cpu().item() if obj.numel() == 1 else obj.cpu().numpy().tolist()\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    elif isinstance(obj, (np.integer, np.int32, np.int64)):\n","        return int(obj)\n","    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n","        return float(obj)\n","    elif isinstance(obj, dict):\n","        return {k: safe_json_serialize(v) for k, v in obj.items()}\n","    elif isinstance(obj, (list, tuple)):\n","        return [safe_json_serialize(item) for item in obj]\n","    elif hasattr(obj, '__dict__'):\n","        return safe_json_serialize(obj.__dict__)\n","    else:\n","        try:\n","            return float(obj) if isinstance(obj, (int, float)) else str(obj)\n","        except:\n","            return str(obj)\n","\n","print(\"\\nCreating CASME II MobileNetV3-Small M2 training datasets...\")\n","\n","train_dataset = CASME2DatasetTraining(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='train',\n","    transform=GLOBAL_CONFIG_CASME2['transform_train'],\n","    use_ram_cache=True\n",")\n","\n","val_dataset = CASME2DatasetTraining(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='val',\n","    transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","    use_ram_cache=True\n",")\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=CASME2_MOBILENET_CONFIG['batch_size'],\n","    shuffle=True,\n","    num_workers=CASME2_MOBILENET_CONFIG['num_workers'],\n","    pin_memory=True,\n","    prefetch_factor=2\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=CASME2_MOBILENET_CONFIG['batch_size'],\n","    shuffle=False,\n","    num_workers=CASME2_MOBILENET_CONFIG['num_workers'],\n","    pin_memory=True,\n","    prefetch_factor=2\n",")\n","\n","print(f\"Training batches: {len(train_loader)} (samples: {len(train_dataset)})\")\n","print(f\"Validation batches: {len(val_loader)} (samples: {len(val_dataset)})\")\n","\n","print(\"\\nInitializing CASME II MobileNetV3-Small model...\")\n","model = MobileNetCASME2Baseline(\n","    num_classes=GLOBAL_CONFIG_CASME2['num_classes'],\n","    dropout_rate=CASME2_MOBILENET_CONFIG['dropout_rate'],\n","    in_channels=1\n",").to(GLOBAL_CONFIG_CASME2['device'])\n","\n","if CASME2_MOBILENET_CONFIG['use_focal_loss']:\n","    criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","        weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","        use_focal_loss=True,\n","        alpha_weights=CASME2_MOBILENET_CONFIG['focal_loss_alpha_weights'],\n","        gamma=CASME2_MOBILENET_CONFIG['focal_loss_gamma']\n","    )\n","else:\n","    criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","        weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","        use_focal_loss=False,\n","        alpha_weights=None,\n","        gamma=2.0\n","    )\n","\n","optimizer, scheduler = GLOBAL_CONFIG_CASME2['optimizer_scheduler_factory'](\n","    model, CASME2_MOBILENET_CONFIG\n",")\n","\n","print(f\"Optimizer: AdamW (LR={CASME2_MOBILENET_CONFIG['learning_rate']})\")\n","print(f\"Scheduler: ReduceLROnPlateau (patience={CASME2_MOBILENET_CONFIG['scheduler_patience']})\")\n","print(f\"Criterion: Optimized Focal Loss\")\n","print(f\"Training: From Scratch (No Pretrained Weights)\")\n","\n","training_history = {\n","    'train_loss': [],\n","    'val_loss': [],\n","    'train_f1': [],\n","    'val_f1': [],\n","    'train_acc': [],\n","    'val_acc': [],\n","    'learning_rate': [],\n","    'epoch_time': []\n","}\n","\n","best_metrics = {\n","    'f1': 0.0,\n","    'loss': float('inf'),\n","    'accuracy': 0.0,\n","    'epoch': 0\n","}\n","\n","print(\"\\nStarting CASME II MobileNetV3-Small M2 training...\")\n","print(f\"Training configuration: {CASME2_MOBILENET_CONFIG['num_epochs']} epochs\")\n","print(f\"Input resolution: 224x224 Pure Grayscale (1 channel)\")\n","print(f\"Training strategy: From Scratch\")\n","print(\"=\" * 70)\n","\n","start_time = time.time()\n","\n","for epoch in range(CASME2_MOBILENET_CONFIG['num_epochs']):\n","    epoch_start_time = time.time()\n","    print(f\"\\nEpoch {epoch+1}/{CASME2_MOBILENET_CONFIG['num_epochs']}\")\n","\n","    train_loss, train_metrics = train_epoch(\n","        model, train_loader, criterion, optimizer,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_MOBILENET_CONFIG['num_epochs']\n","    )\n","\n","    val_loss, val_metrics, val_filenames = validate_epoch(\n","        model, val_loader, criterion,\n","        GLOBAL_CONFIG_CASME2['device'], epoch, CASME2_MOBILENET_CONFIG['num_epochs']\n","    )\n","\n","    if scheduler:\n","        scheduler.step(val_metrics['f1_score'])\n","\n","    epoch_time = time.time() - epoch_start_time\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    training_history['train_loss'].append(float(train_loss))\n","    training_history['val_loss'].append(float(val_loss))\n","    training_history['train_f1'].append(float(train_metrics['f1_score']))\n","    training_history['val_f1'].append(float(val_metrics['f1_score']))\n","    training_history['train_acc'].append(float(train_metrics['accuracy']))\n","    training_history['val_acc'].append(float(val_metrics['accuracy']))\n","    training_history['learning_rate'].append(float(current_lr))\n","    training_history['epoch_time'].append(float(epoch_time))\n","\n","    print(f\"Train - Loss: {train_loss:.4f}, F1: {train_metrics['f1_score']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n","    print(f\"Val   - Loss: {val_loss:.4f}, F1: {val_metrics['f1_score']:.4f}, Acc: {val_metrics['accuracy']:.4f}\")\n","    print(f\"Time  - Epoch: {epoch_time:.1f}s, LR: {current_lr:.2e}\")\n","\n","    save_model = False\n","    improvement_reason = \"\"\n","\n","    if val_metrics['f1_score'] > best_metrics['f1']:\n","        save_model = True\n","        improvement_reason = \"Higher F1\"\n","    elif val_metrics['f1_score'] == best_metrics['f1']:\n","        if val_loss < best_metrics['loss']:\n","            save_model = True\n","            improvement_reason = \"Same F1, Lower Loss\"\n","        elif val_loss == best_metrics['loss'] and val_metrics['accuracy'] > best_metrics['accuracy']:\n","            save_model = True\n","            improvement_reason = \"Same F1&Loss, Higher Accuracy\"\n","\n","    if save_model:\n","        best_metrics['f1'] = val_metrics['f1_score']\n","        best_metrics['loss'] = val_loss\n","        best_metrics['accuracy'] = val_metrics['accuracy']\n","        best_metrics['epoch'] = epoch + 1\n","\n","        best_model_path = save_checkpoint_robust(\n","            model, optimizer, scheduler, epoch,\n","            train_metrics, val_metrics, GLOBAL_CONFIG_CASME2['checkpoint_root'],\n","            best_metrics, CASME2_MOBILENET_CONFIG\n","        )\n","\n","        if best_model_path:\n","            print(f\"New best model: {improvement_reason} - F1: {best_metrics['f1']:.4f}\")\n","        else:\n","            print(f\"Warning: Failed to save checkpoint despite improvement\")\n","\n","    elapsed_time = time.time() - start_time\n","    estimated_total = (elapsed_time / (epoch + 1)) * CASME2_MOBILENET_CONFIG['num_epochs']\n","    remaining_time = estimated_total - elapsed_time\n","    progress_pct = ((epoch + 1) / CASME2_MOBILENET_CONFIG['num_epochs']) * 100\n","\n","    print(f\"Progress: {progress_pct:.1f}% | Best F1: {best_metrics['f1']:.4f} | ETA: {remaining_time/60:.1f}min\")\n","\n","total_time = time.time() - start_time\n","actual_epochs = CASME2_MOBILENET_CONFIG['num_epochs']\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II MOBILENETV3-SMALL M2 MFS-PREP TRAINING COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Training time: {total_time/60:.1f} minutes\")\n","print(f\"Epochs completed: {actual_epochs}\")\n","print(f\"Best validation F1: {best_metrics['f1']:.4f} (epoch {best_metrics['epoch']})\")\n","print(f\"Final train F1: {training_history['train_f1'][-1]:.4f}\")\n","print(f\"Final validation F1: {training_history['val_f1'][-1]:.4f}\")\n","\n","results_dir = GLOBAL_CONFIG_CASME2['results_root']\n","os.makedirs(f\"{results_dir}/training_logs\", exist_ok=True)\n","\n","training_history_path = f\"{results_dir}/training_logs/casme2_mobilenet_mfs_prep_training_history.json\"\n","\n","print(\"\\nExporting training documentation...\")\n","\n","try:\n","    training_summary = {\n","        'experiment_type': 'CASME2_MobileNetV3Small_MFS_PREP_Baseline',\n","        'experiment_configuration': {\n","            'model_architecture': 'MobileNetV3-Small',\n","            'model_parameters': '2.5M',\n","            'dataset_version': CASME2_MOBILENET_CONFIG['dataset_version'],\n","            'methodology': CASME2_MOBILENET_CONFIG['methodology'],\n","            'preprocessing': CASME2_MOBILENET_CONFIG['preprocessing'],\n","            'input_resolution': '224x224 Pure Grayscale (1 channel)',\n","            'training_strategy': CASME2_MOBILENET_CONFIG['training_strategy'],\n","            'use_pretrained_weights': CASME2_MOBILENET_CONFIG['use_pretrained_weights'],\n","            'frame_strategy': CASME2_MOBILENET_CONFIG['frame_strategy'],\n","            'train_augmentation': CASME2_MOBILENET_CONFIG['train_augmentation'],\n","            'loss_function': 'Optimized Focal Loss',\n","            'focal_loss_gamma': CASME2_MOBILENET_CONFIG['focal_loss_gamma'],\n","            'focal_loss_alpha_weights': CASME2_MOBILENET_CONFIG['focal_loss_alpha_weights'],\n","            'model_name': CASME2_MOBILENET_CONFIG['model_name']\n","        },\n","        'training_history': safe_json_serialize(training_history),\n","        'best_val_f1': float(best_metrics['f1']),\n","        'best_val_loss': float(best_metrics['loss']),\n","        'best_val_accuracy': float(best_metrics['accuracy']),\n","        'best_epoch': int(best_metrics['epoch']),\n","        'total_epochs': int(actual_epochs),\n","        'total_time_minutes': float(total_time / 60),\n","        'average_epoch_time_seconds': float(np.mean(training_history['epoch_time'])),\n","        'config': safe_json_serialize(CASME2_MOBILENET_CONFIG),\n","        'final_train_f1': float(training_history['train_f1'][-1]),\n","        'final_val_f1': float(training_history['val_f1'][-1]),\n","        'model_checkpoint': 'casme2_mobilenet_mfs_prep_best_f1.pth',\n","        'dataset_info': {\n","            'name': 'CASME_II',\n","            'version': CASME2_MOBILENET_CONFIG['dataset_version'],\n","            'methodology': CASME2_MOBILENET_CONFIG['methodology'],\n","            'input_resolution': '224x224 Pure Grayscale (1 channel)',\n","            'frame_strategy': CASME2_MOBILENET_CONFIG['frame_strategy'],\n","            'train_augmentation': CASME2_MOBILENET_CONFIG['train_augmentation'],\n","            'train_samples': len(train_dataset),\n","            'val_samples': len(val_dataset),\n","            'num_classes': 7,\n","            'class_names': CASME2_CLASSES\n","        },\n","        'architecture_info': {\n","            'model_type': 'MobileNetCASME2Baseline',\n","            'backbone': CASME2_MOBILENET_CONFIG['model_name'],\n","            'input_size': '224x224 Pure Grayscale (1 channel)',\n","            'classification_head': '576->512->128->7',\n","            'input_channels': 1,\n","            'training_strategy': 'from_scratch'\n","        }\n","    }\n","\n","    with open(training_history_path, 'w') as f:\n","        json.dump(training_summary, f, indent=2)\n","\n","    print(f\"Training documentation saved successfully: {training_history_path}\")\n","    print(f\"Model: {training_summary['experiment_configuration']['model_architecture']}\")\n","    print(f\"Methodology: {training_summary['experiment_configuration']['methodology']}\")\n","    print(f\"Input resolution: {training_summary['experiment_configuration']['input_resolution']}\")\n","    print(f\"Training strategy: {training_summary['experiment_configuration']['training_strategy']}\")\n","\n","except Exception as e:\n","    print(f\"Warning: Could not save training documentation: {e}\")\n","    print(\"Training completed successfully, but documentation export failed\")\n","\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\nNext: Cell 3 - CASME II MobileNetV3-Small M2 Evaluation\")\n","print(\"Training pipeline completed successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"2onklKSFE4Za","executionInfo":{"status":"ok","timestamp":1764311239177,"user_tz":-420,"elapsed":345810,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"5df63dce-8d56-4cbd-bf7d-8af5f3593c46"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II MobileNetV3-Small M2 MFS-PREP Training Pipeline\n","======================================================================\n","Model: MobileNetV3-Small\n","Methodology: M2 (Face-aware preprocessing)\n","Input: 224x224 Pure Grayscale (1 channel)\n","Training: From Scratch (No Pretrained Weights)\n","Loss Function: Focal Loss\n","  Gamma: 2.5\n","  Per-class Alpha: [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","  Alpha Sum: 0.999\n","Dataset Version: v9\n","Frame Strategy: multi_frame_sampling\n","Training epochs: 50\n","Scheduler patience: 5\n","\n","Creating CASME II MobileNetV3-Small M2 training datasets...\n","Loading CASME II train dataset for training...\n","Loaded 2613 CASME II train samples\n","  others: 1027 samples (39.3%)\n","  disgust: 650 samples (24.9%)\n","  happiness: 325 samples (12.4%)\n","  repression: 273 samples (10.4%)\n","  surprise: 260 samples (10.0%)\n","  sadness: 65 samples (2.5%)\n","  fear: 13 samples (0.5%)\n","Preloading 2613 train images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading train to RAM: 100%|██████████| 2613/2613 [00:39<00:00, 65.85it/s] \n"]},{"output_type":"stream","name":"stdout","text":["TRAIN RAM caching completed: 2613/2613 images, ~0.52GB\n","Loading CASME II val dataset for training...\n","Loaded 78 CASME II val samples\n","  others: 30 samples (38.5%)\n","  disgust: 18 samples (23.1%)\n","  happiness: 9 samples (11.5%)\n","  repression: 9 samples (11.5%)\n","  surprise: 6 samples (7.7%)\n","  sadness: 3 samples (3.8%)\n","  fear: 3 samples (3.8%)\n","Preloading 78 val images to RAM with 32 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading val to RAM: 100%|██████████| 78/78 [00:01<00:00, 54.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["VAL RAM caching completed: 78/78 images, ~0.02GB\n","Training batches: 164 (samples: 2613)\n","Validation batches: 5 (samples: 78)\n","\n","Initializing CASME II MobileNetV3-Small model...\n","MobileNetV3-Small feature dimension: 1024\n","Training from scratch with 1-channel input\n","MobileNet CASME II: 1024 -> 512 -> 128 -> 7\n","Architecture: Pure grayscale (1ch) from scratch\n","Using Optimized Focal Loss with gamma=2.5\n","Per-class alpha weights: [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","Alpha sum: 0.999\n","Scheduler: ReduceLROnPlateau monitoring val_f1_macro\n","Optimizer: AdamW (LR=5e-05)\n","Scheduler: ReduceLROnPlateau (patience=5)\n","Criterion: Optimized Focal Loss\n","Training: From Scratch (No Pretrained Weights)\n","\n","Starting CASME II MobileNetV3-Small M2 training...\n","Training configuration: 50 epochs\n","Input resolution: 224x224 Pure Grayscale (1 channel)\n","Training strategy: From Scratch\n","======================================================================\n","\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 1/50: 100%|██████████| 164/164 [00:32<00:00,  5.07it/s, Loss=0.0904, LR=5.00e-05]\n","CASME II Validation Epoch 1/50: 100%|██████████| 5/5 [00:13<00:00,  2.71s/it, Val Loss=0.1270]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0900, F1: 0.2384, Acc: 0.2962\n","Val   - Loss: 0.1224, F1: 0.1967, Acc: 0.2949\n","Time  - Epoch: 45.9s, LR: 5.00e-05\n","Attempt 1: Saving checkpoint to temporary file...\n","Validating checkpoint integrity...\n","Checkpoint validation passed\n","Moving validated checkpoint to final location...\n","Checkpoint saved and validated successfully: casme2_mobilenet_mfs_prep_best_f1.pth\n","  Epoch: 1\n","  Val F1: 0.1967\n","  Val Loss: 0.1224\n","  Val Acc: 0.2949\n","New best model: Higher F1 - F1: 0.1967\n","Progress: 2.0% | Best F1: 0.1967 | ETA: 37.7min\n","\n","Epoch 2/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 2/50: 100%|██████████| 164/164 [00:04<00:00, 33.89it/s, Loss=0.0603, LR=5.00e-05]\n","CASME II Validation Epoch 2/50: 100%|██████████| 5/5 [00:00<00:00,  9.50it/s, Val Loss=0.1455]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0601, F1: 0.4515, Acc: 0.5442\n","Val   - Loss: 0.1396, F1: 0.1248, Acc: 0.1795\n","Time  - Epoch: 5.4s, LR: 5.00e-05\n","Progress: 4.0% | Best F1: 0.1967 | ETA: 20.6min\n","\n","Epoch 3/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 3/50: 100%|██████████| 164/164 [00:04<00:00, 35.17it/s, Loss=0.0377, LR=5.00e-05]\n","CASME II Validation Epoch 3/50: 100%|██████████| 5/5 [00:00<00:00, 10.46it/s, Val Loss=0.1534]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0377, F1: 0.6628, Acc: 0.7202\n","Val   - Loss: 0.1453, F1: 0.1435, Acc: 0.2564\n","Time  - Epoch: 5.2s, LR: 5.00e-05\n","Progress: 6.0% | Best F1: 0.1967 | ETA: 14.8min\n","\n","Epoch 4/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 4/50: 100%|██████████| 164/164 [00:04<00:00, 34.15it/s, Loss=0.0219, LR=5.00e-05]\n","CASME II Validation Epoch 4/50: 100%|██████████| 5/5 [00:00<00:00, 10.55it/s, Val Loss=0.1648]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0219, F1: 0.8186, Acc: 0.8305\n","Val   - Loss: 0.1558, F1: 0.1251, Acc: 0.1923\n","Time  - Epoch: 5.3s, LR: 5.00e-05\n","Progress: 8.0% | Best F1: 0.1967 | ETA: 11.9min\n","\n","Epoch 5/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 5/50: 100%|██████████| 164/164 [00:04<00:00, 34.66it/s, Loss=0.0143, LR=5.00e-05]\n","CASME II Validation Epoch 5/50: 100%|██████████| 5/5 [00:00<00:00, 10.17it/s, Val Loss=0.1678]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0143, F1: 0.9051, Acc: 0.8990\n","Val   - Loss: 0.1602, F1: 0.1484, Acc: 0.2436\n","Time  - Epoch: 5.2s, LR: 5.00e-05\n","Progress: 10.0% | Best F1: 0.1967 | ETA: 10.1min\n","\n","Epoch 6/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 6/50: 100%|██████████| 164/164 [00:04<00:00, 34.66it/s, Loss=0.0100, LR=5.00e-05]\n","CASME II Validation Epoch 6/50: 100%|██████████| 5/5 [00:00<00:00,  9.47it/s, Val Loss=0.1713]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0100, F1: 0.9251, Acc: 0.9334\n","Val   - Loss: 0.1646, F1: 0.1369, Acc: 0.2179\n","Time  - Epoch: 5.3s, LR: 5.00e-05\n","Progress: 12.0% | Best F1: 0.1967 | ETA: 8.9min\n","\n","Epoch 7/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 7/50: 100%|██████████| 164/164 [00:04<00:00, 34.40it/s, Loss=0.0073, LR=5.00e-05]\n","CASME II Validation Epoch 7/50: 100%|██████████| 5/5 [00:00<00:00, 10.75it/s, Val Loss=0.1751]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0075, F1: 0.9503, Acc: 0.9480\n","Val   - Loss: 0.1693, F1: 0.1562, Acc: 0.2436\n","Time  - Epoch: 5.2s, LR: 2.50e-05\n","Progress: 14.0% | Best F1: 0.1967 | ETA: 8.0min\n","\n","Epoch 8/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 8/50: 100%|██████████| 164/164 [00:04<00:00, 34.64it/s, Loss=0.0058, LR=2.50e-05]\n","CASME II Validation Epoch 8/50: 100%|██████████| 5/5 [00:00<00:00,  9.90it/s, Val Loss=0.1691]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0059, F1: 0.9587, Acc: 0.9640\n","Val   - Loss: 0.1655, F1: 0.1896, Acc: 0.2949\n","Time  - Epoch: 5.3s, LR: 2.50e-05\n","Progress: 16.0% | Best F1: 0.1967 | ETA: 7.3min\n","\n","Epoch 9/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 9/50: 100%|██████████| 164/164 [00:04<00:00, 34.22it/s, Loss=0.0046, LR=2.50e-05]\n","CASME II Validation Epoch 9/50: 100%|██████████| 5/5 [00:00<00:00,  9.90it/s, Val Loss=0.1755]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0046, F1: 0.9760, Acc: 0.9713\n","Val   - Loss: 0.1704, F1: 0.1867, Acc: 0.2949\n","Time  - Epoch: 5.3s, LR: 2.50e-05\n","Progress: 18.0% | Best F1: 0.1967 | ETA: 6.7min\n","\n","Epoch 10/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 10/50: 100%|██████████| 164/164 [00:04<00:00, 34.38it/s, Loss=0.0042, LR=2.50e-05]\n","CASME II Validation Epoch 10/50: 100%|██████████| 5/5 [00:00<00:00, 10.70it/s, Val Loss=0.1708]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0047, F1: 0.9664, Acc: 0.9698\n","Val   - Loss: 0.1657, F1: 0.1906, Acc: 0.3333\n","Time  - Epoch: 5.3s, LR: 2.50e-05\n","Progress: 20.0% | Best F1: 0.1967 | ETA: 6.2min\n","\n","Epoch 11/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 11/50: 100%|██████████| 164/164 [00:04<00:00, 33.33it/s, Loss=0.0032, LR=2.50e-05]\n","CASME II Validation Epoch 11/50: 100%|██████████| 5/5 [00:00<00:00, 10.85it/s, Val Loss=0.1760]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0032, F1: 0.9831, Acc: 0.9816\n","Val   - Loss: 0.1721, F1: 0.1592, Acc: 0.2436\n","Time  - Epoch: 5.4s, LR: 2.50e-05\n","Progress: 22.0% | Best F1: 0.1967 | ETA: 5.8min\n","\n","Epoch 12/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 12/50: 100%|██████████| 164/164 [00:04<00:00, 34.37it/s, Loss=0.0034, LR=2.50e-05]\n","CASME II Validation Epoch 12/50: 100%|██████████| 5/5 [00:00<00:00,  9.79it/s, Val Loss=0.1732]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0034, F1: 0.9839, Acc: 0.9812\n","Val   - Loss: 0.1699, F1: 0.2209, Acc: 0.3333\n","Time  - Epoch: 5.3s, LR: 2.50e-05\n","Attempt 1: Saving checkpoint to temporary file...\n","Validating checkpoint integrity...\n","Checkpoint validation passed\n","Moving validated checkpoint to final location...\n","Checkpoint saved and validated successfully: casme2_mobilenet_mfs_prep_best_f1.pth\n","  Epoch: 12\n","  Val F1: 0.2209\n","  Val Loss: 0.1699\n","  Val Acc: 0.3333\n","New best model: Higher F1 - F1: 0.2209\n","Progress: 24.0% | Best F1: 0.2209 | ETA: 5.5min\n","\n","Epoch 13/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 13/50: 100%|██████████| 164/164 [00:04<00:00, 34.25it/s, Loss=0.0028, LR=2.50e-05]\n","CASME II Validation Epoch 13/50: 100%|██████████| 5/5 [00:00<00:00,  9.54it/s, Val Loss=0.1765]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0028, F1: 0.9854, Acc: 0.9824\n","Val   - Loss: 0.1710, F1: 0.1629, Acc: 0.2949\n","Time  - Epoch: 5.3s, LR: 2.50e-05\n","Progress: 26.0% | Best F1: 0.2209 | ETA: 5.2min\n","\n","Epoch 14/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 14/50: 100%|██████████| 164/164 [00:04<00:00, 35.01it/s, Loss=0.0027, LR=2.50e-05]\n","CASME II Validation Epoch 14/50: 100%|██████████| 5/5 [00:00<00:00, 10.65it/s, Val Loss=0.1721]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0028, F1: 0.9828, Acc: 0.9824\n","Val   - Loss: 0.1683, F1: 0.1435, Acc: 0.2436\n","Time  - Epoch: 5.2s, LR: 2.50e-05\n","Progress: 28.0% | Best F1: 0.2209 | ETA: 4.9min\n","\n","Epoch 15/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 15/50: 100%|██████████| 164/164 [00:04<00:00, 34.52it/s, Loss=0.0025, LR=2.50e-05]\n","CASME II Validation Epoch 15/50: 100%|██████████| 5/5 [00:00<00:00,  9.81it/s, Val Loss=0.1761]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0026, F1: 0.9847, Acc: 0.9843\n","Val   - Loss: 0.1724, F1: 0.1917, Acc: 0.3333\n","Time  - Epoch: 5.3s, LR: 2.50e-05\n","Progress: 30.0% | Best F1: 0.2209 | ETA: 4.7min\n","\n","Epoch 16/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 16/50: 100%|██████████| 164/164 [00:04<00:00, 33.48it/s, Loss=0.0022, LR=2.50e-05]\n","CASME II Validation Epoch 16/50: 100%|██████████| 5/5 [00:00<00:00,  9.54it/s, Val Loss=0.1799]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0022, F1: 0.9910, Acc: 0.9878\n","Val   - Loss: 0.1754, F1: 0.2140, Acc: 0.3462\n","Time  - Epoch: 5.4s, LR: 2.50e-05\n","Progress: 32.0% | Best F1: 0.2209 | ETA: 4.5min\n","\n","Epoch 17/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 17/50: 100%|██████████| 164/164 [00:04<00:00, 35.59it/s, Loss=0.0023, LR=2.50e-05]\n","CASME II Validation Epoch 17/50: 100%|██████████| 5/5 [00:00<00:00,  9.78it/s, Val Loss=0.1883]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0023, F1: 0.9852, Acc: 0.9812\n","Val   - Loss: 0.1821, F1: 0.1866, Acc: 0.3077\n","Time  - Epoch: 5.1s, LR: 2.50e-05\n","Progress: 34.0% | Best F1: 0.2209 | ETA: 4.2min\n","\n","Epoch 18/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 18/50: 100%|██████████| 164/164 [00:04<00:00, 33.92it/s, Loss=0.0018, LR=2.50e-05]\n","CASME II Validation Epoch 18/50: 100%|██████████| 5/5 [00:00<00:00,  9.61it/s, Val Loss=0.1822]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0017, F1: 0.9907, Acc: 0.9881\n","Val   - Loss: 0.1773, F1: 0.1862, Acc: 0.3333\n","Time  - Epoch: 5.4s, LR: 1.25e-05\n","Progress: 36.0% | Best F1: 0.2209 | ETA: 4.0min\n","\n","Epoch 19/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 19/50: 100%|██████████| 164/164 [00:04<00:00, 35.83it/s, Loss=0.0013, LR=1.25e-05]\n","CASME II Validation Epoch 19/50: 100%|██████████| 5/5 [00:00<00:00, 10.73it/s, Val Loss=0.1871]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0013, F1: 0.9965, Acc: 0.9954\n","Val   - Loss: 0.1807, F1: 0.1934, Acc: 0.3333\n","Time  - Epoch: 5.1s, LR: 1.25e-05\n","Progress: 38.0% | Best F1: 0.2209 | ETA: 3.8min\n","\n","Epoch 20/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 20/50: 100%|██████████| 164/164 [00:04<00:00, 35.35it/s, Loss=0.0016, LR=1.25e-05]\n","CASME II Validation Epoch 20/50: 100%|██████████| 5/5 [00:00<00:00,  9.50it/s, Val Loss=0.1830]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0017, F1: 0.9911, Acc: 0.9897\n","Val   - Loss: 0.1811, F1: 0.1807, Acc: 0.3333\n","Time  - Epoch: 5.2s, LR: 1.25e-05\n","Progress: 40.0% | Best F1: 0.2209 | ETA: 3.7min\n","\n","Epoch 21/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 21/50: 100%|██████████| 164/164 [00:04<00:00, 34.31it/s, Loss=0.0013, LR=1.25e-05]\n","CASME II Validation Epoch 21/50: 100%|██████████| 5/5 [00:00<00:00, 10.66it/s, Val Loss=0.1868]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0013, F1: 0.9948, Acc: 0.9935\n","Val   - Loss: 0.1817, F1: 0.1625, Acc: 0.2949\n","Time  - Epoch: 5.3s, LR: 1.25e-05\n","Progress: 42.0% | Best F1: 0.2209 | ETA: 3.5min\n","\n","Epoch 22/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 22/50: 100%|██████████| 164/164 [00:04<00:00, 36.26it/s, Loss=0.0014, LR=1.25e-05]\n","CASME II Validation Epoch 22/50: 100%|██████████| 5/5 [00:00<00:00,  9.97it/s, Val Loss=0.1865]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0016, F1: 0.9936, Acc: 0.9935\n","Val   - Loss: 0.1815, F1: 0.1775, Acc: 0.3205\n","Time  - Epoch: 5.0s, LR: 1.25e-05\n","Progress: 44.0% | Best F1: 0.2209 | ETA: 3.3min\n","\n","Epoch 23/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 23/50: 100%|██████████| 164/164 [00:04<00:00, 34.19it/s, Loss=0.0014, LR=1.25e-05]\n","CASME II Validation Epoch 23/50: 100%|██████████| 5/5 [00:00<00:00, 10.68it/s, Val Loss=0.1830]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0014, F1: 0.9927, Acc: 0.9927\n","Val   - Loss: 0.1784, F1: 0.1823, Acc: 0.3333\n","Time  - Epoch: 5.3s, LR: 1.25e-05\n","Progress: 46.0% | Best F1: 0.2209 | ETA: 3.2min\n","\n","Epoch 24/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 24/50: 100%|██████████| 164/164 [00:04<00:00, 35.96it/s, Loss=0.0012, LR=1.25e-05]\n","CASME II Validation Epoch 24/50: 100%|██████████| 5/5 [00:00<00:00,  9.44it/s, Val Loss=0.1810]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0012, F1: 0.9954, Acc: 0.9943\n","Val   - Loss: 0.1790, F1: 0.1909, Acc: 0.3077\n","Time  - Epoch: 5.1s, LR: 6.25e-06\n","Progress: 48.0% | Best F1: 0.2209 | ETA: 3.0min\n","\n","Epoch 25/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 25/50: 100%|██████████| 164/164 [00:04<00:00, 35.23it/s, Loss=0.0012, LR=6.25e-06]\n","CASME II Validation Epoch 25/50: 100%|██████████| 5/5 [00:00<00:00,  9.89it/s, Val Loss=0.1814]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0012, F1: 0.9956, Acc: 0.9939\n","Val   - Loss: 0.1765, F1: 0.1770, Acc: 0.3205\n","Time  - Epoch: 5.2s, LR: 6.25e-06\n","Progress: 50.0% | Best F1: 0.2209 | ETA: 2.9min\n","\n","Epoch 26/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 26/50: 100%|██████████| 164/164 [00:04<00:00, 35.36it/s, Loss=0.0011, LR=6.25e-06]\n","CASME II Validation Epoch 26/50: 100%|██████████| 5/5 [00:00<00:00,  9.57it/s, Val Loss=0.1916]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0011, F1: 0.9946, Acc: 0.9943\n","Val   - Loss: 0.1856, F1: 0.1547, Acc: 0.2821\n","Time  - Epoch: 5.2s, LR: 6.25e-06\n","Progress: 52.0% | Best F1: 0.2209 | ETA: 2.7min\n","\n","Epoch 27/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 27/50: 100%|██████████| 164/164 [00:05<00:00, 32.14it/s, Loss=0.0011, LR=6.25e-06]\n","CASME II Validation Epoch 27/50: 100%|██████████| 5/5 [00:00<00:00, 10.54it/s, Val Loss=0.1876]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0012, F1: 0.9963, Acc: 0.9950\n","Val   - Loss: 0.1851, F1: 0.2142, Acc: 0.3333\n","Time  - Epoch: 5.6s, LR: 6.25e-06\n","Progress: 54.0% | Best F1: 0.2209 | ETA: 2.6min\n","\n","Epoch 28/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 28/50: 100%|██████████| 164/164 [00:04<00:00, 34.87it/s, Loss=0.0012, LR=6.25e-06]\n","CASME II Validation Epoch 28/50: 100%|██████████| 5/5 [00:00<00:00, 10.69it/s, Val Loss=0.1848]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0012, F1: 0.9937, Acc: 0.9916\n","Val   - Loss: 0.1793, F1: 0.2108, Acc: 0.3462\n","Time  - Epoch: 5.2s, LR: 6.25e-06\n","Progress: 56.0% | Best F1: 0.2209 | ETA: 2.5min\n","\n","Epoch 29/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 29/50: 100%|██████████| 164/164 [00:04<00:00, 35.42it/s, Loss=0.0012, LR=6.25e-06]\n","CASME II Validation Epoch 29/50: 100%|██████████| 5/5 [00:00<00:00, 10.02it/s, Val Loss=0.1917]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0012, F1: 0.9945, Acc: 0.9931\n","Val   - Loss: 0.1848, F1: 0.1759, Acc: 0.3205\n","Time  - Epoch: 5.1s, LR: 6.25e-06\n","Progress: 58.0% | Best F1: 0.2209 | ETA: 2.3min\n","\n","Epoch 30/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 30/50: 100%|██████████| 164/164 [00:04<00:00, 33.47it/s, Loss=0.0010, LR=6.25e-06]\n","CASME II Validation Epoch 30/50: 100%|██████████| 5/5 [00:00<00:00, 10.84it/s, Val Loss=0.1951]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0010, F1: 0.9961, Acc: 0.9946\n","Val   - Loss: 0.1880, F1: 0.1496, Acc: 0.3077\n","Time  - Epoch: 5.4s, LR: 3.13e-06\n","Progress: 60.0% | Best F1: 0.2209 | ETA: 2.2min\n","\n","Epoch 31/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 31/50: 100%|██████████| 164/164 [00:04<00:00, 35.34it/s, Loss=0.0012, LR=3.13e-06]\n","CASME II Validation Epoch 31/50: 100%|██████████| 5/5 [00:00<00:00,  9.73it/s, Val Loss=0.1858]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0013, F1: 0.9927, Acc: 0.9916\n","Val   - Loss: 0.1795, F1: 0.1711, Acc: 0.2949\n","Time  - Epoch: 5.2s, LR: 3.13e-06\n","Progress: 62.0% | Best F1: 0.2209 | ETA: 2.1min\n","\n","Epoch 32/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 32/50: 100%|██████████| 164/164 [00:04<00:00, 36.01it/s, Loss=0.0008, LR=3.13e-06]\n","CASME II Validation Epoch 32/50: 100%|██████████| 5/5 [00:00<00:00,  9.04it/s, Val Loss=0.1866]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9972, Acc: 0.9962\n","Val   - Loss: 0.1818, F1: 0.2116, Acc: 0.3462\n","Time  - Epoch: 5.1s, LR: 3.13e-06\n","Progress: 64.0% | Best F1: 0.2209 | ETA: 2.0min\n","\n","Epoch 33/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 33/50: 100%|██████████| 164/164 [00:04<00:00, 35.72it/s, Loss=0.0012, LR=3.13e-06]\n","CASME II Validation Epoch 33/50: 100%|██████████| 5/5 [00:00<00:00,  9.90it/s, Val Loss=0.1909]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0012, F1: 0.9937, Acc: 0.9923\n","Val   - Loss: 0.1827, F1: 0.1651, Acc: 0.3077\n","Time  - Epoch: 5.1s, LR: 3.13e-06\n","Progress: 66.0% | Best F1: 0.2209 | ETA: 1.8min\n","\n","Epoch 34/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 34/50: 100%|██████████| 164/164 [00:04<00:00, 33.83it/s, Loss=0.0010, LR=3.13e-06]\n","CASME II Validation Epoch 34/50: 100%|██████████| 5/5 [00:00<00:00, 10.65it/s, Val Loss=0.1931]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0011, F1: 0.9950, Acc: 0.9935\n","Val   - Loss: 0.1866, F1: 0.1619, Acc: 0.3333\n","Time  - Epoch: 5.3s, LR: 3.13e-06\n","Progress: 68.0% | Best F1: 0.2209 | ETA: 1.7min\n","\n","Epoch 35/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 35/50: 100%|██████████| 164/164 [00:04<00:00, 34.12it/s, Loss=0.0007, LR=3.13e-06]\n","CASME II Validation Epoch 35/50: 100%|██████████| 5/5 [00:00<00:00,  9.72it/s, Val Loss=0.1960]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0008, F1: 0.9968, Acc: 0.9958\n","Val   - Loss: 0.1884, F1: 0.1850, Acc: 0.3333\n","Time  - Epoch: 5.3s, LR: 3.13e-06\n","Progress: 70.0% | Best F1: 0.2209 | ETA: 1.6min\n","\n","Epoch 36/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 36/50: 100%|██████████| 164/164 [00:04<00:00, 35.62it/s, Loss=0.0009, LR=3.13e-06]\n","CASME II Validation Epoch 36/50: 100%|██████████| 5/5 [00:00<00:00, 10.83it/s, Val Loss=0.1957]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0010, F1: 0.9957, Acc: 0.9939\n","Val   - Loss: 0.1879, F1: 0.1738, Acc: 0.3205\n","Time  - Epoch: 5.1s, LR: 1.56e-06\n","Progress: 72.0% | Best F1: 0.2209 | ETA: 1.5min\n","\n","Epoch 37/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 37/50: 100%|██████████| 164/164 [00:04<00:00, 35.23it/s, Loss=0.0009, LR=1.56e-06]\n","CASME II Validation Epoch 37/50: 100%|██████████| 5/5 [00:00<00:00,  9.20it/s, Val Loss=0.1901]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9961, Acc: 0.9943\n","Val   - Loss: 0.1824, F1: 0.1913, Acc: 0.3333\n","Time  - Epoch: 5.2s, LR: 1.56e-06\n","Progress: 74.0% | Best F1: 0.2209 | ETA: 1.4min\n","\n","Epoch 38/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 38/50: 100%|██████████| 164/164 [00:04<00:00, 35.61it/s, Loss=0.0009, LR=1.56e-06]\n","CASME II Validation Epoch 38/50: 100%|██████████| 5/5 [00:00<00:00,  9.53it/s, Val Loss=0.1898]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9955, Acc: 0.9939\n","Val   - Loss: 0.1853, F1: 0.2130, Acc: 0.3333\n","Time  - Epoch: 5.1s, LR: 1.56e-06\n","Progress: 76.0% | Best F1: 0.2209 | ETA: 1.3min\n","\n","Epoch 39/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 39/50: 100%|██████████| 164/164 [00:04<00:00, 34.93it/s, Loss=0.0010, LR=1.56e-06]\n","CASME II Validation Epoch 39/50: 100%|██████████| 5/5 [00:00<00:00,  9.57it/s, Val Loss=0.1910]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0010, F1: 0.9961, Acc: 0.9946\n","Val   - Loss: 0.1861, F1: 0.1990, Acc: 0.3205\n","Time  - Epoch: 5.2s, LR: 1.56e-06\n","Progress: 78.0% | Best F1: 0.2209 | ETA: 1.2min\n","\n","Epoch 40/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 40/50: 100%|██████████| 164/164 [00:04<00:00, 33.92it/s, Loss=0.0010, LR=1.56e-06]\n","CASME II Validation Epoch 40/50: 100%|██████████| 5/5 [00:00<00:00,  9.63it/s, Val Loss=0.1944]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0010, F1: 0.9953, Acc: 0.9958\n","Val   - Loss: 0.1884, F1: 0.1712, Acc: 0.3205\n","Time  - Epoch: 5.4s, LR: 1.56e-06\n","Progress: 80.0% | Best F1: 0.2209 | ETA: 1.0min\n","\n","Epoch 41/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 41/50: 100%|██████████| 164/164 [00:04<00:00, 34.99it/s, Loss=0.0009, LR=1.56e-06]\n","CASME II Validation Epoch 41/50: 100%|██████████| 5/5 [00:00<00:00,  9.07it/s, Val Loss=0.1941]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9962, Acc: 0.9950\n","Val   - Loss: 0.1882, F1: 0.1551, Acc: 0.3205\n","Time  - Epoch: 5.3s, LR: 1.56e-06\n","Progress: 82.0% | Best F1: 0.2209 | ETA: 0.9min\n","\n","Epoch 42/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 42/50: 100%|██████████| 164/164 [00:05<00:00, 28.54it/s, Loss=0.0011, LR=1.56e-06]\n","CASME II Validation Epoch 42/50: 100%|██████████| 5/5 [00:00<00:00,  8.94it/s, Val Loss=0.1961]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0011, F1: 0.9923, Acc: 0.9916\n","Val   - Loss: 0.1901, F1: 0.1661, Acc: 0.3077\n","Time  - Epoch: 6.3s, LR: 1.00e-06\n","Progress: 84.0% | Best F1: 0.2209 | ETA: 0.8min\n","\n","Epoch 43/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 43/50: 100%|██████████| 164/164 [00:04<00:00, 34.09it/s, Loss=0.0008, LR=1.00e-06]\n","CASME II Validation Epoch 43/50: 100%|██████████| 5/5 [00:00<00:00, 10.40it/s, Val Loss=0.1930]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0008, F1: 0.9960, Acc: 0.9950\n","Val   - Loss: 0.1866, F1: 0.1761, Acc: 0.3205\n","Time  - Epoch: 5.3s, LR: 1.00e-06\n","Progress: 86.0% | Best F1: 0.2209 | ETA: 0.7min\n","\n","Epoch 44/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 44/50: 100%|██████████| 164/164 [00:04<00:00, 34.54it/s, Loss=0.0006, LR=1.00e-06]\n","CASME II Validation Epoch 44/50: 100%|██████████| 5/5 [00:00<00:00, 10.69it/s, Val Loss=0.1899]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0006, F1: 0.9985, Acc: 0.9981\n","Val   - Loss: 0.1841, F1: 0.1997, Acc: 0.3462\n","Time  - Epoch: 5.2s, LR: 1.00e-06\n","Progress: 88.0% | Best F1: 0.2209 | ETA: 0.6min\n","\n","Epoch 45/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 45/50: 100%|██████████| 164/164 [00:04<00:00, 35.32it/s, Loss=0.0012, LR=1.00e-06]\n","CASME II Validation Epoch 45/50: 100%|██████████| 5/5 [00:00<00:00,  9.53it/s, Val Loss=0.1917]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0012, F1: 0.9959, Acc: 0.9935\n","Val   - Loss: 0.1882, F1: 0.1813, Acc: 0.3077\n","Time  - Epoch: 5.2s, LR: 1.00e-06\n","Progress: 90.0% | Best F1: 0.2209 | ETA: 0.5min\n","\n","Epoch 46/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 46/50: 100%|██████████| 164/164 [00:04<00:00, 34.85it/s, Loss=0.0009, LR=1.00e-06]\n","CASME II Validation Epoch 46/50: 100%|██████████| 5/5 [00:00<00:00,  9.45it/s, Val Loss=0.1895]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9974, Acc: 0.9966\n","Val   - Loss: 0.1833, F1: 0.1851, Acc: 0.3333\n","Time  - Epoch: 5.3s, LR: 1.00e-06\n","Progress: 92.0% | Best F1: 0.2209 | ETA: 0.4min\n","\n","Epoch 47/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 47/50: 100%|██████████| 164/164 [00:04<00:00, 34.77it/s, Loss=0.0011, LR=1.00e-06]\n","CASME II Validation Epoch 47/50: 100%|██████████| 5/5 [00:00<00:00,  9.67it/s, Val Loss=0.1949]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0018, F1: 0.9931, Acc: 0.9931\n","Val   - Loss: 0.1893, F1: 0.1765, Acc: 0.3333\n","Time  - Epoch: 5.2s, LR: 1.00e-06\n","Progress: 94.0% | Best F1: 0.2209 | ETA: 0.3min\n","\n","Epoch 48/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 48/50: 100%|██████████| 164/164 [00:04<00:00, 34.91it/s, Loss=0.0009, LR=1.00e-06]\n","CASME II Validation Epoch 48/50: 100%|██████████| 5/5 [00:00<00:00, 10.64it/s, Val Loss=0.1925]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0009, F1: 0.9944, Acc: 0.9950\n","Val   - Loss: 0.1859, F1: 0.2066, Acc: 0.3462\n","Time  - Epoch: 5.2s, LR: 1.00e-06\n","Progress: 96.0% | Best F1: 0.2209 | ETA: 0.2min\n","\n","Epoch 49/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 49/50: 100%|██████████| 164/164 [00:04<00:00, 33.61it/s, Loss=0.0008, LR=1.00e-06]\n","CASME II Validation Epoch 49/50: 100%|██████████| 5/5 [00:00<00:00,  9.37it/s, Val Loss=0.1940]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0011, F1: 0.9966, Acc: 0.9943\n","Val   - Loss: 0.1862, F1: 0.1916, Acc: 0.3462\n","Time  - Epoch: 5.4s, LR: 1.00e-06\n","Progress: 98.0% | Best F1: 0.2209 | ETA: 0.1min\n","\n","Epoch 50/50\n"]},{"output_type":"stream","name":"stderr","text":["CASME II Training Epoch 50/50: 100%|██████████| 164/164 [00:04<00:00, 34.81it/s, Loss=0.0009, LR=1.00e-06]\n","CASME II Validation Epoch 50/50: 100%|██████████| 5/5 [00:00<00:00, 10.57it/s, Val Loss=0.1932]"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.0010, F1: 0.9958, Acc: 0.9962\n","Val   - Loss: 0.1862, F1: 0.1862, Acc: 0.3333\n","Time  - Epoch: 5.2s, LR: 1.00e-06\n","Progress: 100.0% | Best F1: 0.2209 | ETA: 0.0min\n","\n","======================================================================\n","CASME II MOBILENETV3-SMALL M2 MFS-PREP TRAINING COMPLETED\n","======================================================================\n","Training time: 5.1 minutes\n","Epochs completed: 50\n","Best validation F1: 0.2209 (epoch 12)\n","Final train F1: 0.9958\n","Final validation F1: 0.1862\n","\n","Exporting training documentation...\n","Training documentation saved successfully: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/09_01_mobilenet_casme2_mfs_prep/training_logs/casme2_mobilenet_mfs_prep_training_history.json\n","Model: MobileNetV3-Small\n","Methodology: M2\n","Input resolution: 224x224 Pure Grayscale (1 channel)\n","Training strategy: from_scratch\n","\n","Next: Cell 3 - CASME II MobileNetV3-Small M2 Evaluation\n","Training pipeline completed successfully!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title Cell 3: CASME II MobileNetV3-Small M2 MFS-PREP Evaluation (Dual Dataset)\n","\n","# File: 09_01_MobileNet_CASME2_MFS_PREP_Cell3.py\n","# Location: experiments/09_01_MobileNet_CASME2-MFS-PREP.ipynb\n","# Purpose: Comprehensive evaluation framework with v7 (AF) and v8 (KFS) test datasets\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from datetime import datetime\n","from collections import defaultdict\n","\n","from sklearn.metrics import (\n","    accuracy_score, precision_recall_fscore_support,\n","    classification_report, confusion_matrix,\n","    roc_curve, auc\n",")\n","from sklearn.preprocessing import label_binarize\n","from concurrent.futures import ThreadPoolExecutor\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","EVALUATE_DATASETS = ['v7', 'v8']\n","\n","print(\"CASME II MobileNetV3-Small M2 MFS-PREP Evaluation Framework\")\n","print(\"=\" * 60)\n","print(f\"Datasets to evaluate: {EVALUATE_DATASETS}\")\n","print(f\"Input: 224x224 Pure Grayscale (1 channel)\")\n","print(f\"Model: From Scratch Training\")\n","print(\"=\" * 60)\n","\n","def get_test_dataset_config(version, project_root):\n","    if version == 'v7':\n","        config = {\n","            'version': 'v7',\n","            'variant': 'AF',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/preprocessed_v7\",\n","            'preprocessing_summary': 'preprocessing_summary.json',\n","            'description': 'Apex Frame with Face-Aware Preprocessing',\n","            'expected_samples': 28,\n","            'frame_strategy': 'apex_frame',\n","            'evaluation_mode': 'frame_level',\n","            'aggregation': None\n","        }\n","    elif version == 'v8':\n","        config = {\n","            'version': 'v8',\n","            'variant': 'KFS',\n","            'dataset_path': f\"{project_root}/datasets/processed_casme2/preprocessed_v8\",\n","            'preprocessing_summary': 'preprocessing_summary.json',\n","            'description': 'Key Frame Sequence with Face-Aware Preprocessing',\n","            'expected_frames': 84,\n","            'expected_videos': 28,\n","            'frame_strategy': 'key_frame_sequence',\n","            'frame_types': ['onset', 'apex', 'offset'],\n","            'evaluation_mode': 'video_level',\n","            'aggregation': 'late_fusion'\n","        }\n","    else:\n","        raise ValueError(f\"Invalid version: {version}. Must be 'v7' or 'v8'\")\n","\n","    return config\n","\n","def extract_video_id_from_filename(filename):\n","    name_without_ext = filename.rsplit('.', 1)[0]\n","\n","    for frame_type in ['onset', 'apex', 'offset']:\n","        if name_without_ext.endswith(f'_{frame_type}'):\n","            video_id = name_without_ext.rsplit(f'_{frame_type}', 1)[0]\n","            return video_id\n","\n","    return name_without_ext\n","\n","class CASME2DatasetEvaluation(Dataset):\n","    def __init__(self, dataset_root, split, transform=None, use_ram_cache=True):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","        self.emotions = []\n","        self.video_ids = []\n","        self.cached_images = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading CASME II {split} dataset for evaluation...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        loaded_count = 0\n","\n","        for filename in sorted(all_files):\n","            emotion_found = None\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                video_id = extract_video_id_from_filename(filename)\n","\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                self.emotions.append(emotion_found)\n","                self.video_ids.append(video_id)\n","                loaded_count += 1\n","\n","        print(f\"Loaded {len(self.images)} CASME II {split} samples for evaluation\")\n","        self._print_evaluation_distribution()\n","\n","        if self.use_ram_cache and len(self.images) > 0:\n","            self._preload_to_ram_evaluation()\n","\n","    def _print_evaluation_distribution(self):\n","        if len(self.labels) == 0:\n","            print(\"No test samples found!\")\n","            return\n","\n","        label_counts = {}\n","        unique_videos = set(self.video_ids)\n","\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        print(\"Test set class distribution:\")\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","        print(f\"Unique video IDs: {len(unique_videos)}\")\n","\n","        missing_classes = []\n","        for i, class_name in enumerate(CASME2_CLASSES):\n","            if i not in label_counts:\n","                missing_classes.append(class_name)\n","\n","        if missing_classes:\n","            print(f\"Missing classes in test set: {missing_classes}\")\n","\n","    def _preload_to_ram_evaluation(self):\n","        if len(self.images) == 0:\n","            return\n","\n","        print(f\"Preloading {len(self.images)} test images to RAM...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","\n","        def load_single_image(idx, img_path):\n","            try:\n","                image = Image.open(img_path)\n","                if image.mode != 'L':\n","                    image = image.convert('L')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except:\n","                return idx, Image.new('L', (224, 224), 128), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=\"Loading test to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","\n","        print(f\"RAM caching completed: {len(self.cached_images)} test images\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx])\n","                if image.mode != 'L':\n","                    image = image.convert('L')\n","                if image.size != (224, 224):\n","                    image = image.resize((224, 224), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('L', (224, 224), 128)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx], self.video_ids[idx]\n","\n","EVALUATION_CONFIG_CASME2 = {\n","    'model_type': 'MobileNetV3Small_CASME2_MFS_PREP_Baseline',\n","    'task_type': 'micro_expression_recognition',\n","    'num_classes': 7,\n","    'class_names': CASME2_CLASSES,\n","    'checkpoint_file': 'casme2_mobilenet_mfs_prep_best_f1.pth',\n","    'dataset_name': 'CASME_II',\n","    'methodology': 'M2',\n","    'input_resolution': '224x224 Pure Grayscale (1 channel)',\n","    'training_strategy': 'from_scratch',\n","    'evaluation_protocol': 'dual_test_v7_v8'\n","}\n","\n","print(f\"\\nCASME II MobileNetV3 M2 Evaluation Configuration:\")\n","print(f\"  Model: {EVALUATION_CONFIG_CASME2['model_type']}\")\n","print(f\"  Methodology: {EVALUATION_CONFIG_CASME2['methodology']}\")\n","print(f\"  Input resolution: {EVALUATION_CONFIG_CASME2['input_resolution']}\")\n","print(f\"  Training strategy: {EVALUATION_CONFIG_CASME2['training_strategy']}\")\n","\n","def load_trained_model_casme2(checkpoint_path, device):\n","    print(f\"Loading trained model from: {checkpoint_path}\")\n","\n","    if not os.path.exists(checkpoint_path):\n","        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n","\n","    checkpoint = None\n","    loading_method = \"unknown\"\n","\n","    try:\n","        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","        loading_method = \"standard\"\n","    except Exception as e1:\n","        try:\n","            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n","            loading_method = \"weights_only_false\"\n","        except Exception as e2:\n","            try:\n","                import pickle\n","                with open(checkpoint_path, 'rb') as f:\n","                    checkpoint = pickle.load(f)\n","                loading_method = \"pickle\"\n","            except Exception as e3:\n","                raise RuntimeError(f\"All loading methods failed: {e1}, {e2}, {e3}\")\n","\n","    print(f\"Checkpoint loaded using: {loading_method}\")\n","\n","    model = MobileNetCASME2Baseline(\n","        num_classes=EVALUATION_CONFIG_CASME2['num_classes'],\n","        dropout_rate=checkpoint['casme2_config']['dropout_rate'],\n","        in_channels=1\n","    ).to(device)\n","\n","    state_dict = checkpoint.get('model_state_dict', checkpoint)\n","\n","    try:\n","        model.load_state_dict(state_dict, strict=True)\n","        print(\"Model state loaded with strict=True\")\n","    except Exception as e:\n","        print(f\"Strict loading failed, trying non-strict: {str(e)[:100]}...\")\n","        try:\n","            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n","            if missing_keys or unexpected_keys:\n","                print(f\"Non-strict loading: Missing {len(missing_keys)}, Unexpected {len(unexpected_keys)}\")\n","            else:\n","                print(\"Model state loaded with strict=False (no key mismatches)\")\n","        except Exception as e2:\n","            raise RuntimeError(f\"Both loading approaches failed: {e2}\")\n","\n","    model.eval()\n","\n","    training_info = {\n","        'best_val_f1': float(checkpoint.get('best_f1', 0.0)),\n","        'best_val_loss': float(checkpoint.get('best_loss', float('inf'))),\n","        'best_val_accuracy': float(checkpoint.get('best_acc', 0.0)),\n","        'best_epoch': int(checkpoint.get('epoch', 0)) + 1,\n","        'model_checkpoint': EVALUATION_CONFIG_CASME2['checkpoint_file'],\n","        'num_classes': EVALUATION_CONFIG_CASME2['num_classes'],\n","        'config': checkpoint.get('casme2_config', {})\n","    }\n","\n","    print(f\"Model loaded successfully:\")\n","    print(f\"  Best validation F1: {training_info['best_val_f1']:.4f}\")\n","    print(f\"  Best validation accuracy: {training_info['best_val_accuracy']:.4f}\")\n","    print(f\"  Best epoch: {training_info['best_epoch']}\")\n","\n","    return model, training_info\n","\n","def run_frame_level_inference(model, test_loader, device):\n","    print(\"Running frame-level inference...\")\n","\n","    model.eval()\n","    all_predictions = []\n","    all_labels = []\n","    all_filenames = []\n","    all_probabilities = []\n","\n","    start_time = time.time()\n","\n","    with torch.no_grad():\n","        for images, labels, filenames, video_ids in tqdm(test_loader, desc=\"Frame-level inference\"):\n","            images = images.to(device)\n","\n","            outputs = model(images)\n","            probabilities = F.softmax(outputs, dim=1)\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","            all_filenames.extend(filenames)\n","            all_probabilities.extend(probabilities.cpu().numpy())\n","\n","    inference_time = time.time() - start_time\n","\n","    return {\n","        'predictions': all_predictions,\n","        'labels': all_labels,\n","        'filenames': all_filenames,\n","        'probabilities': all_probabilities,\n","        'inference_time': inference_time,\n","        'evaluation_mode': 'frame_level'\n","    }\n","\n","def run_video_level_inference_late_fusion(model, test_loader, device):\n","    print(\"Running video-level inference with late fusion...\")\n","\n","    model.eval()\n","\n","    frame_predictions = []\n","    frame_labels = []\n","    frame_filenames = []\n","    frame_video_ids = []\n","    frame_probabilities = []\n","\n","    start_time = time.time()\n","\n","    with torch.no_grad():\n","        for images, labels, filenames, video_ids in tqdm(test_loader, desc=\"Frame inference\"):\n","            images = images.to(device)\n","\n","            outputs = model(images)\n","            probabilities = F.softmax(outputs, dim=1)\n","\n","            _, predicted = torch.max(outputs, 1)\n","\n","            frame_predictions.extend(predicted.cpu().numpy())\n","            frame_labels.extend(labels.numpy())\n","            frame_filenames.extend(filenames)\n","            frame_video_ids.extend(video_ids)\n","            frame_probabilities.extend(probabilities.cpu().numpy())\n","\n","    print(\"Aggregating frame predictions to video level...\")\n","\n","    video_data = {}\n","    for pred, label, filename, video_id, prob in zip(\n","        frame_predictions, frame_labels, frame_filenames, frame_video_ids, frame_probabilities\n","    ):\n","        if video_id not in video_data:\n","            video_data[video_id] = {\n","                'predictions': [],\n","                'probabilities': [],\n","                'label': label,\n","                'filenames': []\n","            }\n","\n","        video_data[video_id]['predictions'].append(pred)\n","        video_data[video_id]['probabilities'].append(prob)\n","        video_data[video_id]['filenames'].append(filename)\n","\n","    video_predictions = []\n","    video_labels = []\n","    video_ids_list = []\n","\n","    for video_id, data in video_data.items():\n","        avg_prob = np.mean(data['probabilities'], axis=0)\n","        final_pred = np.argmax(avg_prob)\n","\n","        video_predictions.append(final_pred)\n","        video_labels.append(data['label'])\n","        video_ids_list.append(video_id)\n","\n","    inference_time = time.time() - start_time\n","\n","    return {\n","        'predictions': video_predictions,\n","        'labels': video_labels,\n","        'video_ids': video_ids_list,\n","        'inference_time': inference_time,\n","        'evaluation_mode': 'video_level',\n","        'kfs_late_fusion_info': {\n","            'total_frames': len(frame_predictions),\n","            'total_videos': len(video_predictions),\n","            'aggregation_method': 'average_probability'\n","        }\n","    }\n","\n","def calculate_comprehensive_metrics(inference_results):\n","    predictions = np.array(inference_results['predictions'])\n","    labels = np.array(inference_results['labels'])\n","\n","    accuracy = accuracy_score(labels, predictions)\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions, average='macro', zero_division=0\n","    )\n","\n","    per_class_precision, per_class_recall, per_class_f1, per_class_support = \\\n","        precision_recall_fscore_support(labels, predictions, average=None, zero_division=0)\n","\n","    cm = confusion_matrix(labels, predictions)\n","\n","    try:\n","        if 'probabilities' in inference_results:\n","            labels_bin = label_binarize(labels, classes=range(len(CASME2_CLASSES)))\n","            auc_scores = []\n","\n","            for i in range(len(CASME2_CLASSES)):\n","                if labels_bin[:, i].sum() > 0:\n","                    fpr, tpr, _ = roc_curve(labels_bin[:, i],\n","                                           np.array(inference_results['probabilities'])[:, i])\n","                    auc_scores.append(auc(fpr, tpr))\n","                else:\n","                    auc_scores.append(0.0)\n","\n","            macro_auc = np.mean([score for score in auc_scores if score > 0])\n","        else:\n","            auc_scores = [0.0] * len(CASME2_CLASSES)\n","            macro_auc = 0.0\n","    except:\n","        auc_scores = [0.0] * len(CASME2_CLASSES)\n","        macro_auc = 0.0\n","\n","    unique_labels = set(labels)\n","    available_classes = [CASME2_CLASSES[i] for i in unique_labels]\n","    missing_classes = [cls for i, cls in enumerate(CASME2_CLASSES) if i not in unique_labels]\n","\n","    per_class_performance = {}\n","    for i, class_name in enumerate(CASME2_CLASSES):\n","        per_class_performance[class_name] = {\n","            'precision': float(per_class_precision[i]) if i < len(per_class_precision) else 0.0,\n","            'recall': float(per_class_recall[i]) if i < len(per_class_recall) else 0.0,\n","            'f1_score': float(per_class_f1[i]) if i < len(per_class_f1) else 0.0,\n","            'support': int(per_class_support[i]) if i < len(per_class_support) else 0,\n","            'auc': float(auc_scores[i]) if i < len(auc_scores) else 0.0,\n","            'in_test_set': i in unique_labels\n","        }\n","\n","    inference_performance = {\n","        'total_time_seconds': inference_results['inference_time'],\n","        'average_time_ms_per_sample': (inference_results['inference_time'] / len(predictions)) * 1000\n","    }\n","\n","    results = {\n","        'evaluation_metadata': {\n","            'dataset': 'CASME_II',\n","            'model_type': 'MobileNetCASME2Baseline',\n","            'methodology': 'M2',\n","            'input_resolution': '224x224 Pure Grayscale (1 channel)',\n","            'training_strategy': 'from_scratch',\n","            'evaluation_timestamp': datetime.now().isoformat(),\n","            'evaluation_mode': inference_results['evaluation_mode'],\n","            'test_samples': len(predictions),\n","            'class_names': CASME2_CLASSES,\n","            'available_classes': available_classes,\n","            'missing_classes': missing_classes\n","        },\n","        'overall_performance': {\n","            'accuracy': float(accuracy),\n","            'macro_precision': float(precision),\n","            'macro_recall': float(recall),\n","            'macro_f1': float(f1),\n","            'macro_auc': float(macro_auc)\n","        },\n","        'per_class_performance': per_class_performance,\n","        'confusion_matrix': cm.tolist(),\n","        'inference_performance': inference_performance\n","    }\n","\n","    if 'kfs_late_fusion_info' in inference_results:\n","        results['kfs_late_fusion_info'] = inference_results['kfs_late_fusion_info']\n","\n","    return results\n","\n","def analyze_wrong_predictions(inference_results):\n","    predictions = np.array(inference_results['predictions'])\n","    labels = np.array(inference_results['labels'])\n","\n","    if 'filenames' in inference_results:\n","        filenames = inference_results['filenames']\n","    elif 'video_ids' in inference_results:\n","        filenames = inference_results['video_ids']\n","    else:\n","        filenames = [f\"sample_{i}\" for i in range(len(predictions))]\n","\n","    wrong_predictions = []\n","    wrong_by_class = defaultdict(int)\n","    confusion_patterns = defaultdict(int)\n","\n","    for i, (pred, true_label) in enumerate(zip(predictions, labels)):\n","        if pred != true_label:\n","            true_class = CASME2_CLASSES[true_label]\n","            pred_class = CASME2_CLASSES[pred]\n","\n","            wrong_predictions.append({\n","                'filename': filenames[i],\n","                'true_label': int(true_label),\n","                'true_class': true_class,\n","                'predicted_label': int(pred),\n","                'predicted_class': pred_class\n","            })\n","\n","            wrong_by_class[true_class] += 1\n","            confusion_patterns[f\"{true_class}->{pred_class}\"] += 1\n","\n","    error_summary = {}\n","    for class_name in CASME2_CLASSES:\n","        class_idx = CLASS_TO_IDX[class_name]\n","        class_mask = labels == class_idx\n","        class_total = class_mask.sum()\n","\n","        if class_total > 0:\n","            class_wrong = wrong_by_class.get(class_name, 0)\n","            error_rate = (class_wrong / class_total) * 100\n","        else:\n","            class_wrong = 0\n","            error_rate = 0.0\n","\n","        error_summary[class_name] = {\n","            'total_samples': int(class_total),\n","            'wrong_predictions': int(class_wrong),\n","            'error_rate_percent': float(error_rate)\n","        }\n","\n","    results = {\n","        'analysis_metadata': {\n","            'total_samples': len(predictions),\n","            'total_wrong_predictions': len(wrong_predictions),\n","            'overall_error_rate': (len(wrong_predictions) / len(predictions) * 100) if len(predictions) > 0 else 0.0\n","        },\n","        'wrong_predictions': wrong_predictions,\n","        'wrong_predictions_by_class': dict(wrong_by_class),\n","        'error_summary': error_summary,\n","        'confusion_patterns': dict(confusion_patterns)\n","    }\n","\n","    return results\n","\n","def save_evaluation_results(evaluation_results, wrong_predictions_results, results_dir, test_version):\n","    os.makedirs(results_dir, exist_ok=True)\n","\n","    results_file = f\"{results_dir}/casme2_mobilenet_mfs_prep_evaluation_results_{test_version}.json\"\n","    with open(results_file, 'w') as f:\n","        json.dump(evaluation_results, f, indent=2, default=str)\n","\n","    wrong_predictions_file = f\"{results_dir}/casme2_mobilenet_mfs_prep_wrong_predictions_{test_version}.json\"\n","    with open(wrong_predictions_file, 'w') as f:\n","        json.dump(wrong_predictions_results, f, indent=2, default=str)\n","\n","    print(f\"Evaluation results saved:\")\n","    print(f\"  Main results: {os.path.basename(results_file)}\")\n","    print(f\"  Wrong predictions: {os.path.basename(wrong_predictions_file)}\")\n","\n","    return results_file, wrong_predictions_file\n","\n","all_evaluation_results = {}\n","\n","checkpoint_path = f\"{GLOBAL_CONFIG_CASME2['checkpoint_root']}/{EVALUATION_CONFIG_CASME2['checkpoint_file']}\"\n","casme2_model, training_info = load_trained_model_casme2(checkpoint_path, GLOBAL_CONFIG_CASME2['device'])\n","\n","results_dir = f\"{GLOBAL_CONFIG_CASME2['results_root']}/evaluation_results\"\n","\n","for dataset_version in EVALUATE_DATASETS:\n","    print(\"\\n\" + \"=\" * 70)\n","    print(f\"EVALUATING DATASET: {dataset_version.upper()}\")\n","    print(\"=\" * 70)\n","\n","    try:\n","        test_config = get_test_dataset_config(dataset_version, PROJECT_ROOT)\n","\n","        print(f\"\\nTest Dataset Configuration:\")\n","        print(f\"  Version: {test_config['version']}\")\n","        print(f\"  Variant: {test_config['variant']}\")\n","        print(f\"  Description: {test_config['description']}\")\n","        print(f\"  Frame strategy: {test_config['frame_strategy']}\")\n","        print(f\"  Evaluation mode: {test_config['evaluation_mode']}\")\n","        if 'aggregation' in test_config and test_config['aggregation']:\n","            print(f\"  Aggregation: {test_config['aggregation']}\")\n","        print(f\"  Dataset path: {test_config['dataset_path']}\")\n","\n","        print(f\"\\nCreating CASME II test dataset from {test_config['variant']}...\")\n","        test_dataset = CASME2DatasetEvaluation(\n","            dataset_root=test_config['dataset_path'],\n","            split='test',\n","            transform=GLOBAL_CONFIG_CASME2['transform_val'],\n","            use_ram_cache=True\n","        )\n","\n","        if len(test_dataset) == 0:\n","            raise ValueError(f\"No test samples found for {dataset_version}!\")\n","\n","        test_loader = DataLoader(\n","            test_dataset,\n","            batch_size=CASME2_MOBILENET_CONFIG['batch_size'],\n","            shuffle=False,\n","            num_workers=CASME2_MOBILENET_CONFIG['num_workers'],\n","            pin_memory=True\n","        )\n","\n","        if test_config['evaluation_mode'] == 'frame_level':\n","            print(f\"\\nRunning frame-level evaluation for {test_config['variant']}...\")\n","            inference_results = run_frame_level_inference(casme2_model, test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","        elif test_config['evaluation_mode'] == 'video_level':\n","            print(f\"\\nRunning video-level evaluation with late fusion for {test_config['variant']}...\")\n","            inference_results = run_video_level_inference_late_fusion(casme2_model, test_loader, GLOBAL_CONFIG_CASME2['device'])\n","\n","        else:\n","            raise ValueError(f\"Unknown evaluation mode: {test_config['evaluation_mode']}\")\n","\n","        evaluation_results = calculate_comprehensive_metrics(inference_results)\n","\n","        wrong_predictions_results = analyze_wrong_predictions(inference_results)\n","\n","        evaluation_results['training_information'] = training_info\n","        evaluation_results['test_configuration'] = test_config\n","\n","        results_file, wrong_file = save_evaluation_results(\n","            evaluation_results, wrong_predictions_results, results_dir, test_config['version']\n","        )\n","\n","        all_evaluation_results[dataset_version] = {\n","            'evaluation': evaluation_results,\n","            'wrong_predictions': wrong_predictions_results,\n","            'config': test_config\n","        }\n","\n","        print(\"\\n\" + \"=\" * 60)\n","        print(f\"EVALUATION RESULTS - {test_config['variant']} ({dataset_version})\")\n","        print(\"=\" * 60)\n","\n","        overall = evaluation_results['overall_performance']\n","        print(f\"\\nOverall Performance:\")\n","        print(f\"  Accuracy:  {overall['accuracy']:.4f}\")\n","        print(f\"  Precision: {overall['macro_precision']:.4f}\")\n","        print(f\"  Recall:    {overall['macro_recall']:.4f}\")\n","        print(f\"  F1 Score:  {overall['macro_f1']:.4f}\")\n","        print(f\"  AUC:       {overall['macro_auc']:.4f}\")\n","\n","        if 'kfs_late_fusion_info' in evaluation_results:\n","            fusion_info = evaluation_results['kfs_late_fusion_info']\n","            print(f\"\\nLate Fusion Info:\")\n","            print(f\"  Total frames processed: {fusion_info['total_frames']}\")\n","            print(f\"  Video-level predictions: {fusion_info['total_videos']}\")\n","            print(f\"  Aggregation method: {fusion_info['aggregation_method']}\")\n","\n","        print(f\"\\nPer-Class Performance:\")\n","        for class_name, metrics in evaluation_results['per_class_performance'].items():\n","            in_test = \"Present\" if metrics['in_test_set'] else \"Missing\"\n","            print(f\"  {class_name} [{in_test}]: F1={metrics['f1_score']:.4f}, \"\n","                  f\"Support={metrics['support']}\")\n","\n","        wrong_meta = wrong_predictions_results['analysis_metadata']\n","        print(f\"\\nWrong Predictions Analysis:\")\n","        print(f\"  Total errors: {wrong_meta['total_wrong_predictions']} / {wrong_meta['total_samples']}\")\n","        print(f\"  Error rate: {wrong_meta['overall_error_rate']:.2f}%\")\n","\n","        print(f\"\\nInference Performance:\")\n","        print(f\"  Total time: {inference_results['inference_time']:.2f}s\")\n","        print(f\"  Speed: {evaluation_results['inference_performance']['average_time_ms_per_sample']:.1f} ms/sample\")\n","\n","    except Exception as e:\n","        print(f\"Evaluation failed for {dataset_version}: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","        continue\n","\n","if len(all_evaluation_results) == 2 and 'v7' in all_evaluation_results and 'v8' in all_evaluation_results:\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"COMPARATIVE ANALYSIS: AF (v7) vs KFS (v8)\")\n","    print(\"=\" * 70)\n","\n","    v7_results = all_evaluation_results['v7']['evaluation']\n","    v8_results = all_evaluation_results['v8']['evaluation']\n","\n","    print(\"\\nOverall Performance Comparison:\")\n","    print(f\"{'Metric':<20} {'AF (v7)':<15} {'KFS (v8)':<15} {'Difference':<15}\")\n","    print(\"-\" * 65)\n","\n","    metrics_to_compare = ['accuracy', 'macro_precision', 'macro_recall', 'macro_f1', 'macro_auc']\n","\n","    for metric in metrics_to_compare:\n","        v7_val = v7_results['overall_performance'][metric]\n","        v8_val = v8_results['overall_performance'][metric]\n","        diff = v8_val - v7_val\n","\n","        print(f\"{metric:<20} {v7_val:<15.4f} {v8_val:<15.4f} {diff:+.4f}\")\n","\n","    print(f\"\\nEvaluation Modes:\")\n","    print(f\"  AF (v7): {v7_results['evaluation_metadata']['evaluation_mode']}\")\n","    print(f\"  KFS (v8): {v8_results['evaluation_metadata']['evaluation_mode']}\")\n","\n","    if 'kfs_late_fusion_info' in v8_results:\n","        print(f\"\\nKFS Late Fusion Strategy:\")\n","        print(f\"  Frames used: {v8_results['kfs_late_fusion_info']['total_frames']}\")\n","        print(f\"  Video predictions: {v8_results['kfs_late_fusion_info']['total_videos']}\")\n","        print(f\"  Aggregation: {v8_results['kfs_late_fusion_info']['aggregation_method']}\")\n","\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"CASME II MOBILENETV3-SMALL M2 MFS-PREP EVALUATION COMPLETED\")\n","print(\"=\" * 70)\n","print(f\"Evaluated datasets: {EVALUATE_DATASETS}\")\n","print(\"Next: Cell 4 - Generate confusion matrices and visualization\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"Dav7rAWXG37P","executionInfo":{"status":"ok","timestamp":1764311271082,"user_tz":-420,"elapsed":31882,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"efab073c-abc2-44f7-a253-5fb7e5c9d692"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II MobileNetV3-Small M2 MFS-PREP Evaluation Framework\n","============================================================\n","Datasets to evaluate: ['v7', 'v8']\n","Input: 224x224 Pure Grayscale (1 channel)\n","Model: From Scratch Training\n","============================================================\n","\n","CASME II MobileNetV3 M2 Evaluation Configuration:\n","  Model: MobileNetV3Small_CASME2_MFS_PREP_Baseline\n","  Methodology: M2\n","  Input resolution: 224x224 Pure Grayscale (1 channel)\n","  Training strategy: from_scratch\n","Loading trained model from: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/09_01_mobilenet_casme2_mfs_prep/casme2_mobilenet_mfs_prep_best_f1.pth\n","Checkpoint loaded using: standard\n","MobileNetV3-Small feature dimension: 1024\n","Training from scratch with 1-channel input\n","MobileNet CASME II: 1024 -> 512 -> 128 -> 7\n","Architecture: Pure grayscale (1ch) from scratch\n","Model state loaded with strict=True\n","Model loaded successfully:\n","  Best validation F1: 0.2209\n","  Best validation accuracy: 0.3333\n","  Best epoch: 12\n","\n","======================================================================\n","EVALUATING DATASET: V7\n","======================================================================\n","\n","Test Dataset Configuration:\n","  Version: v7\n","  Variant: AF\n","  Description: Apex Frame with Face-Aware Preprocessing\n","  Frame strategy: apex_frame\n","  Evaluation mode: frame_level\n","  Dataset path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7\n","\n","Creating CASME II test dataset from AF...\n","Loading CASME II test dataset for evaluation...\n","Found 28 image files in directory\n","Loaded 28 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 10 samples (35.7%)\n","  disgust: 7 samples (25.0%)\n","  happiness: 4 samples (14.3%)\n","  repression: 3 samples (10.7%)\n","  surprise: 3 samples (10.7%)\n","  sadness: 1 samples (3.6%)\n","Unique video IDs: 28\n","Missing classes in test set: ['fear']\n","Preloading 28 test images to RAM...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test to RAM: 100%|██████████| 28/28 [00:00<00:00, 41.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["RAM caching completed: 28 test images\n","\n","Running frame-level evaluation for AF...\n","Running frame-level inference...\n"]},{"output_type":"stream","name":"stderr","text":["Frame-level inference: 100%|██████████| 2/2 [00:13<00:00,  6.76s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Evaluation results saved:\n","  Main results: casme2_mobilenet_mfs_prep_evaluation_results_v7.json\n","  Wrong predictions: casme2_mobilenet_mfs_prep_wrong_predictions_v7.json\n","\n","============================================================\n","EVALUATION RESULTS - AF (v7)\n","============================================================\n","\n","Overall Performance:\n","  Accuracy:  0.3571\n","  Precision: 0.1981\n","  Recall:    0.2202\n","  F1 Score:  0.2037\n","  AUC:       0.4922\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.5556, Support=10\n","  disgust [Present]: F1=0.4444, Support=7\n","  happiness [Present]: F1=0.2222, Support=4\n","  repression [Present]: F1=0.0000, Support=3\n","  surprise [Present]: F1=0.0000, Support=3\n","  sadness [Present]: F1=0.0000, Support=1\n","  fear [Missing]: F1=0.0000, Support=0\n","\n","Wrong Predictions Analysis:\n","  Total errors: 18 / 28\n","  Error rate: 64.29%\n","\n","Inference Performance:\n","  Total time: 13.52s\n","  Speed: 482.9 ms/sample\n","\n","======================================================================\n","EVALUATING DATASET: V8\n","======================================================================\n","\n","Test Dataset Configuration:\n","  Version: v8\n","  Variant: KFS\n","  Description: Key Frame Sequence with Face-Aware Preprocessing\n","  Frame strategy: key_frame_sequence\n","  Evaluation mode: video_level\n","  Aggregation: late_fusion\n","  Dataset path: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8\n","\n","Creating CASME II test dataset from KFS...\n","Loading CASME II test dataset for evaluation...\n","Found 84 image files in directory\n","Loaded 84 CASME II test samples for evaluation\n","Test set class distribution:\n","  others: 30 samples (35.7%)\n","  disgust: 21 samples (25.0%)\n","  happiness: 12 samples (14.3%)\n","  repression: 9 samples (10.7%)\n","  surprise: 9 samples (10.7%)\n","  sadness: 3 samples (3.6%)\n","Unique video IDs: 84\n","Missing classes in test set: ['fear']\n","Preloading 84 test images to RAM...\n"]},{"output_type":"stream","name":"stderr","text":["Loading test to RAM: 100%|██████████| 84/84 [00:02<00:00, 39.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["RAM caching completed: 84 test images\n","\n","Running video-level evaluation with late fusion for KFS...\n","Running video-level inference with late fusion...\n"]},{"output_type":"stream","name":"stderr","text":["Frame inference: 100%|██████████| 6/6 [00:13<00:00,  2.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Aggregating frame predictions to video level...\n","Evaluation results saved:\n","  Main results: casme2_mobilenet_mfs_prep_evaluation_results_v8.json\n","  Wrong predictions: casme2_mobilenet_mfs_prep_wrong_predictions_v8.json\n","\n","============================================================\n","EVALUATION RESULTS - KFS (v8)\n","============================================================\n","\n","Overall Performance:\n","  Accuracy:  0.3452\n","  Precision: 0.2009\n","  Recall:    0.2229\n","  F1 Score:  0.2084\n","  AUC:       0.0000\n","\n","Late Fusion Info:\n","  Total frames processed: 84\n","  Video-level predictions: 84\n","  Aggregation method: average_probability\n","\n","Per-Class Performance:\n","  others [Present]: F1=0.5172, Support=30\n","  disgust [Present]: F1=0.3774, Support=21\n","  happiness [Present]: F1=0.2308, Support=12\n","  repression [Present]: F1=0.1250, Support=9\n","  surprise [Present]: F1=0.0000, Support=9\n","  sadness [Present]: F1=0.0000, Support=3\n","  fear [Missing]: F1=0.0000, Support=0\n","\n","Wrong Predictions Analysis:\n","  Total errors: 55 / 84\n","  Error rate: 65.48%\n","\n","Inference Performance:\n","  Total time: 13.40s\n","  Speed: 159.6 ms/sample\n","\n","======================================================================\n","COMPARATIVE ANALYSIS: AF (v7) vs KFS (v8)\n","======================================================================\n","\n","Overall Performance Comparison:\n","Metric               AF (v7)         KFS (v8)        Difference     \n","-----------------------------------------------------------------\n","accuracy             0.3571          0.3452          -0.0119\n","macro_precision      0.1981          0.2009          +0.0028\n","macro_recall         0.2202          0.2229          +0.0026\n","macro_f1             0.2037          0.2084          +0.0047\n","macro_auc            0.4922          0.0000          -0.4922\n","\n","Evaluation Modes:\n","  AF (v7): frame_level\n","  KFS (v8): video_level\n","\n","KFS Late Fusion Strategy:\n","  Frames used: 84\n","  Video predictions: 84\n","  Aggregation: average_probability\n","\n","======================================================================\n","CASME II MOBILENETV3-SMALL M2 MFS-PREP EVALUATION COMPLETED\n","======================================================================\n","Evaluated datasets: ['v7', 'v8']\n","Next: Cell 4 - Generate confusion matrices and visualization\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title Cell 4: CASME II MobileNetV3-Small M2 MFS-PREP Confusion Matrix Generation\n","\n","# File: 09_01_MobileNet_CASME2_MFS_PREP_Cell4.py\n","# Location: experiments/09_01_MobileNet_CASME2-MFS-PREP.ipynb\n","# Purpose: Generate professional confusion matrix visualizations for v7 (AF) and v8 (KFS) test sets\n","\n","import json\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import seaborn as sns\n","from datetime import datetime\n","\n","print(\"CASME II MobileNetV3-Small M2 MFS-PREP Confusion Matrix Generation\")\n","print(\"=\" * 60)\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/09_01_mobilenet_casme2_mfs_prep\"\n","\n","def find_evaluation_json_files_casme2(results_path):\n","    json_files = {}\n","\n","    eval_dir = f\"{results_path}/evaluation_results\"\n","\n","    if os.path.exists(eval_dir):\n","        for version in ['v7', 'v8']:\n","            eval_pattern = f\"{eval_dir}/casme2_mobilenet_mfs_prep_evaluation_results_{version}.json\"\n","            eval_files = glob.glob(eval_pattern)\n","\n","            if eval_files:\n","                json_files[f'main_{version}'] = eval_files[0]\n","                print(f\"Found {version.upper()} evaluation file: {os.path.basename(eval_files[0])}\")\n","\n","        if not json_files:\n","            print(f\"WARNING: No evaluation results found in {eval_dir}\")\n","            print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","    else:\n","        print(f\"ERROR: Evaluation directory not found: {eval_dir}\")\n","\n","    return json_files\n","\n","def load_evaluation_results_casme2(json_path):\n","    try:\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"Successfully loaded: {os.path.basename(json_path)}\")\n","        return data\n","    except Exception as e:\n","        print(f\"ERROR loading {json_path}: {str(e)}\")\n","        return None\n","\n","def calculate_weighted_f1_casme2(per_class_performance):\n","    total_support = sum([class_data['support'] for class_data in per_class_performance.values()\n","                        if class_data['support'] > 0])\n","\n","    if total_support == 0:\n","        return 0.0\n","\n","    weighted_f1 = 0.0\n","\n","    for class_name, class_data in per_class_performance.items():\n","        if class_data['support'] > 0:\n","            weight = class_data['support'] / total_support\n","            weighted_f1 += class_data['f1_score'] * weight\n","\n","    return weighted_f1\n","\n","def calculate_balanced_accuracy_casme2(confusion_matrix):\n","    cm = np.array(confusion_matrix)\n","    n_classes = cm.shape[0]\n","\n","    per_class_balanced_acc = []\n","\n","    classes_with_samples = []\n","    for i in range(n_classes):\n","        if cm[i, :].sum() > 0:\n","            classes_with_samples.append(i)\n","\n","    for i in classes_with_samples:\n","        tp = cm[i, i]\n","        fn = cm[i, :].sum() - tp\n","        fp = cm[:, i].sum() - tp\n","        tn = cm.sum() - tp - fn - fp\n","\n","        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n","\n","        class_balanced_acc = (sensitivity + specificity) / 2\n","        per_class_balanced_acc.append(class_balanced_acc)\n","\n","    balanced_acc = np.mean(per_class_balanced_acc) if per_class_balanced_acc else 0.0\n","\n","    return balanced_acc\n","\n","def determine_text_color_casme2(color_value, threshold=0.5):\n","    return 'white' if color_value > threshold else 'black'\n","\n","def create_confusion_matrix_plot_casme2(data, output_path, test_version):\n","    meta = data['evaluation_metadata']\n","    class_names = meta['class_names']\n","    cm = np.array(data['confusion_matrix'], dtype=int)\n","    overall = data['overall_performance']\n","    per_class = data['per_class_performance']\n","    test_config = data.get('test_configuration', {})\n","\n","    test_desc = test_config.get('description', test_version)\n","    variant = test_config.get('variant', test_version.upper())\n","    methodology = meta.get('methodology', 'M2')\n","    input_res = meta.get('input_resolution', '224x224 Pure Grayscale (1 channel)')\n","    training_strategy = meta.get('training_strategy', 'from_scratch')\n","\n","    print(f\"Processing confusion matrix for {test_version.upper()}\")\n","    print(f\"Confusion matrix shape: {cm.shape}\")\n","\n","    macro_f1 = overall.get('macro_f1', 0.0)\n","    accuracy = overall.get('accuracy', 0.0)\n","    weighted_f1 = calculate_weighted_f1_casme2(per_class)\n","    balanced_acc = calculate_balanced_accuracy_casme2(cm)\n","\n","    print(f\"Metrics - Macro F1: {macro_f1:.4f}, Weighted F1: {weighted_f1:.4f}, \"\n","          f\"Acc: {accuracy:.4f}, Balanced Acc: {balanced_acc:.4f}\")\n","\n","    row_sums = cm.sum(axis=1, keepdims=True)\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        cm_pct = np.divide(cm, row_sums, where=(row_sums!=0))\n","        cm_pct = np.nan_to_num(cm_pct)\n","\n","    fig, ax = plt.subplots(figsize=(12, 10))\n","\n","    cmap = 'Blues'\n","\n","    im = ax.imshow(cm_pct, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=0.8)\n","\n","    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","    cbar.set_label('True Class Percentage', rotation=270, labelpad=15, fontsize=11)\n","\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            count = cm[i, j]\n","\n","            if row_sums[i, 0] > 0:\n","                percentage = cm_pct[i, j] * 100\n","                text = f\"{count}\\n{percentage:.1f}%\"\n","            else:\n","                text = f\"{count}\\n(N/A)\"\n","\n","            cell_value = cm_pct[i, j]\n","            text_color = determine_text_color_casme2(cell_value, threshold=0.4)\n","\n","            ax.text(j, i, text, ha=\"center\", va=\"center\",\n","                   color=text_color, fontsize=9, fontweight='bold')\n","\n","    ax.set_xticks(np.arange(len(class_names)))\n","    ax.set_yticks(np.arange(len(class_names)))\n","    ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=10)\n","    ax.set_yticklabels(class_names, fontsize=10)\n","    ax.set_xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n","    ax.set_ylabel(\"True Label\", fontsize=12, fontweight='bold')\n","\n","    missing_classes = meta.get('missing_classes', [])\n","    note_text = f\"Test: {test_desc} ({variant})\\n{methodology} | {input_res}\\nTraining: {training_strategy}\"\n","    if missing_classes:\n","        note_text += f\"\\nMissing: {', '.join(missing_classes)}\"\n","\n","    ax.text(0.02, 0.98, note_text, transform=ax.transAxes, fontsize=9,\n","            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n","\n","    title = f\"CASME II MobileNetV3-Small M2 MFS-PREP - {variant}\\n\"\n","    title += f\"Macro F1: {macro_f1:.4f}  |  Weighted F1: {weighted_f1:.4f}  |  Acc: {accuracy:.4f}  |  Balanced Acc: {balanced_acc:.4f}\"\n","    ax.set_title(title, fontsize=12, pad=25, fontweight='bold')\n","\n","    plt.tight_layout()\n","    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n","    plt.close(fig)\n","\n","    print(f\"Confusion matrix saved to: {os.path.basename(output_path)}\")\n","\n","    return {\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        'accuracy': accuracy,\n","        'balanced_accuracy': balanced_acc,\n","        'missing_classes': missing_classes,\n","        'test_version': test_version,\n","        'variant': variant\n","    }\n","\n","json_files = find_evaluation_json_files_casme2(RESULTS_ROOT)\n","\n","if not json_files:\n","    print(f\"ERROR: No evaluation JSON files found in {RESULTS_ROOT}\")\n","    print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","else:\n","    print(f\"\\nFound {len([k for k in json_files.keys() if k.startswith('main_')])} evaluation result(s)\")\n","\n","output_dir = f\"{RESULTS_ROOT}/confusion_matrix_analysis\"\n","Path(output_dir).mkdir(parents=True, exist_ok=True)\n","\n","results_summary = {}\n","generated_files = []\n","\n","for version in ['v7', 'v8']:\n","    main_key = f'main_{version}'\n","\n","    if main_key in json_files:\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Processing {version.upper()} Confusion Matrix\")\n","        print(f\"{'='*60}\")\n","\n","        eval_data = load_evaluation_results_casme2(json_files[main_key])\n","\n","        if eval_data is not None:\n","            try:\n","                cm_output_path = os.path.join(output_dir, f\"confusion_matrix_CASME2_MobileNet_MFS_PREP_{version.upper()}.png\")\n","                metrics = create_confusion_matrix_plot_casme2(eval_data, cm_output_path, version)\n","                generated_files.append(cm_output_path)\n","\n","                results_summary[version] = metrics\n","\n","                print(f\"SUCCESS: {version.upper()} confusion matrix generated\")\n","\n","            except Exception as e:\n","                print(f\"ERROR: Failed to generate {version.upper()} confusion matrix: {str(e)}\")\n","                import traceback\n","                traceback.print_exc()\n","        else:\n","            print(f\"ERROR: Could not load {version.upper()} evaluation data\")\n","\n","if generated_files:\n","    print(f\"\\n\" + \"=\" * 60)\n","    print(\"CASME II MOBILENETV3-SMALL M2 MFS-PREP CONFUSION MATRIX GENERATION COMPLETED\")\n","    print(\"=\" * 60)\n","\n","    print(f\"\\nGenerated confusion matrix files:\")\n","    for file_path in generated_files:\n","        filename = os.path.basename(file_path)\n","        print(f\"  {filename}\")\n","\n","    print(f\"\\nPerformance Summary:\")\n","    for version in ['v7', 'v8']:\n","        if version in results_summary:\n","            metrics = results_summary[version]\n","            variant = metrics.get('variant', version.upper())\n","            print(f\"\\n{variant}:\")\n","            print(f\"  Macro F1:       {metrics['macro_f1']:.4f}\")\n","            print(f\"  Weighted F1:    {metrics['weighted_f1']:.4f}\")\n","            print(f\"  Accuracy:       {metrics['accuracy']:.4f}\")\n","            print(f\"  Balanced Acc:   {metrics['balanced_accuracy']:.4f}\")\n","\n","            if metrics['missing_classes']:\n","                print(f\"  Missing classes: {len(metrics['missing_classes'])}\")\n","\n","    if len(results_summary) == 2:\n","        print(f\"\\nComparative Analysis:\")\n","        v8_f1 = results_summary['v8']['macro_f1']\n","        v7_f1 = results_summary['v7']['macro_f1']\n","        delta_f1 = v8_f1 - v7_f1\n","\n","        v8_variant = results_summary['v8'].get('variant', 'KFS')\n","        v7_variant = results_summary['v7'].get('variant', 'AF')\n","\n","        print(f\"  {v8_variant} vs {v7_variant} (Macro F1): {v8_f1:.4f} vs {v7_f1:.4f}\")\n","        print(f\"  Delta ({v8_variant} - {v7_variant}): {delta_f1:+.4f}\")\n","\n","        if delta_f1 > 0:\n","            improvement_pct = (delta_f1 / v7_f1) * 100\n","            print(f\"  {v8_variant} improves by {improvement_pct:.1f}% over {v7_variant}\")\n","        else:\n","            degradation_pct = (abs(delta_f1) / v8_f1) * 100\n","            print(f\"  {v8_variant} degrades by {degradation_pct:.1f}% from {v7_variant}\")\n","\n","    print(f\"\\nFiles saved in: {output_dir}\")\n","    print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","else:\n","    print(f\"\\nERROR: No confusion matrices were generated\")\n","    print(\"Please check:\")\n","    print(\"1. Cell 3 evaluation results exist\")\n","    print(\"2. JSON file structure is correct\")\n","    print(\"3. No file permission issues\")\n","\n","print(\"\\nCell 4 completed - CASME II MobileNetV3-Small M2 MFS-PREP confusion matrix analysis generated\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"qYbAbZ5jIxnJ","executionInfo":{"status":"ok","timestamp":1764311274290,"user_tz":-420,"elapsed":3161,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"8acc3ee6-9009-452e-f372-69b015a986dc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II MobileNetV3-Small M2 MFS-PREP Confusion Matrix Generation\n","============================================================\n","Found V7 evaluation file: casme2_mobilenet_mfs_prep_evaluation_results_v7.json\n","Found V8 evaluation file: casme2_mobilenet_mfs_prep_evaluation_results_v8.json\n","\n","Found 2 evaluation result(s)\n","\n","============================================================\n","Processing V7 Confusion Matrix\n","============================================================\n","Successfully loaded: casme2_mobilenet_mfs_prep_evaluation_results_v7.json\n","Processing confusion matrix for V7\n","Confusion matrix shape: (6, 6)\n","Metrics - Macro F1: 0.2037, Weighted F1: 0.3413, Acc: 0.3571, Balanced Acc: 0.5412\n","Confusion matrix saved to: confusion_matrix_CASME2_MobileNet_MFS_PREP_V7.png\n","SUCCESS: V7 confusion matrix generated\n","\n","============================================================\n","Processing V8 Confusion Matrix\n","============================================================\n","Successfully loaded: casme2_mobilenet_mfs_prep_evaluation_results_v8.json\n","Processing confusion matrix for V8\n","Confusion matrix shape: (6, 6)\n","Metrics - Macro F1: 0.2084, Weighted F1: 0.3254, Acc: 0.3452, Balanced Acc: 0.5395\n","Confusion matrix saved to: confusion_matrix_CASME2_MobileNet_MFS_PREP_V8.png\n","SUCCESS: V8 confusion matrix generated\n","\n","============================================================\n","CASME II MOBILENETV3-SMALL M2 MFS-PREP CONFUSION MATRIX GENERATION COMPLETED\n","============================================================\n","\n","Generated confusion matrix files:\n","  confusion_matrix_CASME2_MobileNet_MFS_PREP_V7.png\n","  confusion_matrix_CASME2_MobileNet_MFS_PREP_V8.png\n","\n","Performance Summary:\n","\n","AF:\n","  Macro F1:       0.2037\n","  Weighted F1:    0.3413\n","  Accuracy:       0.3571\n","  Balanced Acc:   0.5412\n","  Missing classes: 1\n","\n","KFS:\n","  Macro F1:       0.2084\n","  Weighted F1:    0.3254\n","  Accuracy:       0.3452\n","  Balanced Acc:   0.5395\n","  Missing classes: 1\n","\n","Comparative Analysis:\n","  KFS vs AF (Macro F1): 0.2084 vs 0.2037\n","  Delta (KFS - AF): +0.0047\n","  KFS improves by 2.3% over AF\n","\n","Files saved in: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/09_01_mobilenet_casme2_mfs_prep/confusion_matrix_analysis\n","Analysis completed at: 2025-11-28 06:27:56\n","\n","Cell 4 completed - CASME II MobileNetV3-Small M2 MFS-PREP confusion matrix analysis generated\n"]}]}]}