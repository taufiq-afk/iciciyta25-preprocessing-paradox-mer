{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyPbjs/YuE8R5DwyuNB8+syl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","collapsed":true,"id":"easKbazSLrB_","executionInfo":{"status":"ok","timestamp":1764312466509,"user_tz":-420,"elapsed":50779,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"17938d59-7d9e-4708-bb4a-580db35a7086"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","CASME II CNN BASELINE - ConvNeXT-Tiny M2 MFS-PREP\n","============================================================\n","\n","[1] Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully\n","\n","[2] Importing required libraries...\n","CASME II ConvNeXT-Tiny M2 MFS-PREP - Infrastructure Configuration\n","============================================================\n","Loading CASME II v9 preprocessing metadata...\n","Dataset variant: MFS\n","Processing date: 2025-10-19T08:20:12.098301\n","Preprocessing method: face_bbox_expansion_all_directions\n","Total images processed: 2774\n","Face detection rate: 100.00%\n","Target size: 224x224px\n","BBox expansion: 20px\n","Image format: Grayscale (1 channel)\n","\n","Dataset split information:\n","  Train: 2613 frames\n","  Validation: 78 frames\n","  Test: 83 frames\n","\n","==================================================\n","EXPERIMENT CONFIGURATION - CNN M2 MFS-PREP\n","==================================================\n","Model: ConvNeXT-Tiny (TIMM)\n","Methodology: M2 (Face-Aware Preprocessing)\n","Input Resolution: 224x224 Pure Grayscale (1 channel)\n","Training Strategy: From Scratch (No Pretrained Weights)\n","Preprocessing: Face detection + crop + grayscale\n","Loss Function: Focal Loss\n","  Gamma: 2.5\n","  Alpha Weights: [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","  Alpha Sum: 0.999\n","==================================================\n","\n","Device: cuda\n","GPU: NVIDIA L4 (23.8 GB)\n","L4: Balanced performance configuration\n","RAM preload workers: 32\n","\n","Loading v9 class distribution...\n","\n","v9 Train distribution: [1027, 650, 325, 273, 260, 65, 13]\n","v9 Val distribution: [30, 18, 9, 9, 6, 3, 3]\n","v9 Test distribution: [30, 21, 12, 8, 9, 3, 0]\n","Applied Focal Loss alpha weights: [0.053 0.067 0.094 0.102 0.106 0.201 0.376]\n","Alpha weights sum: 0.999\n","\n","ConvNeXT-Tiny Configuration Summary:\n","  Model: convnext_tiny\n","  Input size: 224x224 Pure Grayscale (1ch)\n","  Methodology: M2 (face-aware preprocessing)\n","  Training: From Scratch (No Pretrained Weights)\n","  Learning rate: 5e-05\n","  Batch size: 12\n","  Dataset version: v9\n","  Frame strategy: multi_frame_sampling\n","\n","Setting up transforms for M2 methodology (224x224 pure grayscale)...\n","M2 transforms configured: 224x224 pure grayscale (1 channel) with standardization\n","\n","Dataset paths:\n","Root: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9\n","  Train: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/train\n","  Validation: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/val\n","  Test: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/test\n","\n","ConvNeXT-Tiny CASME II architecture validation...\n","ConvNeXT-Tiny feature dimension: 768\n","Training from scratch with 1-channel input\n","ConvNeXT CASME II: 768 -> 512 -> 128 -> 7\n","Architecture: Pure grayscale (1ch) from scratch\n","Validation failed: Expected more than 1 value per channel when training, got input size torch.Size([1, 512])\n","\n","============================================================\n","CASME II CONVNEXT-TINY M2 MFS-PREP CONFIGURATION COMPLETE\n","============================================================\n","Loss Configuration:\n","  Function: Optimized Focal Loss\n","  Gamma: 2.5\n","  Per-class Alpha: [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","  Alpha Sum: 0.999\n","\n","Model Configuration:\n","  Architecture: ConvNeXT-Tiny\n","  Parameters: ~28M (largest CNN baseline)\n","  Input Resolution: 224x224 Pure Grayscale (1 channel)\n","  Methodology: M2 (Face-aware preprocessing)\n","  Training Strategy: From Scratch (No Pretrained Weights)\n","  First Conv Layer: Modified to 1-channel input\n","\n","Dataset Configuration:\n","  Version: v9\n","  Frame strategy: multi_frame_sampling\n","  Train augmentation: frame_level_independent\n","  Classes: 7\n","  Train samples: 2613 frames\n","\n","Next: Cell 2 - Dataset Loading and Training Pipeline\n"]}],"source":["# @title Cell 1: CASME II ConvNeXT-Tiny M2 MFS-PREP Infrastructure Configuration\n","\n","# File: 09_03_ConvNeXT_CASME2_MFS_PREP_Cell1.py\n","# Location: experiments/09_03_ConvNeXT_CASME2-MFS-PREP.ipynb\n","# Purpose: ConvNeXT-Tiny for CASME II micro-expression recognition with M2 preprocessed methodology\n","\n","from google.colab import drive\n","print(\"=\" * 60)\n","print(\"CASME II CNN BASELINE - ConvNeXT-Tiny M2 MFS-PREP\")\n","print(\"=\" * 60)\n","print(\"\\n[1] Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Google Drive mounted successfully\")\n","\n","print(\"\\n[2] Importing required libraries...\")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import timm\n","import json\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import time\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","DATASET_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/preprocessed_v9\"\n","CHECKPOINT_ROOT = f\"{PROJECT_ROOT}/models/09_03_convnext_casme2_mfs_prep\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/09_03_convnext_casme2_mfs_prep\"\n","\n","PREPROCESSING_SUMMARY = f\"{DATASET_ROOT}/preprocessing_summary.json\"\n","\n","print(\"CASME II ConvNeXT-Tiny M2 MFS-PREP - Infrastructure Configuration\")\n","print(\"=\" * 60)\n","\n","if not os.path.exists(PREPROCESSING_SUMMARY):\n","    raise FileNotFoundError(f\"v9 preprocessing summary not found: {PREPROCESSING_SUMMARY}\")\n","\n","print(\"Loading CASME II v9 preprocessing metadata...\")\n","with open(PREPROCESSING_SUMMARY, 'r') as f:\n","    preprocessing_info = json.load(f)\n","\n","print(f\"Dataset variant: {preprocessing_info['variant']}\")\n","print(f\"Processing date: {preprocessing_info['processing_date']}\")\n","print(f\"Preprocessing method: {preprocessing_info['preprocessing_method']}\")\n","print(f\"Total images processed: {preprocessing_info['total_processed']}\")\n","print(f\"Face detection rate: {preprocessing_info['face_detection_stats']['detection_rate']:.2%}\")\n","\n","preproc_params = preprocessing_info['preprocessing_parameters']\n","print(f\"Target size: {preproc_params['target_size']}x{preproc_params['target_size']}px\")\n","print(f\"BBox expansion: {preproc_params['bbox_expansion']}px\")\n","print(f\"Image format: Grayscale (1 channel)\")\n","\n","print(f\"\\nDataset split information:\")\n","print(f\"  Train: {preprocessing_info['splits']['train']['total_images']} frames\")\n","print(f\"  Validation: {preprocessing_info['splits']['val']['total_images']} frames\")\n","print(f\"  Test: {preprocessing_info['splits']['test']['total_images']} frames\")\n","\n","USE_FOCAL_LOSS = True\n","FOCAL_LOSS_GAMMA = 2.5\n","\n","CROSSENTROPY_CLASS_WEIGHTS = [1.00, 1.25, 1.76, 1.91, 1.99, 3.76, 7.04]\n","FOCAL_LOSS_ALPHA_WEIGHTS = [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","\n","CONVNEXT_MODEL_NAME = 'convnext_tiny'\n","USE_PURE_GRAYSCALE = True\n","\n","print(\"\\n\" + \"=\" * 50)\n","print(\"EXPERIMENT CONFIGURATION - CNN M2 MFS-PREP\")\n","print(\"=\" * 50)\n","print(f\"Model: ConvNeXT-Tiny (TIMM)\")\n","print(f\"Methodology: M2 (Face-Aware Preprocessing)\")\n","print(f\"Input Resolution: 224x224 Pure Grayscale (1 channel)\")\n","print(f\"Training Strategy: From Scratch (No Pretrained Weights)\")\n","print(f\"Preprocessing: Face detection + crop + grayscale\")\n","print(f\"Loss Function: Focal Loss\")\n","print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","print(f\"  Alpha Weights: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","print(\"=\" * 50)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n","gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n","\n","print(f\"\\nDevice: {device}\")\n","print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n","\n","if 'A100' in gpu_name:\n","    BATCH_SIZE = 16\n","    NUM_WORKERS = 8\n","    torch.backends.cudnn.benchmark = True\n","    print(\"A100: Optimized batch size for ConvNeXT-Tiny (largest model)\")\n","elif 'L4' in gpu_name:\n","    BATCH_SIZE = 12\n","    NUM_WORKERS = 8\n","    torch.backends.cudnn.benchmark = True\n","    print(\"L4: Balanced performance configuration\")\n","else:\n","    BATCH_SIZE = 8\n","    NUM_WORKERS = 8\n","    print(\"Default GPU: Conservative settings for large model\")\n","\n","RAM_PRELOAD_WORKERS = 32\n","print(f\"RAM preload workers: {RAM_PRELOAD_WORKERS}\")\n","\n","CASME2_CLASSES = ['others', 'disgust', 'happiness', 'repression', 'surprise', 'sadness', 'fear']\n","CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CASME2_CLASSES)}\n","\n","print(\"\\nLoading v9 class distribution...\")\n","train_dist = preprocessing_info['splits']['train']['emotion_distribution']\n","val_dist = preprocessing_info['splits']['val']['emotion_distribution']\n","test_dist = preprocessing_info['splits']['test']['emotion_distribution']\n","\n","def emotion_dist_to_list(emotion_dict, class_names):\n","    return [emotion_dict.get(cls, 0) for cls in class_names]\n","\n","train_dist_list = emotion_dist_to_list(train_dist, CASME2_CLASSES)\n","val_dist_list = emotion_dist_to_list(val_dist, CASME2_CLASSES)\n","test_dist_list = emotion_dist_to_list(test_dist, CASME2_CLASSES)\n","\n","print(f\"\\nv9 Train distribution: {train_dist_list}\")\n","print(f\"v9 Val distribution: {val_dist_list}\")\n","print(f\"v9 Test distribution: {test_dist_list}\")\n","\n","if USE_FOCAL_LOSS:\n","    class_weights = torch.tensor(FOCAL_LOSS_ALPHA_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied Focal Loss alpha weights: {class_weights.cpu().numpy()}\")\n","    print(f\"Alpha weights sum: {class_weights.sum().item():.3f}\")\n","else:\n","    class_weights = torch.tensor(CROSSENTROPY_CLASS_WEIGHTS, dtype=torch.float32).to(device)\n","    print(f\"Applied CrossEntropy class weights: {class_weights.cpu().numpy()}\")\n","\n","CASME2_CONVNEXT_CONFIG = {\n","    'model_name': CONVNEXT_MODEL_NAME,\n","    'input_size': (224, 224),\n","    'num_classes': 7,\n","    'dropout_rate': 0.3,\n","\n","    'learning_rate': 5e-5,\n","    'weight_decay': 1e-5,\n","    'gradient_clip': 1.0,\n","    'num_epochs': 50,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'device': device,\n","\n","    'scheduler_type': 'plateau',\n","    'scheduler_mode': 'max',\n","    'scheduler_factor': 0.5,\n","    'scheduler_patience': 5,\n","    'scheduler_min_lr': 1e-6,\n","    'scheduler_monitor': 'val_f1_macro',\n","\n","    'use_focal_loss': USE_FOCAL_LOSS,\n","    'focal_loss_gamma': FOCAL_LOSS_GAMMA,\n","    'focal_loss_alpha_weights': FOCAL_LOSS_ALPHA_WEIGHTS,\n","    'crossentropy_class_weights': CROSSENTROPY_CLASS_WEIGHTS,\n","    'class_weights': class_weights,\n","\n","    'use_macro_avg': True,\n","    'early_stopping': False,\n","    'save_best_f1': True,\n","    'save_strategy': 'best_only',\n","\n","    'dataset_version': 'v9',\n","    'methodology': 'M2',\n","    'preprocessing': 'face_aware_preprocessing',\n","    'frame_strategy': 'multi_frame_sampling',\n","    'train_augmentation': 'frame_level_independent',\n","    'image_format': 'pure_grayscale_1channel',\n","    'use_pure_grayscale': USE_PURE_GRAYSCALE,\n","    'use_pretrained_weights': False,\n","    'training_strategy': 'from_scratch',\n","    'preprocessing_details': {\n","        'face_detection': True,\n","        'crop_method': 'face_bbox_expansion',\n","        'target_size': preproc_params['target_size'],\n","        'bbox_expansion': preproc_params['bbox_expansion'],\n","        'grayscale_conversion': True,\n","        'input_channels': 1\n","    }\n","}\n","\n","print(f\"\\nConvNeXT-Tiny Configuration Summary:\")\n","print(f\"  Model: {CASME2_CONVNEXT_CONFIG['model_name']}\")\n","print(f\"  Input size: {CASME2_CONVNEXT_CONFIG['input_size'][0]}x{CASME2_CONVNEXT_CONFIG['input_size'][1]} Pure Grayscale (1ch)\")\n","print(f\"  Methodology: {CASME2_CONVNEXT_CONFIG['methodology']} (face-aware preprocessing)\")\n","print(f\"  Training: From Scratch (No Pretrained Weights)\")\n","print(f\"  Learning rate: {CASME2_CONVNEXT_CONFIG['learning_rate']}\")\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","print(f\"  Dataset version: {CASME2_CONVNEXT_CONFIG['dataset_version']}\")\n","print(f\"  Frame strategy: {CASME2_CONVNEXT_CONFIG['frame_strategy']}\")\n","\n","class OptimizedFocalLoss(nn.Module):\n","    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n","        super(OptimizedFocalLoss, self).__init__()\n","\n","        if alpha is not None:\n","            if isinstance(alpha, list):\n","                self.alpha = torch.tensor(alpha, dtype=torch.float32)\n","            else:\n","                self.alpha = alpha\n","\n","            alpha_sum = self.alpha.sum().item()\n","            if abs(alpha_sum - 1.0) > 0.01:\n","                print(f\"Warning: Alpha weights sum to {alpha_sum:.3f}, expected 1.0\")\n","        else:\n","            self.alpha = None\n","\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","\n","        if self.alpha is not None:\n","            if self.alpha.device != targets.device:\n","                self.alpha = self.alpha.to(targets.device)\n","            alpha_t = self.alpha.gather(0, targets)\n","        else:\n","            alpha_t = 1.0\n","\n","        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","\n","class ConvNeXTCASME2Baseline(nn.Module):\n","    def __init__(self, num_classes, dropout_rate=0.3, in_channels=1):\n","        super(ConvNeXTCASME2Baseline, self).__init__()\n","\n","        self.convnext = timm.create_model(\n","            CONVNEXT_MODEL_NAME,\n","            pretrained=False,\n","            num_classes=0,\n","            global_pool='avg',\n","            in_chans=in_channels\n","        )\n","\n","        for param in self.convnext.parameters():\n","            param.requires_grad = True\n","\n","        with torch.no_grad():\n","            test_input = torch.randn(1, in_channels, 224, 224)\n","            test_output = self.convnext(test_input)\n","            self.convnext_feature_dim = test_output.shape[1]\n","\n","        print(f\"ConvNeXT-Tiny feature dimension: {self.convnext_feature_dim}\")\n","        print(f\"Training from scratch with {in_channels}-channel input\")\n","\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.convnext_feature_dim, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(dropout_rate),\n","\n","            nn.Linear(512, 128),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(dropout_rate),\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","        print(f\"ConvNeXT CASME II: {self.convnext_feature_dim} -> 512 -> 128 -> {num_classes}\")\n","        print(f\"Architecture: Pure grayscale (1ch) from scratch\")\n","\n","    def forward(self, x):\n","        features = self.convnext(x)\n","        processed_features = self.classifier_layers(features)\n","        output = self.classifier(processed_features)\n","        return output\n","\n","def create_optimizer_scheduler_casme2(model, config):\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=config['learning_rate'],\n","        weight_decay=config['weight_decay'],\n","        betas=(0.9, 0.999)\n","    )\n","\n","    if config['scheduler_type'] == 'plateau':\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer,\n","            mode=config['scheduler_mode'],\n","            factor=config['scheduler_factor'],\n","            patience=config['scheduler_patience'],\n","            min_lr=config['scheduler_min_lr']\n","        )\n","        print(f\"Scheduler: ReduceLROnPlateau monitoring {config['scheduler_monitor']}\")\n","    else:\n","        scheduler = None\n","\n","    return optimizer, scheduler\n","\n","print(\"\\nSetting up transforms for M2 methodology (224x224 pure grayscale)...\")\n","\n","convnext_transform_train = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=1),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])\n","])\n","\n","convnext_transform_val = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=1),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])\n","])\n","\n","print(\"M2 transforms configured: 224x224 pure grayscale (1 channel) with standardization\")\n","\n","class CASME2Dataset(Dataset):\n","    def __init__(self, dataset_root, split, transform=None):\n","        self.dataset_root = dataset_root\n","        self.split = split\n","        self.transform = transform\n","        self.images = []\n","        self.labels = []\n","        self.filenames = []\n","\n","        split_path = os.path.join(dataset_root, split)\n","\n","        print(f\"Loading {split} dataset from {split_path}...\")\n","\n","        if not os.path.exists(split_path):\n","            raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n","\n","        all_files = [f for f in os.listdir(split_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","        print(f\"Found {len(all_files)} image files in directory\")\n","\n","        loaded_count = 0\n","\n","        for filename in sorted(all_files):\n","            emotion_found = None\n","            name_without_ext = filename.rsplit('.', 1)[0]\n","\n","            for emotion_class in CASME2_CLASSES:\n","                if emotion_class in name_without_ext.lower():\n","                    emotion_found = emotion_class\n","                    break\n","\n","            if emotion_found and emotion_found in CLASS_TO_IDX:\n","                image_path = os.path.join(split_path, filename)\n","                self.images.append(image_path)\n","                self.labels.append(CLASS_TO_IDX[emotion_found])\n","                self.filenames.append(filename)\n","                loaded_count += 1\n","\n","        print(f\"Loaded {len(self.images)} samples for {split} split\")\n","        self._print_distribution()\n","\n","    def _print_distribution(self):\n","        if len(self.labels) == 0:\n","            print(\"No samples to display distribution\")\n","            return\n","\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        for label, count in sorted(label_counts.items()):\n","            class_name = CASME2_CLASSES[label]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.images[idx]\n","        image = Image.open(image_path)\n","\n","        if image.mode != 'L':\n","            image = image.convert('L')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.filenames[idx]\n","\n","os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/training_logs\", exist_ok=True)\n","os.makedirs(f\"{RESULTS_ROOT}/evaluation_results\", exist_ok=True)\n","\n","TRAIN_PATH = DATASET_ROOT\n","VAL_PATH = DATASET_ROOT\n","TEST_PATH = DATASET_ROOT\n","\n","print(f\"\\nDataset paths:\")\n","print(f\"Root: {DATASET_ROOT}\")\n","print(f\"  Train: {TRAIN_PATH}/train\")\n","print(f\"  Validation: {VAL_PATH}/val\")\n","print(f\"  Test: {TEST_PATH}/test\")\n","\n","print(\"\\nConvNeXT-Tiny CASME II architecture validation...\")\n","\n","try:\n","    test_model = ConvNeXTCASME2Baseline(num_classes=7, dropout_rate=0.3, in_channels=1).to(device)\n","    test_input = torch.randn(1, 1, 224, 224).to(device)\n","    test_output = test_model(test_input)\n","\n","    print(f\"Validation successful: Output shape {test_output.shape}\")\n","    print(f\"Expected output shape: [1, 7] for CASME II 7 classes\")\n","    print(f\"Pure grayscale (1 channel) architecture validated\")\n","\n","    del test_model, test_input, test_output\n","    torch.cuda.empty_cache()\n","\n","except Exception as e:\n","    print(f\"Validation failed: {e}\")\n","\n","def create_criterion_casme2(weights, use_focal_loss=False, alpha_weights=None, gamma=2.0):\n","    if use_focal_loss:\n","        print(f\"Using Optimized Focal Loss with gamma={gamma}\")\n","        if alpha_weights:\n","            print(f\"Per-class alpha weights: {alpha_weights}\")\n","            print(f\"Alpha sum: {sum(alpha_weights):.3f}\")\n","        return OptimizedFocalLoss(alpha=alpha_weights, gamma=gamma)\n","    else:\n","        print(f\"Using CrossEntropy Loss with optimized class weights\")\n","        print(f\"Class weights: {weights.cpu().numpy()}\")\n","        return nn.CrossEntropyLoss(weight=weights)\n","\n","GLOBAL_CONFIG_CASME2 = {\n","    'device': device,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'num_classes': 7,\n","    'class_weights': class_weights,\n","    'class_names': CASME2_CLASSES,\n","    'class_to_idx': CLASS_TO_IDX,\n","    'transform_train': convnext_transform_train,\n","    'transform_val': convnext_transform_val,\n","    'convnext_config': CASME2_CONVNEXT_CONFIG,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'results_root': RESULTS_ROOT,\n","    'dataset_root': DATASET_ROOT,\n","    'train_path': TRAIN_PATH,\n","    'val_path': VAL_PATH,\n","    'test_path': TEST_PATH,\n","    'optimizer_scheduler_factory': create_optimizer_scheduler_casme2,\n","    'criterion_factory': create_criterion_casme2\n","}\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"CASME II CONVNEXT-TINY M2 MFS-PREP CONFIGURATION COMPLETE\")\n","print(\"=\" * 60)\n","\n","print(f\"Loss Configuration:\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Function: Optimized Focal Loss\")\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Per-class Alpha: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Alpha Sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f}\")\n","else:\n","    print(f\"  Function: CrossEntropy with Optimized Weights\")\n","    print(f\"  Class Weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","\n","print(f\"\\nModel Configuration:\")\n","print(f\"  Architecture: ConvNeXT-Tiny\")\n","print(f\"  Parameters: ~28M (largest CNN baseline)\")\n","print(f\"  Input Resolution: 224x224 Pure Grayscale (1 channel)\")\n","print(f\"  Methodology: M2 (Face-aware preprocessing)\")\n","print(f\"  Training Strategy: From Scratch (No Pretrained Weights)\")\n","print(f\"  First Conv Layer: Modified to 1-channel input\")\n","\n","print(f\"\\nDataset Configuration:\")\n","print(f\"  Version: {CASME2_CONVNEXT_CONFIG['dataset_version']}\")\n","print(f\"  Frame strategy: {CASME2_CONVNEXT_CONFIG['frame_strategy']}\")\n","print(f\"  Train augmentation: {CASME2_CONVNEXT_CONFIG['train_augmentation']}\")\n","print(f\"  Classes: {len(CASME2_CLASSES)}\")\n","print(f\"  Train samples: {preprocessing_info['splits']['train']['total_images']} frames\")\n","\n","print(\"\\nNext: Cell 2 - Dataset Loading and Training Pipeline\")"]},{"cell_type":"code","source":["# @title Cell 2: CASME II ConvNeXT-Tiny M2 MFS-PREP Training Pipeline\n","\n","# File: 09_03_ConvNeXT_CASME2_MFS_PREP_Cell2.py\n","# Location: experiments/09_03_ConvNeXT_CASME2-MFS-PREP.ipynb\n","# Purpose: Training loop with RAM caching, robust checkpointing, and comprehensive logging\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from sklearn.metrics import f1_score, precision_recall_fscore_support\n","import time\n","import json\n","import os\n","import shutil\n","from datetime import datetime\n","from concurrent.futures import ThreadPoolExecutor\n","from PIL import Image\n","\n","print(\"CASME II ConvNeXT-Tiny M2 MFS-PREP Training Pipeline\")\n","print(\"=\" * 60)\n","\n","class RAMCachedDataset(torch.utils.data.Dataset):\n","    def __init__(self, base_dataset, preload=True, num_workers=32):\n","        self.base_dataset = base_dataset\n","        self.transform = base_dataset.transform\n","        self.cached_images = {}\n","        self.labels = base_dataset.labels\n","        self.filenames = base_dataset.filenames\n","\n","        if preload:\n","            print(f\"Preloading {len(base_dataset)} images to RAM using {num_workers} workers...\")\n","            start_time = time.time()\n","\n","            with ThreadPoolExecutor(max_workers=num_workers) as executor:\n","                futures = []\n","                for idx in range(len(base_dataset)):\n","                    future = executor.submit(self._load_image, idx)\n","                    futures.append((idx, future))\n","\n","                for idx, future in futures:\n","                    self.cached_images[idx] = future.result()\n","\n","            elapsed = time.time() - start_time\n","\n","            sample_image = list(self.cached_images.values())[0]\n","            if isinstance(sample_image, Image.Image):\n","                image_array = np.array(sample_image)\n","                bytes_per_image = image_array.nbytes\n","            else:\n","                bytes_per_image = sample_image.size[0] * sample_image.size[1]\n","\n","            total_memory_mb = (bytes_per_image * len(self.cached_images)) / (1024 * 1024)\n","\n","            print(f\"Preloading completed in {elapsed:.2f}s\")\n","            print(f\"Cached {len(self.cached_images)} images (~{total_memory_mb:.2f} MB)\")\n","\n","    def _load_image(self, idx):\n","        image_path = self.base_dataset.images[idx]\n","        image = Image.open(image_path)\n","\n","        if image.mode != 'L':\n","            image = image.convert('L')\n","\n","        return image\n","\n","    def __len__(self):\n","        return len(self.base_dataset)\n","\n","    def __getitem__(self, idx):\n","        if idx in self.cached_images:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            image = self._load_image(idx)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        label = self.labels[idx]\n","        filename = self.filenames[idx]\n","\n","        return image, label, filename\n","\n","print(\"\\nInitializing datasets...\")\n","train_dataset_base = CASME2Dataset(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='train',\n","    transform=None\n",")\n","\n","val_dataset_base = CASME2Dataset(\n","    dataset_root=GLOBAL_CONFIG_CASME2['dataset_root'],\n","    split='val',\n","    transform=None\n",")\n","\n","print(\"\\nCreating RAM-cached datasets...\")\n","train_dataset = RAMCachedDataset(\n","    train_dataset_base,\n","    preload=True,\n","    num_workers=RAM_PRELOAD_WORKERS\n",")\n","train_dataset.transform = GLOBAL_CONFIG_CASME2['transform_train']\n","\n","val_dataset = RAMCachedDataset(\n","    val_dataset_base,\n","    preload=True,\n","    num_workers=RAM_PRELOAD_WORKERS\n",")\n","val_dataset.transform = GLOBAL_CONFIG_CASME2['transform_val']\n","\n","print(\"\\nCreating data loaders...\")\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=GLOBAL_CONFIG_CASME2['batch_size'],\n","    shuffle=True,\n","    num_workers=0,\n","    pin_memory=True\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=GLOBAL_CONFIG_CASME2['batch_size'],\n","    shuffle=False,\n","    num_workers=0,\n","    pin_memory=True\n",")\n","\n","print(f\"Train batches: {len(train_loader)}\")\n","print(f\"Validation batches: {len(val_loader)}\")\n","\n","print(\"\\nInitializing ConvNeXT-Tiny model...\")\n","model = ConvNeXTCASME2Baseline(\n","    num_classes=GLOBAL_CONFIG_CASME2['num_classes'],\n","    dropout_rate=CASME2_CONVNEXT_CONFIG['dropout_rate'],\n","    in_channels=1\n",").to(GLOBAL_CONFIG_CASME2['device'])\n","\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f\"Total parameters: {total_params:,}\")\n","print(f\"Trainable parameters: {trainable_params:,}\")\n","print(f\"Model size: ~{total_params / 1e6:.1f}M parameters\")\n","\n","criterion = GLOBAL_CONFIG_CASME2['criterion_factory'](\n","    weights=GLOBAL_CONFIG_CASME2['class_weights'],\n","    use_focal_loss=CASME2_CONVNEXT_CONFIG['use_focal_loss'],\n","    alpha_weights=CASME2_CONVNEXT_CONFIG['focal_loss_alpha_weights'],\n","    gamma=CASME2_CONVNEXT_CONFIG['focal_loss_gamma']\n",")\n","\n","optimizer, scheduler = GLOBAL_CONFIG_CASME2['optimizer_scheduler_factory'](\n","    model, CASME2_CONVNEXT_CONFIG\n",")\n","\n","print(f\"\\nOptimizer: AdamW\")\n","print(f\"Learning rate: {CASME2_CONVNEXT_CONFIG['learning_rate']}\")\n","print(f\"Weight decay: {CASME2_CONVNEXT_CONFIG['weight_decay']}\")\n","\n","def train_one_epoch_casme2(model, train_loader, criterion, optimizer, device, epoch, gradient_clip=1.0):\n","    model.train()\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    for batch_idx, (images, labels, _) in enumerate(train_loader):\n","        images = images.to(device, non_blocking=True)\n","        labels = labels.to(device, non_blocking=True)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","\n","        if gradient_clip > 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n","\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","        _, predicted = torch.max(outputs, 1)\n","        all_preds.extend(predicted.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    epoch_loss = running_loss / len(train_loader)\n","\n","    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n","\n","    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n","\n","    return epoch_loss, accuracy, macro_f1\n","\n","def validate_casme2(model, val_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels, _ in val_loader:\n","            images = images.to(device, non_blocking=True)\n","            labels = labels.to(device, non_blocking=True)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs, 1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    val_loss = running_loss / len(val_loader)\n","\n","    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n","\n","    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n","\n","    precision, recall, f1_per_class, support = precision_recall_fscore_support(\n","        all_labels, all_preds, average=None, zero_division=0\n","    )\n","\n","    return val_loss, accuracy, macro_f1, precision, recall, f1_per_class, support\n","\n","def save_checkpoint_atomic_casme2(state, filepath, max_retries=3):\n","    temp_filepath = filepath + '.tmp'\n","\n","    for attempt in range(max_retries):\n","        try:\n","            torch.save(state, temp_filepath)\n","\n","            if os.path.exists(temp_filepath):\n","                file_size = os.path.getsize(temp_filepath)\n","                if file_size > 1000:\n","                    shutil.move(temp_filepath, filepath)\n","                    return True\n","                else:\n","                    print(f\"Warning: Checkpoint file too small ({file_size} bytes), retrying...\")\n","\n","        except Exception as e:\n","            print(f\"Save attempt {attempt + 1} failed: {str(e)}\")\n","            if attempt < max_retries - 1:\n","                time.sleep(2 ** attempt)\n","            else:\n","                print(f\"ERROR: Failed to save checkpoint after {max_retries} attempts\")\n","                if os.path.exists(temp_filepath):\n","                    os.remove(temp_filepath)\n","                return False\n","\n","    return False\n","\n","training_history = {\n","    'train_loss': [],\n","    'train_accuracy': [],\n","    'train_f1_macro': [],\n","    'val_loss': [],\n","    'val_accuracy': [],\n","    'val_f1_macro': [],\n","    'val_f1_per_class': [],\n","    'learning_rates': [],\n","    'epoch_times': []\n","}\n","\n","best_f1 = 0.0\n","best_loss = float('inf')\n","best_accuracy = 0.0\n","\n","checkpoint_filename = 'casme2_convnext_mfs_prep_best_f1.pth'\n","checkpoint_path = os.path.join(GLOBAL_CONFIG_CASME2['checkpoint_root'], checkpoint_filename)\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"STARTING TRAINING - CONVNEXT-TINY M2 MFS-PREP\")\n","print(\"=\" * 60)\n","print(f\"Total epochs: {CASME2_CONVNEXT_CONFIG['num_epochs']}\")\n","print(f\"Batch size: {CASME2_CONVNEXT_CONFIG['batch_size']}\")\n","print(f\"Train samples: {len(train_dataset)}\")\n","print(f\"Validation samples: {len(val_dataset)}\")\n","print(f\"Training strategy: From Scratch (No Pretrained Weights)\")\n","print(f\"Input: 224x224 Pure Grayscale (1 channel)\")\n","print(f\"Checkpoint: {checkpoint_filename}\")\n","print(\"=\" * 60)\n","\n","for epoch in range(CASME2_CONVNEXT_CONFIG['num_epochs']):\n","    epoch_start_time = time.time()\n","\n","    train_loss, train_acc, train_f1 = train_one_epoch_casme2(\n","        model, train_loader, criterion, optimizer,\n","        GLOBAL_CONFIG_CASME2['device'], epoch,\n","        gradient_clip=CASME2_CONVNEXT_CONFIG['gradient_clip']\n","    )\n","\n","    val_loss, val_acc, val_f1, val_precision, val_recall, val_f1_per_class, val_support = validate_casme2(\n","        model, val_loader, criterion, GLOBAL_CONFIG_CASME2['device']\n","    )\n","\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    if scheduler is not None:\n","        if CASME2_CONVNEXT_CONFIG['scheduler_type'] == 'plateau':\n","            scheduler.step(val_f1)\n","\n","    epoch_time = time.time() - epoch_start_time\n","\n","    training_history['train_loss'].append(float(train_loss))\n","    training_history['train_accuracy'].append(float(train_acc))\n","    training_history['train_f1_macro'].append(float(train_f1))\n","    training_history['val_loss'].append(float(val_loss))\n","    training_history['val_accuracy'].append(float(val_acc))\n","    training_history['val_f1_macro'].append(float(val_f1))\n","    training_history['val_f1_per_class'].append([float(f1) for f1 in val_f1_per_class])\n","    training_history['learning_rates'].append(float(current_lr))\n","    training_history['epoch_times'].append(float(epoch_time))\n","\n","    print(f\"\\nEpoch [{epoch+1}/{CASME2_CONVNEXT_CONFIG['num_epochs']}] - {epoch_time:.2f}s\")\n","    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n","    print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n","    print(f\"LR: {current_lr:.2e}\")\n","\n","    should_save = False\n","    save_reason = \"\"\n","\n","    if val_f1 > best_f1:\n","        should_save = True\n","        save_reason = f\"F1 improved: {best_f1:.4f} -> {val_f1:.4f}\"\n","        best_f1 = val_f1\n","    elif val_f1 == best_f1 and val_loss < best_loss:\n","        should_save = True\n","        save_reason = f\"F1 tied, Loss improved: {best_loss:.4f} -> {val_loss:.4f}\"\n","        best_loss = val_loss\n","    elif val_f1 == best_f1 and val_loss == best_loss and val_acc > best_accuracy:\n","        should_save = True\n","        save_reason = f\"F1 & Loss tied, Acc improved: {best_accuracy:.4f} -> {val_acc:.4f}\"\n","        best_accuracy = val_acc\n","\n","    if should_save:\n","        checkpoint = {\n","            'epoch': epoch + 1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","            'train_loss': train_loss,\n","            'train_accuracy': train_acc,\n","            'train_f1_macro': train_f1,\n","            'val_loss': val_loss,\n","            'val_accuracy': val_acc,\n","            'val_f1_macro': val_f1,\n","            'val_f1_per_class': val_f1_per_class.tolist(),\n","            'val_precision': val_precision.tolist(),\n","            'val_recall': val_recall.tolist(),\n","            'val_support': val_support.tolist(),\n","            'best_f1': best_f1,\n","            'best_loss': best_loss,\n","            'best_accuracy': best_accuracy,\n","            'config': CASME2_CONVNEXT_CONFIG,\n","            'class_names': GLOBAL_CONFIG_CASME2['class_names']\n","        }\n","\n","        success = save_checkpoint_atomic_casme2(checkpoint, checkpoint_path)\n","\n","        if success:\n","            print(f\"✓ Checkpoint saved: {save_reason}\")\n","        else:\n","            print(f\"✗ Failed to save checkpoint\")\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"TRAINING COMPLETED - CONVNEXT-TINY M2 MFS-PREP\")\n","print(\"=\" * 60)\n","print(f\"Best validation F1: {best_f1:.4f}\")\n","print(f\"Best validation Loss: {best_loss:.4f}\")\n","print(f\"Best validation Accuracy: {best_accuracy:.4f}\")\n","print(f\"Total training time: {sum(training_history['epoch_times']):.2f}s\")\n","print(f\"Average epoch time: {np.mean(training_history['epoch_times']):.2f}s\")\n","print(f\"Checkpoint saved: {checkpoint_path}\")\n","\n","training_log_path = os.path.join(\n","    GLOBAL_CONFIG_CASME2['results_root'],\n","    'training_logs',\n","    'casme2_convnext_mfs_prep_training_history.json'\n",")\n","\n","training_log_data = {\n","    'model_name': 'ConvNeXT-Tiny',\n","    'methodology': 'M2',\n","    'preprocessing': 'face_aware_preprocessing',\n","    'input_resolution': '224x224_pure_grayscale_1ch',\n","    'training_strategy': 'from_scratch',\n","    'dataset_version': 'v9',\n","    'training_config': {\n","        'num_epochs': CASME2_CONVNEXT_CONFIG['num_epochs'],\n","        'batch_size': CASME2_CONVNEXT_CONFIG['batch_size'],\n","        'learning_rate': CASME2_CONVNEXT_CONFIG['learning_rate'],\n","        'weight_decay': CASME2_CONVNEXT_CONFIG['weight_decay'],\n","        'gradient_clip': CASME2_CONVNEXT_CONFIG['gradient_clip'],\n","        'dropout_rate': CASME2_CONVNEXT_CONFIG['dropout_rate'],\n","        'use_focal_loss': CASME2_CONVNEXT_CONFIG['use_focal_loss'],\n","        'focal_loss_gamma': CASME2_CONVNEXT_CONFIG['focal_loss_gamma'],\n","        'focal_loss_alpha_weights': CASME2_CONVNEXT_CONFIG['focal_loss_alpha_weights']\n","    },\n","    'best_metrics': {\n","        'best_f1_macro': float(best_f1),\n","        'best_loss': float(best_loss),\n","        'best_accuracy': float(best_accuracy)\n","    },\n","    'training_history': training_history,\n","    'training_completed_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","}\n","\n","with open(training_log_path, 'w') as f:\n","    json.dump(training_log_data, f, indent=2)\n","\n","print(f\"\\nTraining history saved: {training_log_path}\")\n","print(\"\\nNext: Cell 3 - Dual Test Evaluation (v7 AF + v8 KFS)\")"],"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Rt21b7czNoX6","executionInfo":{"status":"ok","timestamp":1764313645000,"user_tz":-420,"elapsed":1178493,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"c73a78fa-1de9-4f1b-c874-5537bb6b859c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II ConvNeXT-Tiny M2 MFS-PREP Training Pipeline\n","============================================================\n","\n","Initializing datasets...\n","Loading train dataset from /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/train...\n","Found 2613 image files in directory\n","Loaded 2613 samples for train split\n","  others: 1027 samples (39.3%)\n","  disgust: 650 samples (24.9%)\n","  happiness: 325 samples (12.4%)\n","  repression: 273 samples (10.4%)\n","  surprise: 260 samples (10.0%)\n","  sadness: 65 samples (2.5%)\n","  fear: 13 samples (0.5%)\n","Loading val dataset from /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v9/val...\n","Found 78 image files in directory\n","Loaded 78 samples for val split\n","  others: 30 samples (38.5%)\n","  disgust: 18 samples (23.1%)\n","  happiness: 9 samples (11.5%)\n","  repression: 9 samples (11.5%)\n","  surprise: 6 samples (7.7%)\n","  sadness: 3 samples (3.8%)\n","  fear: 3 samples (3.8%)\n","\n","Creating RAM-cached datasets...\n","Preloading 2613 images to RAM using 32 workers...\n","Preloading completed in 43.13s\n","Cached 2613 images (~125.04 MB)\n","Preloading 78 images to RAM using 32 workers...\n","Preloading completed in 1.26s\n","Cached 78 images (~3.73 MB)\n","\n","Creating data loaders...\n","Train batches: 218\n","Validation batches: 7\n","\n","Initializing ConvNeXT-Tiny model...\n","ConvNeXT-Tiny feature dimension: 768\n","Training from scratch with 1-channel input\n","ConvNeXT CASME II: 768 -> 512 -> 128 -> 7\n","Architecture: Pure grayscale (1ch) from scratch\n","Total parameters: 28,278,631\n","Trainable parameters: 28,278,631\n","Model size: ~28.3M parameters\n","Using Optimized Focal Loss with gamma=2.5\n","Per-class alpha weights: [0.053, 0.067, 0.094, 0.102, 0.106, 0.201, 0.376]\n","Alpha sum: 0.999\n","Scheduler: ReduceLROnPlateau monitoring val_f1_macro\n","\n","Optimizer: AdamW\n","Learning rate: 5e-05\n","Weight decay: 1e-05\n","\n","============================================================\n","STARTING TRAINING - CONVNEXT-TINY M2 MFS-PREP\n","============================================================\n","Total epochs: 50\n","Batch size: 12\n","Train samples: 2613\n","Validation samples: 78\n","Training strategy: From Scratch (No Pretrained Weights)\n","Input: 224x224 Pure Grayscale (1 channel)\n","Checkpoint: casme2_convnext_mfs_prep_best_f1.pth\n","============================================================\n","\n","Epoch [1/50] - 51.98s\n","Train - Loss: 0.0800, Acc: 0.3877, F1: 0.3231\n","Val   - Loss: 0.1264, Acc: 0.3205, F1: 0.0729\n","LR: 5.00e-05\n","✓ Checkpoint saved: F1 improved: 0.0000 -> 0.0729\n","\n","Epoch [2/50] - 21.91s\n","Train - Loss: 0.0479, Acc: 0.6582, F1: 0.6118\n","Val   - Loss: 0.1272, Acc: 0.2692, F1: 0.0652\n","LR: 5.00e-05\n","\n","Epoch [3/50] - 22.14s\n","Train - Loss: 0.0307, Acc: 0.7899, F1: 0.7642\n","Val   - Loss: 0.1232, Acc: 0.2436, F1: 0.0997\n","LR: 5.00e-05\n","✓ Checkpoint saved: F1 improved: 0.0729 -> 0.0997\n","\n","Epoch [4/50] - 21.95s\n","Train - Loss: 0.0196, Acc: 0.8653, F1: 0.8717\n","Val   - Loss: 0.1267, Acc: 0.2308, F1: 0.1312\n","LR: 5.00e-05\n","✓ Checkpoint saved: F1 improved: 0.0997 -> 0.1312\n","\n","Epoch [5/50] - 21.91s\n","Train - Loss: 0.0133, Acc: 0.9066, F1: 0.9150\n","Val   - Loss: 0.1427, Acc: 0.2051, F1: 0.1046\n","LR: 5.00e-05\n","\n","Epoch [6/50] - 21.90s\n","Train - Loss: 0.0093, Acc: 0.9300, F1: 0.9358\n","Val   - Loss: 0.2441, Acc: 0.0897, F1: 0.0320\n","LR: 5.00e-05\n","\n","Epoch [7/50] - 21.92s\n","Train - Loss: 0.0068, Acc: 0.9480, F1: 0.9441\n","Val   - Loss: 0.2476, Acc: 0.3077, F1: 0.0771\n","LR: 5.00e-05\n","\n","Epoch [8/50] - 21.93s\n","Train - Loss: 0.0059, Acc: 0.9591, F1: 0.9612\n","Val   - Loss: 0.1525, Acc: 0.2308, F1: 0.1368\n","LR: 5.00e-05\n","✓ Checkpoint saved: F1 improved: 0.1312 -> 0.1368\n","\n","Epoch [9/50] - 21.97s\n","Train - Loss: 0.0045, Acc: 0.9617, F1: 0.9649\n","Val   - Loss: 0.1466, Acc: 0.2692, F1: 0.1709\n","LR: 5.00e-05\n","✓ Checkpoint saved: F1 improved: 0.1368 -> 0.1709\n","\n","Epoch [10/50] - 21.94s\n","Train - Loss: 0.0039, Acc: 0.9629, F1: 0.9678\n","Val   - Loss: 0.1831, Acc: 0.2692, F1: 0.1627\n","LR: 5.00e-05\n","\n","Epoch [11/50] - 21.91s\n","Train - Loss: 0.0031, Acc: 0.9755, F1: 0.9781\n","Val   - Loss: 0.1849, Acc: 0.2821, F1: 0.1501\n","LR: 5.00e-05\n","\n","Epoch [12/50] - 21.91s\n","Train - Loss: 0.0026, Acc: 0.9755, F1: 0.9733\n","Val   - Loss: 0.1619, Acc: 0.2564, F1: 0.1371\n","LR: 5.00e-05\n","\n","Epoch [13/50] - 21.93s\n","Train - Loss: 0.0024, Acc: 0.9786, F1: 0.9822\n","Val   - Loss: 0.1881, Acc: 0.2821, F1: 0.1287\n","LR: 5.00e-05\n","\n","Epoch [14/50] - 21.91s\n","Train - Loss: 0.0022, Acc: 0.9805, F1: 0.9758\n","Val   - Loss: 0.1843, Acc: 0.2564, F1: 0.1354\n","LR: 5.00e-05\n","\n","Epoch [15/50] - 21.91s\n","Train - Loss: 0.0015, Acc: 0.9908, F1: 0.9900\n","Val   - Loss: 0.1868, Acc: 0.2949, F1: 0.1869\n","LR: 5.00e-05\n","✓ Checkpoint saved: F1 improved: 0.1709 -> 0.1869\n","\n","Epoch [16/50] - 21.92s\n","Train - Loss: 0.0015, Acc: 0.9820, F1: 0.9848\n","Val   - Loss: 0.1743, Acc: 0.3462, F1: 0.1899\n","LR: 5.00e-05\n","✓ Checkpoint saved: F1 improved: 0.1869 -> 0.1899\n","\n","Epoch [17/50] - 21.92s\n","Train - Loss: 0.0016, Acc: 0.9832, F1: 0.9848\n","Val   - Loss: 0.1930, Acc: 0.2949, F1: 0.1345\n","LR: 5.00e-05\n","\n","Epoch [18/50] - 21.94s\n","Train - Loss: 0.0013, Acc: 0.9847, F1: 0.9867\n","Val   - Loss: 0.1657, Acc: 0.2436, F1: 0.1742\n","LR: 5.00e-05\n","\n","Epoch [19/50] - 21.96s\n","Train - Loss: 0.0014, Acc: 0.9881, F1: 0.9884\n","Val   - Loss: 0.2245, Acc: 0.3333, F1: 0.1629\n","LR: 5.00e-05\n","\n","Epoch [20/50] - 21.93s\n","Train - Loss: 0.0011, Acc: 0.9862, F1: 0.9883\n","Val   - Loss: 0.2044, Acc: 0.3077, F1: 0.1632\n","LR: 5.00e-05\n","\n","Epoch [21/50] - 21.93s\n","Train - Loss: 0.0009, Acc: 0.9943, F1: 0.9959\n","Val   - Loss: 0.2355, Acc: 0.3077, F1: 0.1496\n","LR: 5.00e-05\n","\n","Epoch [22/50] - 21.91s\n","Train - Loss: 0.0006, Acc: 0.9950, F1: 0.9953\n","Val   - Loss: 0.2164, Acc: 0.2821, F1: 0.1387\n","LR: 5.00e-05\n","\n","Epoch [23/50] - 21.91s\n","Train - Loss: 0.0008, Acc: 0.9935, F1: 0.9952\n","Val   - Loss: 0.2065, Acc: 0.2821, F1: 0.2043\n","LR: 2.50e-05\n","✓ Checkpoint saved: F1 improved: 0.1899 -> 0.2043\n","\n","Epoch [24/50] - 21.92s\n","Train - Loss: 0.0006, Acc: 0.9946, F1: 0.9951\n","Val   - Loss: 0.1980, Acc: 0.3077, F1: 0.1855\n","LR: 2.50e-05\n","\n","Epoch [25/50] - 21.91s\n","Train - Loss: 0.0006, Acc: 0.9954, F1: 0.9963\n","Val   - Loss: 0.2018, Acc: 0.3077, F1: 0.2073\n","LR: 2.50e-05\n","✓ Checkpoint saved: F1 improved: 0.2043 -> 0.2073\n","\n","Epoch [26/50] - 21.94s\n","Train - Loss: 0.0004, Acc: 0.9969, F1: 0.9973\n","Val   - Loss: 0.2182, Acc: 0.2436, F1: 0.1274\n","LR: 2.50e-05\n","\n","Epoch [27/50] - 21.92s\n","Train - Loss: 0.0004, Acc: 0.9950, F1: 0.9961\n","Val   - Loss: 0.2188, Acc: 0.3205, F1: 0.1962\n","LR: 2.50e-05\n","\n","Epoch [28/50] - 21.92s\n","Train - Loss: 0.0004, Acc: 0.9962, F1: 0.9967\n","Val   - Loss: 0.2193, Acc: 0.3333, F1: 0.2473\n","LR: 2.50e-05\n","✓ Checkpoint saved: F1 improved: 0.2073 -> 0.2473\n","\n","Epoch [29/50] - 21.93s\n","Train - Loss: 0.0003, Acc: 0.9973, F1: 0.9982\n","Val   - Loss: 0.2183, Acc: 0.2949, F1: 0.1765\n","LR: 2.50e-05\n","\n","Epoch [30/50] - 21.92s\n","Train - Loss: 0.0004, Acc: 0.9939, F1: 0.9953\n","Val   - Loss: 0.2083, Acc: 0.3205, F1: 0.2042\n","LR: 2.50e-05\n","\n","Epoch [31/50] - 21.95s\n","Train - Loss: 0.0005, Acc: 0.9973, F1: 0.9978\n","Val   - Loss: 0.2161, Acc: 0.2821, F1: 0.1619\n","LR: 2.50e-05\n","\n","Epoch [32/50] - 21.91s\n","Train - Loss: 0.0002, Acc: 0.9992, F1: 0.9983\n","Val   - Loss: 0.2265, Acc: 0.3077, F1: 0.1915\n","LR: 2.50e-05\n","\n","Epoch [33/50] - 21.91s\n","Train - Loss: 0.0005, Acc: 0.9969, F1: 0.9970\n","Val   - Loss: 0.2230, Acc: 0.3333, F1: 0.2227\n","LR: 2.50e-05\n","\n","Epoch [34/50] - 21.92s\n","Train - Loss: 0.0003, Acc: 0.9985, F1: 0.9986\n","Val   - Loss: 0.2175, Acc: 0.2949, F1: 0.1341\n","LR: 2.50e-05\n","\n","Epoch [35/50] - 21.92s\n","Train - Loss: 0.0002, Acc: 0.9996, F1: 0.9995\n","Val   - Loss: 0.2198, Acc: 0.2564, F1: 0.1238\n","LR: 1.25e-05\n","\n","Epoch [36/50] - 21.93s\n","Train - Loss: 0.0003, Acc: 0.9985, F1: 0.9985\n","Val   - Loss: 0.2159, Acc: 0.3205, F1: 0.1868\n","LR: 1.25e-05\n","\n","Epoch [37/50] - 21.99s\n","Train - Loss: 0.0002, Acc: 0.9989, F1: 0.9991\n","Val   - Loss: 0.2206, Acc: 0.3077, F1: 0.1500\n","LR: 1.25e-05\n","\n","Epoch [38/50] - 21.92s\n","Train - Loss: 0.0001, Acc: 0.9996, F1: 0.9997\n","Val   - Loss: 0.2206, Acc: 0.2564, F1: 0.1262\n","LR: 1.25e-05\n","\n","Epoch [39/50] - 21.91s\n","Train - Loss: 0.0002, Acc: 0.9981, F1: 0.9983\n","Val   - Loss: 0.2222, Acc: 0.3077, F1: 0.1640\n","LR: 1.25e-05\n","\n","Epoch [40/50] - 21.91s\n","Train - Loss: 0.0002, Acc: 0.9977, F1: 0.9980\n","Val   - Loss: 0.2202, Acc: 0.2949, F1: 0.1346\n","LR: 1.25e-05\n","\n","Epoch [41/50] - 21.93s\n","Train - Loss: 0.0002, Acc: 0.9992, F1: 0.9995\n","Val   - Loss: 0.2145, Acc: 0.3077, F1: 0.2047\n","LR: 6.25e-06\n","\n","Epoch [42/50] - 21.91s\n","Train - Loss: 0.0003, Acc: 0.9985, F1: 0.9986\n","Val   - Loss: 0.2257, Acc: 0.2821, F1: 0.1327\n","LR: 6.25e-06\n","\n","Epoch [43/50] - 21.93s\n","Train - Loss: 0.0002, Acc: 0.9989, F1: 0.9991\n","Val   - Loss: 0.2243, Acc: 0.2821, F1: 0.1609\n","LR: 6.25e-06\n","\n","Epoch [44/50] - 21.92s\n","Train - Loss: 0.0001, Acc: 0.9989, F1: 0.9991\n","Val   - Loss: 0.2354, Acc: 0.3077, F1: 0.1434\n","LR: 6.25e-06\n","\n","Epoch [45/50] - 21.91s\n","Train - Loss: 0.0001, Acc: 0.9992, F1: 0.9995\n","Val   - Loss: 0.2191, Acc: 0.3077, F1: 0.1661\n","LR: 6.25e-06\n","\n","Epoch [46/50] - 21.91s\n","Train - Loss: 0.0001, Acc: 0.9996, F1: 0.9996\n","Val   - Loss: 0.2239, Acc: 0.2949, F1: 0.1922\n","LR: 6.25e-06\n","\n","Epoch [47/50] - 21.91s\n","Train - Loss: 0.0001, Acc: 0.9992, F1: 0.9992\n","Val   - Loss: 0.2270, Acc: 0.2692, F1: 0.1305\n","LR: 3.13e-06\n","\n","Epoch [48/50] - 21.91s\n","Train - Loss: 0.0001, Acc: 0.9985, F1: 0.9989\n","Val   - Loss: 0.2257, Acc: 0.2949, F1: 0.1756\n","LR: 3.13e-06\n","\n","Epoch [49/50] - 21.91s\n","Train - Loss: 0.0001, Acc: 0.9996, F1: 0.9998\n","Val   - Loss: 0.2315, Acc: 0.2949, F1: 0.1753\n","LR: 3.13e-06\n","\n","Epoch [50/50] - 21.91s\n","Train - Loss: 0.0001, Acc: 1.0000, F1: 1.0000\n","Val   - Loss: 0.2295, Acc: 0.2949, F1: 0.1737\n","LR: 3.13e-06\n","\n","============================================================\n","TRAINING COMPLETED - CONVNEXT-TINY M2 MFS-PREP\n","============================================================\n","Best validation F1: 0.2473\n","Best validation Loss: inf\n","Best validation Accuracy: 0.0000\n","Total training time: 1126.48s\n","Average epoch time: 22.53s\n","Checkpoint saved: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/09_03_convnext_casme2_mfs_prep/casme2_convnext_mfs_prep_best_f1.pth\n","\n","Training history saved: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/09_03_convnext_casme2_mfs_prep/training_logs/casme2_convnext_mfs_prep_training_history.json\n","\n","Next: Cell 3 - Dual Test Evaluation (v7 AF + v8 KFS)\n"]}]},{"cell_type":"code","source":["# @title Cell 3: CASME II ConvNeXT-Tiny M2 MFS-PREP Dual Test Evaluation\n","\n","# File: 09_03_ConvNeXT_CASME2_MFS_PREP_Cell3.py\n","# Location: experiments/09_03_ConvNeXT_CASME2-MFS-PREP.ipynb\n","# Purpose: Comprehensive evaluation on v7 (AF) and v8 (KFS) test sets with late fusion\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from sklearn.metrics import (\n","    f1_score, precision_recall_fscore_support, accuracy_score,\n","    confusion_matrix, roc_auc_score\n",")\n","import json\n","import os\n","from collections import defaultdict\n","from datetime import datetime\n","\n","print(\"CASME II ConvNeXT-Tiny M2 MFS-PREP Dual Test Evaluation\")\n","print(\"=\" * 60)\n","\n","TEST_V7_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/preprocessed_v7\"\n","TEST_V8_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/preprocessed_v8\"\n","\n","print(f\"\\nTest dataset paths:\")\n","print(f\"v7 (AF): {TEST_V7_ROOT}\")\n","print(f\"v8 (KFS): {TEST_V8_ROOT}\")\n","\n","if not os.path.exists(TEST_V7_ROOT):\n","    raise FileNotFoundError(f\"v7 test dataset not found: {TEST_V7_ROOT}\")\n","if not os.path.exists(TEST_V8_ROOT):\n","    raise FileNotFoundError(f\"v8 test dataset not found: {TEST_V8_ROOT}\")\n","\n","checkpoint_path = os.path.join(GLOBAL_CONFIG_CASME2['checkpoint_root'], 'casme2_convnext_mfs_prep_best_f1.pth')\n","\n","if not os.path.exists(checkpoint_path):\n","    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n","\n","print(f\"\\nLoading checkpoint: {checkpoint_path}\")\n","checkpoint = torch.load(checkpoint_path, map_location=GLOBAL_CONFIG_CASME2['device'], weights_only=False)\n","\n","print(f\"Checkpoint from epoch: {checkpoint['epoch']}\")\n","print(f\"Validation F1: {checkpoint['val_f1_macro']:.4f}\")\n","print(f\"Validation Accuracy: {checkpoint['val_accuracy']:.4f}\")\n","\n","model = ConvNeXTCASME2Baseline(\n","    num_classes=GLOBAL_CONFIG_CASME2['num_classes'],\n","    dropout_rate=CASME2_CONVNEXT_CONFIG['dropout_rate'],\n","    in_channels=1\n",").to(GLOBAL_CONFIG_CASME2['device'])\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.eval()\n","\n","print(\"Model loaded successfully and set to evaluation mode\")\n","\n","def extract_video_id(filename):\n","    parts = filename.rsplit('_', 1)\n","    if len(parts) == 2:\n","        video_id = parts[0]\n","        return video_id\n","    return filename.rsplit('.', 1)[0]\n","\n","def evaluate_test_set_casme2(model, test_dataset, test_loader, device, class_names, test_version='v7'):\n","    model.eval()\n","\n","    all_preds = []\n","    all_labels = []\n","    all_probs = []\n","    all_filenames = []\n","\n","    print(f\"\\nEvaluating {test_version.upper()} test set...\")\n","\n","    with torch.no_grad():\n","        for images, labels, filenames in test_loader:\n","            images = images.to(device, non_blocking=True)\n","            labels = labels.to(device, non_blocking=True)\n","\n","            outputs = model(images)\n","            probs = torch.softmax(outputs, dim=1)\n","            _, predicted = torch.max(outputs, 1)\n","\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","            all_probs.extend(probs.cpu().numpy())\n","            all_filenames.extend(filenames)\n","\n","    all_preds = np.array(all_preds)\n","    all_labels = np.array(all_labels)\n","    all_probs = np.array(all_probs)\n","\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n","    weighted_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n","\n","    precision, recall, f1_per_class, support = precision_recall_fscore_support(\n","        all_labels, all_preds, average=None, zero_division=0\n","    )\n","\n","    cm = confusion_matrix(all_labels, all_preds)\n","\n","    present_classes = np.unique(all_labels)\n","\n","    try:\n","        if len(present_classes) > 1:\n","            auc_scores = []\n","            for i in range(len(class_names)):\n","                if i in present_classes:\n","                    binary_labels = (all_labels == i).astype(int)\n","                    binary_probs = all_probs[:, i]\n","\n","                    if len(np.unique(binary_labels)) > 1:\n","                        auc = roc_auc_score(binary_labels, binary_probs)\n","                        auc_scores.append(auc)\n","                    else:\n","                        auc_scores.append(0.0)\n","                else:\n","                    auc_scores.append(0.0)\n","        else:\n","            auc_scores = [0.0] * len(class_names)\n","    except Exception as e:\n","        print(f\"Warning: Could not calculate AUC scores: {e}\")\n","        auc_scores = [0.0] * len(class_names)\n","\n","    print(f\"\\n{test_version.upper()} Results:\")\n","    print(f\"  Accuracy: {accuracy:.4f}\")\n","    print(f\"  Macro F1: {macro_f1:.4f}\")\n","    print(f\"  Weighted F1: {weighted_f1:.4f}\")\n","\n","    results = {\n","        'test_configuration': {\n","            'version': test_version,\n","            'variant': 'AF' if test_version == 'v7' else 'KFS',\n","            'description': 'Apex Frame' if test_version == 'v7' else 'Key Frame Sequence',\n","            'total_samples': len(all_labels)\n","        },\n","        'overall_performance': {\n","            'accuracy': float(accuracy),\n","            'macro_f1': float(macro_f1),\n","            'weighted_f1': float(weighted_f1)\n","        },\n","        'per_class_performance': {},\n","        'confusion_matrix': cm.tolist(),\n","        'predictions': {\n","            'filenames': all_filenames,\n","            'true_labels': all_labels.tolist(),\n","            'predicted_labels': all_preds.tolist(),\n","            'prediction_probabilities': all_probs.tolist()\n","        },\n","        'evaluation_metadata': {\n","            'model': 'ConvNeXT-Tiny',\n","            'methodology': 'M2',\n","            'preprocessing': 'face_aware_preprocessing',\n","            'input_resolution': '224x224 Pure Grayscale (1 channel)',\n","            'training_strategy': 'from_scratch',\n","            'class_names': class_names,\n","            'checkpoint_epoch': checkpoint['epoch'],\n","            'evaluated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","        }\n","    }\n","\n","    missing_classes = []\n","    for i, class_name in enumerate(class_names):\n","        if i not in present_classes:\n","            missing_classes.append(class_name)\n","            results['per_class_performance'][class_name] = {\n","                'precision': 0.0,\n","                'recall': 0.0,\n","                'f1_score': 0.0,\n","                'support': 0,\n","                'auc': 0.0,\n","                'missing': True\n","            }\n","        else:\n","            results['per_class_performance'][class_name] = {\n","                'precision': float(precision[i]),\n","                'recall': float(recall[i]),\n","                'f1_score': float(f1_per_class[i]),\n","                'support': int(support[i]),\n","                'auc': float(auc_scores[i]),\n","                'missing': False\n","            }\n","\n","    if missing_classes:\n","        print(f\"  Missing classes: {missing_classes}\")\n","        results['evaluation_metadata']['missing_classes'] = missing_classes\n","    else:\n","        results['evaluation_metadata']['missing_classes'] = []\n","\n","    wrong_predictions = []\n","    for i, (true_label, pred_label, filename) in enumerate(zip(all_labels, all_preds, all_filenames)):\n","        if true_label != pred_label:\n","            wrong_predictions.append({\n","                'filename': filename,\n","                'true_class': class_names[true_label],\n","                'predicted_class': class_names[pred_label],\n","                'true_label_idx': int(true_label),\n","                'predicted_label_idx': int(pred_label),\n","                'prediction_confidence': float(all_probs[i][pred_label]),\n","                'true_class_probability': float(all_probs[i][true_label])\n","            })\n","\n","    results['wrong_predictions'] = {\n","        'count': len(wrong_predictions),\n","        'percentage': (len(wrong_predictions) / len(all_labels)) * 100,\n","        'details': wrong_predictions\n","    }\n","\n","    print(f\"  Wrong predictions: {len(wrong_predictions)}/{len(all_labels)} ({results['wrong_predictions']['percentage']:.1f}%)\")\n","\n","    return results\n","\n","def evaluate_with_late_fusion_casme2(model, test_dataset, test_loader, device, class_names, test_version='v8'):\n","    model.eval()\n","\n","    video_data = defaultdict(lambda: {'probs': [], 'true_label': None, 'filenames': []})\n","\n","    print(f\"\\nCollecting frame predictions for {test_version.upper()} late fusion...\")\n","\n","    with torch.no_grad():\n","        for images, labels, filenames in test_loader:\n","            images = images.to(device, non_blocking=True)\n","            labels = labels.to(device, non_blocking=True)\n","\n","            outputs = model(images)\n","            probs = torch.softmax(outputs, dim=1)\n","\n","            probs_np = probs.cpu().numpy()\n","            labels_np = labels.cpu().numpy()\n","\n","            for prob, label, filename in zip(probs_np, labels_np, filenames):\n","                video_id = extract_video_id(filename)\n","                video_data[video_id]['probs'].append(prob)\n","                video_data[video_id]['true_label'] = label\n","                video_data[video_id]['filenames'].append(filename)\n","\n","    print(f\"Collected {sum(len(v['probs']) for v in video_data.values())} frames from {len(video_data)} videos\")\n","\n","    all_preds = []\n","    all_labels = []\n","    all_probs = []\n","    all_video_ids = []\n","\n","    for video_id, data in sorted(video_data.items()):\n","        avg_probs = np.mean(data['probs'], axis=0)\n","        pred_label = np.argmax(avg_probs)\n","        true_label = data['true_label']\n","\n","        all_preds.append(pred_label)\n","        all_labels.append(true_label)\n","        all_probs.append(avg_probs)\n","        all_video_ids.append(video_id)\n","\n","    all_preds = np.array(all_preds)\n","    all_labels = np.array(all_labels)\n","    all_probs = np.array(all_probs)\n","\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n","    weighted_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n","\n","    precision, recall, f1_per_class, support = precision_recall_fscore_support(\n","        all_labels, all_preds, average=None, zero_division=0\n","    )\n","\n","    cm = confusion_matrix(all_labels, all_preds)\n","\n","    present_classes = np.unique(all_labels)\n","\n","    try:\n","        if len(present_classes) > 1:\n","            auc_scores = []\n","            for i in range(len(class_names)):\n","                if i in present_classes:\n","                    binary_labels = (all_labels == i).astype(int)\n","                    binary_probs = all_probs[:, i]\n","\n","                    if len(np.unique(binary_labels)) > 1:\n","                        auc = roc_auc_score(binary_labels, binary_probs)\n","                        auc_scores.append(auc)\n","                    else:\n","                        auc_scores.append(0.0)\n","                else:\n","                    auc_scores.append(0.0)\n","        else:\n","            auc_scores = [0.0] * len(class_names)\n","    except Exception as e:\n","        print(f\"Warning: Could not calculate AUC scores: {e}\")\n","        auc_scores = [0.0] * len(class_names)\n","\n","    print(f\"\\n{test_version.upper()} Late Fusion Results:\")\n","    print(f\"  Videos evaluated: {len(all_video_ids)}\")\n","    print(f\"  Accuracy: {accuracy:.4f}\")\n","    print(f\"  Macro F1: {macro_f1:.4f}\")\n","    print(f\"  Weighted F1: {weighted_f1:.4f}\")\n","\n","    results = {\n","        'test_configuration': {\n","            'version': test_version,\n","            'variant': 'KFS',\n","            'description': 'Key Frame Sequence with Late Fusion',\n","            'fusion_method': 'average_probability',\n","            'total_videos': len(all_video_ids),\n","            'total_frames': sum(len(v['probs']) for v in video_data.values())\n","        },\n","        'overall_performance': {\n","            'accuracy': float(accuracy),\n","            'macro_f1': float(macro_f1),\n","            'weighted_f1': float(weighted_f1)\n","        },\n","        'per_class_performance': {},\n","        'confusion_matrix': cm.tolist(),\n","        'video_predictions': {\n","            'video_ids': all_video_ids,\n","            'true_labels': all_labels.tolist(),\n","            'predicted_labels': all_preds.tolist(),\n","            'prediction_probabilities': all_probs.tolist()\n","        },\n","        'evaluation_metadata': {\n","            'model': 'ConvNeXT-Tiny',\n","            'methodology': 'M2',\n","            'preprocessing': 'face_aware_preprocessing',\n","            'input_resolution': '224x224 Pure Grayscale (1 channel)',\n","            'training_strategy': 'from_scratch',\n","            'class_names': class_names,\n","            'checkpoint_epoch': checkpoint['epoch'],\n","            'evaluated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","        }\n","    }\n","\n","    missing_classes = []\n","    for i, class_name in enumerate(class_names):\n","        if i not in present_classes:\n","            missing_classes.append(class_name)\n","            results['per_class_performance'][class_name] = {\n","                'precision': 0.0,\n","                'recall': 0.0,\n","                'f1_score': 0.0,\n","                'support': 0,\n","                'auc': 0.0,\n","                'missing': True\n","            }\n","        else:\n","            results['per_class_performance'][class_name] = {\n","                'precision': float(precision[i]),\n","                'recall': float(recall[i]),\n","                'f1_score': float(f1_per_class[i]),\n","                'support': int(support[i]),\n","                'auc': float(auc_scores[i]),\n","                'missing': False\n","            }\n","\n","    if missing_classes:\n","        print(f\"  Missing classes: {missing_classes}\")\n","        results['evaluation_metadata']['missing_classes'] = missing_classes\n","    else:\n","        results['evaluation_metadata']['missing_classes'] = []\n","\n","    wrong_predictions = []\n","    for video_id, true_label, pred_label, probs in zip(all_video_ids, all_labels, all_preds, all_probs):\n","        if true_label != pred_label:\n","            wrong_predictions.append({\n","                'video_id': video_id,\n","                'true_class': class_names[true_label],\n","                'predicted_class': class_names[pred_label],\n","                'true_label_idx': int(true_label),\n","                'predicted_label_idx': int(pred_label),\n","                'prediction_confidence': float(probs[pred_label]),\n","                'true_class_probability': float(probs[true_label]),\n","                'num_frames': len(video_data[video_id]['probs'])\n","            })\n","\n","    results['wrong_predictions'] = {\n","        'count': len(wrong_predictions),\n","        'percentage': (len(wrong_predictions) / len(all_video_ids)) * 100,\n","        'details': wrong_predictions\n","    }\n","\n","    print(f\"  Wrong predictions: {len(wrong_predictions)}/{len(all_video_ids)} ({results['wrong_predictions']['percentage']:.1f}%)\")\n","\n","    return results\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"EVALUATING V7 TEST SET (APEX FRAME)\")\n","print(\"=\" * 60)\n","\n","test_v7_dataset = CASME2Dataset(\n","    dataset_root=TEST_V7_ROOT,\n","    split='test',\n","    transform=GLOBAL_CONFIG_CASME2['transform_val']\n",")\n","\n","test_v7_loader = DataLoader(\n","    test_v7_dataset,\n","    batch_size=GLOBAL_CONFIG_CASME2['batch_size'],\n","    shuffle=False,\n","    num_workers=0,\n","    pin_memory=True\n",")\n","\n","v7_results = evaluate_test_set_casme2(\n","    model, test_v7_dataset, test_v7_loader,\n","    GLOBAL_CONFIG_CASME2['device'],\n","    GLOBAL_CONFIG_CASME2['class_names'],\n","    test_version='v7'\n",")\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"EVALUATING V8 TEST SET (KEY FRAME SEQUENCE WITH LATE FUSION)\")\n","print(\"=\" * 60)\n","\n","test_v8_dataset = CASME2Dataset(\n","    dataset_root=TEST_V8_ROOT,\n","    split='test',\n","    transform=GLOBAL_CONFIG_CASME2['transform_val']\n",")\n","\n","test_v8_loader = DataLoader(\n","    test_v8_dataset,\n","    batch_size=GLOBAL_CONFIG_CASME2['batch_size'],\n","    shuffle=False,\n","    num_workers=0,\n","    pin_memory=True\n",")\n","\n","v8_results = evaluate_with_late_fusion_casme2(\n","    model, test_v8_dataset, test_v8_loader,\n","    GLOBAL_CONFIG_CASME2['device'],\n","    GLOBAL_CONFIG_CASME2['class_names'],\n","    test_version='v8'\n",")\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"DUAL TEST EVALUATION COMPLETED\")\n","print(\"=\" * 60)\n","\n","eval_results_dir = os.path.join(GLOBAL_CONFIG_CASME2['results_root'], 'evaluation_results')\n","os.makedirs(eval_results_dir, exist_ok=True)\n","\n","v7_output_path = os.path.join(eval_results_dir, 'casme2_convnext_mfs_prep_evaluation_results_v7.json')\n","with open(v7_output_path, 'w') as f:\n","    json.dump(v7_results, f, indent=2)\n","print(f\"\\nv7 results saved: {v7_output_path}\")\n","\n","v8_output_path = os.path.join(eval_results_dir, 'casme2_convnext_mfs_prep_evaluation_results_v8.json')\n","with open(v8_output_path, 'w') as f:\n","    json.dump(v8_results, f, indent=2)\n","print(f\"v8 results saved: {v8_output_path}\")\n","\n","print(\"\\nComparative Summary:\")\n","print(f\"v7 (AF)  - Macro F1: {v7_results['overall_performance']['macro_f1']:.4f}, Acc: {v7_results['overall_performance']['accuracy']:.4f}\")\n","print(f\"v8 (KFS) - Macro F1: {v8_results['overall_performance']['macro_f1']:.4f}, Acc: {v8_results['overall_performance']['accuracy']:.4f}\")\n","\n","v8_v7_delta = v8_results['overall_performance']['macro_f1'] - v7_results['overall_performance']['macro_f1']\n","print(f\"Delta (v8 - v7): {v8_v7_delta:+.4f}\")\n","\n","if v8_v7_delta > 0:\n","    improvement_pct = (v8_v7_delta / v7_results['overall_performance']['macro_f1']) * 100\n","    print(f\"KFS improves by {improvement_pct:.1f}% over AF\")\n","else:\n","    degradation_pct = (abs(v8_v7_delta) / v8_results['overall_performance']['macro_f1']) * 100\n","    print(f\"KFS degrades by {degradation_pct:.1f}% from AF\")\n","\n","print(\"\\nNext: Cell 4 - Confusion Matrix Generation\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"cellView":"form","id":"_LT4vqpxORMP","executionInfo":{"status":"ok","timestamp":1764313866557,"user_tz":-420,"elapsed":46743,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"54124157-d08b-4ae2-f94c-16e4c8a68d1d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II ConvNeXT-Tiny M2 MFS-PREP Dual Test Evaluation\n","============================================================\n","\n","Test dataset paths:\n","v7 (AF): /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7\n","v8 (KFS): /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8\n","\n","Loading checkpoint: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/09_03_convnext_casme2_mfs_prep/casme2_convnext_mfs_prep_best_f1.pth\n","Checkpoint from epoch: 28\n","Validation F1: 0.2473\n","Validation Accuracy: 0.3333\n","ConvNeXT-Tiny feature dimension: 768\n","Training from scratch with 1-channel input\n","ConvNeXT CASME II: 768 -> 512 -> 128 -> 7\n","Architecture: Pure grayscale (1ch) from scratch\n","Model loaded successfully and set to evaluation mode\n","\n","============================================================\n","EVALUATING V7 TEST SET (APEX FRAME)\n","============================================================\n","Loading test dataset from /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v7/test...\n","Found 28 image files in directory\n","Loaded 28 samples for test split\n","  others: 10 samples (35.7%)\n","  disgust: 7 samples (25.0%)\n","  happiness: 4 samples (14.3%)\n","  repression: 3 samples (10.7%)\n","  surprise: 3 samples (10.7%)\n","  sadness: 1 samples (3.6%)\n","\n","Evaluating V7 test set...\n","\n","V7 Results:\n","  Accuracy: 0.4286\n","  Macro F1: 0.3439\n","  Weighted F1: 0.4229\n","  Missing classes: ['fear']\n","  Wrong predictions: 16/28 (57.1%)\n","\n","============================================================\n","EVALUATING V8 TEST SET (KEY FRAME SEQUENCE WITH LATE FUSION)\n","============================================================\n","Loading test dataset from /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/preprocessed_v8/test...\n","Found 84 image files in directory\n","Loaded 84 samples for test split\n","  others: 30 samples (35.7%)\n","  disgust: 21 samples (25.0%)\n","  happiness: 12 samples (14.3%)\n","  repression: 9 samples (10.7%)\n","  surprise: 9 samples (10.7%)\n","  sadness: 3 samples (3.6%)\n","\n","Collecting frame predictions for V8 late fusion...\n","Collected 84 frames from 84 videos\n","\n","V8 Late Fusion Results:\n","  Videos evaluated: 84\n","  Accuracy: 0.4286\n","  Macro F1: 0.3317\n","  Weighted F1: 0.4172\n","  Missing classes: ['fear']\n","  Wrong predictions: 48/84 (57.1%)\n","\n","============================================================\n","DUAL TEST EVALUATION COMPLETED\n","============================================================\n","\n","v7 results saved: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/09_03_convnext_casme2_mfs_prep/evaluation_results/casme2_convnext_mfs_prep_evaluation_results_v7.json\n","v8 results saved: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/09_03_convnext_casme2_mfs_prep/evaluation_results/casme2_convnext_mfs_prep_evaluation_results_v8.json\n","\n","Comparative Summary:\n","v7 (AF)  - Macro F1: 0.3439, Acc: 0.4286\n","v8 (KFS) - Macro F1: 0.3317, Acc: 0.4286\n","Delta (v8 - v7): -0.0122\n","KFS degrades by 3.7% from AF\n","\n","Next: Cell 4 - Confusion Matrix Generation\n"]}]},{"cell_type":"code","source":["# @title Cell 4: CASME II ConvNeXT-Tiny M2 MFS-PREP Confusion Matrix Generation\n","\n","# File: 09_03_ConvNeXT_CASME2_MFS_PREP_Cell4.py\n","# Location: experiments/09_03_ConvNeXT_CASME2-MFS-PREP.ipynb\n","# Purpose: Generate professional confusion matrix visualizations for v7 (AF) and v8 (KFS) test sets\n","\n","import json\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import seaborn as sns\n","from datetime import datetime\n","\n","print(\"CASME II ConvNeXT-Tiny M2 MFS-PREP Confusion Matrix Generation\")\n","print(\"=\" * 60)\n","\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/09_03_convnext_casme2_mfs_prep\"\n","\n","def find_evaluation_json_files_casme2(results_path):\n","    json_files = {}\n","\n","    eval_dir = f\"{results_path}/evaluation_results\"\n","\n","    if os.path.exists(eval_dir):\n","        for version in ['v7', 'v8']:\n","            eval_pattern = f\"{eval_dir}/casme2_convnext_mfs_prep_evaluation_results_{version}.json\"\n","            eval_files = glob.glob(eval_pattern)\n","\n","            if eval_files:\n","                json_files[f'main_{version}'] = eval_files[0]\n","                print(f\"Found {version.upper()} evaluation file: {os.path.basename(eval_files[0])}\")\n","\n","        if not json_files:\n","            print(f\"WARNING: No evaluation results found in {eval_dir}\")\n","            print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","    else:\n","        print(f\"ERROR: Evaluation directory not found: {eval_dir}\")\n","\n","    return json_files\n","\n","def load_evaluation_results_casme2(json_path):\n","    try:\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"Successfully loaded: {os.path.basename(json_path)}\")\n","        return data\n","    except Exception as e:\n","        print(f\"ERROR loading {json_path}: {str(e)}\")\n","        return None\n","\n","def calculate_weighted_f1_casme2(per_class_performance):\n","    total_support = sum([class_data['support'] for class_data in per_class_performance.values()\n","                        if class_data['support'] > 0])\n","\n","    if total_support == 0:\n","        return 0.0\n","\n","    weighted_f1 = 0.0\n","\n","    for class_name, class_data in per_class_performance.items():\n","        if class_data['support'] > 0:\n","            weight = class_data['support'] / total_support\n","            weighted_f1 += class_data['f1_score'] * weight\n","\n","    return weighted_f1\n","\n","def calculate_balanced_accuracy_casme2(confusion_matrix):\n","    cm = np.array(confusion_matrix)\n","    n_classes = cm.shape[0]\n","\n","    per_class_balanced_acc = []\n","\n","    classes_with_samples = []\n","    for i in range(n_classes):\n","        if cm[i, :].sum() > 0:\n","            classes_with_samples.append(i)\n","\n","    for i in classes_with_samples:\n","        tp = cm[i, i]\n","        fn = cm[i, :].sum() - tp\n","        fp = cm[:, i].sum() - tp\n","        tn = cm.sum() - tp - fn - fp\n","\n","        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n","\n","        class_balanced_acc = (sensitivity + specificity) / 2\n","        per_class_balanced_acc.append(class_balanced_acc)\n","\n","    balanced_acc = np.mean(per_class_balanced_acc) if per_class_balanced_acc else 0.0\n","\n","    return balanced_acc\n","\n","def determine_text_color_casme2(color_value, threshold=0.5):\n","    return 'white' if color_value > threshold else 'black'\n","\n","def create_confusion_matrix_plot_casme2(data, output_path, test_version):\n","    meta = data['evaluation_metadata']\n","    class_names = meta['class_names']\n","    cm = np.array(data['confusion_matrix'], dtype=int)\n","    overall = data['overall_performance']\n","    per_class = data['per_class_performance']\n","    test_config = data.get('test_configuration', {})\n","\n","    test_desc = test_config.get('description', test_version)\n","    variant = test_config.get('variant', test_version.upper())\n","    methodology = meta.get('methodology', 'M2')\n","    input_res = meta.get('input_resolution', '224x224 Pure Grayscale (1 channel)')\n","    training_strategy = meta.get('training_strategy', 'from_scratch')\n","\n","    print(f\"Processing confusion matrix for {test_version.upper()}\")\n","    print(f\"Confusion matrix shape: {cm.shape}\")\n","\n","    macro_f1 = overall.get('macro_f1', 0.0)\n","    accuracy = overall.get('accuracy', 0.0)\n","    weighted_f1 = calculate_weighted_f1_casme2(per_class)\n","    balanced_acc = calculate_balanced_accuracy_casme2(cm)\n","\n","    print(f\"Metrics - Macro F1: {macro_f1:.4f}, Weighted F1: {weighted_f1:.4f}, \"\n","          f\"Acc: {accuracy:.4f}, Balanced Acc: {balanced_acc:.4f}\")\n","\n","    row_sums = cm.sum(axis=1, keepdims=True)\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        cm_pct = np.divide(cm, row_sums, where=(row_sums!=0))\n","        cm_pct = np.nan_to_num(cm_pct)\n","\n","    fig, ax = plt.subplots(figsize=(12, 10))\n","\n","    cmap = 'Blues'\n","\n","    im = ax.imshow(cm_pct, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=0.8)\n","\n","    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","    cbar.set_label('True Class Percentage', rotation=270, labelpad=15, fontsize=11)\n","\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            count = cm[i, j]\n","\n","            if row_sums[i, 0] > 0:\n","                percentage = cm_pct[i, j] * 100\n","                text = f\"{count}\\n{percentage:.1f}%\"\n","            else:\n","                text = f\"{count}\\n(N/A)\"\n","\n","            cell_value = cm_pct[i, j]\n","            text_color = determine_text_color_casme2(cell_value, threshold=0.4)\n","\n","            ax.text(j, i, text, ha=\"center\", va=\"center\",\n","                   color=text_color, fontsize=9, fontweight='bold')\n","\n","    ax.set_xticks(np.arange(len(class_names)))\n","    ax.set_yticks(np.arange(len(class_names)))\n","    ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=10)\n","    ax.set_yticklabels(class_names, fontsize=10)\n","    ax.set_xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n","    ax.set_ylabel(\"True Label\", fontsize=12, fontweight='bold')\n","\n","    missing_classes = meta.get('missing_classes', [])\n","    note_text = f\"Test: {test_desc} ({variant})\\n{methodology} | {input_res}\\nTraining: {training_strategy}\"\n","    if missing_classes:\n","        note_text += f\"\\nMissing: {', '.join(missing_classes)}\"\n","\n","    ax.text(0.02, 0.98, note_text, transform=ax.transAxes, fontsize=9,\n","            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n","\n","    title = f\"CASME II ConvNeXT-Tiny M2 MFS-PREP - {variant}\\n\"\n","    title += f\"Macro F1: {macro_f1:.4f}  |  Weighted F1: {weighted_f1:.4f}  |  Acc: {accuracy:.4f}  |  Balanced Acc: {balanced_acc:.4f}\"\n","    ax.set_title(title, fontsize=12, pad=25, fontweight='bold')\n","\n","    plt.tight_layout()\n","    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n","    plt.close(fig)\n","\n","    print(f\"Confusion matrix saved to: {os.path.basename(output_path)}\")\n","\n","    return {\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        'accuracy': accuracy,\n","        'balanced_accuracy': balanced_acc,\n","        'missing_classes': missing_classes,\n","        'test_version': test_version,\n","        'variant': variant\n","    }\n","\n","json_files = find_evaluation_json_files_casme2(RESULTS_ROOT)\n","\n","if not json_files:\n","    print(f\"ERROR: No evaluation JSON files found in {RESULTS_ROOT}\")\n","    print(\"Make sure Cell 3 (evaluation) has been executed first!\")\n","else:\n","    print(f\"\\nFound {len([k for k in json_files.keys() if k.startswith('main_')])} evaluation result(s)\")\n","\n","output_dir = f\"{RESULTS_ROOT}/confusion_matrix_analysis\"\n","Path(output_dir).mkdir(parents=True, exist_ok=True)\n","\n","results_summary = {}\n","generated_files = []\n","\n","for version in ['v7', 'v8']:\n","    main_key = f'main_{version}'\n","\n","    if main_key in json_files:\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Processing {version.upper()} Confusion Matrix\")\n","        print(f\"{'='*60}\")\n","\n","        eval_data = load_evaluation_results_casme2(json_files[main_key])\n","\n","        if eval_data is not None:\n","            try:\n","                cm_output_path = os.path.join(output_dir, f\"confusion_matrix_CASME2_ConvNeXT_MFS_PREP_{version.upper()}.png\")\n","                metrics = create_confusion_matrix_plot_casme2(eval_data, cm_output_path, version)\n","                generated_files.append(cm_output_path)\n","\n","                results_summary[version] = metrics\n","\n","                print(f\"SUCCESS: {version.upper()} confusion matrix generated\")\n","\n","            except Exception as e:\n","                print(f\"ERROR: Failed to generate {version.upper()} confusion matrix: {str(e)}\")\n","                import traceback\n","                traceback.print_exc()\n","        else:\n","            print(f\"ERROR: Could not load {version.upper()} evaluation data\")\n","\n","if generated_files:\n","    print(f\"\\n\" + \"=\" * 60)\n","    print(\"CASME II CONVNEXT-TINY M2 MFS-PREP CONFUSION MATRIX GENERATION COMPLETED\")\n","    print(\"=\" * 60)\n","\n","    print(f\"\\nGenerated confusion matrix files:\")\n","    for file_path in generated_files:\n","        filename = os.path.basename(file_path)\n","        print(f\"  {filename}\")\n","\n","    print(f\"\\nPerformance Summary:\")\n","    for version in ['v7', 'v8']:\n","        if version in results_summary:\n","            metrics = results_summary[version]\n","            variant = metrics.get('variant', version.upper())\n","            print(f\"\\n{variant}:\")\n","            print(f\"  Macro F1:       {metrics['macro_f1']:.4f}\")\n","            print(f\"  Weighted F1:    {metrics['weighted_f1']:.4f}\")\n","            print(f\"  Accuracy:       {metrics['accuracy']:.4f}\")\n","            print(f\"  Balanced Acc:   {metrics['balanced_accuracy']:.4f}\")\n","\n","            if metrics['missing_classes']:\n","                print(f\"  Missing classes: {len(metrics['missing_classes'])}\")\n","\n","    if len(results_summary) == 2:\n","        print(f\"\\nComparative Analysis:\")\n","        v8_f1 = results_summary['v8']['macro_f1']\n","        v7_f1 = results_summary['v7']['macro_f1']\n","        delta_f1 = v8_f1 - v7_f1\n","\n","        v8_variant = results_summary['v8'].get('variant', 'KFS')\n","        v7_variant = results_summary['v7'].get('variant', 'AF')\n","\n","        print(f\"  {v8_variant} vs {v7_variant} (Macro F1): {v8_f1:.4f} vs {v7_f1:.4f}\")\n","        print(f\"  Delta ({v8_variant} - {v7_variant}): {delta_f1:+.4f}\")\n","\n","        if delta_f1 > 0:\n","            improvement_pct = (delta_f1 / v7_f1) * 100\n","            print(f\"  {v8_variant} improves by {improvement_pct:.1f}% over {v7_variant}\")\n","        else:\n","            degradation_pct = (abs(delta_f1) / v8_f1) * 100\n","            print(f\"  {v8_variant} degrades by {degradation_pct:.1f}% from {v7_variant}\")\n","\n","    print(f\"\\nFiles saved in: {output_dir}\")\n","    print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","else:\n","    print(f\"\\nERROR: No confusion matrices were generated\")\n","    print(\"Please check:\")\n","    print(\"1. Cell 3 evaluation results exist\")\n","    print(\"2. JSON file structure is correct\")\n","    print(\"3. No file permission issues\")\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"CNN BASELINE M2 EXPERIMENTS COMPLETED\")\n","print(\"=\" * 60)\n","print(\"\\nAll 3 CNN baseline models with M2 preprocessing have been successfully implemented:\")\n","print(\"  1. MobileNetV3-Small M2 MFS-PREP (~2.5M params)\")\n","print(\"  2. EfficientNet-B0 M2 MFS-PREP (~5.3M params)\")\n","print(\"  3. ConvNeXT-Tiny M2 MFS-PREP (~28M params)\")\n","print(\"\\nEach model:\")\n","print(\"  - Trained from scratch with pure grayscale (1-channel) input\")\n","print(\"  - Evaluated on dual test sets (v7 AF + v8 KFS)\")\n","print(\"  - Generated confusion matrices and comprehensive metrics\")\n","print(\"\\nNext steps:\")\n","print(\"  - Comparative analysis across all 3 CNN models\")\n","print(\"  - Cross-comparison with ViT baseline experiments (M1 vs M2)\")\n","print(\"  - Preprocessing paradox validation on CNN architectures\")\n","\n","print(\"\\nCell 4 completed - CASME II ConvNeXT-Tiny M2 MFS-PREP confusion matrix analysis generated\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"KAWIwMgHOURR","executionInfo":{"status":"ok","timestamp":1764313869727,"user_tz":-420,"elapsed":3156,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"e91d311d-cb3b-4592-e064-a06d4543f61f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["CASME II ConvNeXT-Tiny M2 MFS-PREP Confusion Matrix Generation\n","============================================================\n","Found V7 evaluation file: casme2_convnext_mfs_prep_evaluation_results_v7.json\n","Found V8 evaluation file: casme2_convnext_mfs_prep_evaluation_results_v8.json\n","\n","Found 2 evaluation result(s)\n","\n","============================================================\n","Processing V7 Confusion Matrix\n","============================================================\n","Successfully loaded: casme2_convnext_mfs_prep_evaluation_results_v7.json\n","Processing confusion matrix for V7\n","Confusion matrix shape: (6, 6)\n","Metrics - Macro F1: 0.3439, Weighted F1: 0.4229, Acc: 0.4286, Balanced Acc: 0.6182\n","Confusion matrix saved to: confusion_matrix_CASME2_ConvNeXT_MFS_PREP_V7.png\n","SUCCESS: V7 confusion matrix generated\n","\n","============================================================\n","Processing V8 Confusion Matrix\n","============================================================\n","Successfully loaded: casme2_convnext_mfs_prep_evaluation_results_v8.json\n","Processing confusion matrix for V8\n","Confusion matrix shape: (6, 6)\n","Metrics - Macro F1: 0.3317, Weighted F1: 0.4172, Acc: 0.4286, Balanced Acc: 0.6117\n","Confusion matrix saved to: confusion_matrix_CASME2_ConvNeXT_MFS_PREP_V8.png\n","SUCCESS: V8 confusion matrix generated\n","\n","============================================================\n","CASME II CONVNEXT-TINY M2 MFS-PREP CONFUSION MATRIX GENERATION COMPLETED\n","============================================================\n","\n","Generated confusion matrix files:\n","  confusion_matrix_CASME2_ConvNeXT_MFS_PREP_V7.png\n","  confusion_matrix_CASME2_ConvNeXT_MFS_PREP_V8.png\n","\n","Performance Summary:\n","\n","AF:\n","  Macro F1:       0.3439\n","  Weighted F1:    0.4229\n","  Accuracy:       0.4286\n","  Balanced Acc:   0.6182\n","  Missing classes: 1\n","\n","KFS:\n","  Macro F1:       0.3317\n","  Weighted F1:    0.4172\n","  Accuracy:       0.4286\n","  Balanced Acc:   0.6117\n","  Missing classes: 1\n","\n","Comparative Analysis:\n","  KFS vs AF (Macro F1): 0.3317 vs 0.3439\n","  Delta (KFS - AF): -0.0122\n","  KFS degrades by 3.7% from AF\n","\n","Files saved in: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/09_03_convnext_casme2_mfs_prep/confusion_matrix_analysis\n","Analysis completed at: 2025-11-28 07:11:11\n","\n","============================================================\n","CNN BASELINE M2 EXPERIMENTS COMPLETED\n","============================================================\n","\n","All 3 CNN baseline models with M2 preprocessing have been successfully implemented:\n","  1. MobileNetV3-Small M2 MFS-PREP (~2.5M params)\n","  2. EfficientNet-B0 M2 MFS-PREP (~5.3M params)\n","  3. ConvNeXT-Tiny M2 MFS-PREP (~28M params)\n","\n","Each model:\n","  - Trained from scratch with pure grayscale (1-channel) input\n","  - Evaluated on dual test sets (v7 AF + v8 KFS)\n","  - Generated confusion matrices and comprehensive metrics\n","\n","Next steps:\n","  - Comparative analysis across all 3 CNN models\n","  - Cross-comparison with ViT baseline experiments (M1 vs M2)\n","  - Preprocessing paradox validation on CNN architectures\n","\n","Cell 4 completed - CASME II ConvNeXT-Tiny M2 MFS-PREP confusion matrix analysis generated\n"]}]}]}