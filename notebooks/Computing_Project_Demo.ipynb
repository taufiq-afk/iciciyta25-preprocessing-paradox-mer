{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO7HaxPSHrLfXUVOv8b46Jl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"58b00db4f9aa47a7ada71bf44fa20203":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7444319e3b5a446ebb183cc3f9602000","IPY_MODEL_5f2a2a29dd3a4501b194fd68586efed7","IPY_MODEL_8f2fdc59d6d24d63a56d7c7db1e1897f"],"layout":"IPY_MODEL_0b5997e5aa8c497dae4472ae4dfc21a0"}},"7444319e3b5a446ebb183cc3f9602000":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f6a158d04954715814a83ebcc5052be","placeholder":"​","style":"IPY_MODEL_3fc2b7e4b4fa461983f1ecaf0c9880a5","value":"preprocessor_config.json: 100%"}},"5f2a2a29dd3a4501b194fd68586efed7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_63e083940c8a4519a4859c16f01d5751","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb79ff7ae25c4dc5a8e915c7bb6b813b","value":160}},"8f2fdc59d6d24d63a56d7c7db1e1897f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2be49976e3514ac5bb907249cbdb0095","placeholder":"​","style":"IPY_MODEL_1eecc9e887e6400cbe3cc240fbf7fbb7","value":" 160/160 [00:00&lt;00:00, 10.9kB/s]"}},"0b5997e5aa8c497dae4472ae4dfc21a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f6a158d04954715814a83ebcc5052be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fc2b7e4b4fa461983f1ecaf0c9880a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63e083940c8a4519a4859c16f01d5751":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb79ff7ae25c4dc5a8e915c7bb6b813b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2be49976e3514ac5bb907249cbdb0095":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1eecc9e887e6400cbe3cc240fbf7fbb7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","collapsed":true,"id":"_K2vPEIo1KPq","outputId":"25651c53-292d-4de0-8c08-0986a0a880b9","executionInfo":{"status":"ok","timestamp":1767432129936,"user_tz":-420,"elapsed":75474,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","MULTIMEDIA LABORATORY - TELKOM UNIVERSITY\n","Interactive Project Demonstrations\n","================================================================================\n","\n","[1/4] Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully\n","\n","[2/4] Installing dependencies...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"]},{"output_type":"stream","name":"stdout","text":["All packages available\n","\n","[3/4] Configuring project paths...\n","Device: cpu\n","Project metadata configured\n","Styling configured\n","\n","[4/4] Infrastructure setup complete\n","================================================================================\n","Ready to build interface\n","================================================================================\n","\n","Building Gradio interface...\n","Landing page interface built\n","\n","================================================================================\n","CELL 1 COMPLETE - ORANGE THEME\n","================================================================================\n","\n","Changes from red theme:\n","  ✓ Primary color: #dc2626 → #FF8C00 (Orange)\n","  ✓ Dark color: #991b1b → #CC7A00 (Dark Orange)\n","  ✓ Hover color: #b91c1c → #E67E00 (Orange hover)\n","  ✓ Light backgrounds updated to cream/light orange\n","  ✓ Gradient updated to orange tones\n","  ✓ All CSS classes updated with orange colors\n","\n","Next steps:\n","1. Run Cell 2: HER2 Model Loading and Preprocessing\n","2. Run Cell 3: HER2 Interpretability Functions\n","3. Run Cell 4: HER2 Interactive Interface\n","4. Run Cell 5: MER Models (With Transformers)\n","5. Run Cell 6: MER Preprocessing\n","6. Run Cell 7: MER Interface\n","7. Run Cell 8: Launch Integrated Demo\n","================================================================================\n"]}],"source":["# @title Cell 1: Infrastructure Setup + Landing Page - ORANGE THEME\n","\n","# File: MMLab_Project_Demo.ipynb - Cell 1\n","# Location: Thesis_MER_Project/MMLab_Project_Demo.ipynb\n","# Purpose: Combined infrastructure and landing page - ORANGE THEMED\n","\n","import os\n","import sys\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"=\" * 80)\n","print(\"MULTIMEDIA LABORATORY - TELKOM UNIVERSITY\")\n","print(\"Interactive Project Demonstrations\")\n","print(\"=\" * 80)\n","\n","# ============================================================================\n","# SECTION 1: GOOGLE DRIVE MOUNTING\n","# ============================================================================\n","print(\"\\n[1/4] Mounting Google Drive...\")\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","print(\"Google Drive mounted successfully\")\n","\n","# ============================================================================\n","# SECTION 2: DEPENDENCY INSTALLATION\n","# ============================================================================\n","print(\"\\n[2/4] Installing dependencies...\")\n","try:\n","    import gradio as gr\n","    from transformers import ViTModel, ViTImageProcessor\n","    import torch\n","    import torchvision\n","    import timm\n","    print(\"All packages available\")\n","except ImportError:\n","    print(\"Installing missing packages...\")\n","    import subprocess\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n","                          \"gradio\", \"transformers\", \"timm\", \"opencv-python\"])\n","    import gradio as gr\n","    from transformers import ViTModel, ViTImageProcessor\n","    import torch\n","    import torchvision\n","    import timm\n","    print(\"Dependencies installed successfully\")\n","\n","# Core imports\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import numpy as np\n","import cv2\n","import json\n","import matplotlib.pyplot as plt\n","\n","# ============================================================================\n","# SECTION 3: PROJECT CONFIGURATIONS (ORIGINAL PATHS - UNCHANGED)\n","# ============================================================================\n","print(\"\\n[3/4] Configuring project paths...\")\n","\n","# Base paths\n","PROJECT_BASE = \"/content/drive/MyDrive\"\n","\n","# HER2 Project paths (ORIGINAL - DO NOT CHANGE)\n","HER2_PROJECT_ROOT = f\"{PROJECT_BASE}/SOURCE_CODE\"\n","HER2_MODELS_ROOT = f\"{HER2_PROJECT_ROOT}/Models\"\n","\n","# MER Project paths\n","MER_PROJECT_ROOT = f\"{PROJECT_BASE}/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","MER_MODELS_ROOT = f\"{MER_PROJECT_ROOT}/models\"\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if torch.cuda.is_available():\n","    gpu_name = torch.cuda.get_device_name(0)\n","    torch.backends.cudnn.benchmark = True\n","    print(f\"Device: {device} ({gpu_name})\")\n","else:\n","    print(f\"Device: {device}\")\n","\n","# ============================================================================\n","# SECTION 4: PROJECT METADATA (DUAL STRUCTURE FOR COMPATIBILITY)\n","# ============================================================================\n","\n","# Simple PROJECT_INFO for Cell 8 compatibility - UPDATED COLOR\n","PROJECT_INFO = {\n","    'lab_name': 'Multimedia Laboratory',\n","    'university': 'Telkom University',\n","    'location': 'Bandung, Indonesia',\n","    'primary_color': '#FF8C00'  # Changed from red to orange\n","}\n","\n","# Detailed project information for landing page cards\n","PROJECT_DETAILS = {\n","    'her2': {\n","        'title': 'HER2 Status Classification',\n","        'subtitle': 'Deep Learning for Gastroesophageal Cancer Diagnosis',\n","        'description': '''\n","        Automated classification of HER2 status in gastroesophageal adenocarcinoma\n","        from tissue microarray images using fusion of CNN and Vision Transformer models.\n","        ''',\n","        'features': [\n","            '4 Model Architectures (MobileNetV3, ViT, Fusion Concat, Fusion Addition)',\n","            'Dual Preprocessing Variants (Standard & Medical Enhancement)',\n","            'Interactive Attention Visualization (Grad-CAM & Transformer Attention)',\n","            'IHC Score (0-3) and HER2 Status (Negative/Positive) Classification'\n","        ],\n","        'metrics': {\n","            'Dataset': 'TMA Images',\n","            'Best Model': 'Fusion (Concatenation)',\n","            'Accuracy': '~85% (IHC), ~90% (HER2)',\n","            'Status': 'Production Ready'\n","        },\n","        'thumbnail': 'https://via.placeholder.com/400x250/FF8C00/ffffff?text=HER2+Classification',\n","        'demo_available': True,\n","        'publication': 'Under Review - IEEE/ACM Conference'\n","    },\n","    'mer': {\n","        'title': 'Micro-Expression Recognition',\n","        'subtitle': 'CNN and Transformer Baselines for CASME II Dataset',\n","        'description': '''\n","        Comparative study of CNN and Transformer architectures for micro-expression recognition\n","        with investigation of preprocessing paradox phenomenon.\n","        ''',\n","        'features': [\n","            '6 Model Architectures (3 CNNs + 3 Transformers)',\n","            'Dual Methodologies (M1 Raw RGB, M2 Preprocessed)',\n","            '7-Category CASME II Classification (Extreme Class Imbalance)',\n","            'Real-time Webcam Classification'\n","        ],\n","        'metrics': {\n","            'Dataset': 'CASME II (255 videos)',\n","            'Best Model': 'MobileNetV3-Small M1',\n","            'Macro F1': '0.3880',\n","            'Status': 'Research Phase'\n","        },\n","        'thumbnail': 'https://via.placeholder.com/400x250/3b82f6/ffffff?text=Micro-Expression+Recognition',\n","        'demo_available': True,\n","        'publication': 'IEEE ICICyTA 2025 (Accepted)'\n","    }\n","}\n","\n","print(\"Project metadata configured\")\n","\n","# ============================================================================\n","# SECTION 5: GLOBAL STYLING & THEME - ORANGE THEME\n","# ============================================================================\n","\n","# Telkom University inspired theme - ORANGE COLOR SCHEME\n","MMLAB_THEME = gr.themes.Soft(\n","    primary_hue=\"orange\",  # Changed from red\n","    secondary_hue=\"slate\",\n","    neutral_hue=\"slate\",\n","    font=gr.themes.GoogleFont(\"Inter\"),\n","    font_mono=gr.themes.GoogleFont(\"JetBrains Mono\")\n",").set(\n","    # Light mode - ORANGE COLORS\n","    body_background_fill=\"white\",\n","    body_text_color=\"#1f2937\",\n","    button_primary_background_fill=\"#FF8C00\",  # Orange\n","    button_primary_background_fill_hover=\"#E67E00\",  # Darker orange\n","    button_primary_text_color=\"white\",\n","    block_background_fill=\"#f9fafb\",\n","    block_border_color=\"#e5e7eb\",\n","    block_border_width=\"1px\",\n","    block_label_background_fill=\"#FFF4E6\",  # Light orange/cream\n","    block_label_text_color=\"#CC7A00\",  # Dark orange\n","\n","    # Dark mode - ORANGE COLORS\n","    body_background_fill_dark=\"#111827\",\n","    body_text_color_dark=\"#f3f4f6\",\n","    button_primary_background_fill_dark=\"#FF8C00\",  # Orange\n","    button_primary_background_fill_hover_dark=\"#E67E00\",  # Darker orange\n","    block_background_fill_dark=\"#1f2937\",\n","    block_border_color_dark=\"#374151\",\n","    block_label_background_fill_dark=\"#CC7A00\",  # Dark orange\n","    block_label_text_color_dark=\"#FFB347\"  # Light orange\n",")\n","\n","# Custom CSS - ORANGE COLOR SCHEME\n","CUSTOM_CSS = \"\"\"\n",".project-card {\n","    border: 1px solid #e5e7eb;\n","    border-radius: 12px;\n","    padding: 24px;\n","    background: white;\n","    box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n","    transition: all 0.3s ease;\n","}\n","\n",".project-card:hover {\n","    box-shadow: 0 10px 25px rgba(255,140,0,0.2);\n","    transform: translateY(-4px);\n","}\n","\n",".project-title {\n","    font-size: 24px;\n","    font-weight: 700;\n","    color: #1f2937;\n","    margin-bottom: 8px;\n","}\n","\n",".project-subtitle {\n","    font-size: 14px;\n","    color: #6b7280;\n","    font-style: italic;\n","    margin-bottom: 16px;\n","}\n","\n",".project-description {\n","    font-size: 15px;\n","    line-height: 1.6;\n","    color: #374151;\n","    margin-bottom: 20px;\n","}\n","\n",".feature-list {\n","    list-style: none;\n","    padding: 0;\n","    margin: 16px 0;\n","}\n","\n",".feature-item {\n","    padding: 8px 0;\n","    padding-left: 24px;\n","    position: relative;\n","    color: #4b5563;\n","}\n","\n",".feature-item:before {\n","    content: \"✓\";\n","    position: absolute;\n","    left: 0;\n","    color: #FF8C00;\n","    font-weight: bold;\n","}\n","\n",".metrics-grid {\n","    display: grid;\n","    grid-template-columns: repeat(2, 1fr);\n","    gap: 12px;\n","    margin: 16px 0;\n","    padding: 16px;\n","    background: #f9fafb;\n","    border-radius: 8px;\n","}\n","\n",".metric-item {\n","    padding: 8px;\n","}\n","\n",".metric-label {\n","    font-size: 12px;\n","    color: #6b7280;\n","    text-transform: uppercase;\n","    letter-spacing: 0.5px;\n","}\n","\n",".metric-value {\n","    font-size: 16px;\n","    font-weight: 600;\n","    color: #1f2937;\n","    margin-top: 4px;\n","}\n","\n",".status-badge {\n","    display: inline-block;\n","    padding: 4px 12px;\n","    border-radius: 12px;\n","    font-size: 12px;\n","    font-weight: 600;\n","    text-transform: uppercase;\n","}\n","\n",".status-ready {\n","    background: #dcfce7;\n","    color: #166534;\n","}\n","\n",".status-research {\n","    background: #dbeafe;\n","    color: #1e40af;\n","}\n","\n",".hero-section {\n","    text-align: center;\n","    padding: 48px 24px;\n","    background: linear-gradient(135deg, #FF8C00 0%, #CC7A00 100%);\n","    color: white;\n","    border-radius: 16px;\n","    margin-bottom: 32px;\n","}\n","\n",".hero-title {\n","    font-size: 42px;\n","    font-weight: 800;\n","    margin-bottom: 16px;\n","    text-shadow: 2px 2px 4px rgba(0,0,0,0.2);\n","}\n","\n",".hero-subtitle {\n","    font-size: 18px;\n","    opacity: 0.95;\n","    margin-bottom: 8px;\n","}\n","\n",".demo-button {\n","    background: #FF8C00 !important;\n","    color: white !important;\n","    border: none !important;\n","    padding: 12px 24px !important;\n","    font-weight: 600 !important;\n","    border-radius: 8px !important;\n","    cursor: pointer !important;\n","    transition: all 0.3s ease !important;\n","}\n","\n",".demo-button:hover {\n","    background: #E67E00 !important;\n","    transform: scale(1.05);\n","}\n","\n",".demo-button:disabled {\n","    background: #9ca3af !important;\n","    cursor: not-allowed !important;\n","    transform: none !important;\n","}\n","\n","@media (max-width: 768px) {\n","    .hero-title {\n","        font-size: 32px;\n","    }\n","    .metrics-grid {\n","        grid-template-columns: 1fr;\n","    }\n","}\n","\"\"\"\n","\n","print(\"Styling configured\")\n","\n","# ============================================================================\n","# SECTION 6: HELPER FUNCTIONS FOR PROJECT CARDS\n","# ============================================================================\n","\n","def create_project_card_html(project_key):\n","    \"\"\"Generate HTML for project card\"\"\"\n","    info = PROJECT_DETAILS[project_key]\n","\n","    # Features list\n","    features_html = \"\".join([\n","        f'<li class=\"feature-item\">{feature}</li>'\n","        for feature in info['features']\n","    ])\n","\n","    # Metrics grid\n","    metrics_html = \"\".join([\n","        f'''\n","        <div class=\"metric-item\">\n","            <div class=\"metric-label\">{label}</div>\n","            <div class=\"metric-value\">{value}</div>\n","        </div>\n","        '''\n","        for label, value in info['metrics'].items()\n","    ])\n","\n","    # Status badge\n","    status_class = \"status-ready\" if info['demo_available'] else \"status-research\"\n","    status_text = info['metrics']['Status']\n","\n","    card_html = f'''\n","    <div class=\"project-card\">\n","        <div class=\"project-title\">{info['title']}</div>\n","        <div class=\"project-subtitle\">{info['subtitle']}</div>\n","        <div class=\"project-description\">{info['description']}</div>\n","\n","        <div style=\"margin: 16px 0;\">\n","            <strong style=\"color: #374151;\">Key Features:</strong>\n","            <ul class=\"feature-list\">\n","                {features_html}\n","            </ul>\n","        </div>\n","\n","        <div class=\"metrics-grid\">\n","            {metrics_html}\n","        </div>\n","\n","        <div style=\"margin-top: 16px; display: flex; justify-content: space-between; align-items: center;\">\n","            <span class=\"status-badge {status_class}\">{status_text}</span>\n","            <span style=\"font-size: 12px; color: #6b7280;\">{info['publication']}</span>\n","        </div>\n","    </div>\n","    '''\n","\n","    return card_html\n","\n","# ============================================================================\n","# SECTION 7: TAB NAVIGATION STATE MANAGEMENT\n","# ============================================================================\n","\n","current_tab_state = gr.State(value=\"landing\")\n","\n","def switch_to_her2_demo():\n","    \"\"\"Switch to HER2 demo tab\"\"\"\n","    return 1\n","\n","def switch_to_landing():\n","    \"\"\"Switch back to landing page\"\"\"\n","    return 0\n","\n","print(\"\\n[4/4] Infrastructure setup complete\")\n","print(\"=\" * 80)\n","print(\"Ready to build interface\")\n","print(\"=\" * 80)\n","\n","# ============================================================================\n","# SECTION 8: GRADIO INTERFACE STRUCTURE\n","# ============================================================================\n","\n","print(\"\\nBuilding Gradio interface...\")\n","\n","with gr.Blocks(\n","    title=\"Multimedia Laboratory - Telkom University\",\n","    theme=MMLAB_THEME,\n","    css=CUSTOM_CSS\n",") as demo:\n","\n","    with gr.Tabs() as main_tabs:\n","\n","        # Tab 1: Landing Page\n","        with gr.Tab(\"Projects\", id=0):\n","            gr.HTML(\"\"\"\n","            <div class=\"hero-section\">\n","                <div class=\"hero-title\">Multimedia Laboratory</div>\n","                <div class=\"hero-subtitle\">School of Computing - Telkom University</div>\n","                <div style=\"margin-top: 16px; font-size: 16px; opacity: 0.9;\">\n","                    Advancing AI Research in Medical Imaging and Affective Computing\n","                </div>\n","            </div>\n","            \"\"\")\n","\n","            gr.Markdown(\"## Featured Research Projects\")\n","\n","            gr.Markdown(\"\"\"\n","            Explore our cutting-edge deep learning research projects.\n","            Each project demonstrates practical applications of AI in healthcare and human behavior analysis.\n","            \"\"\")\n","\n","            with gr.Row(equal_height=True):\n","                with gr.Column(scale=1):\n","                    gr.HTML(create_project_card_html('her2'))\n","\n","                    her2_demo_btn = gr.Button(\n","                        \"Try HER2 Demo\",\n","                        variant=\"primary\",\n","                        size=\"lg\",\n","                        elem_classes=\"demo-button\"\n","                    )\n","\n","                with gr.Column(scale=1):\n","                    gr.HTML(create_project_card_html('mer'))\n","\n","                    mer_demo_btn = gr.Button(\n","                        \"Try MER Demo\",\n","                        variant=\"primary\",\n","                        size=\"lg\",\n","                        elem_classes=\"demo-button\"\n","                    )\n","\n","            gr.Markdown(\"---\")\n","\n","            with gr.Row():\n","                with gr.Column(scale=1):\n","                    gr.Markdown(\"\"\"\n","                    ### About the Lab\n","\n","                    The Multimedia Laboratory focuses on developing intelligent systems\n","                    for medical image analysis, affective computing, and human-computer interaction.\n","\n","                    **Research Areas:**\n","                    - Medical Image Classification\n","                    - Micro-Expression Recognition\n","                    - Deep Learning Architectures\n","                    - Multimodal Fusion Systems\n","                    \"\"\")\n","\n","                with gr.Column(scale=1):\n","                    gr.Markdown(\"\"\"\n","                    ### Contact and Resources\n","\n","                    **Location:**\n","                    School of Computing, Telkom University\n","                    Bandung, Indonesia\n","\n","                    **Collaboration Inquiries:**\n","                    For research collaboration or dataset access,\n","                    please contact the lab coordinator.\n","\n","                    **Publications:**\n","                    Visit our publications page for latest research outputs.\n","                    \"\"\")\n","\n","        # Tab 2: HER2 Demo (Placeholder - will be filled by Cell 4)\n","        with gr.Tab(\"HER2 Classification Demo\", id=1):\n","            gr.Markdown(\"\"\"\n","            # HER2 Status Classification\n","\n","            **Note:** This demo will be activated by Cell 4.\n","            Please run Cell 2, Cell 3, and Cell 4 to enable full functionality.\n","\n","            The HER2 demo provides:\n","            - Medical image upload and preprocessing\n","            - Model selection (MobileNetV3, ViT, Fusion models)\n","            - Real-time classification with confidence scores\n","            - Attention visualization (Grad-CAM and Transformer attention)\n","            \"\"\")\n","\n","            her2_placeholder = gr.Markdown(\"\"\"\n","            ### Initializing Demo Components...\n","\n","            Run the following cells to activate:\n","            - Cell 2: Model Loading and Preprocessing Functions\n","            - Cell 3: Interpretability Functions\n","            - Cell 4: Interactive Interface\n","            \"\"\")\n","\n","        # Tab 3: MER Demo (Placeholder - will be filled by Cell 8)\n","        with gr.Tab(\"MER Analysis Demo\", id=2):\n","            gr.Markdown(\"\"\"\n","            # Micro-Expression Recognition Analysis\n","\n","            **Note:** This demo will be activated by Cell 8.\n","            Please run Cells 5-8 to enable full functionality.\n","\n","            The MER demo provides:\n","            - Webcam capture for real-time classification\n","            - 6 model architectures (3 CNNs + 3 Transformers)\n","            - Dual preprocessing methodologies (M1 Raw, M2 Preprocessed)\n","            - 7-class emotion probabilities\n","            \"\"\")\n","\n","    # Event Handlers\n","    her2_demo_btn.click(\n","        fn=lambda: gr.Tabs.update(selected=1),\n","        inputs=None,\n","        outputs=main_tabs\n","    )\n","\n","    mer_demo_btn.click(\n","        fn=lambda: gr.Tabs.update(selected=2),\n","        inputs=None,\n","        outputs=main_tabs\n","    )\n","\n","print(\"Landing page interface built\")\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 1 COMPLETE - ORANGE THEME\")\n","print(\"=\" * 80)\n","print(\"\\nChanges from red theme:\")\n","print(\"  ✓ Primary color: #dc2626 → #FF8C00 (Orange)\")\n","print(\"  ✓ Dark color: #991b1b → #CC7A00 (Dark Orange)\")\n","print(\"  ✓ Hover color: #b91c1c → #E67E00 (Orange hover)\")\n","print(\"  ✓ Light backgrounds updated to cream/light orange\")\n","print(\"  ✓ Gradient updated to orange tones\")\n","print(\"  ✓ All CSS classes updated with orange colors\")\n","print(\"\\nNext steps:\")\n","print(\"1. Run Cell 2: HER2 Model Loading and Preprocessing\")\n","print(\"2. Run Cell 3: HER2 Interpretability Functions\")\n","print(\"3. Run Cell 4: HER2 Interactive Interface\")\n","print(\"4. Run Cell 5: MER Models (With Transformers)\")\n","print(\"5. Run Cell 6: MER Preprocessing\")\n","print(\"6. Run Cell 7: MER Interface\")\n","print(\"7. Run Cell 8: Launch Integrated Demo\")\n","print(\"=\" * 80)"]},{"cell_type":"code","source":["# @title Cell 2: HER2 Model Loading & Preprocessing Functions\n","\n","# File: MMLab_Project_Demo.ipynb - Cell 2\n","# Location: Thesis_MER_Project/MMLab_Project_Demo.ipynb\n","# Purpose: HER2 model architectures, preprocessing, and loading functions\n","\n","import pickle\n","from typing import Tuple, Optional, Dict, Any\n","\n","print(\"=\" * 80)\n","print(\"HER2 PROJECT - MODEL LOADING & PREPROCESSING\")\n","print(\"=\" * 80)\n","\n","# ============================================================================\n","# SECTION 1: HER2 CHECKPOINT PATHS\n","# ============================================================================\n","\n","print(\"\\n[1/6] Configuring HER2 model checkpoints...\")\n","\n","HER2_CHECKPOINT_PATHS = {\n","    # Original preprocessing models (8 models)\n","    'MobileNetV3_IHC_orig': f\"{HER2_MODELS_ROOT}/MobileNetV3/ihc_mobilenetv3_orig_WOA_val_F1.pth\",\n","    'MobileNetV3_HER2_orig': f\"{HER2_MODELS_ROOT}/MobileNetV3/her2_mobilenetv3_orig_WOA_val_F1.pth\",\n","    'ViT_IHC_orig': f\"{HER2_MODELS_ROOT}/ViT/ihc_vit_orig_WOA_val_F1.pth\",\n","    'ViT_HER2_orig': f\"{HER2_MODELS_ROOT}/ViT/her2_vit_orig_WOA_val_F1.pth\",\n","    'FusionConcat_IHC_orig': f\"{HER2_MODELS_ROOT}/Fusion Concat/ihc_concat_mobilenetv3_vit_orig_WOA_val_F1.pth\",\n","    'FusionConcat_HER2_orig': f\"{HER2_MODELS_ROOT}/Fusion Concat/her2_concat_mobilenetv3_vit_orig_WOA_val_F1.pth\",\n","    'FusionAddition_IHC_orig': f\"{HER2_MODELS_ROOT}/Fusion Addition/ihc_addition_mobilenetv3_vit_orig_WOA_val_F1.pth\",\n","    'FusionAddition_HER2_orig': f\"{HER2_MODELS_ROOT}/Fusion Addition/her2_addition_mobilenetv3_vit_orig_WOA_val_F1.pth\",\n","\n","    # Medical preprocessing models (8 models)\n","    'MobileNetV3_IHC_prep': f\"{HER2_MODELS_ROOT}/MobileNetV3/ihc_mobilenetv3_prep_WOA_val_F1.pth\",\n","    'MobileNetV3_HER2_prep': f\"{HER2_MODELS_ROOT}/MobileNetV3/her2_mobilenetv3_prep_WOA_val_F1.pth\",\n","    'ViT_IHC_prep': f\"{HER2_MODELS_ROOT}/ViT/ihc_vit_prep_WOA_val_F1.pth\",\n","    'ViT_HER2_prep': f\"{HER2_MODELS_ROOT}/ViT/her2_vit_prep_WOA_val_F1.pth\",\n","    'FusionConcat_IHC_prep': f\"{HER2_MODELS_ROOT}/Fusion Concat/ihc_concat_mobilenetv3_vit_prep_WOA_val_F1.pth\",\n","    'FusionConcat_HER2_prep': f\"{HER2_MODELS_ROOT}/Fusion Concat/her2_concat_mobilenetv3_vit_prep_WOA_val_F1.pth\",\n","    'FusionAddition_IHC_prep': f\"{HER2_MODELS_ROOT}/Fusion Addition/ihc_addition_mobilenetv3_vit_prep_WOA_val_F1.pth\",\n","    'FusionAddition_HER2_prep': f\"{HER2_MODELS_ROOT}/Fusion Addition/her2_addition_mobilenetv3_vit_prep_WOA_val_F1.pth\"\n","}\n","\n","print(f\"✓ Configured {len(HER2_CHECKPOINT_PATHS)} model checkpoints\")\n","print(f\"  - Original preprocessing: 8 models\")\n","print(f\"  - Medical preprocessing: 8 models\")\n","\n","# ============================================================================\n","# SECTION 2: HER2 PREPROCESSING CONFIGURATION\n","# ============================================================================\n","\n","print(\"\\n[2/6] Setting up preprocessing pipelines...\")\n","\n","# Image settings\n","TARGET_SIZE = 1024\n","IMAGENET_MEAN = [0.485, 0.456, 0.406]\n","IMAGENET_STD = [0.229, 0.224, 0.225]\n","\n","# Standard transform for model input\n","standard_transform = transforms.Compose([\n","    transforms.Resize((TARGET_SIZE, TARGET_SIZE), interpolation=transforms.InterpolationMode.LANCZOS),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n","])\n","\n","# ViT processor\n","vit_processor = ViTImageProcessor.from_pretrained(\n","    'google/vit-base-patch32-224-in21k',\n","    do_resize=False,\n","    do_normalize=True,\n","    do_rescale=True,\n","    do_center_crop=False\n",")\n","\n","print(\"✓ Preprocessing pipelines configured\")\n","\n","# ============================================================================\n","# SECTION 3: HER2 TASK CONFIGURATIONS\n","# ============================================================================\n","\n","HER2_TASK_CONFIGS = {\n","    'IHC': {\n","        'full_name': 'IHC Score Classification',\n","        'num_classes': 4,\n","        'class_names': ['IHC_0', 'IHC_1', 'IHC_2', 'IHC_3'],\n","        'description': 'Immunohistochemistry score (0-3)'\n","    },\n","    'HER2': {\n","        'full_name': 'HER2 Status Classification',\n","        'num_classes': 2,\n","        'class_names': ['HER2_negative', 'HER2_positive'],\n","        'description': 'HER2 receptor status'\n","    }\n","}\n","\n","print(f\"✓ Task configurations: {list(HER2_TASK_CONFIGS.keys())}\")\n","\n","# ============================================================================\n","# SECTION 4: HER2 MODEL ARCHITECTURES\n","# ============================================================================\n","\n","print(\"\\n[3/6] Defining model architectures...\")\n","\n","class MobileNetV3_Architecture(nn.Module):\n","    \"\"\"MobileNetV3 baseline for medical image classification\"\"\"\n","\n","    def __init__(self, num_classes, dropout_rate=0.2):\n","        super(MobileNetV3_Architecture, self).__init__()\n","\n","        self.mobilenet = timm.create_model(\n","            'mobilenetv3_large_100',\n","            pretrained=True,\n","            num_classes=0,\n","            global_pool='avg'\n","        )\n","\n","        for param in self.mobilenet.parameters():\n","            param.requires_grad = True\n","\n","        self.mobilenet_feature_dim = 1280\n","\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.mobilenet_feature_dim, 512),\n","            nn.LayerNorm(512),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(512, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        mobilenet_features = self.mobilenet(x)\n","        processed_features = self.classifier_layers(mobilenet_features)\n","        output = self.classifier(processed_features)\n","        return output\n","\n","\n","class ViT_Architecture(nn.Module):\n","    \"\"\"Vision Transformer with attention extraction support\"\"\"\n","\n","    def __init__(self, num_classes, dropout_rate=0.2):\n","        super(ViT_Architecture, self).__init__()\n","\n","        self.vit = ViTModel.from_pretrained(\n","            'google/vit-base-patch32-224-in21k',\n","            add_pooling_layer=False,\n","            output_attentions=True\n","        )\n","\n","        self.vit.config.output_attentions = True\n","\n","        for param in self.vit.parameters():\n","            param.requires_grad = True\n","\n","        self.vit_feature_dim = self.vit.config.hidden_size\n","\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.vit_feature_dim, 512),\n","            nn.LayerNorm(512),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(512, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","    def forward(self, pixel_values, output_attentions=False):\n","        vit_outputs = self.vit(\n","            pixel_values=pixel_values,\n","            interpolate_pos_encoding=True,\n","            output_attentions=output_attentions\n","        )\n","        vit_features = vit_outputs.last_hidden_state[:, 0]\n","        processed_features = self.classifier_layers(vit_features)\n","        output = self.classifier(processed_features)\n","\n","        if output_attentions:\n","            return output, vit_outputs.attentions\n","        return output\n","\n","\n","class FusionConcat_Architecture(nn.Module):\n","    \"\"\"Fusion model with concatenation strategy\"\"\"\n","\n","    def __init__(self, num_classes, dropout_rate=0.2):\n","        super(FusionConcat_Architecture, self).__init__()\n","\n","        self.mobilenet = timm.create_model(\n","            'mobilenetv3_large_100',\n","            pretrained=True,\n","            num_classes=0,\n","            global_pool='avg'\n","        )\n","\n","        self.vit = ViTModel.from_pretrained(\n","            'google/vit-base-patch32-224-in21k',\n","            add_pooling_layer=False,\n","            output_attentions=True\n","        )\n","\n","        self.vit.config.output_attentions = True\n","\n","        for param in self.mobilenet.parameters():\n","            param.requires_grad = True\n","        for param in self.vit.parameters():\n","            param.requires_grad = True\n","\n","        self.mobilenet_dim = 1280\n","        self.vit_dim = self.vit.config.hidden_size\n","        self.projection_dim = 512\n","\n","        self.mobilenet_projector = nn.Sequential(\n","            nn.Linear(self.mobilenet_dim, self.projection_dim),\n","            nn.LayerNorm(self.projection_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.vit_projector = nn.Sequential(\n","            nn.Linear(self.vit_dim, self.projection_dim),\n","            nn.LayerNorm(self.projection_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.fusion_classifier = nn.Sequential(\n","            nn.Linear(1024, 512),\n","            nn.LayerNorm(512),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(512, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","    def forward(self, images, pixel_values, return_features=False):\n","        mobilenet_features = self.mobilenet(images)\n","        mobilenet_projected = self.mobilenet_projector(mobilenet_features)\n","\n","        vit_outputs = self.vit(\n","            pixel_values=pixel_values,\n","            interpolate_pos_encoding=True,\n","            output_attentions=True\n","        )\n","        vit_features = vit_outputs.last_hidden_state[:, 0]\n","        vit_projected = self.vit_projector(vit_features)\n","\n","        mobilenet_weighted = mobilenet_projected * 0.5\n","        vit_weighted = vit_projected * 0.5\n","        fused_features = torch.cat([mobilenet_weighted, vit_weighted], dim=1)\n","\n","        processed_features = self.fusion_classifier(fused_features)\n","        output = self.classifier(processed_features)\n","\n","        if return_features:\n","            feature_dict = {\n","                'mobilenet_features': mobilenet_features,\n","                'vit_features': vit_features,\n","                'mobilenet_projected': mobilenet_projected,\n","                'vit_projected': vit_projected,\n","                'mobilenet_weighted': mobilenet_weighted,\n","                'vit_weighted': vit_weighted,\n","                'fused_features': fused_features,\n","                'vit_attentions': vit_outputs.attentions\n","            }\n","            return output, feature_dict\n","\n","        return output\n","\n","\n","class FusionAddition_Architecture(nn.Module):\n","    \"\"\"Fusion model with addition strategy\"\"\"\n","\n","    def __init__(self, num_classes, dropout_rate=0.2):\n","        super(FusionAddition_Architecture, self).__init__()\n","\n","        self.mobilenet = timm.create_model(\n","            'mobilenetv3_large_100',\n","            pretrained=True,\n","            num_classes=0,\n","            global_pool='avg'\n","        )\n","\n","        self.vit = ViTModel.from_pretrained(\n","            'google/vit-base-patch32-224-in21k',\n","            add_pooling_layer=False,\n","            output_attentions=True\n","        )\n","\n","        self.vit.config.output_attentions = True\n","\n","        for param in self.mobilenet.parameters():\n","            param.requires_grad = True\n","        for param in self.vit.parameters():\n","            param.requires_grad = True\n","\n","        self.mobilenet_dim = 1280\n","        self.vit_dim = self.vit.config.hidden_size\n","        self.projection_dim = 512\n","\n","        self.mobilenet_projector = nn.Sequential(\n","            nn.Linear(self.mobilenet_dim, self.projection_dim),\n","            nn.LayerNorm(self.projection_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.vit_projector = nn.Sequential(\n","            nn.Linear(self.vit_dim, self.projection_dim),\n","            nn.LayerNorm(self.projection_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.fusion_classifier = nn.Sequential(\n","            nn.Linear(512, 256),\n","            nn.LayerNorm(256),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(256, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","    def forward(self, images, pixel_values, return_features=False):\n","        mobilenet_features = self.mobilenet(images)\n","        mobilenet_projected = self.mobilenet_projector(mobilenet_features)\n","\n","        vit_outputs = self.vit(\n","            pixel_values=pixel_values,\n","            interpolate_pos_encoding=True,\n","            output_attentions=True\n","        )\n","        vit_features = vit_outputs.last_hidden_state[:, 0]\n","        vit_projected = self.vit_projector(vit_features)\n","\n","        mobilenet_weighted = mobilenet_projected * 0.5\n","        vit_weighted = vit_projected * 0.5\n","        fused_features = mobilenet_weighted + vit_weighted\n","\n","        processed_features = self.fusion_classifier(fused_features)\n","        output = self.classifier(processed_features)\n","\n","        if return_features:\n","            feature_dict = {\n","                'mobilenet_features': mobilenet_features,\n","                'vit_features': vit_features,\n","                'mobilenet_projected': mobilenet_projected,\n","                'vit_projected': vit_projected,\n","                'mobilenet_weighted': mobilenet_weighted,\n","                'vit_weighted': vit_weighted,\n","                'fused_features': fused_features,\n","                'vit_attentions': vit_outputs.attentions\n","            }\n","            return output, feature_dict\n","\n","        return output\n","\n","\n","# Model registry\n","HER2_MODEL_REGISTRY = {\n","    'MobileNetV3': {\n","        'architecture_class': MobileNetV3_Architecture,\n","        'display_name': 'MobileNetV3-Large',\n","        'requires_dual_input': False,\n","        'supports_attention': True,\n","        'attention_method': 'grad_cam'\n","    },\n","    'ViT': {\n","        'architecture_class': ViT_Architecture,\n","        'display_name': 'Vision Transformer',\n","        'requires_dual_input': False,\n","        'supports_attention': True,\n","        'attention_method': 'native_transformer'\n","    },\n","    'FusionConcat': {\n","        'architecture_class': FusionConcat_Architecture,\n","        'display_name': 'Fusion (Concatenation)',\n","        'requires_dual_input': True,\n","        'supports_attention': True,\n","        'attention_method': 'dual_branch'\n","    },\n","    'FusionAddition': {\n","        'architecture_class': FusionAddition_Architecture,\n","        'display_name': 'Fusion (Addition)',\n","        'requires_dual_input': True,\n","        'supports_attention': True,\n","        'attention_method': 'dual_branch'\n","    }\n","}\n","\n","print(f\"✓ Defined {len(HER2_MODEL_REGISTRY)} model architectures\")\n","\n","# ============================================================================\n","# SECTION 5: PREPROCESSING FUNCTIONS\n","# ============================================================================\n","\n","print(\"\\n[4/6] Defining preprocessing functions...\")\n","\n","def apply_standard_preprocessing(image: Image.Image) -> Image.Image:\n","    \"\"\"Apply standard preprocessing: resize to 1024px with LANCZOS\"\"\"\n","    if image.size != (TARGET_SIZE, TARGET_SIZE):\n","        image = image.resize(\n","            (TARGET_SIZE, TARGET_SIZE),\n","            Image.Resampling.LANCZOS\n","        )\n","    return image\n","\n","\n","def apply_medical_preprocessing(image: Image.Image) -> Image.Image:\n","    \"\"\"Apply medical preprocessing: CLAHE + tissue mask + background removal\"\"\"\n","    img_array = np.array(image)\n","\n","    if img_array.shape[:2] != (TARGET_SIZE, TARGET_SIZE):\n","        img_array = cv2.resize(\n","            img_array,\n","            (TARGET_SIZE, TARGET_SIZE),\n","            interpolation=cv2.INTER_LANCZOS4\n","        )\n","\n","    hsv_image = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)\n","\n","    hsv_lower = np.array([0, 5, 5], dtype=np.uint8)\n","    hsv_upper = np.array([180, 255, 250], dtype=np.uint8)\n","    mask = cv2.inRange(hsv_image, hsv_lower, hsv_upper)\n","\n","    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n","    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n","    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n","\n","    enhanced_channels = []\n","    for channel in range(3):\n","        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n","        enhanced = clahe.apply(img_array[:, :, channel].astype(np.uint8))\n","        enhanced_channels.append(enhanced)\n","\n","    enhanced_img = np.stack(enhanced_channels, axis=2)\n","    enhanced_img[mask == 0] = [240, 240, 240]\n","\n","    return Image.fromarray(enhanced_img.astype(np.uint8))\n","\n","\n","def prepare_model_input(\n","    image: Image.Image,\n","    model_name: str\n",") -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n","    \"\"\"Prepare model input tensors\"\"\"\n","    if image.mode != 'RGB':\n","        image = image.convert('RGB')\n","\n","    processed_image = apply_standard_preprocessing(image)\n","\n","    model_info = HER2_MODEL_REGISTRY[model_name]\n","    requires_dual = model_info['requires_dual_input']\n","\n","    if requires_dual:\n","        standard_tensor = standard_transform(processed_image).unsqueeze(0).to(device)\n","        vit_inputs = vit_processor(processed_image, return_tensors=\"pt\")\n","        vit_tensor = vit_inputs['pixel_values'].to(device)\n","        return standard_tensor, vit_tensor\n","    else:\n","        if model_name == 'ViT':\n","            vit_inputs = vit_processor(processed_image, return_tensors=\"pt\")\n","            vit_tensor = vit_inputs['pixel_values'].to(device)\n","            return vit_tensor, None\n","        else:\n","            standard_tensor = standard_transform(processed_image).unsqueeze(0).to(device)\n","            return standard_tensor, None\n","\n","\n","print(\"✓ Preprocessing functions defined\")\n","\n","# ============================================================================\n","# SECTION 6: MODEL LOADING & INFERENCE\n","# ============================================================================\n","\n","print(\"\\n[5/6] Defining model loading functions...\")\n","\n","def load_her2_model_checkpoint(\n","    model_name: str,\n","    task: str,\n","    num_classes: int,\n","    preprocessing_variant: str = 'orig'\n",") -> Tuple[nn.Module, Dict[str, Any]]:\n","    \"\"\"Load HER2 model checkpoint with variant support\"\"\"\n","\n","    if preprocessing_variant not in ['orig', 'prep']:\n","        raise ValueError(f\"Invalid preprocessing_variant: {preprocessing_variant}\")\n","\n","    checkpoint_key = f\"{model_name}_{task}_{preprocessing_variant}\"\n","\n","    if checkpoint_key not in HER2_CHECKPOINT_PATHS:\n","        raise KeyError(f\"Checkpoint not found: {checkpoint_key}\")\n","\n","    checkpoint_path = HER2_CHECKPOINT_PATHS[checkpoint_key]\n","\n","    if not os.path.exists(checkpoint_path):\n","        raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n","\n","    print(f\"Loading {model_name} for {task} task ({preprocessing_variant})...\")\n","\n","    checkpoint = None\n","    try:\n","        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","        loading_method = \"standard\"\n","    except Exception:\n","        try:\n","            with open(checkpoint_path, 'rb') as f:\n","                checkpoint = pickle.load(f)\n","            loading_method = \"pickle\"\n","        except Exception:\n","            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n","            loading_method = \"weights_only_false\"\n","\n","    architecture_class = HER2_MODEL_REGISTRY[model_name]['architecture_class']\n","    model = architecture_class(num_classes=num_classes, dropout_rate=0.2).to(device)\n","\n","    state_dict = checkpoint.get('model_state_dict', checkpoint)\n","\n","    try:\n","        model.load_state_dict(state_dict, strict=True)\n","        load_status = \"strict\"\n","    except Exception:\n","        model.load_state_dict(state_dict, strict=False)\n","        load_status = \"non-strict\"\n","\n","    model.eval()\n","\n","    training_info = {\n","        'best_val_f1': float(checkpoint.get('best_f1', 0.0)),\n","        'best_epoch': int(checkpoint.get('epoch', 0)) + 1,\n","        'checkpoint_file': os.path.basename(checkpoint_path),\n","        'loading_method': loading_method,\n","        'load_status': load_status,\n","        'model_type': model_name,\n","        'task': task,\n","        'num_classes': num_classes,\n","        'preprocessing_variant': preprocessing_variant\n","    }\n","\n","    print(f\"  ✓ Loaded ({load_status}), Val F1: {training_info['best_val_f1']:.4f}\")\n","\n","    return model, training_info\n","\n","\n","def run_her2_inference(\n","    model: nn.Module,\n","    model_name: str,\n","    input_tensor: torch.Tensor,\n","    vit_tensor: Optional[torch.Tensor] = None\n",") -> Tuple[torch.Tensor, torch.Tensor]:\n","    \"\"\"Run HER2 model inference\"\"\"\n","    model.eval()\n","\n","    with torch.no_grad():\n","        model_info = HER2_MODEL_REGISTRY[model_name]\n","\n","        if model_info['requires_dual_input']:\n","            if vit_tensor is None:\n","                raise ValueError(f\"{model_name} requires dual input\")\n","            outputs = model(input_tensor, vit_tensor)\n","        elif model_name == 'ViT':\n","            outputs = model(input_tensor)\n","        else:\n","            outputs = model(input_tensor)\n","\n","        if isinstance(outputs, torch.Tensor):\n","            logits = outputs\n","        elif isinstance(outputs, (tuple, list)):\n","            logits = outputs[0]\n","        else:\n","            logits = outputs\n","\n","        probabilities = torch.softmax(logits, dim=1)\n","        predictions = torch.argmax(probabilities, dim=1)\n","\n","    return predictions, probabilities\n","\n","\n","def format_her2_results(\n","    predictions: torch.Tensor,\n","    probabilities: torch.Tensor,\n","    task: str\n",") -> Dict[str, Any]:\n","    \"\"\"Format HER2 prediction results\"\"\"\n","    pred_class_idx = predictions.item()\n","    class_probs = probabilities[0].cpu().numpy()\n","\n","    task_config = HER2_TASK_CONFIGS[task]\n","    class_names = task_config['class_names']\n","\n","    results = {\n","        'predicted_class': class_names[pred_class_idx],\n","        'predicted_index': pred_class_idx,\n","        'confidence': float(class_probs[pred_class_idx]),\n","        'all_probabilities': {\n","            class_names[i]: float(class_probs[i])\n","            for i in range(len(class_names))\n","        },\n","        'task': task,\n","        'task_description': task_config['description']\n","    }\n","\n","    return results\n","\n","\n","# Model cache system\n","class HER2ModelCache:\n","    \"\"\"Cache for loaded HER2 models\"\"\"\n","\n","    def __init__(self):\n","        self.cache = {}\n","        self.stats = {'hits': 0, 'misses': 0}\n","\n","    def get(self, model_name, task, num_classes, preprocessing_variant='orig'):\n","        cache_key = f\"{model_name}_{task}_{num_classes}_{preprocessing_variant}\"\n","        if cache_key in self.cache:\n","            self.stats['hits'] += 1\n","            print(f\"  ✓ Cache hit: {cache_key}\")\n","            return self.cache[cache_key]\n","        else:\n","            self.stats['misses'] += 1\n","            return None\n","\n","    def set(self, model_name, task, model, info):\n","        num_classes = info.get('num_classes', 2)\n","        preprocessing_variant = info.get('preprocessing_variant', 'orig')\n","        cache_key = f\"{model_name}_{task}_{num_classes}_{preprocessing_variant}\"\n","        self.cache[cache_key] = (model, info)\n","        print(f\"  ✓ Cached: {cache_key}\")\n","\n","    def clear(self):\n","        self.cache.clear()\n","        self.stats = {'hits': 0, 'misses': 0}\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","\n","her2_model_cache = HER2ModelCache()\n","\n","print(\"✓ Model loading functions defined\")\n","\n","# ============================================================================\n","# COMPLETION\n","# ============================================================================\n","\n","print(\"\\n[6/6] Verification...\")\n","print(f\"✓ Checkpoints: {len(HER2_CHECKPOINT_PATHS)}\")\n","print(f\"✓ Architectures: {len(HER2_MODEL_REGISTRY)}\")\n","print(f\"✓ Tasks: {len(HER2_TASK_CONFIGS)}\")\n","print(f\"✓ Preprocessing: standard + medical\")\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 2 COMPLETE - HER2 MODEL SYSTEM READY\")\n","print(\"=\" * 80)\n","print(\"\\nNext: Run Cell 3 for interpretability functions\")\n","print(\"=\" * 80)"],"metadata":{"cellView":"form","collapsed":true,"id":"e5Ze93r6AIUr","colab":{"base_uri":"https://localhost:8080/","height":656,"referenced_widgets":["58b00db4f9aa47a7ada71bf44fa20203","7444319e3b5a446ebb183cc3f9602000","5f2a2a29dd3a4501b194fd68586efed7","8f2fdc59d6d24d63a56d7c7db1e1897f","0b5997e5aa8c497dae4472ae4dfc21a0","5f6a158d04954715814a83ebcc5052be","3fc2b7e4b4fa461983f1ecaf0c9880a5","63e083940c8a4519a4859c16f01d5751","eb79ff7ae25c4dc5a8e915c7bb6b813b","2be49976e3514ac5bb907249cbdb0095","1eecc9e887e6400cbe3cc240fbf7fbb7"]},"executionInfo":{"status":"ok","timestamp":1767432130988,"user_tz":-420,"elapsed":1051,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"1a832eb0-b6cc-4280-9845-9c7ea3a3c3d9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","HER2 PROJECT - MODEL LOADING & PREPROCESSING\n","================================================================================\n","\n","[1/6] Configuring HER2 model checkpoints...\n","✓ Configured 16 model checkpoints\n","  - Original preprocessing: 8 models\n","  - Medical preprocessing: 8 models\n","\n","[2/6] Setting up preprocessing pipelines...\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58b00db4f9aa47a7ada71bf44fa20203"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✓ Preprocessing pipelines configured\n","✓ Task configurations: ['IHC', 'HER2']\n","\n","[3/6] Defining model architectures...\n","✓ Defined 4 model architectures\n","\n","[4/6] Defining preprocessing functions...\n","✓ Preprocessing functions defined\n","\n","[5/6] Defining model loading functions...\n","✓ Model loading functions defined\n","\n","[6/6] Verification...\n","✓ Checkpoints: 16\n","✓ Architectures: 4\n","✓ Tasks: 2\n","✓ Preprocessing: standard + medical\n","\n","================================================================================\n","CELL 2 COMPLETE - HER2 MODEL SYSTEM READY\n","================================================================================\n","\n","Next: Run Cell 3 for interpretability functions\n","================================================================================\n"]}]},{"cell_type":"code","source":["# @title Cell 3: HER2 Model Interpretability Functions\n","\n","# File: MMLab_Project_Demo.ipynb - Cell 3\n","# Location: Thesis_MER_Project/MMLab_Project_Demo.ipynb\n","# Purpose: Attention visualization and interpretability for HER2 models\n","\n","import matplotlib.cm as cm\n","import torch.nn.functional as F\n","\n","print(\"=\" * 80)\n","print(\"HER2 PROJECT - MODEL INTERPRETABILITY\")\n","print(\"=\" * 80)\n","\n","# ============================================================================\n","# SECTION 1: GRAD-CAM EXTRACTOR (MobileNetV3)\n","# ============================================================================\n","\n","print(\"\\n[1/4] Defining Grad-CAM extractor...\")\n","\n","class GradCAMExtractor:\n","    \"\"\"Grad-CAM implementation for MobileNetV3 attention visualization\"\"\"\n","\n","    def __init__(self, model, device, target_layer_names=None, max_layers=3):\n","        self.model = model\n","        self.device = device if isinstance(device, torch.device) else torch.device(device)\n","        self.model.to(self.device).eval()\n","\n","        self.hooks = []\n","        self.activations = {}\n","        self.gradients = {}\n","\n","        if target_layer_names:\n","            self.target_layers = self._resolve_layer_names(target_layer_names)\n","        else:\n","            self.target_layers = self._auto_detect_layers(max_layers=max_layers)\n","\n","        if len(self.target_layers) == 0:\n","            raise RuntimeError(\"No suitable target layers found\")\n","\n","        print(f\"  ✓ Initialized with {len(self.target_layers)} target layers\")\n","\n","        for name, module in self.target_layers.items():\n","            h = module.register_forward_hook(self._make_forward_hook(name))\n","            self.hooks.append(h)\n","\n","    def _resolve_layer_names(self, names):\n","        found = {}\n","        for n in names:\n","            layer = self._get_layer_by_path(n)\n","            if layer is not None:\n","                found[n] = layer\n","        return found\n","\n","    def _get_layer_by_path(self, path):\n","        try:\n","            parts = path.split('.')\n","            cur = self.model\n","            for p in parts:\n","                if p.isdigit():\n","                    cur = cur[int(p)]\n","                else:\n","                    cur = getattr(cur, p)\n","            return cur\n","        except Exception:\n","            return None\n","\n","    def _auto_detect_layers(self, max_layers=3):\n","        candidates = [\n","            'mobilenet.blocks.4', 'mobilenet.blocks.5', 'mobilenet.blocks.6',\n","            'mobilenet.conv_head', 'mobilenet.conv_stem'\n","        ]\n","\n","        found = {}\n","        for name in candidates:\n","            layer = self._get_layer_by_path(name)\n","            if layer is not None:\n","                last_conv = self._find_last_conv_in_module(layer, prefix=name)\n","                if last_conv:\n","                    found[last_conv[0]] = last_conv[1]\n","                elif isinstance(layer, nn.Conv2d):\n","                    found[name] = layer\n","            if len(found) >= max_layers:\n","                break\n","\n","        if len(found) == 0:\n","            last_conv = self._find_last_conv_in_module(self.model, prefix='')\n","            if last_conv:\n","                found[last_conv[0]] = last_conv[1]\n","\n","        limited = {}\n","        for i, (k, v) in enumerate(found.items()):\n","            if i >= max_layers:\n","                break\n","            limited[k] = v\n","\n","        return limited\n","\n","    def _find_last_conv_in_module(self, module, prefix=''):\n","        last = None\n","        for name, child in module.named_modules():\n","            full_name = f\"{prefix}.{name}\" if prefix and name else (name or prefix)\n","            if isinstance(child, nn.Conv2d):\n","                last = (full_name.strip('.'), child)\n","        return last\n","\n","    def _make_forward_hook(self, layer_name):\n","        def hook(module, input, output):\n","            act = output\n","            self.activations[layer_name] = act\n","\n","            def _grad_hook(grad):\n","                self.gradients[layer_name] = grad\n","\n","            try:\n","                if torch.is_tensor(act):\n","                    act.register_hook(_grad_hook)\n","                elif isinstance(act, (tuple, list)) and len(act) > 0 and torch.is_tensor(act[0]):\n","                    act[0].register_hook(_grad_hook)\n","            except Exception:\n","                pass\n","\n","        return hook\n","\n","    def _clear(self):\n","        self.activations.clear()\n","        self.gradients.clear()\n","\n","    def generate_cam(self, input_tensor, target_class):\n","        self._clear()\n","\n","        input_tensor = input_tensor.to(self.device)\n","        self.model.zero_grad()\n","        output = self.model(input_tensor)\n","\n","        if isinstance(output, (tuple, list)):\n","            logits = output[0]\n","        else:\n","            logits = output\n","\n","        score = logits[0, int(target_class)]\n","        score.backward(retain_graph=False)\n","\n","        layer_cams = []\n","        layer_weights = []\n","\n","        for name in self.target_layers.keys():\n","            if name in self.activations and name in self.gradients:\n","                act = self.activations[name]\n","                grad = self.gradients[name]\n","\n","                if isinstance(act, (tuple, list)):\n","                    act = act[0]\n","                if isinstance(grad, (tuple, list)):\n","                    grad = grad[0]\n","\n","                activations = act[0]\n","                gradients = grad[0]\n","\n","                weights = torch.mean(gradients.view(gradients.size(0), -1), dim=1)\n","\n","                cam = torch.zeros(activations.shape[1:], dtype=torch.float32, device=activations.device)\n","                for c, w in enumerate(weights):\n","                    cam += w * activations[c]\n","\n","                cam = F.relu(cam)\n","                if cam.max() > 0:\n","                    cam = cam - cam.min()\n","                    cam = cam / (cam.max() + 1e-8)\n","\n","                layer_cams.append(cam.detach().cpu().numpy())\n","                layer_weights.append(torch.mean(torch.abs(gradients)).item())\n","\n","        if len(layer_cams) == 0:\n","            raise RuntimeError(\"No CAMs generated\")\n","\n","        if len(layer_cams) == 1:\n","            final_cam = layer_cams[0]\n","        else:\n","            weights = np.array(layer_weights)\n","            if weights.sum() == 0:\n","                weights = np.ones_like(weights)\n","            weights = weights / weights.sum()\n","\n","            target_shape = max(cam.shape for cam in layer_cams)\n","            resized_cams = []\n","            for cam in layer_cams:\n","                if cam.shape != target_shape:\n","                    cam_resized = cv2.resize(cam, (target_shape[1], target_shape[0]),\n","                                           interpolation=cv2.INTER_LINEAR)\n","                    resized_cams.append(cam_resized)\n","                else:\n","                    resized_cams.append(cam)\n","\n","            final_cam = np.zeros_like(resized_cams[0])\n","            for cam, w in zip(resized_cams, weights):\n","                final_cam += w * cam\n","\n","        final_cam = np.clip(final_cam, 0, 1)\n","        if final_cam.max() > 0:\n","            final_cam = final_cam / final_cam.max()\n","\n","        final_cam = cv2.GaussianBlur(final_cam, (3, 3), 0)\n","\n","        if final_cam.max() > 0:\n","            final_cam = final_cam / final_cam.max()\n","\n","        return final_cam\n","\n","    def release(self):\n","        for h in self.hooks:\n","            try:\n","                h.remove()\n","            except Exception:\n","                pass\n","        self.hooks = []\n","\n","\n","print(\"✓ Grad-CAM extractor defined\")\n","\n","# ============================================================================\n","# SECTION 2: VIT ATTENTION EXTRACTOR\n","# ============================================================================\n","\n","print(\"\\n[2/4] Defining ViT attention extractor...\")\n","\n","class ViTAttentionExtractor:\n","    \"\"\"Native ViT attention extraction and visualization\"\"\"\n","\n","    def __init__(self, model, device, patch_size=32, image_size=1024):\n","        self.model = model\n","        self.device = device\n","        self.patch_size = patch_size\n","        self.image_size = image_size\n","        self.num_patches_per_side = image_size // patch_size\n","        self.num_patches = self.num_patches_per_side ** 2\n","\n","        self.model.eval()\n","        print(f\"  ✓ Initialized for {self.num_patches_per_side}x{self.num_patches_per_side} patches\")\n","\n","    def extract_attention(self, pixel_values):\n","        pixel_values = pixel_values.to(self.device)\n","\n","        with torch.no_grad():\n","            try:\n","                if hasattr(self.model, 'vit'):\n","                    outputs = self.model.vit(\n","                        pixel_values=pixel_values,\n","                        interpolate_pos_encoding=True,\n","                        output_attentions=True,\n","                        return_dict=True\n","                    )\n","                    attentions = outputs.attentions\n","                else:\n","                    output, attentions = self.model(pixel_values, output_attentions=True)\n","\n","                if attentions is None:\n","                    raise RuntimeError(\"Model did not return attention weights\")\n","\n","                return attentions\n","\n","            except Exception as e:\n","                raise RuntimeError(f\"Attention extraction failed: {e}\")\n","\n","    def compute_attention_map(self, attentions):\n","        if attentions is None:\n","            return None\n","\n","        num_layers = min(4, len(attentions))\n","        selected_layers = attentions[-num_layers:]\n","\n","        layer_attentions = []\n","        layer_weights = [0.4, 0.3, 0.2, 0.1][:num_layers]\n","\n","        for layer_att in selected_layers:\n","            layer_att = layer_att[0]\n","\n","            head_vars = torch.var(layer_att.view(layer_att.size(0), -1), dim=1)\n","            num_keep = max(1, layer_att.size(0) // 4)\n","            top_heads = torch.argsort(head_vars, descending=True)[:num_keep]\n","\n","            selected_att = layer_att[top_heads].mean(dim=0)\n","            cls_att = selected_att[0, 1:]\n","            layer_attentions.append(cls_att)\n","\n","        weights = np.array(layer_weights[:len(layer_attentions)])\n","        weights = weights / weights.sum()\n","\n","        combined_att = torch.zeros_like(layer_attentions[0])\n","        for att, w in zip(layer_attentions, weights):\n","            if not torch.is_tensor(att):\n","                att = torch.tensor(att, device=combined_att.device, dtype=combined_att.dtype)\n","            combined_att += float(w) * att\n","\n","        attention_np = combined_att.detach().cpu().numpy()\n","\n","        threshold = np.percentile(attention_np, 15)\n","        attention_np = np.where(attention_np < threshold, 0, attention_np)\n","\n","        if attention_np.max() > 0:\n","            attention_np = attention_np / attention_np.max()\n","\n","        return attention_np\n","\n","    def apply_tissue_filtering(self, attention_weights, original_image_np):\n","        tissue_mask = self.create_tissue_mask(original_image_np).astype(np.uint8)\n","        tissue_bool = (tissue_mask > 0).astype(bool)\n","\n","        h, w = tissue_bool.shape\n","        patch_h = max(1, h // self.num_patches_per_side)\n","        patch_w = max(1, w // self.num_patches_per_side)\n","\n","        patch_tissue = np.zeros((self.num_patches_per_side, self.num_patches_per_side), dtype=float)\n","\n","        for r in range(self.num_patches_per_side):\n","            for c in range(self.num_patches_per_side):\n","                y0, y1 = r * patch_h, min(h, (r + 1) * patch_h)\n","                x0, x1 = c * patch_w, min(w, (c + 1) * patch_w)\n","                region = tissue_bool[y0:y1, x0:x1]\n","                if region.size > 0:\n","                    patch_tissue[r, c] = region.mean()\n","\n","        patch_tissue_flat = patch_tissue.ravel()\n","\n","        tissue_threshold = 0.10\n","        attention_filtered = attention_weights.copy()\n","        attention_filtered[patch_tissue_flat < tissue_threshold] = 0.0\n","\n","        if attention_filtered.sum() > 0:\n","            attention_filtered = attention_filtered / attention_filtered.sum()\n","        else:\n","            attention_filtered = attention_weights\n","\n","        return attention_filtered\n","\n","    def attention_to_heatmap(self, attention_weights, original_image_np):\n","        if attention_weights is None:\n","            return None\n","\n","        attention_filtered = self.apply_tissue_filtering(attention_weights, original_image_np)\n","\n","        attention_2d = attention_filtered.reshape(\n","            self.num_patches_per_side,\n","            self.num_patches_per_side\n","        )\n","\n","        padded = np.pad(attention_2d, pad_width=2, mode='edge')\n","        heatmap_large = cv2.resize(\n","            padded,\n","            (self.image_size + 64, self.image_size + 64),\n","            interpolation=cv2.INTER_CUBIC\n","        )\n","\n","        heatmap = heatmap_large[32:-32, 32:-32]\n","\n","        if heatmap.shape != (self.image_size, self.image_size):\n","            heatmap = cv2.resize(heatmap, (self.image_size, self.image_size))\n","\n","        if heatmap.max() > heatmap.min():\n","            heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n","        else:\n","            heatmap = np.ones_like(heatmap) * 0.5\n","\n","        heatmap = cv2.GaussianBlur(heatmap, (3, 3), 0.5)\n","\n","        if heatmap.max() > 0:\n","            heatmap = heatmap / heatmap.max()\n","\n","        return heatmap\n","\n","    def create_tissue_mask(self, image_rgb):\n","        hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)\n","\n","        saturation_thresh = 15\n","        value_thresh = 35\n","\n","        mask = (hsv[:,:,1] > saturation_thresh) & (hsv[:,:,2] > value_thresh)\n","        mask = mask.astype(np.uint8) * 255\n","\n","        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n","        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n","        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n","        mask = cv2.GaussianBlur(mask, (5, 5), 0)\n","\n","        return mask\n","\n","\n","print(\"✓ ViT attention extractor defined\")\n","\n","# ============================================================================\n","# SECTION 3: FUSION ATTENTION EXTRACTOR\n","# ============================================================================\n","\n","print(\"\\n[3/4] Defining Fusion attention extractor...\")\n","\n","class FusionAttentionExtractor:\n","    \"\"\"Dual-branch attention extraction for Fusion models\"\"\"\n","\n","    def __init__(self, model, device, fusion_type='concat', patch_size=32, image_size=1024):\n","        self.model = model\n","        self.device = device\n","        self.fusion_type = fusion_type\n","        self.patch_size = patch_size\n","        self.image_size = image_size\n","        self.num_patches_per_side = image_size // patch_size\n","        self.num_patches = self.num_patches_per_side ** 2\n","\n","        self.gradients = {}\n","        self.activations = {}\n","        self.hooks = []\n","\n","        self.model.eval()\n","        print(f\"  ✓ Initialized for {fusion_type} fusion\")\n","\n","    def _register_hooks(self):\n","        def make_hook(name):\n","            def hook(module, input, output):\n","                self.activations[name] = output\n","                def grad_hook(grad):\n","                    self.gradients[name] = grad\n","                if torch.is_tensor(output):\n","                    output.register_hook(grad_hook)\n","                elif isinstance(output, (tuple, list)) and len(output) > 0:\n","                    if torch.is_tensor(output[0]):\n","                        output[0].register_hook(grad_hook)\n","            return hook\n","\n","        h = self.model.mobilenet_projector.register_forward_hook(make_hook('mobilenet_projected'))\n","        self.hooks.append(h)\n","\n","    def _clear_hooks(self):\n","        for h in self.hooks:\n","            try:\n","                h.remove()\n","            except:\n","                pass\n","        self.hooks = []\n","        self.gradients.clear()\n","        self.activations.clear()\n","\n","    def create_tissue_mask(self, image_rgb):\n","        hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)\n","\n","        saturation_thresh = 15\n","        value_thresh = 35\n","\n","        mask = (hsv[:,:,1] > saturation_thresh) & (hsv[:,:,2] > value_thresh)\n","        mask = mask.astype(np.uint8) * 255\n","\n","        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n","        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n","        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n","        mask = cv2.GaussianBlur(mask, (5, 5), 0)\n","\n","        return mask\n","\n","    def extract_mobilenet_attention(self, images, target_class, extracted_features):\n","        self._register_hooks()\n","\n","        try:\n","            self.model.zero_grad()\n","            mobilenet_weighted = extracted_features['mobilenet_weighted']\n","\n","            dummy_scores = torch.sum(mobilenet_weighted, dim=1)\n","            target_score = dummy_scores[0]\n","            target_score.backward(retain_graph=True)\n","\n","            if 'mobilenet_projected' in self.gradients:\n","                gradients = self.gradients['mobilenet_projected'][0]\n","                activations = self.activations['mobilenet_projected'][0]\n","\n","                weights = torch.abs(gradients)\n","                weighted_features = activations * weights\n","\n","                spatial_size = 32\n","                if weighted_features.shape[0] >= spatial_size:\n","                    spatial_map = weighted_features[:spatial_size].view(int(np.sqrt(spatial_size)), int(np.sqrt(spatial_size)))\n","                else:\n","                    padded = F.pad(weighted_features, (0, spatial_size - weighted_features.shape[0]))\n","                    spatial_map = padded.view(int(np.sqrt(spatial_size)), int(np.sqrt(spatial_size)))\n","\n","                spatial_np = spatial_map.detach().cpu().numpy()\n","                spatial_np = spatial_np - spatial_np.min()\n","                if spatial_np.max() > 0:\n","                    spatial_np = spatial_np / spatial_np.max()\n","\n","                return spatial_np\n","            else:\n","                mobilenet_weighted_np = mobilenet_weighted[0].detach().cpu().numpy()\n","                spatial_size = int(np.sqrt(min(512, 256)))\n","                if mobilenet_weighted_np.shape[0] >= spatial_size * spatial_size:\n","                    spatial_map = mobilenet_weighted_np[:spatial_size*spatial_size].reshape(spatial_size, spatial_size)\n","                else:\n","                    needed = spatial_size * spatial_size\n","                    padded = np.pad(mobilenet_weighted_np, (0, max(0, needed - len(mobilenet_weighted_np))))\n","                    spatial_map = padded[:needed].reshape(spatial_size, spatial_size)\n","\n","                spatial_map = spatial_map - spatial_map.min()\n","                if spatial_map.max() > 0:\n","                    spatial_map = spatial_map / spatial_map.max()\n","\n","                return spatial_map\n","\n","        except Exception as e:\n","            print(f\"  Warning: MobileNet attention extraction failed: {e}\")\n","            return np.ones((16, 16)) * 0.5\n","\n","        finally:\n","            self._clear_hooks()\n","\n","    def extract_vit_attention(self, extracted_features, original_image_np):\n","        try:\n","            vit_attentions = extracted_features['vit_attentions']\n","\n","            if vit_attentions is None or len(vit_attentions) == 0:\n","                return None\n","\n","            num_layers_to_use = min(4, len(vit_attentions))\n","            selected_layers = vit_attentions[-num_layers_to_use:]\n","\n","            layer_attentions = []\n","            layer_weights = [0.4, 0.3, 0.2, 0.1]\n","\n","            for layer_attention in selected_layers:\n","                layer_att = layer_attention[0]\n","\n","                head_variances = torch.var(layer_att.view(layer_att.size(0), -1), dim=1)\n","                num_keep = max(1, layer_att.size(0) // 4)\n","                top_heads = torch.argsort(head_variances, descending=True)[:num_keep]\n","\n","                selected_heads_att = layer_att[top_heads].mean(dim=0)\n","                cls_att = selected_heads_att[0, 1:]\n","                layer_attentions.append(cls_att)\n","\n","            if len(layer_attentions) == 1:\n","                combined_attention = layer_attentions[0]\n","            else:\n","                weights = layer_weights[:len(layer_attentions)]\n","                weights = np.array(weights, dtype=float)\n","                weights = weights / weights.sum()\n","\n","                combined_attention = torch.zeros_like(layer_attentions[0])\n","                for att, weight in zip(layer_attentions, weights):\n","                    if not torch.is_tensor(att):\n","                        att = torch.tensor(att, device=combined_attention.device, dtype=combined_attention.dtype)\n","                    combined_attention = combined_attention + float(weight) * att\n","\n","            attention_np = combined_attention.detach().cpu().numpy()\n","\n","            discard_ratio = 0.15\n","            p_discard = np.percentile(attention_np, discard_ratio * 100)\n","            attention_filtered = np.where(attention_np < p_discard, 0, attention_np)\n","\n","            if attention_filtered.max() > 0:\n","                attention_filtered = attention_filtered / attention_filtered.max()\n","\n","            attention_tissue_filtered = self.apply_tissue_filtering_vit(attention_filtered, original_image_np)\n","\n","            return attention_tissue_filtered\n","\n","        except Exception as e:\n","            print(f\"  Warning: ViT attention extraction failed: {e}\")\n","            return None\n","\n","    def apply_tissue_filtering_vit(self, attention_weights, original_image_np):\n","        try:\n","            tissue_mask = self.create_tissue_mask(original_image_np).astype(np.uint8)\n","            tissue_bool = (tissue_mask > 0).astype(bool)\n","\n","            h, w = tissue_bool.shape\n","            patch_h, patch_w = max(1, h // self.num_patches_per_side), max(1, w // self.num_patches_per_side)\n","\n","            patch_tissue = np.zeros((self.num_patches_per_side, self.num_patches_per_side), dtype=float)\n","\n","            for r in range(self.num_patches_per_side):\n","                for c in range(self.num_patches_per_side):\n","                    y0, y1 = r * patch_h, min(h, (r + 1) * patch_h)\n","                    x0, x1 = c * patch_w, min(w, (c + 1) * patch_w)\n","                    region = tissue_bool[y0:y1, x0:x1]\n","                    if region.size > 0:\n","                        patch_tissue[r, c] = region.mean()\n","\n","            patch_tissue_flat = patch_tissue.ravel()\n","\n","            tissue_threshold = 0.10\n","            attention_filtered = attention_weights.copy()\n","            attention_filtered[patch_tissue_flat < tissue_threshold] = 0.0\n","\n","            if attention_filtered.sum() > 0:\n","                attention_filtered = attention_filtered / attention_filtered.sum()\n","            else:\n","                attention_filtered = attention_weights\n","\n","            return attention_filtered\n","\n","        except Exception:\n","            return attention_weights\n","\n","    def attention_to_heatmap(self, attention_weights, target_size=(1024, 1024)):\n","        if attention_weights is None:\n","            return None\n","\n","        try:\n","            if len(attention_weights.shape) == 1:\n","                if len(attention_weights) == self.num_patches:\n","                    attention_2d = attention_weights.reshape(self.num_patches_per_side, self.num_patches_per_side)\n","                else:\n","                    side = int(np.sqrt(len(attention_weights)))\n","                    if side * side == len(attention_weights):\n","                        attention_2d = attention_weights.reshape(side, side)\n","                    else:\n","                        needed = side * side if side > 0 else 16 * 16\n","                        padded = np.pad(attention_weights, (0, max(0, needed - len(attention_weights))))\n","                        side = int(np.sqrt(needed))\n","                        attention_2d = padded[:needed].reshape(side, side)\n","            else:\n","                attention_2d = attention_weights\n","\n","            padded_attention = np.pad(attention_2d, pad_width=2, mode='edge')\n","            heatmap_large = cv2.resize(\n","                padded_attention,\n","                (target_size[0] + 64, target_size[1] + 64),\n","                interpolation=cv2.INTER_CUBIC\n","            )\n","\n","            crop_size = 32\n","            heatmap = heatmap_large[crop_size:-crop_size, crop_size:-crop_size]\n","\n","            if heatmap.shape != target_size:\n","                heatmap = cv2.resize(heatmap, target_size, interpolation=cv2.INTER_CUBIC)\n","\n","            heatmap_min, heatmap_max = heatmap.min(), heatmap.max()\n","            if heatmap_max > heatmap_min:\n","                heatmap = (heatmap - heatmap_min) / (heatmap_max - heatmap_min)\n","            else:\n","                heatmap = np.ones_like(heatmap) * 0.5\n","\n","            heatmap = cv2.GaussianBlur(heatmap, (3, 3), 0.5)\n","\n","            if heatmap.max() > 0:\n","                heatmap = heatmap / heatmap.max()\n","\n","            return heatmap\n","\n","        except Exception as e:\n","            print(f\"  Warning: Heatmap conversion failed: {e}\")\n","            return None\n","\n","    def generate_fusion_attention(self, images, pixel_values, target_class, original_image):\n","        images = images.to(self.device)\n","        pixel_values = pixel_values.to(self.device)\n","        original_image_np = np.array(original_image)\n","\n","        try:\n","            output, extracted_features = self.model(images, pixel_values, return_features=True)\n","\n","            mobilenet_attention = self.extract_mobilenet_attention(images, target_class, extracted_features)\n","            vit_attention = self.extract_vit_attention(extracted_features, original_image_np)\n","\n","            mobilenet_heatmap = self.attention_to_heatmap(mobilenet_attention)\n","            vit_heatmap = self.attention_to_heatmap(vit_attention) if vit_attention is not None else None\n","\n","            fusion_heatmap = None\n","            if mobilenet_heatmap is not None and vit_heatmap is not None:\n","                fusion_heatmap = 0.5 * mobilenet_heatmap + 0.5 * vit_heatmap\n","\n","                if fusion_heatmap.max() > 0:\n","                    fusion_heatmap = fusion_heatmap / fusion_heatmap.max()\n","            elif mobilenet_heatmap is not None:\n","                fusion_heatmap = mobilenet_heatmap\n","            elif vit_heatmap is not None:\n","                fusion_heatmap = vit_heatmap\n","\n","            return {\n","                'mobilenet_heatmap': mobilenet_heatmap,\n","                'vit_heatmap': vit_heatmap,\n","                'fusion_heatmap': fusion_heatmap\n","            }\n","\n","        except Exception as e:\n","            print(f\"  Warning: Fusion attention generation failed: {e}\")\n","            return {\n","                'mobilenet_heatmap': None,\n","                'vit_heatmap': None,\n","                'fusion_heatmap': None\n","            }\n","\n","\n","print(\"✓ Fusion attention extractor defined\")\n","\n","# ============================================================================\n","# SECTION 4: OVERLAY & INTERPRETATION FUNCTIONS\n","# ============================================================================\n","\n","print(\"\\n[4/4] Defining visualization functions...\")\n","\n","def create_medical_overlay(\n","    original_image,\n","    heatmap,\n","    alpha: float = 0.4,\n","    colormap: str = 'magma',\n","    apply_tissue_mask: bool = True\n",") -> Image.Image:\n","    \"\"\"Create medical overlay with robust size/channel handling\"\"\"\n","\n","    # Convert original to RGB uint8\n","    if isinstance(original_image, Image.Image):\n","        original_np = np.array(original_image.convert('RGB'))\n","    else:\n","        original_np = np.array(original_image)\n","        if original_np.ndim == 2:\n","            original_np = cv2.cvtColor(original_np, cv2.COLOR_GRAY2RGB)\n","        elif original_np.shape[2] == 4:\n","            original_np = original_np[:, :, :3]\n","\n","    if original_np.dtype != np.uint8:\n","        if original_np.max() <= 1.0:\n","            original_np = (original_np * 255).astype(np.uint8)\n","        else:\n","            original_np = original_np.astype(np.uint8)\n","\n","    H, W = original_np.shape[:2]\n","\n","    if heatmap is None:\n","        return Image.fromarray(original_np)\n","\n","    if isinstance(heatmap, Image.Image):\n","        heat_np = np.array(heatmap)\n","    else:\n","        heat_np = np.array(heatmap)\n","\n","    if heat_np.ndim == 3:\n","        if heat_np.shape[2] == 4:\n","            heat_rgb = heat_np[:, :, :3]\n","        elif heat_np.shape[2] == 3:\n","            heat_rgb = heat_np\n","        else:\n","            heat_rgb = heat_np[:, :, :3]\n","\n","        if np.issubdtype(heat_rgb.dtype, np.floating):\n","            heat_rgb = np.clip(heat_rgb, 0.0, 1.0)\n","            heat_rgb = (heat_rgb * 255).astype(np.uint8)\n","        else:\n","            heat_rgb = heat_rgb.astype(np.uint8)\n","\n","        if heat_rgb.shape[:2] != (H, W):\n","            heatmap_rgb = cv2.resize(heat_rgb, (W, H), interpolation=cv2.INTER_CUBIC)\n","        else:\n","            heatmap_rgb = heat_rgb\n","\n","    elif heat_np.ndim == 2:\n","        heat_single = heat_np.astype(np.float32)\n","\n","        if heat_single.max() > 1.1:\n","            heat_single = heat_single / 255.0\n","        heat_single = np.clip(heat_single, 0.0, 1.0)\n","\n","        if heat_single.shape != (H, W):\n","            heat_single = cv2.resize(heat_single, (W, H), interpolation=cv2.INTER_CUBIC)\n","\n","        if colormap == 'magma':\n","            cmap = cm.magma\n","        elif colormap == 'inferno':\n","            cmap = cm.inferno\n","        elif colormap == 'viridis':\n","            cmap = cm.viridis\n","        else:\n","            cmap = cm.jet\n","\n","        heat_rgba = cmap(heat_single)\n","        heat_rgbf = heat_rgba[:, :, :3]\n","        heatmap_rgb = (heat_rgbf * 255).astype(np.uint8)\n","\n","    else:\n","        return Image.fromarray(original_np)\n","\n","    if heatmap_rgb.shape[:2] != (H, W):\n","        heatmap_rgb = cv2.resize(heatmap_rgb, (W, H), interpolation=cv2.INTER_CUBIC)\n","\n","    if heatmap_rgb.shape[2] != 3:\n","        heatmap_rgb = heatmap_rgb[:, :, :3]\n","\n","    if heatmap_rgb.dtype != np.uint8:\n","        heatmap_rgb = heatmap_rgb.astype(np.uint8)\n","\n","    try:\n","        overlaid = cv2.addWeighted(original_np, 1 - alpha, heatmap_rgb, alpha, 0)\n","    except Exception as e:\n","        print(f\"  Warning: Overlay blend failed: {e}\")\n","        return Image.fromarray(original_np)\n","\n","    return Image.fromarray(overlaid)\n","\n","\n","def generate_her2_interpretation(\n","    model,\n","    model_name: str,\n","    input_tensor: torch.Tensor,\n","    vit_tensor: Optional[torch.Tensor],\n","    target_class: int,\n","    original_image: Image.Image,\n","    device: torch.device\n",") -> Dict[str, Optional[Image.Image]]:\n","    \"\"\"Generate HER2 model interpretability visualizations\"\"\"\n","    results = {}\n","\n","    try:\n","        if model_name == 'MobileNetV3':\n","            gradcam = GradCAMExtractor(model, device)\n","            cam = gradcam.generate_cam(input_tensor, target_class)\n","\n","            overlay = create_medical_overlay(original_image, cam, alpha=0.35, colormap='magma')\n","            results['attention'] = overlay\n","\n","            gradcam.release()\n","\n","        elif model_name == 'ViT':\n","            vit_extractor = ViTAttentionExtractor(model, device)\n","\n","            attentions = vit_extractor.extract_attention(input_tensor)\n","            attention_map = vit_extractor.compute_attention_map(attentions)\n","\n","            original_np = np.array(original_image)\n","            heatmap = vit_extractor.attention_to_heatmap(attention_map, original_np)\n","\n","            if heatmap is not None:\n","                overlay = create_medical_overlay(original_image, heatmap, alpha=0.4, colormap='magma')\n","                results['attention'] = overlay\n","\n","        elif model_name in ['FusionConcat', 'FusionAddition']:\n","            fusion_type = 'concat' if model_name == 'FusionConcat' else 'addition'\n","            fusion_extractor = FusionAttentionExtractor(model, device, fusion_type=fusion_type)\n","\n","            fusion_results = fusion_extractor.generate_fusion_attention(\n","                input_tensor, vit_tensor, target_class, original_image\n","            )\n","\n","            if fusion_results['mobilenet_heatmap'] is not None:\n","                results['mobilenet_attention'] = create_medical_overlay(\n","                    original_image,\n","                    fusion_results['mobilenet_heatmap'],\n","                    alpha=0.35,\n","                    colormap='magma'\n","                )\n","\n","            if fusion_results['vit_heatmap'] is not None:\n","                results['vit_attention'] = create_medical_overlay(\n","                    original_image,\n","                    fusion_results['vit_heatmap'],\n","                    alpha=0.35,\n","                    colormap='magma'\n","                )\n","\n","            if fusion_results['fusion_heatmap'] is not None:\n","                results['fusion_attention'] = create_medical_overlay(\n","                    original_image,\n","                    fusion_results['fusion_heatmap'],\n","                    alpha=0.35,\n","                    colormap='magma'\n","                )\n","\n","    except Exception as e:\n","        print(f\"  Warning: Interpretation generation failed: {e}\")\n","        results['error'] = str(e)\n","\n","    return results\n","\n","\n","print(\"✓ Visualization functions defined\")\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 3 COMPLETE - HER2 INTERPRETABILITY READY\")\n","print(\"=\" * 80)\n","print(\"\\nNext: Run Cell 4 for interactive interface\")\n","print(\"=\" * 80)"],"metadata":{"cellView":"form","collapsed":true,"id":"VP5aVREBBNpP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767432131714,"user_tz":-420,"elapsed":724,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"2915d140-3698-45fa-afae-80cf6b9afdbb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","HER2 PROJECT - MODEL INTERPRETABILITY\n","================================================================================\n","\n","[1/4] Defining Grad-CAM extractor...\n","✓ Grad-CAM extractor defined\n","\n","[2/4] Defining ViT attention extractor...\n","✓ ViT attention extractor defined\n","\n","[3/4] Defining Fusion attention extractor...\n","✓ Fusion attention extractor defined\n","\n","[4/4] Defining visualization functions...\n","✓ Visualization functions defined\n","\n","================================================================================\n","CELL 3 COMPLETE - HER2 INTERPRETABILITY READY\n","================================================================================\n","\n","Next: Run Cell 4 for interactive interface\n","================================================================================\n"]}]},{"cell_type":"code","source":["# @title Cell 4: HER2 Prediction Pipeline\n","\n","# File: MMLab_Project_Demo.ipynb - Cell 4\n","# Location: Thesis_MER_Project/MMLab_Project_Demo.ipynb\n","# Purpose: HER2 prediction pipeline with flexible model name handling\n","\n","import time\n","\n","print(\"=\" * 80)\n","print(\"HER2 PROJECT - PREDICTION PIPELINE (FIXED)\")\n","print(\"=\" * 80)\n","\n","# ============================================================================\n","# SECTION 1: HER2 PREDICTION PIPELINE (FIXED MODEL HANDLING)\n","# ============================================================================\n","\n","print(\"\\n[1/2] Defining HER2 prediction pipeline...\")\n","\n","def her2_predict_pipeline(\n","    image,\n","    preprocessing_choice,\n","    model_choice,\n","    task_choice\n","):\n","    \"\"\"\n","    Complete HER2 prediction pipeline with flexible model name handling\n","\n","    Accepts both display names and internal keys for backward compatibility\n","    \"\"\"\n","    try:\n","        # Step 1: Image validation\n","        if image is None:\n","            return (\n","                None,\n","                \"### Error\\n\\nPlease upload an image first.\",\n","                {},\n","                None, None, None,\n","                gr.update(visible=False),\n","                \"\", \"\", \"\"\n","            )\n","\n","        if isinstance(image, np.ndarray):\n","            image = Image.fromarray(image)\n","\n","        if image.mode != 'RGB':\n","            image = image.convert('RGB')\n","\n","        # Step 2: Model name handling (FIXED)\n","        # Check if model_choice is already an internal key\n","        if model_choice in HER2_MODEL_REGISTRY:\n","            model_name = model_choice\n","            print(f\"Using internal key: {model_name}\")\n","        else:\n","            # Try mapping from display name\n","            model_map = {\n","                'MobileNetV3-Large': 'MobileNetV3',\n","                'Vision Transformer (ViT)': 'ViT',\n","                'Fusion (Concatenation)': 'FusionConcat',\n","                'Fusion (Addition)': 'FusionAddition'\n","            }\n","\n","            if model_choice in model_map:\n","                model_name = model_map[model_choice]\n","                print(f\"Mapped display name: {model_choice} -> {model_name}\")\n","            else:\n","                raise ValueError(f\"Unknown model: {model_choice}\")\n","\n","        # Step 3: Task handling (FIXED)\n","        if task_choice in HER2_TASK_CONFIGS:\n","            task = task_choice\n","        else:\n","            task_map = {\n","                'IHC Score (0-3)': 'IHC',\n","                'IHC Intensity': 'IHC',\n","                'HER2 Status (Neg/Pos)': 'HER2',\n","                'HER2 Status': 'HER2'\n","            }\n","            task = task_map.get(task_choice, task_choice)\n","\n","        # Step 4: Preprocessing handling (FIXED)\n","        if preprocessing_choice in ['Standard Preprocessing', 'Medical Preprocessing']:\n","            preprocessing_type = 'medical' if 'Medical' in preprocessing_choice else 'standard'\n","            preprocessing_variant = 'prep' if 'Medical' in preprocessing_choice else 'orig'\n","        else:\n","            # Handle Cell 8 format\n","            preprocessing_type = 'medical' if 'Medical' in preprocessing_choice or 'Optimized' in preprocessing_choice else 'standard'\n","            preprocessing_variant = 'prep' if preprocessing_type == 'medical' else 'orig'\n","\n","        # Step 5: Task configuration\n","        task_config = HER2_TASK_CONFIGS[task]\n","        num_classes = task_config['num_classes']\n","\n","        # Step 6: Get model info\n","        model_info = HER2_MODEL_REGISTRY[model_name]\n","        is_fusion = model_info['requires_dual_input']\n","\n","        # Step 7: Create preprocessing comparison\n","        if preprocessing_type == 'medical':\n","            medical_processed = apply_medical_preprocessing(image)\n","            standard_resized = apply_standard_preprocessing(image)\n","\n","            comparison = Image.new('RGB', (TARGET_SIZE * 2, TARGET_SIZE))\n","            comparison.paste(standard_resized, (0, 0))\n","            comparison.paste(medical_processed, (TARGET_SIZE, 0))\n","\n","            preprocessing_note = \"Medical preprocessing applied (CLAHE enhancement + tissue detection).\"\n","        else:\n","            standard_resized = apply_standard_preprocessing(image)\n","            comparison = standard_resized\n","            preprocessing_note = \"Standard preprocessing (resize to 1024px + ImageNet normalization).\"\n","\n","        # Step 8: Model loading\n","        print(f\"Loading {model_name} for {task} ({preprocessing_variant})...\")\n","        cached = her2_model_cache.get(model_name, task, num_classes, preprocessing_variant)\n","        if cached:\n","            model, training_info = cached\n","            print(\"  Model from cache\")\n","        else:\n","            model, training_info = load_her2_model_checkpoint(\n","                model_name,\n","                task,\n","                num_classes,\n","                preprocessing_variant\n","            )\n","            her2_model_cache.set(model_name, task, model, training_info)\n","            print(\"  Model loaded from checkpoint\")\n","\n","        # Step 9: Input preparation\n","        input_tensor, vit_tensor = prepare_model_input(image, model_name)\n","\n","        # Step 10: Inference\n","        print(\"Running inference...\")\n","        predictions, probabilities = run_her2_inference(model, model_name, input_tensor, vit_tensor)\n","        results = format_her2_results(predictions, probabilities, task)\n","        print(f\"  Prediction: {results['predicted_class']} ({results['confidence']:.1%})\")\n","\n","        # Step 11: Interpretability generation\n","        print(\"Generating attention visualizations...\")\n","        interpretations = {}\n","        interpretation_status = \"\"\n","\n","        try:\n","            interpretations = generate_her2_interpretation(\n","                model,\n","                model_name,\n","                input_tensor,\n","                vit_tensor,\n","                int(predictions.item()),\n","                image,\n","                device\n","            )\n","\n","            generated = []\n","            if 'attention' in interpretations and interpretations['attention'] is not None:\n","                generated.append(\"single attention\")\n","            if 'mobilenet_attention' in interpretations and interpretations['mobilenet_attention'] is not None:\n","                generated.append(\"MobileNet\")\n","            if 'vit_attention' in interpretations and interpretations['vit_attention'] is not None:\n","                generated.append(\"ViT\")\n","            if 'fusion_attention' in interpretations and interpretations['fusion_attention'] is not None:\n","                generated.append(\"fusion\")\n","\n","            if generated:\n","                interpretation_status = f\"Generated: {', '.join(generated)}\"\n","                print(f\"  {interpretation_status}\")\n","            else:\n","                interpretation_status = \"Warning: No visualizations generated\"\n","                print(f\"  {interpretation_status}\")\n","\n","        except Exception as e:\n","            interpretation_status = f\"Visualization failed: {str(e)}\"\n","            print(f\"  {interpretation_status}\")\n","\n","        # Step 12: Format results\n","        variant_display = \"Medical Preprocessing\" if preprocessing_variant == 'prep' else \"Standard Preprocessing\"\n","\n","        prediction_text = f\"\"\"\n","### Prediction Results\n","\n","**Model:** {model_info['display_name']}\n","**Task:** {task_config['full_name']}\n","**Preprocessing:** {variant_display}\n","\n","---\n","\n","**Predicted Class:** `{results['predicted_class']}`\n","**Confidence:** {results['confidence']:.1%}\n","\n","---\n","\n","**Model Info:**\n","- Validation F1: {training_info['best_val_f1']:.4f}\n","- Best Epoch: {training_info['best_epoch']}\n","- {preprocessing_note}\n","\n","---\n","\n","{interpretation_status}\n","\"\"\"\n","\n","        prob_dict = results['all_probabilities']\n","\n","        # Step 13: Dynamic return based on model type\n","        if is_fusion:\n","            return (\n","                comparison,\n","                prediction_text,\n","                prob_dict,\n","                interpretations.get('mobilenet_attention'),\n","                interpretations.get('vit_attention'),\n","                interpretations.get('fusion_attention'),\n","                gr.update(visible=True),\n","                \"MobileNet Branch Attention\",\n","                \"ViT Branch Attention\",\n","                \"Combined Fusion Attention\"\n","            )\n","        else:\n","            attention_method = model_info['attention_method']\n","            if attention_method == 'grad_cam':\n","                label = \"Grad-CAM Attention Map\"\n","            elif attention_method == 'native_transformer':\n","                label = \"Transformer Attention Map\"\n","            else:\n","                label = \"Attention Map\"\n","\n","            return (\n","                comparison,\n","                prediction_text,\n","                prob_dict,\n","                interpretations.get('attention'),\n","                None,\n","                None,\n","                gr.update(visible=True),\n","                label,\n","                \"\",\n","                \"\"\n","            )\n","\n","    except Exception as e:\n","        error_msg = f\"\"\"\n","### Critical Error\n","\n","**Error Type:** {type(e).__name__}\n","**Message:** {str(e)}\n","\n","### Debug Info\n","\n","**Model Choice Received:** {model_choice}\n","**Available Models:** {list(HER2_MODEL_REGISTRY.keys())}\n","\n","### Troubleshooting\n","\n","1. Verify Cell 2 executed successfully\n","2. Check HER2_MODEL_REGISTRY defined\n","3. Verify model name mapping\n","4. Check terminal for detailed traceback\n","\n","\"\"\"\n","        print(f\"\\nERROR: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","        return (\n","            None, error_msg, {},\n","            None, None, None,\n","            gr.update(visible=False),\n","            \"\", \"\", \"\"\n","        )\n","\n","\n","print(\"HER2 prediction pipeline defined with flexible model handling\")\n","\n","# ============================================================================\n","# SECTION 2: COMPLETION\n","# ============================================================================\n","\n","print(\"\\n[2/2] Pipeline ready...\")\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 4 COMPLETE - HER2 PIPELINE READY (FIXED)\")\n","print(\"=\" * 80)\n","print(\"\\nFixed: Flexible model name handling\")\n","print(\"  - Accepts internal keys directly\")\n","print(\"  - Accepts display names with mapping\")\n","print(\"  - Backward compatible with both Cell 7 and Cell 8\")\n","print(\"\\nNext: Run Cell 8 to launch demo\")\n","print(\"=\" * 80)"],"metadata":{"collapsed":true,"cellView":"form","id":"7bZjhtfmCoOI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767432131761,"user_tz":-420,"elapsed":45,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"602226a1-b56b-4e55-c8c8-a0f08b800230"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","HER2 PROJECT - PREDICTION PIPELINE (FIXED)\n","================================================================================\n","\n","[1/2] Defining HER2 prediction pipeline...\n","HER2 prediction pipeline defined with flexible model handling\n","\n","[2/2] Pipeline ready...\n","\n","================================================================================\n","CELL 4 COMPLETE - HER2 PIPELINE READY (FIXED)\n","================================================================================\n","\n","Fixed: Flexible model name handling\n","  - Accepts internal keys directly\n","  - Accepts display names with mapping\n","  - Backward compatible with both Cell 7 and Cell 8\n","\n","Next: Run Cell 8 to launch demo\n","================================================================================\n"]}]},{"cell_type":"code","source":["# @title Cell 5: MER Models & Pipeline\n","\n","# File: MMLab_Project_Demo.ipynb - Cell 5\n","# Location: Thesis_MER_Project/MMLab_Project_Demo.ipynb\n","# Purpose: MER models with correct channel handling per model type\n","\n","print(\"=\" * 80)\n","print(\"MER PROJECT - MODELS & PIPELINE\")\n","print(\"=\" * 80)\n","\n","# ============================================================================\n","# SECTION 1: MER CHECKPOINT PATHS\n","# ============================================================================\n","\n","print(\"\\n[1/7] Configuring MER model checkpoints...\")\n","\n","MER_CHECKPOINT_PATHS = {\n","    'MobileNetV3_M1': f\"{MER_MODELS_ROOT}/08_01_mobilenet_casme2_mfs/casme2_mobilenet_mfs_best_f1.pth\",\n","    'EfficientNet_M1': f\"{MER_MODELS_ROOT}/08_02_efficientnet_casme2_mfs/casme2_efficientnet_mfs_best_f1.pth\",\n","    'ConvNeXt_M1': f\"{MER_MODELS_ROOT}/08_03_convnext_casme2_mfs/casme2_convnext_mfs_best_f1.pth\",\n","\n","    'MobileNetV3_M2': f\"{MER_MODELS_ROOT}/09_01_mobilenet_casme2_mfs_prep/casme2_mobilenet_mfs_prep_best_f1.pth\",\n","    'EfficientNet_M2': f\"{MER_MODELS_ROOT}/09_02_efficientnet_casme2_mfs_prep/casme2_efficientnet_mfs_prep_best_f1.pth\",\n","    'ConvNeXt_M2': f\"{MER_MODELS_ROOT}/09_03_convnext_casme2_mfs_prep/casme2_convnext_mfs_prep_best_f1.pth\",\n","\n","    'ViT_M1': f\"{MER_MODELS_ROOT}/02_01_vit_casme2-af/casme2_vit_direct_best_f1.pth\",\n","    'SwinTransformer_M1': f\"{MER_MODELS_ROOT}/02_02_swint_casme2-af/casme2_swint_direct_best_f1.pth\",\n","    'PoolFormer_M1': f\"{MER_MODELS_ROOT}/04_03_poolformer_casme2_mfs/casme2_poolformer_multiframe_best_f1.pth\",\n","\n","    'ViT_M2': f\"{MER_MODELS_ROOT}/05_01_vit_casme2_af_prep/casme2_vit_apex_frame_best_f1.pth\",\n","    'SwinTransformer_M2': f\"{MER_MODELS_ROOT}/07_02_swint_casme2_mfs_prep/casme2_swint_mfs_best_f1.pth\",\n","    'PoolFormer_M2': f\"{MER_MODELS_ROOT}/07_03_poolformer_casme2_mfs_prep/casme2_poolformer_mfs_best_f1.pth\",\n","}\n","\n","print(f\"Configured {len(MER_CHECKPOINT_PATHS)} model checkpoints\")\n","\n","# ============================================================================\n","# SECTION 2: MER TASK CONFIGURATION\n","# ============================================================================\n","\n","MER_TASK_CONFIG = {\n","    'num_classes': 7,\n","    'class_names': ['Others', 'Disgust', 'Happiness', 'Repression', 'Surprise', 'Sadness', 'Fear'],\n","    'dataset': 'CASME II',\n","    'description': 'Micro-expression recognition (7 emotions)',\n","    'evaluation_phase': 'AF'\n","}\n","\n","MER_PREPROCESSING_CONFIGS = {\n","    'M1': {\n","        'name': 'M1 (Raw RGB)',\n","        'description_cnn': 'Minimal preprocessing - RGB 640×480',\n","        'description_transformer': 'Minimal preprocessing - RGB 384×384',\n","        'image_size_cnn': (640, 480),\n","        'image_size_transformer': 384,\n","        'channels': 3,\n","        'mode': 'RGB'\n","    },\n","    'M2': {\n","        'name': 'M2 (Preprocessed)',\n","        'description': 'Face-aware preprocessing - Grayscale 224×224',\n","        'image_size': 224,\n","        'channels': 1,\n","        'mode': 'Grayscale'\n","    }\n","}\n","\n","print(f\"Task configuration: {MER_TASK_CONFIG['dataset']} - {MER_TASK_CONFIG['num_classes']} classes\")\n","\n","# ============================================================================\n","# SECTION 3: CNN MODEL ARCHITECTURES (UNCHANGED)\n","# ============================================================================\n","\n","print(\"\\n[2/7] Defining CNN model architectures...\")\n","\n","class MER_MobileNetV3(nn.Module):\n","    def __init__(self, num_classes=7, dropout_rate=0.3, input_channels=3):\n","        super(MER_MobileNetV3, self).__init__()\n","\n","        if input_channels == 1:\n","            self.mobilenet = timm.create_model(\n","                'mobilenetv3_small_100',\n","                pretrained=False,\n","                num_classes=0,\n","                global_pool='',\n","                in_chans=1\n","            )\n","        else:\n","            self.mobilenet = timm.create_model(\n","                'mobilenetv3_small_100',\n","                pretrained=True,\n","                num_classes=0,\n","                global_pool=''\n","            )\n","\n","        self.conv_head = nn.Conv2d(576, 1024, kernel_size=1, bias=True)\n","        self.act_head = nn.ReLU(inplace=True)\n","        self.global_pool = nn.AdaptiveAvgPool2d(1)\n","        self.flatten = nn.Flatten(1)\n","\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(512, 128),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        features = self.mobilenet.forward_features(x)\n","        features = self.conv_head(features)\n","        features = self.act_head(features)\n","        features = self.global_pool(features)\n","        features = self.flatten(features)\n","        x = self.classifier_layers(features)\n","        output = self.classifier(x)\n","        return output\n","\n","\n","class MER_EfficientNet(nn.Module):\n","    def __init__(self, num_classes=7, dropout_rate=0.3, input_channels=3):\n","        super(MER_EfficientNet, self).__init__()\n","\n","        if input_channels == 1:\n","            self.efficientnet = timm.create_model(\n","                'efficientnet_b0',\n","                pretrained=False,\n","                num_classes=0,\n","                in_chans=1\n","            )\n","        else:\n","            self.efficientnet = timm.create_model(\n","                'efficientnet_b0',\n","                pretrained=True,\n","                num_classes=0\n","            )\n","\n","        self.feature_dim = 1280\n","\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.feature_dim, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(512, 128),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        features = self.efficientnet(x)\n","        x = self.classifier_layers(features)\n","        output = self.classifier(x)\n","        return output\n","\n","\n","class MER_ConvNeXt(nn.Module):\n","    def __init__(self, num_classes=7, dropout_rate=0.3, input_channels=3):\n","        super(MER_ConvNeXt, self).__init__()\n","\n","        if input_channels == 1:\n","            self.convnext = timm.create_model(\n","                'convnext_tiny',\n","                pretrained=False,\n","                num_classes=0,\n","                in_chans=1\n","            )\n","        else:\n","            self.convnext = timm.create_model(\n","                'convnext_tiny',\n","                pretrained=True,\n","                num_classes=0\n","            )\n","\n","        self.feature_dim = 768\n","\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.feature_dim, 512),\n","            nn.LayerNorm(512),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(512, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        features = self.convnext(x)\n","        x = self.classifier_layers(features)\n","        output = self.classifier(x)\n","        return output\n","\n","\n","print(\"Defined 3 CNN architectures\")\n","\n","# ============================================================================\n","# SECTION 4: TRANSFORMER MODEL ARCHITECTURES (FIXED)\n","# ============================================================================\n","\n","print(\"\\n[3/7] Defining Transformer model architectures...\")\n","\n","try:\n","    from transformers import ViTModel, ViTConfig\n","    from transformers import SwinModel, SwinConfig\n","    from transformers import PoolFormerModel, PoolFormerConfig\n","    print(\"Transformers library imported\")\n","    TRANSFORMERS_AVAILABLE = True\n","except ImportError:\n","    print(\"Installing transformers...\")\n","    import subprocess\n","    import sys\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\"])\n","    from transformers import ViTModel, ViTConfig\n","    from transformers import SwinModel, SwinConfig\n","    from transformers import PoolFormerModel, PoolFormerConfig\n","    print(\"Transformers library installed\")\n","    TRANSFORMERS_AVAILABLE = True\n","\n","\n","class MER_ViT(nn.Module):\n","    def __init__(self, num_classes=7, dropout_rate=0.3, input_channels=3, methodology='M1'):\n","        super(MER_ViT, self).__init__()\n","\n","        # Both M1 and M2 use 224×224 RGB\n","        image_size = 224\n","        classifier_dim = 512 if methodology == 'M1' else 256\n","\n","        config = ViTConfig(\n","            hidden_size=768,\n","            num_hidden_layers=12,\n","            num_attention_heads=12,\n","            intermediate_size=3072,\n","            hidden_dropout_prob=dropout_rate,\n","            attention_probs_dropout_prob=dropout_rate,\n","            num_channels=input_channels,\n","            image_size=image_size,\n","            patch_size=32,\n","            num_labels=num_classes\n","        )\n","\n","        self.vit = ViTModel(config)\n","        self.feature_dim = 768\n","\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.feature_dim, classifier_dim),\n","            nn.LayerNorm(classifier_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(classifier_dim, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        outputs = self.vit(pixel_values=x)\n","        features = outputs.last_hidden_state[:, 0]\n","        x = self.classifier_layers(features)\n","        output = self.classifier(x)\n","        return output\n","\n","\n","class MER_SwinTransformer(nn.Module):\n","    def __init__(self, num_classes=7, dropout_rate=0.3, input_channels=3, methodology='M1'):\n","        super(MER_SwinTransformer, self).__init__()\n","\n","        # Both M1 and M2 use 224×224\n","        image_size = 224\n","\n","        config = SwinConfig(\n","            embed_dim=128,\n","            depths=[2, 2, 18, 2],\n","            num_heads=[4, 8, 16, 32],\n","            window_size=7,\n","            mlp_ratio=4.0,\n","            drop_rate=dropout_rate,\n","            attn_drop_rate=dropout_rate,\n","            num_channels=input_channels,\n","            image_size=image_size,\n","            patch_size=4,\n","            num_labels=num_classes\n","        )\n","\n","        self.swin = SwinModel(config)\n","\n","        # M1: 1024 feature dim, M2: 768 feature dim\n","        self.feature_dim = 1024 if methodology == 'M1' else 768\n","\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.feature_dim, 512),\n","            nn.LayerNorm(512),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(512, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        outputs = self.swin(pixel_values=x)\n","        features = outputs.pooler_output\n","        x = self.classifier_layers(features)\n","        output = self.classifier(x)\n","        return output\n","\n","\n","class MER_PoolFormer(nn.Module):\n","    def __init__(self, num_classes=7, dropout_rate=0.3, input_channels=3, methodology='M1'):\n","        super(MER_PoolFormer, self).__init__()\n","\n","        # Both M1 and M2 use 224×224\n","        image_size = 224\n","\n","        config = PoolFormerConfig(\n","            hidden_sizes=[96, 192, 384, 768],\n","            depths=[2, 2, 18, 2],\n","            mlp_ratio=4.0,\n","            drop_rate=dropout_rate,\n","            num_channels=input_channels,\n","            image_size=image_size,\n","            patch_size=7,\n","            num_labels=num_classes\n","        )\n","\n","        self.poolformer = PoolFormerModel(config)\n","        self.feature_dim = 768\n","\n","        # FIXED: Add explicit pooling\n","        self.global_pool = nn.AdaptiveAvgPool1d(1)\n","\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.feature_dim, 512),\n","            nn.LayerNorm(512),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(512, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        outputs = self.poolformer(pixel_values=x)\n","\n","        # FIXED: Proper spatial pooling\n","        # outputs.last_hidden_state shape: (batch, seq_len, 768)\n","        features = outputs.last_hidden_state\n","\n","        # Transpose for pooling: (batch, 768, seq_len)\n","        features = features.transpose(1, 2)\n","\n","        # Global average pooling: (batch, 768, 1)\n","        features = self.global_pool(features)\n","\n","        # Squeeze: (batch, 768)\n","        features = features.squeeze(-1)\n","\n","        x = self.classifier_layers(features)\n","        output = self.classifier(x)\n","        return output\n","\n","\n","print(\"Defined 3 Transformer architectures\")\n","\n","# ============================================================================\n","# SECTION 5: MODEL REGISTRY\n","# ============================================================================\n","\n","MER_MODEL_REGISTRY = {\n","    'MobileNetV3': {\n","        'architecture_class': MER_MobileNetV3,\n","        'display_name': 'MobileNetV3-Small',\n","        'description': 'Lightweight CNN - Best overall (F1: 0.3880 M1-AF)',\n","        'model_type': 'CNN'\n","    },\n","    'EfficientNet': {\n","        'architecture_class': MER_EfficientNet,\n","        'display_name': 'EfficientNet-B0',\n","        'description': 'Compound scaling - Balanced performance',\n","        'model_type': 'CNN'\n","    },\n","    'ConvNeXt': {\n","        'architecture_class': MER_ConvNeXt,\n","        'display_name': 'ConvNeXt-Tiny',\n","        'description': 'Modernized CNN - Benefits from M2 (41% improvement)',\n","        'model_type': 'CNN'\n","    },\n","    'ViT': {\n","        'architecture_class': MER_ViT,\n","        'display_name': 'Vision Transformer (ViT)',\n","        'description': 'Attention-based - Global pattern analysis (F1: 0.2298 M1)',\n","        'model_type': 'Transformer'\n","    },\n","    'SwinTransformer': {\n","        'architecture_class': MER_SwinTransformer,\n","        'display_name': 'Swin Transformer',\n","        'description': 'Window attention - Hierarchical features (F1: 0.2619 M1)',\n","        'model_type': 'Transformer'\n","    },\n","    'PoolFormer': {\n","        'architecture_class': MER_PoolFormer,\n","        'display_name': 'PoolFormer',\n","        'description': 'Pooling-based - Simple and efficient (F1: 0.1734 M1)',\n","        'model_type': 'Transformer'\n","    }\n","}\n","\n","print(f\"Model registry: {len(MER_MODEL_REGISTRY)} architectures\")\n","\n","# ============================================================================\n","# SECTION 6: MODEL LOADING & INFERENCE (FIXED CHANNEL LOGIC)\n","# ============================================================================\n","\n","print(\"\\n[4/7] Defining model loading and inference functions...\")\n","\n","def load_mer_model_checkpoint(model_name, methodology):\n","    checkpoint_key = f\"{model_name}_{methodology}\"\n","\n","    if checkpoint_key not in MER_CHECKPOINT_PATHS:\n","        raise KeyError(f\"Checkpoint not found: {checkpoint_key}\")\n","\n","    checkpoint_path = MER_CHECKPOINT_PATHS[checkpoint_key]\n","\n","    if not os.path.exists(checkpoint_path):\n","        raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n","\n","    print(f\"Loading {model_name} ({methodology})...\")\n","\n","    try:\n","        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","    except Exception:\n","        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n","\n","    model_type = MER_MODEL_REGISTRY[model_name]['model_type']\n","\n","    if methodology == 'M1':\n","        input_channels = 3\n","    else:\n","        input_channels = 1 if model_type == 'CNN' else 3\n","\n","    print(f\"  Model type: {model_type}\")\n","    print(f\"  Input channels: {input_channels} ({'RGB' if input_channels == 3 else 'Grayscale'})\")\n","\n","    architecture_class = MER_MODEL_REGISTRY[model_name]['architecture_class']\n","\n","    if model_type == 'Transformer':\n","        model = architecture_class(\n","            num_classes=MER_TASK_CONFIG['num_classes'],\n","            dropout_rate=0.3 if methodology == 'M2' else 0.2,\n","            input_channels=input_channels,\n","            methodology=methodology\n","        ).to(device)\n","    else:\n","        model = architecture_class(\n","            num_classes=MER_TASK_CONFIG['num_classes'],\n","            dropout_rate=0.3 if methodology == 'M2' else 0.2,\n","            input_channels=input_channels\n","        ).to(device)\n","\n","    state_dict = checkpoint.get('model_state_dict', checkpoint)\n","\n","    try:\n","        model.load_state_dict(state_dict, strict=True)\n","        load_status = \"strict\"\n","    except Exception as e:\n","        print(f\"  Warning: Strict loading failed: {e}\")\n","        model.load_state_dict(state_dict, strict=False)\n","        load_status = \"non-strict\"\n","\n","    model.eval()\n","\n","    training_info = {\n","        'model_name': model_name,\n","        'methodology': methodology,\n","        'checkpoint_file': os.path.basename(checkpoint_path),\n","        'load_status': load_status,\n","        'input_channels': input_channels,\n","        'best_f1': checkpoint.get('best_f1', 'N/A')\n","    }\n","\n","    print(f\"  Loaded ({load_status})\")\n","    if 'best_f1' in checkpoint:\n","        print(f\"  Training F1: {checkpoint['best_f1']:.4f}\")\n","\n","    return model, training_info\n","\n","\n","def run_mer_inference(model, input_tensor):\n","    \"\"\"Run MER inference\"\"\"\n","    model.eval()\n","\n","    with torch.no_grad():\n","        outputs = model(input_tensor)\n","        probabilities = torch.softmax(outputs, dim=1)\n","        predictions = torch.argmax(probabilities, dim=1)\n","\n","    return predictions, probabilities\n","\n","\n","def format_mer_results(predictions, probabilities):\n","    \"\"\"Format MER prediction results\"\"\"\n","\n","    pred_class_idx = predictions.item()\n","    class_probs = probabilities[0].cpu().numpy()\n","\n","    class_names = MER_TASK_CONFIG['class_names']\n","\n","    results = {\n","        'predicted_emotion': class_names[pred_class_idx],\n","        'predicted_index': pred_class_idx,\n","        'confidence': float(class_probs[pred_class_idx]),\n","        'all_probabilities': {\n","            class_names[i]: float(class_probs[i])\n","            for i in range(len(class_names))\n","        }\n","    }\n","\n","    return results\n","\n","\n","class MERModelCache:\n","    \"\"\"Cache for loaded MER models\"\"\"\n","\n","    def __init__(self):\n","        self.cache = {}\n","        self.stats = {'hits': 0, 'misses': 0}\n","\n","    def get(self, model_name, methodology):\n","        cache_key = f\"{model_name}_{methodology}\"\n","        if cache_key in self.cache:\n","            self.stats['hits'] += 1\n","            print(f\"  Cache hit: {cache_key}\")\n","            return self.cache[cache_key]\n","        else:\n","            self.stats['misses'] += 1\n","            return None\n","\n","    def set(self, model_name, methodology, model, info):\n","        cache_key = f\"{model_name}_{methodology}\"\n","        self.cache[cache_key] = (model, info)\n","        print(f\"  Cached: {cache_key}\")\n","\n","    def clear(self):\n","        self.cache.clear()\n","        self.stats = {'hits': 0, 'misses': 0}\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","\n","\n","# CRITICAL: Instantiate the cache\n","mer_model_cache = MERModelCache()\n","\n","print(\"Model loading, inference, and cache functions defined\")\n","\n","# ============================================================================\n","# SECTION 7: PREDICTION PIPELINE (FIXED DESCRIPTION)\n","# ============================================================================\n","\n","print(\"\\n[5/7] Defining prediction pipeline...\")\n","\n","def mer_predict_pipeline(image, model_choice, methodology_choice):\n","    try:\n","        if image is None:\n","            return (None, \"### Error\\n\\nPlease upload or capture an image first.\", {})\n","\n","        if isinstance(image, np.ndarray):\n","            image = Image.fromarray(image)\n","\n","        if model_choice in MER_MODEL_REGISTRY:\n","            model_name = model_choice\n","            print(f\"Using internal key: {model_name}\")\n","        else:\n","            model_map = {\n","                'MobileNetV3-Small': 'MobileNetV3',\n","                'EfficientNet-B0': 'EfficientNet',\n","                'ConvNeXt-Tiny': 'ConvNeXt',\n","                'Vision Transformer (ViT)': 'ViT',\n","                'Swin Transformer': 'SwinTransformer',\n","                'PoolFormer': 'PoolFormer'\n","            }\n","\n","            if model_choice in model_map:\n","                model_name = model_map[model_choice]\n","                print(f\"Mapped display name: {model_choice} -> {model_name}\")\n","            else:\n","                raise ValueError(f\"Unknown model: {model_choice}\")\n","\n","        if methodology_choice in ['M1', 'M2']:\n","            methodology = methodology_choice\n","        else:\n","            methodology = 'M1' if 'M1' in methodology_choice else 'M2'\n","\n","        comparison = create_preprocessing_comparison_mer_updated(image, methodology, model_name)\n","\n","        print(f\"Loading {model_name} ({methodology})...\")\n","        cached = mer_model_cache.get(model_name, methodology)\n","        if cached:\n","            model, training_info = cached\n","            print(\"  From cache\")\n","        else:\n","            model, training_info = load_mer_model_checkpoint(model_name, methodology)\n","            mer_model_cache.set(model_name, methodology, model, training_info)\n","            print(\"  From checkpoint\")\n","\n","        print(f\"Preprocessing with {methodology} method...\")\n","        model_type = MER_MODEL_REGISTRY[model_name]['model_type']\n","\n","        if methodology == 'M1':\n","            input_tensor = preprocess_mer_m1_webcam(image, model_type)\n","            print(f\"  M1 preprocessing: shape {input_tensor.shape}\")\n","        else:\n","            input_tensor = preprocess_mer_m2_webcam(image, model_type)\n","            print(f\"  M2 preprocessing: shape {input_tensor.shape}\")\n","\n","        print(\"Running inference...\")\n","        predictions, probabilities = run_mer_inference(model, input_tensor)\n","        results = format_mer_results(predictions, probabilities)\n","        print(f\"  Prediction: {results['predicted_emotion']} ({results['confidence']:.1%})\")\n","\n","        model_info = MER_MODEL_REGISTRY[model_name]\n","        preprocessing_info = MER_PREPROCESSING_CONFIGS[methodology]\n","\n","        # FIXED: Fallback to 'description' if 'description_cnn' missing\n","        if model_type == 'CNN':\n","            prep_desc = preprocessing_info.get('description_cnn', preprocessing_info.get('description', 'CNN preprocessing'))\n","        else:\n","            prep_desc = preprocessing_info.get('description_transformer', preprocessing_info.get('description', 'Transformer preprocessing'))\n","\n","        prediction_text = f\"\"\"\n","### Micro-Expression Classification Results\n","\n","**Model:** {model_info['display_name']}\n","**Type:** {model_type}\n","**Methodology:** {preprocessing_info['name']}\n","**Evaluation:** Apex Frame (Single Snapshot)\n","\n","---\n","\n","**Predicted Emotion:** `{results['predicted_emotion']}`\n","**Confidence:** {results['confidence']:.1%}\n","\n","---\n","\n","**Model Info:**\n","- {model_info['description']}\n","- Training F1: {training_info.get('best_f1', 'N/A')}\n","- Input: {prep_desc}\n","- Dataset: CASME II (7 emotion categories)\n","\"\"\"\n","\n","        prob_dict = results['all_probabilities']\n","\n","        return (comparison, prediction_text, prob_dict)\n","\n","    except Exception as e:\n","        error_msg = f\"\"\"\n","### Error\n","\n","**Type:** {type(e).__name__}\n","**Message:** {str(e)}\n","\n","**Debug Info:**\n","- Model Choice: {model_choice}\n","- Available Models: {list(MER_MODEL_REGISTRY.keys())}\n","\n","Check terminal for traceback.\n","\"\"\"\n","        print(f\"\\nERROR: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","        return (None, error_msg, {})\n","\n","\n","print(\"Prediction pipeline defined\")\n","\n","# ============================================================================\n","# SECTION 8: VERIFICATION\n","# ============================================================================\n","\n","print(\"\\n[6/7] Verifying dependencies...\")\n","\n","required_functions = [\n","    'preprocess_mer_m1_webcam',\n","    'preprocess_mer_m2_webcam',\n","    'create_preprocessing_comparison_mer_updated'\n","]\n","\n","missing = [f for f in required_functions if f not in globals()]\n","if missing:\n","    print(f\"WARNING: Missing preprocessing functions from Cell 6: {missing}\")\n","else:\n","    print(\"All preprocessing functions available\")\n","\n","print(\"\\n[7/7] System ready...\")\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 5 COMPLETE - MER SYSTEM (FINAL FIX)\")\n","print(\"=\" * 80)\n","print(\"\\nFixed:\")\n","print(\"  - M1 All: RGB (3 channels)\")\n","print(\"  - M2 CNN: Grayscale (1 channel)\")\n","print(\"  - M2 Transformer: RGB (3 channels) ← CRITICAL FIX\")\n","print(\"  - PoolFormer: Explicit spatial pooling\")\n","print(\"  - Description fallback for M2\")\n","print(\"\\nNext: Update Cell 6 for conditional M2 preprocessing\")\n","print(\"=\" * 80)"],"metadata":{"collapsed":true,"cellView":"form","id":"1p_XS5vOn3l1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767432131938,"user_tz":-420,"elapsed":174,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"985f3df3-51f4-477d-d06b-8998baf5bddb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","MER PROJECT - MODELS & PIPELINE\n","================================================================================\n","\n","[1/7] Configuring MER model checkpoints...\n","Configured 12 model checkpoints\n","Task configuration: CASME II - 7 classes\n","\n","[2/7] Defining CNN model architectures...\n","Defined 3 CNN architectures\n","\n","[3/7] Defining Transformer model architectures...\n","Transformers library imported\n","Defined 3 Transformer architectures\n","Model registry: 6 architectures\n","\n","[4/7] Defining model loading and inference functions...\n","Model loading, inference, and cache functions defined\n","\n","[5/7] Defining prediction pipeline...\n","Prediction pipeline defined\n","\n","[6/7] Verifying dependencies...\n","WARNING: Missing preprocessing functions from Cell 6: ['preprocess_mer_m1_webcam', 'preprocess_mer_m2_webcam', 'create_preprocessing_comparison_mer_updated']\n","\n","[7/7] System ready...\n","\n","================================================================================\n","CELL 5 COMPLETE - MER SYSTEM (FINAL FIX)\n","================================================================================\n","\n","Fixed:\n","  - M1 All: RGB (3 channels)\n","  - M2 CNN: Grayscale (1 channel)\n","  - M2 Transformer: RGB (3 channels) ← CRITICAL FIX\n","  - PoolFormer: Explicit spatial pooling\n","  - Description fallback for M2\n","\n","Next: Update Cell 6 for conditional M2 preprocessing\n","================================================================================\n"]}]},{"cell_type":"code","source":["# @title Cell 6: MER Preprocessing Functions\n","\n","# File: MMLab_Project_Demo.ipynb - Cell 6\n","# Location: Thesis_MER_Project/MMLab_Project_Demo.ipynb\n","# Purpose: Preprocessing with CORRECT grayscale conversion for M2\n","\n","print(\"=\" * 80)\n","print(\"MER PREPROCESSING FUNCTIONS - GRAYSCALE FIX\")\n","print(\"=\" * 80)\n","\n","# ============================================================================\n","# SECTION 1: DLIB FACE DETECTOR\n","# ============================================================================\n","\n","print(\"\\n[1/4] Initializing Dlib face detector...\")\n","\n","try:\n","    import dlib\n","    mer_face_detector = dlib.get_frontal_face_detector()\n","    DLIB_AVAILABLE = True\n","    print(\"Dlib face detector loaded successfully\")\n","except ImportError:\n","    DLIB_AVAILABLE = False\n","    print(\"Dlib not available - M2 will use center crop fallback\")\n","\n","# ============================================================================\n","# SECTION 2: M1 PREPROCESSING\n","# ============================================================================\n","\n","print(\"\\n[2/4] Defining M1 preprocessing...\")\n","\n","def preprocess_mer_m1_webcam(image, model_type='CNN'):\n","    \"\"\"\n","    M1 preprocessing with model-specific sizing.\n","\n","    CNN models: RGB 640×480\n","    Transformer models: RGB 224×224 (NOT 384×384!)\n","\n","    Args:\n","        image: PIL Image or numpy array\n","        model_type: 'CNN' or 'Transformer'\n","\n","    Returns:\n","        Torch tensor (1, 3, H, W) RGB\n","    \"\"\"\n","    if isinstance(image, Image.Image):\n","        image = np.array(image)\n","\n","    if len(image.shape) == 2:\n","        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n","    elif image.shape[2] == 4:\n","        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n","\n","    h, w = image.shape[:2]\n","\n","    # FIXED: Transformer M1 uses 224×224, NOT 384×384!\n","    if model_type == 'Transformer':\n","        target_w, target_h = 224, 224\n","        aspect_target = 1.0\n","    else:  # CNN\n","        target_w, target_h = 640, 480\n","        aspect_target = target_w / target_h\n","\n","    aspect_current = w / h\n","\n","    if aspect_target == 1.0:\n","        crop_size = min(h, w)\n","        start_y = (h - crop_size) // 2\n","        start_x = (w - crop_size) // 2\n","        cropped = image[start_y:start_y+crop_size, start_x:start_x+crop_size]\n","    else:\n","        if aspect_current > aspect_target:\n","            new_w = int(h * aspect_target)\n","            x_start = (w - new_w) // 2\n","            cropped = image[:, x_start:x_start+new_w]\n","        elif aspect_current < aspect_target:\n","            new_h = int(w / aspect_target)\n","            y_start = (h - new_h) // 2\n","            cropped = image[y_start:y_start+new_h, :]\n","        else:\n","            cropped = image\n","\n","    resized = cv2.resize(cropped, (target_w, target_h),\n","                        interpolation=cv2.INTER_LANCZOS4)\n","\n","    pil_image = Image.fromarray(resized)\n","\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    tensor = transform(pil_image).unsqueeze(0).to(device)\n","\n","    return tensor\n","\n","\n","print(\"M1 preprocessing defined: CNN 640×480, Transformer 224×224\")\n","\n","# ============================================================================\n","# SECTION 3: M2 PREPROCESSING (GRAYSCALE FIX)\n","# ============================================================================\n","\n","print(\"\\n[3/4] Defining M2 preprocessing with grayscale conversion...\")\n","\n","def detect_face_with_expansion_exact(image, expansion=20):\n","    if not DLIB_AVAILABLE:\n","        return None\n","\n","    try:\n","        if len(image.shape) == 3:\n","            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n","        else:\n","            gray = image\n","\n","        faces = mer_face_detector(gray, 1)\n","\n","        if len(faces) == 0:\n","            return None\n","\n","        if len(faces) > 1:\n","            faces = sorted(faces, key=lambda r: r.width() * r.height(), reverse=True)\n","\n","        face = faces[0]\n","\n","        img_h, img_w = gray.shape[:2]\n","\n","        x1 = max(0, face.left() - expansion)\n","        y1 = max(0, face.top() - expansion)\n","        x2 = min(img_w, face.right() + expansion)\n","        y2 = min(img_h, face.bottom() + expansion)\n","\n","        return (x1, y1, x2, y2)\n","\n","    except Exception:\n","        return None\n","\n","\n","def ensure_minimum_size_exact(image, min_size=224):\n","    h, w = image.shape[:2]\n","\n","    if h >= min_size and w >= min_size:\n","        return image\n","\n","    scale_factor = min_size / min(h, w)\n","    new_width = int(w * scale_factor)\n","    new_height = int(h * scale_factor)\n","\n","    resized = cv2.resize(image, (new_width, new_height),\n","                        interpolation=cv2.INTER_LANCZOS4)\n","\n","    return resized\n","\n","\n","def preprocess_mer_m2_webcam(image, model_type='CNN'):\n","    \"\"\"\n","    M2 preprocessing: Conditional output based on model_type.\n","\n","    CNN:         Grayscale 224x224 (1 channel), single-channel normalization\n","    Transformer: RGB 224x224 (3 channels), ImageNet RGB normalization\n","\n","    Returns:\n","        Torch tensor (1, C, 224, 224) where C = 1 for CNN, 3 for Transformer.\n","    \"\"\"\n","    # Accept PIL or numpy\n","    if isinstance(image, Image.Image):\n","        image = np.array(image)\n","\n","    # Ensure RGB first (so face detector gets consistent input)\n","    if len(image.shape) == 2:\n","        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n","    elif image.shape[2] == 4:\n","        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n","\n","    # Ensure minimum size\n","    image = ensure_minimum_size_exact(image, min_size=224)\n","\n","    h, w = image.shape[:2]\n","\n","    # Face detection (may return None)\n","    face_bbox = detect_face_with_expansion_exact(image, expansion=20)\n","\n","    if face_bbox is not None:\n","        x1, y1, x2, y2 = face_bbox\n","        cropped = image[y1:y2, x1:x2]\n","    else:\n","        # center-crop fallback\n","        center_size = min(h, w)\n","        start_y = (h - center_size) // 2\n","        start_x = (w - center_size) // 2\n","        cropped = image[start_y:start_y+center_size, start_x:start_x+center_size]\n","\n","    # Resize to exact 224x224\n","    if cropped.shape[0] != 224 or cropped.shape[1] != 224:\n","        cropped_224 = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_LANCZOS4)\n","    else:\n","        cropped_224 = cropped\n","\n","    # Decide branch: CNN -> grayscale (single channel). Otherwise treat as Transformer (RGB).\n","    is_cnn = str(model_type).upper().startswith('C')  # permissive check\n","\n","    if is_cnn:\n","        # Convert RGB -> Grayscale and use single-channel normalization\n","        cropped_224_final = cv2.cvtColor(cropped_224, cv2.COLOR_RGB2GRAY)\n","        pil_image = Image.fromarray(cropped_224_final, mode='L')\n","\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),                     # (1, H, W)\n","            transforms.Normalize(mean=[0.485], std=[0.229])\n","        ])\n","    else:\n","        # Keep RGB and use ImageNet normalization\n","        pil_image = Image.fromarray(cropped_224)       # mode 'RGB'\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),                     # (3, H, W)\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    tensor = transform(pil_image).unsqueeze(0).to(device)  # (1, C, 224, 224)\n","\n","    return tensor\n","\n","\n","\n","print(\"M2 preprocessing defined: Grayscale 224×224 (1 channel)\")\n","\n","# ============================================================================\n","# SECTION 4: PREPROCESSING COMPARISON VISUALIZATION\n","# ============================================================================\n","\n","print(\"\\n[4/4] Defining preprocessing comparison...\")\n","\n","def create_preprocessing_comparison_mer_updated(original_image, methodology, model_name='MobileNetV3'):\n","    \"\"\"\n","    Create side-by-side comparison with correct processing.\n","\n","    Args:\n","        original_image: PIL Image\n","        methodology: 'M1' or 'M2'\n","        model_name: Model name\n","\n","    Returns:\n","        PIL Image with comparison\n","    \"\"\"\n","    if isinstance(original_image, Image.Image):\n","        original_np = np.array(original_image)\n","    else:\n","        original_np = original_image\n","\n","    if len(original_np.shape) == 2:\n","        original_np = cv2.cvtColor(original_np, cv2.COLOR_GRAY2RGB)\n","    elif original_np.shape[2] == 4:\n","        original_np = cv2.cvtColor(original_np, cv2.COLOR_RGBA2RGB)\n","\n","    if methodology == 'M1':\n","        model_type = 'Transformer' if model_name in ['ViT', 'SwinTransformer', 'PoolFormer'] else 'CNN'\n","\n","        if model_type == 'Transformer':\n","            # Square 224×224\n","            h, w = original_np.shape[:2]\n","            crop_size = min(h, w)\n","            start_y = (h - crop_size) // 2\n","            start_x = (w - crop_size) // 2\n","            cropped = original_np[start_y:start_y+crop_size, start_x:start_x+crop_size]\n","            resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_LANCZOS4)\n","\n","            comparison = Image.new('RGB', (224 * 2, 224))\n","            orig_resized = cv2.resize(original_np, (224, 224), interpolation=cv2.INTER_LANCZOS4)\n","            comparison.paste(Image.fromarray(orig_resized), (0, 0))\n","            comparison.paste(Image.fromarray(resized), (224, 0))\n","\n","        else:\n","            # Rectangular 640×480\n","            h, w = original_np.shape[:2]\n","            aspect_current = w / h\n","            aspect_target = 640 / 480\n","\n","            if aspect_current > aspect_target:\n","                new_w = int(h * aspect_target)\n","                x_start = (w - new_w) // 2\n","                cropped = original_np[:, x_start:x_start+new_w]\n","            elif aspect_current < aspect_target:\n","                new_h = int(w / aspect_target)\n","                y_start = (h - new_h) // 2\n","                cropped = original_np[y_start:y_start+new_h, :]\n","            else:\n","                cropped = original_np\n","\n","            resized = cv2.resize(cropped, (640, 480), interpolation=cv2.INTER_LANCZOS4)\n","\n","            comparison = Image.new('RGB', (640 * 2, 480))\n","            orig_resized = cv2.resize(original_np, (640, 480), interpolation=cv2.INTER_LANCZOS4)\n","            comparison.paste(Image.fromarray(orig_resized), (0, 0))\n","            comparison.paste(Image.fromarray(resized), (640, 0))\n","\n","    else:  # M2\n","        # Grayscale 224×224 with face detection\n","        image = ensure_minimum_size_exact(original_np, min_size=224)\n","\n","        face_bbox = detect_face_with_expansion_exact(image, expansion=20)\n","\n","        if face_bbox is not None:\n","            x1, y1, x2, y2 = face_bbox\n","            cropped = image[y1:y2, x1:x2]\n","        else:\n","            h, w = image.shape[:2]\n","            center_size = min(h, w)\n","            start_y = (h - center_size) // 2\n","            start_x = (w - center_size) // 2\n","            cropped = image[start_y:start_y+center_size, start_x:start_x+center_size]\n","\n","        cropped_224 = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_LANCZOS4)\n","\n","        # Convert to grayscale for display\n","        cropped_224_gray = cv2.cvtColor(cropped_224, cv2.COLOR_RGB2GRAY)\n","        cropped_224_gray_rgb = cv2.cvtColor(cropped_224_gray, cv2.COLOR_GRAY2RGB)\n","\n","        # Create comparison\n","        comparison = Image.new('RGB', (224 * 2, 224))\n","        orig_224 = cv2.resize(original_np, (224, 224), interpolation=cv2.INTER_LANCZOS4)\n","        comparison.paste(Image.fromarray(orig_224), (0, 0))\n","        comparison.paste(Image.fromarray(cropped_224_gray_rgb), (224, 0))\n","\n","    return comparison\n","\n","\n","print(\"Preprocessing comparison defined\")\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 6 COMPLETE - MER PREPROCESSING (GRAYSCALE FIX)\")\n","print(\"=\" * 80)\n","print(\"\\nM1 Preprocessing:\")\n","print(\"  - CNN: RGB 640×480\")\n","print(\"  - Transformer: RGB 224×224 (CORRECTED from 384×384)\")\n","print(\"  - ImageNet RGB normalization\")\n","print(\"\\nM2 Preprocessing:\")\n","print(\"  - Grayscale 224×224 (1 channel) ← FIXED\")\n","print(\"  - Face detection + 20px expansion\")\n","print(\"  - Single channel normalization\")\n","print(\"\\nDlib status:\", \"Available\" if DLIB_AVAILABLE else \"Fallback mode\")\n","print(\"\\nNext: Update Cell 5 Transformer image sizes\")\n","print(\"=\" * 80)"],"metadata":{"collapsed":true,"cellView":"form","id":"G5XJD030D2Og","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767432133217,"user_tz":-420,"elapsed":1190,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"ae067a2f-d148-41ad-b8fd-09d140493b00"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","MER PREPROCESSING FUNCTIONS - GRAYSCALE FIX\n","================================================================================\n","\n","[1/4] Initializing Dlib face detector...\n","Dlib face detector loaded successfully\n","\n","[2/4] Defining M1 preprocessing...\n","M1 preprocessing defined: CNN 640×480, Transformer 224×224\n","\n","[3/4] Defining M2 preprocessing with grayscale conversion...\n","M2 preprocessing defined: Grayscale 224×224 (1 channel)\n","\n","[4/4] Defining preprocessing comparison...\n","Preprocessing comparison defined\n","\n","================================================================================\n","CELL 6 COMPLETE - MER PREPROCESSING (GRAYSCALE FIX)\n","================================================================================\n","\n","M1 Preprocessing:\n","  - CNN: RGB 640×480\n","  - Transformer: RGB 224×224 (CORRECTED from 384×384)\n","  - ImageNet RGB normalization\n","\n","M2 Preprocessing:\n","  - Grayscale 224×224 (1 channel) ← FIXED\n","  - Face detection + 20px expansion\n","  - Single channel normalization\n","\n","Dlib status: Available\n","\n","Next: Update Cell 5 Transformer image sizes\n","================================================================================\n"]}]},{"cell_type":"code","source":["# @title Cell 7: MER Interface\n","\n","# File: MMLab_Project_Demo.ipynb - Cell 7\n","# Location: Thesis_MER_Project/MMLab_Project_Demo.ipynb\n","# Purpose: MER interface with CSS-based\n","\n","print(\"=\" * 80)\n","print(\"MER INTERFACE CONFIGURATION - CSS FIXED\")\n","print(\"=\" * 80)\n","\n","# ============================================================================\n","# SECTION 1: ENHANCED MODEL DESCRIPTIONS WITH CARD STYLING\n","# ============================================================================\n","\n","print(\"\\n[1/4] Defining enhanced model descriptions...\")\n","\n","def get_mer_model_description(model_name):\n","    \"\"\"Get styled model description with card formatting\"\"\"\n","\n","    descriptions = {\n","        'MobileNetV3-Small': \"\"\"\n","<div style='padding: 16px; background: #f9fafb; border-left: 4px solid #dc2626; border-radius: 8px; margin: 8px 0;'>\n","<h3 style='margin-top: 0; color: #1f2937;'>MobileNetV3-Small (CNN)</h3>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Architecture:</strong> Lightweight CNN with inverted residuals and efficient channel attention\n","</div>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Performance:</strong>\n","<ul style='margin: 8px 0; padding-left: 20px;'>\n","<li>Best Overall: Macro F1 = 0.3880 (M1-AF)</li>\n","<li>Parameters: 2.5M (lightweight)</li>\n","<li>Strength: Excellent single-frame classification</li>\n","<li>Weakness: Severe preprocessing degradation (-47.5% with M2)</li>\n","</ul>\n","</div>\n","\n","<div style='background: #dcfce7; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong style='color: #166534;'>Recommended:</strong> Use with M1 (Raw RGB) for best results\n","</div>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Technical Details:</strong>\n","<ul style='margin: 8px 0; padding-left: 20px;'>\n","<li>Input: RGB 640×480 (M1) or RGB 224×224 (M2)</li>\n","<li>Feature dimension: 1024</li>\n","<li>Classifier: 1024 → 512 → 128 → 7</li>\n","</ul>\n","</div>\n","</div>\n","\"\"\",\n","\n","        'EfficientNet-B0': \"\"\"\n","<div style='padding: 16px; background: #f9fafb; border-left: 4px solid #dc2626; border-radius: 8px; margin: 8px 0;'>\n","<h3 style='margin-top: 0; color: #1f2937;'>EfficientNet-B0 (CNN)</h3>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Architecture:</strong> Compound scaling CNN with efficient depth/width/resolution balance\n","</div>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Performance:</strong>\n","<ul style='margin: 8px 0; padding-left: 20px;'>\n","<li>Macro F1 = 0.3278 (M1-AF)</li>\n","<li>Parameters: 5.3M (balanced)</li>\n","<li>Strength: Consistent across methodologies</li>\n","<li>Weakness: Moderate preprocessing sensitivity (-33.7% with M2)</li>\n","</ul>\n","</div>\n","\n","<div style='background: #dcfce7; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong style='color: #166534;'>Recommended:</strong> Use with M1 for webcam snapshots\n","</div>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Technical Details:</strong>\n","<ul style='margin: 8px 0; padding-left: 20px;'>\n","<li>Input: RGB 640×480 (M1) or RGB 224×224 (M2)</li>\n","<li>Feature dimension: 1280</li>\n","<li>Classifier: 1280 → 512 → 128 → 7</li>\n","</ul>\n","</div>\n","</div>\n","\"\"\",\n","\n","        'ConvNeXt-Tiny': \"\"\"\n","<div style='padding: 16px; background: #f9fafb; border-left: 4px solid #dc2626; border-radius: 8px; margin: 8px 0;'>\n","<h3 style='margin-top: 0; color: #1f2937;'>ConvNeXt-Tiny (CNN)</h3>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Architecture:</strong> Modernized CNN inspired by Vision Transformers\n","</div>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Performance:</strong>\n","<ul style='margin: 8px 0; padding-left: 20px;'>\n","<li>M1: Macro F1 = 0.2437 (AF)</li>\n","<li>M2: Macro F1 = 0.3439 (AF, +41.1% improvement)</li>\n","<li>Parameters: 28.6M (large)</li>\n","<li>Strength: Unique benefit from preprocessing</li>\n","<li>Weakness: Poor M1 performance</li>\n","</ul>\n","</div>\n","\n","<div style='background: #dcfce7; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong style='color: #166534;'>Recommended:</strong> Use with M2 (Preprocessed) for optimal results\n","</div>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Technical Details:</strong>\n","<ul style='margin: 8px 0; padding-left: 20px;'>\n","<li>Input: RGB 640×480 (M1) or RGB 224×224 (M2)</li>\n","<li>Feature dimension: 768</li>\n","<li>Classifier: 768 → 512 → 128 → 7</li>\n","</ul>\n","</div>\n","</div>\n","\"\"\",\n","\n","        'Vision Transformer (ViT)': \"\"\"\n","<div style='padding: 16px; background: #f9fafb; border-left: 4px solid #3b82f6; border-radius: 8px; margin: 8px 0;'>\n","<h3 style='margin-top: 0; color: #1f2937;'>Vision Transformer (ViT)</h3>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Architecture:</strong> Pure attention-based model with patch embeddings\n","</div>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Performance:</strong>\n","<ul style='margin: 8px 0; padding-left: 20px;'>\n","<li>M1: Macro F1 = 0.2298 (direct)</li>\n","<li>M2: Macro F1 = 0.2347 (preprocessed)</li>\n","<li>Parameters: 88M (large)</li>\n","<li>Strength: Global pattern analysis via self-attention</li>\n","<li>Weakness: Requires large datasets for optimal performance</li>\n","</ul>\n","</div>\n","\n","<div style='background: #dbeafe; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong style='color: #1e40af;'>Research Note:</strong> Baseline for attention mechanisms\n","</div>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Technical Details:</strong>\n","<ul style='margin: 8px 0; padding-left: 20px;'>\n","<li>Input: RGB 384×384 (M1) or RGB 224×224 (M2)</li>\n","<li>Patch size: 32×32</li>\n","<li>Feature dimension: 768</li>\n","<li>Classifier: 768 → 512 → 128 → 7</li>\n","</ul>\n","</div>\n","</div>\n","\"\"\",\n","\n","        'Swin Transformer': \"\"\"\n","<div style='padding: 16px; background: #f9fafb; border-left: 4px solid #3b82f6; border-radius: 8px; margin: 8px 0;'>\n","<h3 style='margin-top: 0; color: #1f2937;'>Swin Transformer</h3>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Architecture:</strong> Hierarchical vision transformer with shifted windows\n","</div>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Performance:</strong>\n","<ul style='margin: 8px 0; padding-left: 20px;'>\n","<li>M1: Macro F1 = 0.2619 (direct, best Transformer)</li>\n","<li>Parameters: 87M (large)</li>\n","<li>Strength: Window-based attention for local patterns</li>\n","<li>Weakness: Computational complexity</li>\n","</ul>\n","</div>\n","\n","<div style='background: #dcfce7; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong style='color: #166534;'>Recommended:</strong> Best Transformer for micro-expressions\n","</div>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Technical Details:</strong>\n","<ul style='margin: 8px 0; padding-left: 20px;'>\n","<li>Input: RGB 384×384 (M1) or RGB 224×224 (M2)</li>\n","<li>Window size: 7×7</li>\n","<li>Feature dimension: 768</li>\n","<li>Classifier: 768 → 512 → 128 → 7</li>\n","</ul>\n","</div>\n","</div>\n","\"\"\",\n","\n","        'PoolFormer': \"\"\"\n","<div style='padding: 16px; background: #f9fafb; border-left: 4px solid #3b82f6; border-radius: 8px; margin: 8px 0;'>\n","<h3 style='margin-top: 0; color: #1f2937;'>PoolFormer</h3>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Architecture:</strong> MetaFormer with simple pooling instead of attention\n","</div>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Performance:</strong>\n","<ul style='margin: 8px 0; padding-left: 20px;'>\n","<li>M1: Macro F1 = 0.1734 (multiframe)</li>\n","<li>Parameters: 73M (efficient)</li>\n","<li>Strength: Simple pooling-based token mixing</li>\n","<li>Weakness: Lower accuracy compared to other Transformers</li>\n","</ul>\n","</div>\n","\n","<div style='background: #dbeafe; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong style='color: #1e40af;'>Research Note:</strong> Comparison for pooling vs attention\n","</div>\n","\n","<div style='background: white; padding: 12px; border-radius: 6px; margin: 12px 0;'>\n","<strong>Technical Details:</strong>\n","<ul style='margin: 8px 0; padding-left: 20px;'>\n","<li>Input: RGB 384×384 (M1) or RGB 224×224 (M2)</li>\n","<li>Pooling: 3×3 average pooling</li>\n","<li>Feature dimension: 768</li>\n","<li>Classifier: 768 → 512 → 128 → 7</li>\n","</ul>\n","</div>\n","</div>\n","\"\"\"\n","    }\n","\n","    return descriptions.get(model_name, \"<p>No description available</p>\")\n","\n","\n","print(\"Enhanced model descriptions defined for 6 architectures\")\n","\n","# ============================================================================\n","# SECTION 2: CUSTOM CSS FOR TEXT ALIGNMENT FIX\n","# ============================================================================\n","\n","print(\"\\n[2/4] Defining custom CSS...\")\n","\n","# CSS to remove extra left margin/padding from labels and info text\n","MER_CUSTOM_CSS = \"\"\"\n","/* Fix text alignment - remove extra left margin */\n",".align-left .block-label,\n",".align-left .info {\n","    margin-left: 8px !important;\n","    padding-left: 0 !important;\n","}\n","\n","/* Ensure radio buttons themselves stay properly indented */\n",".align-left .wrap {\n","    padding-left: 8px;\n","}\n","\"\"\"\n","\n","print(\"Custom CSS defined for text alignment\")\n","\n","# ============================================================================\n","# SECTION 3: MER INTERFACE LAYOUT\n","# ============================================================================\n","\n","print(\"\\n[3/4] Building MER interface\")\n","\n","with gr.Blocks(css=MER_CUSTOM_CSS) as mer_interface:\n","    gr.Markdown(\"\"\"\n","    ## Micro-Expression Recognition (MER) Analysis\n","\n","    Capture your facial expression using webcam for real-time classification.\n","    \"\"\")\n","\n","    with gr.Row():\n","        # Left column - Configuration\n","        with gr.Column(scale=1):\n","            gr.Markdown(\"### Configuration\")\n","\n","            # Model Selection (CSS class applied)\n","            with gr.Group():\n","                mer_model_select = gr.Radio(\n","                    choices=[\n","                        'MobileNetV3-Small',\n","                        'EfficientNet-B0',\n","                        'ConvNeXt-Tiny',\n","                        'Vision Transformer (ViT)',\n","                        'Swin Transformer',\n","                        'PoolFormer'\n","                    ],\n","                    value='MobileNetV3-Small',\n","                    label=\"Model Architecture\",\n","                    info=\"Select CNN or Transformer model\",\n","                    elem_classes=\"align-left\"\n","                )\n","\n","            # Methodology Selection (CSS class applied)\n","            with gr.Group():\n","                mer_methodology_select = gr.Radio(\n","                    choices=[\n","                        'M1 (Raw RGB 640×480)',\n","                        'M2 (Preprocessed Grayscale 224×224)'\n","                    ],\n","                    value='M1 (Raw RGB 640×480)',\n","                    label=\"Preprocessing Methodology\",\n","                    info=\"M1 for MobileNet/EfficientNet, M2 for ConvNeXt\",\n","                    elem_classes=\"align-left\"\n","                )\n","\n","            # Classify Button\n","            mer_classify_btn = gr.Button(\n","                \"Classify Expression\",\n","                variant=\"primary\",\n","                size=\"lg\"\n","            )\n","\n","            # Evaluation Info\n","            with gr.Accordion(\"Evaluation\", open=True):\n","                gr.Markdown(\"\"\"\n","                **Evaluation Phase:** Apex Frame (single snapshot)\n","\n","                **Processing:** Check terminal for detailed progress\n","\n","                **Dataset:** CASME II (7 emotion categories)\n","                \"\"\")\n","\n","            # Model Info Display (Open by default with styled cards)\n","            with gr.Accordion(\"Model Information\", open=True):\n","                mer_model_info = gr.HTML(\n","                    value=get_mer_model_description('MobileNetV3-Small'),\n","                    elem_id=\"mer_model_info_display\"\n","                )\n","\n","            # Research Findings\n","            with gr.Accordion(\"Research Findings\", open=False):\n","                gr.HTML(\"\"\"\n","                <div style='padding: 12px;'>\n","                <h3 style='color: #1f2937; margin-top: 0;'>Key Research Findings</h3>\n","\n","                <div style='background: #fef2f2; padding: 12px; border-radius: 8px; margin: 12px 0; border-left: 4px solid #dc2626;'>\n","                <strong style='color: #991b1b;'>Preprocessing Paradox Discovery:</strong>\n","                <ul style='margin: 8px 0; padding-left: 20px; color: #374151;'>\n","                <li>MobileNetV3 M1→M2: -47.5% degradation</li>\n","                <li>EfficientNet M1→M2: -33.7% degradation</li>\n","                <li>ConvNeXt M1→M2: +41.1% improvement (unique)</li>\n","                </ul>\n","                </div>\n","\n","                <div style='background: #dcfce7; padding: 12px; border-radius: 8px; margin: 12px 0; border-left: 4px solid #059669;'>\n","                <strong style='color: #047857;'>Best Model Combinations:</strong>\n","                <ol style='margin: 8px 0; padding-left: 20px; color: #374151;'>\n","                <li>MobileNetV3 + M1: F1 = 0.3880 (Best Overall)</li>\n","                <li>ConvNeXt + M2: F1 = 0.3439 (Best M2)</li>\n","                <li>EfficientNet + M1: F1 = 0.3278</li>\n","                <li>Swin Transformer + M1: F1 = 0.2619 (Best Transformer)</li>\n","                <li>ViT + M2: F1 = 0.2347</li>\n","                </ol>\n","                </div>\n","\n","                <div style='background: #dbeafe; padding: 12px; border-radius: 8px; margin: 12px 0; border-left: 4px solid #3b82f6;'>\n","                <strong style='color: #1e40af;'>Transformer Insights:</strong>\n","                <ul style='margin: 8px 0; padding-left: 20px; color: #374151;'>\n","                <li>Swin Transformer: Best Transformer performance (0.2619)</li>\n","                <li>ViT: Global attention patterns (0.2298)</li>\n","                <li>PoolFormer: Pooling-based alternative (0.1734)</li>\n","                <li>All Transformers lag behind best CNN (MobileNetV3)</li>\n","                </ul>\n","                </div>\n","\n","                <div style='background: #fef3c7; padding: 12px; border-radius: 8px; margin: 12px 0; border-left: 4px solid #f59e0b;'>\n","                <strong style='color: #92400e;'>Dataset Challenges:</strong>\n","                <ul style='margin: 8px 0; padding-left: 20px; color: #374151;'>\n","                <li>Training samples: 201</li>\n","                <li>Test samples: 26</li>\n","                <li>Class imbalance: 49.5:1 (Others vs Fear)</li>\n","                <li>Rare emotions: Fear (0 test), Sadness (1 test)</li>\n","                </ul>\n","                </div>\n","\n","                <div style='background: #f3f4f6; padding: 12px; border-radius: 8px; margin: 12px 0;'>\n","                <strong style='color: #1f2937;'>Publication Status:</strong>\n","                <p style='margin: 8px 0; color: #374151;'>\n","                IEEE ICICyTA 2025 (Accepted)<br>\n","                Paper: \"Preprocessing Paradox in Vision Transformer-Based Micro-Expression Recognition\"\n","                </p>\n","                </div>\n","                </div>\n","                \"\"\")\n","\n","            # Methodology Comparison\n","            with gr.Accordion(\"Methodology Comparison\", open=False):\n","                gr.HTML(\"\"\"\n","                <div style='padding: 12px;'>\n","                <h3 style='color: #1f2937; margin-top: 0;'>Methodology Comparison</h3>\n","\n","                <div style='background: white; padding: 16px; border-radius: 8px; margin: 12px 0; border: 2px solid #dc2626;'>\n","                <h4 style='color: #dc2626; margin-top: 0;'>M1 (Raw RGB)</h4>\n","\n","                <div style='background: #f9fafb; padding: 12px; border-radius: 6px; margin: 8px 0;'>\n","                <strong>Pipeline:</strong>\n","                <ol style='margin: 8px 0; padding-left: 20px;'>\n","                <li>Minimal preprocessing</li>\n","                <li>CNN: Center crop to 640×480 (4:3 ratio)</li>\n","                <li>Transformer: Center crop to 384×384 (square)</li>\n","                <li>RGB normalization (ImageNet stats)</li>\n","                </ol>\n","                </div>\n","\n","                <div style='background: #f9fafb; padding: 12px; border-radius: 6px; margin: 8px 0;'>\n","                <strong>Characteristics:</strong>\n","                <ul style='margin: 8px 0; padding-left: 20px;'>\n","                <li>Input pixels: CNN 307K, Transformer 147K</li>\n","                <li>Face centering: 84% approximate</li>\n","                <li>Best for: MobileNetV3, EfficientNet, Swin Transformer</li>\n","                </ul>\n","                </div>\n","                </div>\n","\n","                <div style='background: white; padding: 16px; border-radius: 8px; margin: 12px 0; border: 2px solid #3b82f6;'>\n","                <h4 style='color: #3b82f6; margin-top: 0;'>M2 (Preprocessed)</h4>\n","\n","                <div style='background: #f9fafb; padding: 12px; border-radius: 6px; margin: 8px 0;'>\n","                <strong>Pipeline:</strong>\n","                <ol style='margin: 8px 0; padding-left: 20px;'>\n","                <li>Face detection (Dlib)</li>\n","                <li>Bounding box expansion (+20px)</li>\n","                <li>Crop + resize to 224×224</li>\n","                <li>RGB normalization (ImageNet stats)</li>\n","                </ol>\n","                </div>\n","\n","                <div style='background: #f9fafb; padding: 12px; border-radius: 6px; margin: 8px 0;'>\n","                <strong>Characteristics:</strong>\n","                <ul style='margin: 8px 0; padding-left: 20px;'>\n","                <li>Input pixels: 50K (-84% vs M1 CNN)</li>\n","                <li>Face centering: 100% (when detected)</li>\n","                <li>Best for: ConvNeXt</li>\n","                <li>Degrades: MobileNetV3, EfficientNet</li>\n","                </ul>\n","                </div>\n","                </div>\n","                </div>\n","                \"\"\")\n","\n","        # Right column - Input and Results\n","        with gr.Column(scale=1):\n","            gr.Markdown(\"### Webcam Input\")\n","\n","            mer_webcam_input = gr.Image(\n","                sources=[\"webcam\"],\n","                type=\"pil\",\n","                label=\"Capture Your Expression\",\n","                height=400\n","            )\n","\n","            gr.Markdown(\"\"\"\n","            **Instructions:**\n","            1. Click camera icon to activate webcam\n","            2. Position your face in frame\n","            3. Capture snapshot of your expression\n","            4. Select model and methodology\n","            5. Click \"Classify Expression\"\n","            \"\"\")\n","\n","            gr.Markdown(\"### Classification Results\")\n","\n","            mer_preprocessing_comparison = gr.Image(\n","                label=\"Preprocessing Comparison\",\n","                height=300\n","            )\n","\n","            mer_prediction_output = gr.Markdown(\n","                value=\"### Micro-Expression Classification Results\\n\\nCapture an image and click 'Classify Expression' to see results.\"\n","            )\n","\n","            mer_probabilities_output = gr.Label(\n","                label=\"Emotion Probabilities (7 Classes)\",\n","                num_top_classes=7\n","            )\n","\n","    # Event Handlers\n","    mer_model_select.change(\n","        fn=get_mer_model_description,\n","        inputs=[mer_model_select],\n","        outputs=[mer_model_info]\n","    )\n","\n","    mer_classify_btn.click(\n","        fn=mer_predict_pipeline,\n","        inputs=[\n","            mer_webcam_input,\n","            mer_model_select,\n","            mer_methodology_select\n","        ],\n","        outputs=[\n","            mer_preprocessing_comparison,\n","            mer_prediction_output,\n","            mer_probabilities_output\n","        ]\n","    )\n","\n","print(\"MER interface with CSS fix complete\")\n","\n","# ============================================================================\n","# SECTION 4: VERIFICATION\n","# ============================================================================\n","\n","print(\"\\n[4/4] Interface verification...\")\n","print(\"Model options: 6 (3 CNN + 3 Transformer)\")\n","print(\"Methodology options: 2 (M1, M2)\")\n","print(\"Total configurations: 12\")\n","print(\"CSS fixes applied:\")\n","print(\"  - Custom CSS removes extra left margin from labels\")\n","print(\"  - Text alignment: margin-left: 0, padding-left: 0\")\n","print(\"  - Model Information accordion: open by default\")\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"CELL 7 COMPLETE - MER INTERFACE READY (FINAL)\")\n","print(\"=\" * 80)\n","print(\"\\nFixed issues:\")\n","print(\"  - Text perfectly aligned (no left indent)\")\n","print(\"  - Model Information visible and styled\")\n","print(\"  - All accordions functional\")\n","print(\"  - CSS-based solution for clean styling\")\n","print(\"\\nNext: Cell 8 launch demo\")\n","print(\"=\" * 80)"],"metadata":{"collapsed":true,"cellView":"form","id":"WvcFoME1rjOQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767432133763,"user_tz":-420,"elapsed":532,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"bdf0b030-ef20-4c36-d5a8-8b76c3e19194"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","MER INTERFACE CONFIGURATION - CSS FIXED\n","================================================================================\n","\n","[1/4] Defining enhanced model descriptions...\n","Enhanced model descriptions defined for 6 architectures\n","\n","[2/4] Defining custom CSS...\n","Custom CSS defined for text alignment\n","\n","[3/4] Building MER interface\n","MER interface with CSS fix complete\n","\n","[4/4] Interface verification...\n","Model options: 6 (3 CNN + 3 Transformer)\n","Methodology options: 2 (M1, M2)\n","Total configurations: 12\n","CSS fixes applied:\n","  - Custom CSS removes extra left margin from labels\n","  - Text alignment: margin-left: 0, padding-left: 0\n","  - Model Information accordion: open by default\n","\n","================================================================================\n","CELL 7 COMPLETE - MER INTERFACE READY (FINAL)\n","================================================================================\n","\n","Fixed issues:\n","  - Text perfectly aligned (no left indent)\n","  - Model Information visible and styled\n","  - All accordions functional\n","  - CSS-based solution for clean styling\n","\n","Next: Cell 8 launch demo\n","================================================================================\n"]}]},{"cell_type":"code","source":["# @title Cell 8: Launch Integrated Demo\n","\n","# File: MMLab_Project_Demo.ipynb - Cell 8\n","# Location: Thesis_MER_Project/MMLab_Project_Demo.ipynb\n","# Purpose: Launch demo with proper attention visualization\n","\n","print(\"=\" * 80)\n","print(\"LAUNCHING INTEGRATED DEMO\")\n","print(\"=\" * 80)\n","\n","# ============================================================================\n","# SECTION 1: VERIFY AND DEFINE REQUIRED VARIABLES\n","# ============================================================================\n","\n","print(\"\\n[1/5] Verifying system variables...\")\n","\n","if 'PROJECT_INFO' not in globals():\n","    PROJECT_INFO = {\n","        'lab_name': 'Multimedia Laboratory',\n","        'university': 'Telkom University',\n","        'location': 'Bandung, Indonesia',\n","        'primary_color': '#C41E3A'\n","    }\n","    print(\"PROJECT_INFO defined\")\n","\n","if 'MMLAB_THEME' not in globals():\n","    MMLAB_THEME = gr.themes.Soft(\n","        primary_hue=\"red\",\n","        secondary_hue=\"slate\",\n","        neutral_hue=\"slate\",\n","        font=gr.themes.GoogleFont(\"Inter\")\n","    )\n","    print(\"MMLAB_THEME defined\")\n","\n","required_functions = ['her2_predict_pipeline', 'mer_predict_pipeline']\n","missing_functions = [f for f in required_functions if f not in globals()]\n","\n","if missing_functions:\n","    print(f\"WARNING: Missing functions: {', '.join(missing_functions)}\")\n","    raise RuntimeError(\"Required functions not available\")\n","\n","print(\"System variables verified\")\n","\n","# ============================================================================\n","# SECTION 2: MAPPING FUNCTIONS\n","# ============================================================================\n","\n","print(\"\\n[2/5] Defining mapping functions...\")\n","\n","def map_her2_model_display_to_key(display_name):\n","    mapping = {\n","        'MobileNetV3-Large': 'MobileNetV3',\n","        'Vision Transformer (ViT)': 'ViT',\n","        'Fusion (Concatenation)': 'FusionConcat',\n","        'Fusion (Addition)': 'FusionAddition'\n","    }\n","    return mapping.get(display_name, display_name)\n","\n","def map_her2_preprocessing_display_to_key(display_name):\n","    mapping = {\n","        'Standard': 'Standard Preprocessing',\n","        'Medical-Optimized': 'Medical Preprocessing'\n","    }\n","    return mapping.get(display_name, display_name)\n","\n","def map_her2_task_display_to_key(display_name):\n","    mapping = {\n","        'IHC Score (0-3)': 'IHC',\n","        'HER2 Status (Neg/Pos)': 'HER2'\n","    }\n","    return mapping.get(display_name, display_name)\n","\n","def map_mer_model_display_to_key(display_name):\n","    mapping = {\n","        'MobileNetV3-Small': 'MobileNetV3',\n","        'EfficientNet-B0': 'EfficientNet',\n","        'ConvNeXt-Tiny': 'ConvNeXt',\n","        'Vision Transformer (ViT)': 'ViT',\n","        'Swin Transformer': 'SwinTransformer',\n","        'PoolFormer': 'PoolFormer'\n","    }\n","    return mapping.get(display_name, display_name)\n","\n","def map_mer_methodology_display_to_key(display_name):\n","    if 'M1' in display_name:\n","        return 'M1'\n","    elif 'M2' in display_name:\n","        return 'M2'\n","    return display_name\n","\n","print(\"Mapping functions defined\")\n","\n","# ============================================================================\n","# SECTION 3: WRAPPER FUNCTIONS\n","# ============================================================================\n","\n","print(\"\\n[3/5] Defining wrapper functions...\")\n","\n","def her2_predict_with_mapping(image, preprocessing_display, model_display, task_display):\n","    preprocessing_key = map_her2_preprocessing_display_to_key(preprocessing_display)\n","    model_key = map_her2_model_display_to_key(model_display)\n","    task_key = map_her2_task_display_to_key(task_display)\n","\n","    print(f\"\\nHER2 Mapping:\")\n","    print(f\"  Preprocessing: {preprocessing_display} -> {preprocessing_key}\")\n","    print(f\"  Model: {model_display} -> {model_key}\")\n","    print(f\"  Task: {task_display} -> {task_key}\")\n","\n","    return her2_predict_pipeline(image, preprocessing_key, model_key, task_key)\n","\n","def mer_predict_with_mapping(image, model_display, methodology_display):\n","    model_key = map_mer_model_display_to_key(model_display)\n","    methodology_key = map_mer_methodology_display_to_key(methodology_display)\n","\n","    print(f\"\\nMER Mapping:\")\n","    print(f\"  Model: {model_display} -> {model_key}\")\n","    print(f\"  Methodology: {methodology_display} -> {methodology_key}\")\n","\n","    return mer_predict_pipeline(image, model_key, methodology_key)\n","\n","print(\"Wrapper functions defined\")\n","\n","# ============================================================================\n","# SECTION 4: INTEGRATED DEMO INTERFACE\n","# ============================================================================\n","\n","print(\"\\n[4/5] Building integrated interface...\")\n","\n","with gr.Blocks(\n","    theme=MMLAB_THEME,\n","    title=\"Multimedia Laboratory - Research Showcase\"\n",") as demo:\n","\n","    # ========================================================================\n","    # TAB 0: LANDING PAGE\n","    # ========================================================================\n","\n","    with gr.Tab(\"Projects Overview\", id=0):\n","        gr.HTML(f\"\"\"\n","        <div style='text-align: center; padding: 48px 24px; background: linear-gradient(135deg, {PROJECT_INFO['primary_color']} 0%, #991b1b 100%); border-radius: 16px; margin-bottom: 32px;'>\n","            <h1 style='font-size: 42px; font-weight: 800; color: white; margin-bottom: 16px; text-shadow: 2px 2px 4px rgba(0,0,0,0.2);'>\n","                {PROJECT_INFO['lab_name']}\n","            </h1>\n","            <p style='font-size: 18px; color: rgba(255,255,255,0.95); margin-bottom: 8px;'>\n","                {PROJECT_INFO['university']}\n","            </p>\n","            <p style='font-size: 16px; color: rgba(255,255,255,0.85);'>\n","                Advancing AI Research in Medical Imaging and Affective Computing\n","            </p>\n","        </div>\n","        \"\"\")\n","\n","        gr.Markdown(\"## Featured Research Projects\")\n","\n","        with gr.Row():\n","            with gr.Column(scale=1):\n","                gr.Markdown(\"\"\"\n","                ### HER2 Status Classification\n","\n","                **Domain:** Medical Imaging (Gastroesophageal Cancer)\n","\n","                **Models:** 4 architectures with dual preprocessing\n","\n","                **Innovation:** Medical-optimized preprocessing and fusion architectures\n","\n","                **Status:** Under Review - International Conference\n","                \"\"\")\n","\n","                her2_demo_btn = gr.Button(\"Launch HER2 Demo\", variant=\"primary\", size=\"lg\")\n","\n","            with gr.Column(scale=1):\n","                gr.Markdown(\"\"\"\n","                ### Micro-Expression Recognition (MER)\n","\n","                **Domain:** Affective Computing (Emotion Analysis)\n","\n","                **Models:** 6 architectures with dual methodologies\n","\n","                **Innovation:** Preprocessing paradox discovery\n","\n","                **Status:** Accepted - IEEE ICICyTA 2025\n","                \"\"\")\n","\n","                mer_demo_btn = gr.Button(\"Launch MER Demo\", variant=\"primary\", size=\"lg\")\n","\n","    # ========================================================================\n","    # TAB 1: HER2 DEMO (ATTENTION VISUALIZATION\n","    # ========================================================================\n","\n","    with gr.Tab(\"HER2 Classification\", id=1):\n","        gr.Markdown(\"\"\"\n","        ## HER2 Status Classification for Gastroesophageal Cancer\n","\n","        Upload histopathology tissue microarray (TMA) images for automated HER2 status prediction.\n","        \"\"\")\n","\n","        with gr.Row():\n","            with gr.Column(scale=1):\n","                gr.Markdown(\"### Configuration\")\n","\n","                with gr.Group():\n","                    her2_preprocessing_select = gr.Radio(\n","                        choices=['Standard', 'Medical-Optimized'],\n","                        value='Medical-Optimized',\n","                        label=\"Preprocessing Strategy\",\n","                        info=\"Medical-Optimized includes CLAHE enhancement\"\n","                    )\n","\n","                with gr.Group():\n","                    her2_model_select = gr.Radio(\n","                        choices=[\n","                            'MobileNetV3-Large',\n","                            'Vision Transformer (ViT)',\n","                            'Fusion (Concatenation)',\n","                            'Fusion (Addition)'\n","                        ],\n","                        value='Fusion (Concatenation)',\n","                        label=\"Model Architecture\",\n","                        info=\"Fusion models combine CNN and Transformer\"\n","                    )\n","\n","                with gr.Group():\n","                    her2_task_select = gr.Radio(\n","                        choices=['IHC Score (0-3)', 'HER2 Status (Neg/Pos)'],\n","                        value='HER2 Status (Neg/Pos)',\n","                        label=\"Classification Task\",\n","                        info=\"IHC intensity or final HER2 status\"\n","                    )\n","\n","                her2_classify_btn = gr.Button(\n","                    \"Classify Image\",\n","                    variant=\"primary\",\n","                    size=\"lg\"\n","                )\n","\n","                with gr.Accordion(\"System Status\", open=True):\n","                    gr.Markdown(\"\"\"\n","                    **Ready:** All models loaded and cached\n","\n","                    **Processing:** Check terminal for progress\n","\n","                    **Dataset:** TMA images from gastroesophageal adenocarcinoma\n","                    \"\"\")\n","\n","            with gr.Column(scale=1):\n","                gr.Markdown(\"### Image Input\")\n","\n","                her2_image_input = gr.Image(\n","                    type=\"pil\",\n","                    label=\"Upload Histopathology TMA Image\",\n","                    height=400\n","                )\n","\n","                gr.Markdown(\"### Results\")\n","\n","                her2_preprocessing_comparison = gr.Image(\n","                    label=\"Preprocessing Comparison (Original vs Processed)\",\n","                    height=250\n","                )\n","\n","                her2_prediction_output = gr.Markdown(\n","                    value=\"### Prediction Results\\n\\nUpload image and click 'Classify Image'.\"\n","                )\n","\n","                her2_probabilities_output = gr.Label(\n","                    label=\"Class Probabilities\",\n","                    num_top_classes=4\n","                )\n","\n","                gr.Markdown(\"### Attention Visualization\")\n","\n","                # FIXED: Single row with conditional visibility\n","                with gr.Row() as her2_attention_row:\n","                    her2_attention_1 = gr.Image(\n","                        label=\"Attention Map\",\n","                        height=300\n","                    )\n","                    her2_attention_2 = gr.Image(\n","                        label=\"Branch Attention 2\",\n","                        visible=False,\n","                        height=300\n","                    )\n","                    her2_attention_3 = gr.Image(\n","                        label=\"Fusion Attention\",\n","                        visible=False,\n","                        height=300\n","                    )\n","\n","        # HER2 Event Handler (FIXED - proper output mapping)\n","        her2_classify_btn.click(\n","            fn=her2_predict_with_mapping,\n","            inputs=[\n","                her2_image_input,\n","                her2_preprocessing_select,\n","                her2_model_select,\n","                her2_task_select\n","            ],\n","            outputs=[\n","                her2_preprocessing_comparison,\n","                her2_prediction_output,\n","                her2_probabilities_output,\n","                her2_attention_1,\n","                her2_attention_2,\n","                her2_attention_3,\n","                her2_attention_row,\n","                gr.Markdown(),  # label_1 (not displayed, placeholder)\n","                gr.Markdown(),  # label_2 (not displayed, placeholder)\n","                gr.Markdown()   # label_3 (not displayed, placeholder)\n","            ]\n","        )\n","\n","    # ========================================================================\n","    # TAB 2: MER DEMO\n","    # ========================================================================\n","\n","    with gr.Tab(\"MER Analysis\", id=2):\n","        gr.Markdown(\"\"\"\n","        ## Micro-Expression Recognition Analysis\n","\n","        Capture facial expression using webcam for real-time emotion classification.\n","        \"\"\")\n","\n","        with gr.Row():\n","            with gr.Column(scale=1):\n","                gr.Markdown(\"### Configuration\")\n","\n","                with gr.Group():\n","                    mer_model_select = gr.Radio(\n","                        choices=[\n","                            'MobileNetV3-Small',\n","                            'EfficientNet-B0',\n","                            'ConvNeXt-Tiny',\n","                            'Vision Transformer (ViT)',\n","                            'Swin Transformer',\n","                            'PoolFormer'\n","                        ],\n","                        value='MobileNetV3-Small',\n","                        label=\"Model Architecture\"\n","                    )\n","\n","                with gr.Group():\n","                    mer_methodology_select = gr.Radio(\n","                        choices=[\n","                            'M1 (Raw RGB 640x480)',\n","                            'M2 (Preprocessed Gray Scale 224x224)'\n","                        ],\n","                        value='M1 (Raw RGB 640x480)',\n","                        label=\"Preprocessing Methodology\"\n","                    )\n","\n","                mer_classify_btn = gr.Button(\n","                    \"Classify Expression\",\n","                    variant=\"primary\",\n","                    size=\"lg\"\n","                )\n","\n","                with gr.Accordion(\"Evaluation Info\", open=True):\n","                    gr.Markdown(\"\"\"\n","                    **Evaluation:** Apex Frame (single snapshot)\n","\n","                    **Dataset:** CASME II (7 emotion categories)\n","                    \"\"\")\n","\n","            with gr.Column(scale=1):\n","                gr.Markdown(\"### Webcam Input\")\n","\n","                mer_image_input = gr.Image(\n","                    sources=[\"upload\", \"webcam\"],\n","                    type=\"pil\",\n","                    label=\"Capture Your Expression\",\n","                    height=400\n","                )\n","\n","                gr.Markdown(\"\"\"\n","                **Instructions:**\n","                1. Activate webcam\n","                2. Position face in frame\n","                3. Capture snapshot\n","                4. Click 'Classify Expression'\n","                \"\"\")\n","\n","                gr.Markdown(\"### Classification Results\")\n","\n","                mer_preprocessing_comparison = gr.Image(\n","                    label=\"Preprocessing Comparison\",\n","                    height=250\n","                )\n","\n","                mer_prediction_output = gr.Markdown(\n","                    value=\"### Results\\n\\nCapture image and classify.\"\n","                )\n","\n","                mer_probabilities_output = gr.Label(\n","                    label=\"Emotion Probabilities (7 Classes)\",\n","                    num_top_classes=7\n","                )\n","\n","        # MER Event Handler\n","        mer_classify_btn.click(\n","            fn=mer_predict_with_mapping,\n","            inputs=[\n","                mer_image_input,\n","                mer_model_select,\n","                mer_methodology_select\n","            ],\n","            outputs=[\n","                mer_preprocessing_comparison,\n","                mer_prediction_output,\n","                mer_probabilities_output\n","            ]\n","        )\n","\n","    # Navigation\n","    her2_demo_btn.click(fn=lambda: gr.Tabs(selected=1), inputs=None, outputs=None)\n","    mer_demo_btn.click(fn=lambda: gr.Tabs(selected=2), inputs=None, outputs=None)\n","\n","print(\"Interface built\")\n","\n","# ============================================================================\n","# SECTION 5: LAUNCH\n","# ============================================================================\n","\n","print(\"\\n[5/5] Launching demo...\")\n","\n","print(\"\\nSystem Configuration:\")\n","print(\"  - HER2: 4 models with attention visualization\")\n","print(\"  - MER: 6 models (3 CNN + 3 Transformer)\")\n","print(\"  - Attention maps: Now properly rendered\")\n","\n","demo.launch(share=True, debug=False, show_error=True)\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"DEMO LAUNCHED - ATTENTION VISUALIZATION FIXED\")\n","print(\"=\" * 80)\n","print(\"\\nFixed: Attention images now visible when generated\")\n","print(\"Next: Test HER2 attention rendering, then MER system\")\n","print(\"=\" * 80)"],"metadata":{"collapsed":true,"cellView":"form","id":"Tas4CeCasvix","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1767432136774,"user_tz":-420,"elapsed":3009,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"7f7a0e3f-548d-4aff-bf6d-dca6ede48635"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","LAUNCHING INTEGRATED DEMO\n","================================================================================\n","\n","[1/5] Verifying system variables...\n","System variables verified\n","\n","[2/5] Defining mapping functions...\n","Mapping functions defined\n","\n","[3/5] Defining wrapper functions...\n","Wrapper functions defined\n","\n","[4/5] Building integrated interface...\n","Interface built\n","\n","[5/5] Launching demo...\n","\n","System Configuration:\n","  - HER2: 4 models with attention visualization\n","  - MER: 6 models (3 CNN + 3 Transformer)\n","  - Attention maps: Now properly rendered\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://6a3f07b95fffbaf1e7.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://6a3f07b95fffbaf1e7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","DEMO LAUNCHED - ATTENTION VISUALIZATION FIXED\n","================================================================================\n","\n","Fixed: Attention images now visible when generated\n","Next: Test HER2 attention rendering, then MER system\n","================================================================================\n"]}]}]}