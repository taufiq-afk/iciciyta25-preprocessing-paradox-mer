{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOxaFtPndPQSBRAKC0lp4oD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"45985dbc79544733aaa138eee17710fd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4fee02a885474210bd94090fe599e236","IPY_MODEL_46b981fef03a4208bfe29d6f860ad94e","IPY_MODEL_29c1eddb35a24f9c899626d478a2b889"],"layout":"IPY_MODEL_d0c137d6562f4d8a97e74c46562aef97"}},"4fee02a885474210bd94090fe599e236":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_908795c7146e4aee84158519d6eb0648","placeholder":"​","style":"IPY_MODEL_a3d59b2177d04c089722eca55062b3ee","value":"preprocessor_config.json: 100%"}},"46b981fef03a4208bfe29d6f860ad94e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd5629798e6444d5854a2a64251906f9","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a2bf0efa9f3e4177ae26f40432ed1f98","value":160}},"29c1eddb35a24f9c899626d478a2b889":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a5aabe586794e91bba4116963684d20","placeholder":"​","style":"IPY_MODEL_173619bec73b4643ac9574d1327cff9f","value":" 160/160 [00:00&lt;00:00, 17.6kB/s]"}},"d0c137d6562f4d8a97e74c46562aef97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"908795c7146e4aee84158519d6eb0648":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3d59b2177d04c089722eca55062b3ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd5629798e6444d5854a2a64251906f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2bf0efa9f3e4177ae26f40432ed1f98":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a5aabe586794e91bba4116963684d20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"173619bec73b4643ac9574d1327cff9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d91764b896624ad9a94b9cd24b2bc11a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_191c70876b864461917b48436f276b56","IPY_MODEL_9e67e1b169f94adeb7174a0e39cd07cc","IPY_MODEL_5cbc3d4070fe472bbab271db6dde2b86"],"layout":"IPY_MODEL_487b781e2fba43e4b92038744747b906"}},"191c70876b864461917b48436f276b56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6991a161b3d4102ae4cb147394e2f08","placeholder":"​","style":"IPY_MODEL_e062562263cd4c20be17fba408660eb3","value":"config.json: 100%"}},"9e67e1b169f94adeb7174a0e39cd07cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_49b9ea25724144ac97c5e7e00ca7b596","max":502,"min":0,"orientation":"horizontal","style":"IPY_MODEL_49900e478ff0437ead578bcb579edda2","value":502}},"5cbc3d4070fe472bbab271db6dde2b86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf623e3bbd8d41c39976fb9fe66dec82","placeholder":"​","style":"IPY_MODEL_5d1da8ff6b8946ee97f51074bcfc0e39","value":" 502/502 [00:00&lt;00:00, 62.4kB/s]"}},"487b781e2fba43e4b92038744747b906":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6991a161b3d4102ae4cb147394e2f08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e062562263cd4c20be17fba408660eb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49b9ea25724144ac97c5e7e00ca7b596":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49900e478ff0437ead578bcb579edda2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf623e3bbd8d41c39976fb9fe66dec82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d1da8ff6b8946ee97f51074bcfc0e39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92f673b827d742d2b32bfd2b1b671ace":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96106d47e7454144846cdad3de397553","IPY_MODEL_f471599d11aa4b84afbff9514b76453e","IPY_MODEL_131940333f6941cfb4de8574ecd7bde0"],"layout":"IPY_MODEL_0880e1688d524fccbd73266306d382a0"}},"96106d47e7454144846cdad3de397553":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ae237caa5db4c3dbf6d7fdbc69dd451","placeholder":"​","style":"IPY_MODEL_10e04fb8fd4c4922b91a488855367574","value":"model.safetensors: 100%"}},"f471599d11aa4b84afbff9514b76453e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e60ab12ce1e4de3b039cc866f669b84","max":352205741,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86cbb72af4a44dbe80dc49ced489d857","value":352205741}},"131940333f6941cfb4de8574ecd7bde0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f75745b5c334c03b566e5ebebb64ed0","placeholder":"​","style":"IPY_MODEL_b488456f4a6b47cd92f26f9ffc549f83","value":" 352M/352M [00:12&lt;00:00, 33.0MB/s]"}},"0880e1688d524fccbd73266306d382a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ae237caa5db4c3dbf6d7fdbc69dd451":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10e04fb8fd4c4922b91a488855367574":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e60ab12ce1e4de3b039cc866f669b84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86cbb72af4a44dbe80dc49ced489d857":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f75745b5c334c03b566e5ebebb64ed0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b488456f4a6b47cd92f26f9ffc549f83":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# @title Cell 1: ViT Transfer Learning Infrastructure Configuration (Fixed & Optimized)\n","\n","# File: 05_01_ViT_RAF-DB_CASME2-AF.ipynb - Cell 1\n","# Location: experiments/05_01_ViT_RAF-DB_CASME2-AF.ipynb\n","# Purpose: Transfer learning from RAF-DB macro-expressions to CASME2 micro-expressions (Apex Frame)\n","#          with optimized loss functions and class weights\n","\n","from google.colab import drive\n","print(\"=\" * 70)\n","print(\"VIT TRANSFER LEARNING: RAF-DB → CASME2 APEX FRAME (OPTIMIZED)\")\n","print(\"=\" * 70)\n","print(\"\\n[1] Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"    Google Drive mounted successfully\")\n","\n","print(\"\\n[2] Importing required libraries...\")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import timm\n","import json\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import time\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Project paths configuration\n","PROJECT_ROOT = \"/content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project\"\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"MODEL VARIANT CONFIGURATION\")\n","print(\"=\" * 70)\n","\n","# ViT Model Variant Selection\n","VIT_MODEL_VARIANT = 'patch32'\n","\n","if VIT_MODEL_VARIANT == 'patch16':\n","    VIT_MODEL_NAME = 'google/vit-base-patch16-224-in21k'\n","    PATCH_SIZE = 16\n","    print(\"\\nSelected: ViT-Base Patch16\")\n","    print(\"  Characteristics: Fine-grained feature extraction\")\n","    print(\"  Tokens at 384px: 576 tokens (24×24 grid)\")\n","    print(\"  Best for: Subtle micro-expression details\")\n","elif VIT_MODEL_VARIANT == 'patch32':\n","    VIT_MODEL_NAME = 'google/vit-base-patch32-224-in21k'\n","    PATCH_SIZE = 32\n","    print(\"\\nSelected: ViT-Base Patch32\")\n","    print(\"  Characteristics: Efficient feature extraction\")\n","    print(\"  Tokens at 384px: 144 tokens (12×12 grid)\")\n","    print(\"  Best for: Balanced performance and speed\")\n","else:\n","    raise ValueError(f\"Invalid VIT_MODEL_VARIANT: {VIT_MODEL_VARIANT}. Use 'patch16' or 'patch32'\")\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"TRANSFER LEARNING STAGE CONFIGURATION\")\n","print(\"=\" * 70)\n","\n","# Training stage toggle\n","TRAINING_STAGE = 'finetune'\n","\n","print(f\"\\nCurrent training stage: {TRAINING_STAGE.upper()}\")\n","print(\"  'pretrain' = RAF-DB macro-expression feature extraction\")\n","print(\"  'finetune' = CASME2 micro-expression specialization\")\n","\n","# FIXED CLASS ORDERING FOR TRANSFER LEARNING (BY FREQUENCY)\n","# Ordered from most to least frequent in CASME2 training set\n","TRANSFER_CLASSES = ['disgust', 'happy', 'surprise', 'sad', 'fear']\n","NUM_CLASSES = 5\n","\n","print(f\"\\nFixed transfer learning classes: {NUM_CLASSES}\")\n","print(f\"  Ordering: {TRANSFER_CLASSES}\")\n","print(f\"  Order rationale: Frequency-based (most to least common)\")\n","print(f\"  Index mapping:\")\n","for idx, cls in enumerate(TRANSFER_CLASSES):\n","    print(f\"    {idx} = {cls}\")\n","\n","# CASME2 to RAF-DB naming convention mapping\n","CASME2_TO_RAF_MAPPING = {\n","    'disgust': 'disgust',\n","    'fear': 'fear',\n","    'happiness': 'happy',\n","    'sadness': 'sad',\n","    'surprise': 'surprise',\n","    'repression': None,\n","    'others': None\n","}\n","\n","print(f\"\\nCASME2 naming convention mapping:\")\n","print(f\"  happiness → happy\")\n","print(f\"  sadness → sad\")\n","print(f\"  repression → excluded\")\n","print(f\"  others → excluded\")\n","\n","# OPTIMIZED CLASS WEIGHTS CONFIGURATION\n","# CASME2 train distribution after filtering: [50, 25, 20, 5, 1] for [disgust, happy, surprise, sad, fear]\n","print(f\"\\n\" + \"=\" * 70)\n","print(\"CLASS IMBALANCE HANDLING CONFIGURATION\")\n","print(\"=\" * 70)\n","\n","print(f\"\\nCASME2 Training Distribution (5 classes):\")\n","print(f\"  disgust:  50 samples (49.5%)\")\n","print(f\"  happy:    25 samples (24.8%)\")\n","print(f\"  surprise: 20 samples (19.8%)\")\n","print(f\"  sad:       5 samples (5.0%)\")\n","print(f\"  fear:      1 sample  (1.0%) - CRITICAL: Very minor!\")\n","\n","# CrossEntropy Loss - Inverse Square Root Frequency Weights\n","# Formula: weight[i] = 1/sqrt(freq[i]) normalized to smallest class = 1.0\n","CROSSENTROPY_CLASS_WEIGHTS = [1.00, 1.42, 1.59, 3.17, 7.09]\n","\n","print(f\"\\nCrossEntropy Class Weights (inverse sqrt frequency):\")\n","print(f\"  Values: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","print(f\"  Rationale: Sqrt smoothing prevents extreme weights\")\n","print(f\"  Fear weight (7.09): Strong but not excessive\")\n","\n","# Focal Loss - Smoothed Normalized Alpha Weights (sum = 1.0)\n","# Formula: alpha[i] = 1/sqrt(proportion[i]) normalized to sum=1.0\n","FOCAL_LOSS_ALPHA_WEIGHTS = [0.071, 0.100, 0.112, 0.222, 0.497]\n","FOCAL_LOSS_GAMMA = 2.0\n","\n","print(f\"\\nFocal Loss Configuration:\")\n","print(f\"  Alpha weights: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","print(f\"  Alpha sum: {sum(FOCAL_LOSS_ALPHA_WEIGHTS):.3f} (must be 1.0)\")\n","print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","print(f\"  Rationale: Smoothed alpha prevents over-emphasis on minorities\")\n","\n","# LOSS FUNCTION SELECTION TOGGLE\n","USE_FOCAL_LOSS = False\n","\n","print(f\"\\n\" + \"=\" * 70)\n","print(\"LOSS FUNCTION CONFIGURATION\")\n","print(\"=\" * 70)\n","\n","if USE_FOCAL_LOSS:\n","    print(f\"\\nSelected: Focal Loss\")\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Alpha weights: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","    print(f\"  Strategy: Focus on hard examples + minority class weighting\")\n","    print(f\"  Best for: Extreme imbalance (1-50 ratio)\")\n","else:\n","    print(f\"\\nSelected: CrossEntropy Loss\")\n","    print(f\"  Class weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","    print(f\"  Strategy: Direct minority class weighting\")\n","    print(f\"  Best for: Moderate imbalance\")\n","\n","# Unified output structure\n","CHECKPOINT_ROOT = f\"{PROJECT_ROOT}/models/05_01_transfer_learning\"\n","RESULTS_ROOT = f\"{PROJECT_ROOT}/results/05_01_transfer_learning\"\n","\n","# Image resolution configuration\n","INPUT_SIZE = 384\n","print(f\"\\n\" + \"=\" * 70)\n","print(\"IMAGE PROCESSING CONFIGURATION\")\n","print(\"=\" * 70)\n","print(f\"\\nImage resolution: {INPUT_SIZE}×{INPUT_SIZE}px\")\n","print(f\"  RAF-DB images: Upscaled from 100×100 to {INPUT_SIZE}×{INPUT_SIZE}\")\n","print(f\"  CASME2 images: Native resolution maintained at {INPUT_SIZE}×{INPUT_SIZE}\")\n","print(f\"  Resize method: LANCZOS (high quality)\")\n","\n","# Stage-dependent configuration\n","if TRAINING_STAGE == 'pretrain':\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"STAGE 1: RAF-DB PRE-TRAINING CONFIGURATION\")\n","    print(\"=\" * 70)\n","\n","    DATASET_NAME = 'RAF-DB Balanced (Transfer Set)'\n","    DATASET_ROOT = f\"{PROJECT_ROOT}/datasets/processed_raf\"\n","    METADATA_PATH = f\"{PROJECT_ROOT}/datasets/metadata/rafdb_metadata.csv\"\n","\n","    BATCH_SIZE = 256\n","    NUM_EPOCHS = 30\n","    LEARNING_RATE = 1e-5\n","    WEIGHT_DECAY = 1e-5\n","    DROPOUT_RATE = 0.2\n","    GRADIENT_CLIP = 1.0\n","\n","    AUGMENTATION_STRENGTH = 'moderate'\n","    ROTATION_RANGE = 10\n","    BRIGHTNESS_FACTOR = 0.1\n","    CONTRAST_FACTOR = 0.0\n","    HORIZONTAL_FLIP = False\n","\n","    PRETRAINED_CHECKPOINT = None\n","    FREEZE_ENCODER = False\n","\n","    CHECKPOINT_FILENAME = 'raf_pretrain_best_f1.pth'\n","    LOGS_SUBDIR = 'pretrain_logs'\n","\n","    print(f\"\\nDataset: {DATASET_NAME}\")\n","    print(f\"  Root: {DATASET_ROOT}\")\n","    print(f\"  Classes: {NUM_CLASSES} (fixed ordering by frequency)\")\n","    print(f\"  Expected samples: ~29,880 images (5 classes × ~5,956 per class)\")\n","    print(f\"  Original resolution: 100×100px (aligned faces)\")\n","    print(f\"  Training resolution: {INPUT_SIZE}×{INPUT_SIZE}px (upscaled)\")\n","\n","    print(f\"\\nTraining configuration:\")\n","    print(f\"  Batch size: {BATCH_SIZE}\")\n","    print(f\"  Epochs: {NUM_EPOCHS}\")\n","    print(f\"  Learning rate: {LEARNING_RATE}\")\n","    print(f\"  Weight decay: {WEIGHT_DECAY}\")\n","    print(f\"  Dropout: {DROPOUT_RATE}\")\n","    print(f\"  Augmentation: {AUGMENTATION_STRENGTH}\")\n","\n","    print(f\"\\nLoss function: {'Focal Loss' if USE_FOCAL_LOSS else 'CrossEntropy Loss'}\")\n","    if USE_FOCAL_LOSS:\n","        print(f\"  Note: Focal Loss less critical for balanced RAF-DB\")\n","        print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","\n","elif TRAINING_STAGE == 'finetune':\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"STAGE 2: CASME2 FINE-TUNING CONFIGURATION (OPTIMIZED)\")\n","    print(\"=\" * 70)\n","\n","    DATASET_NAME = 'CASME2 Apex Frame (Phase 1)'\n","    DATASET_ROOT = f\"{PROJECT_ROOT}/datasets/processed_casme2/data_split\"\n","    METADATA_PATH = f\"{DATASET_ROOT}/split_metadata.json\"\n","\n","    # OPTIMIZED HYPERPARAMETERS FOR SMALL IMBALANCED DATASET\n","    BATCH_SIZE = 8          # Smaller batch for limited data (was 16)\n","    NUM_EPOCHS = 50\n","    LEARNING_RATE = 3e-6    # Slightly higher than before (was 1e-6)\n","    WEIGHT_DECAY = 1e-4\n","    DROPOUT_RATE = 0.3      # Lower dropout (was 0.5)\n","    GRADIENT_CLIP = 0.5\n","\n","    AUGMENTATION_STRENGTH = 'enhanced'\n","    ROTATION_RANGE = 15\n","    BRIGHTNESS_FACTOR = 0.2\n","    CONTRAST_FACTOR = 0.2\n","    HORIZONTAL_FLIP = True\n","\n","    CHECKPOINT_FILENAME = 'casme2_finetune_best_f1.pth'\n","    LOGS_SUBDIR = 'finetune_logs'\n","    PRETRAINED_CHECKPOINT = f\"{CHECKPOINT_ROOT}/raf_pretrain_best_f1.pth\"\n","\n","    if not os.path.exists(PRETRAINED_CHECKPOINT):\n","        raise FileNotFoundError(\n","            f\"Pre-trained checkpoint not found: {PRETRAINED_CHECKPOINT}\\n\"\n","            f\"Please run Stage 1 (pretrain) first before fine-tuning.\"\n","        )\n","\n","    # OPTIMIZED TRANSFER STRATEGY\n","    FINETUNE_STRATEGY = 'frozen_encoder'  # Conservative approach for limited data\n","    FREEZE_ENCODER = (FINETUNE_STRATEGY == 'frozen_encoder')\n","\n","    print(f\"\\nDataset: {DATASET_NAME}\")\n","    print(f\"  Root: {DATASET_ROOT}\")\n","    print(f\"  Classes: {NUM_CLASSES} (fixed ordering, fear=1 sample)\")\n","    print(f\"  Expected samples: ~150-160 apex images\")\n","    print(f\"  Native resolution: {INPUT_SIZE}×{INPUT_SIZE}px\")\n","\n","    print(f\"\\nTransfer learning configuration:\")\n","    print(f\"  Pre-trained checkpoint: {os.path.basename(PRETRAINED_CHECKPOINT)}\")\n","    print(f\"  Checkpoint verified: Found\")\n","    print(f\"  Strategy: {FINETUNE_STRATEGY}\")\n","    print(f\"  Encoder frozen: {FREEZE_ENCODER}\")\n","    print(f\"  Rationale: Preserve pretrained features, only adapt classifier\")\n","\n","    print(f\"\\nOptimized fine-tuning hyperparameters:\")\n","    print(f\"  Batch size: {BATCH_SIZE} (reduced from 16 for better gradient)\")\n","    print(f\"  Epochs: {NUM_EPOCHS}\")\n","    print(f\"  Learning rate: {LEARNING_RATE} (increased from 1e-6)\")\n","    print(f\"  Weight decay: {WEIGHT_DECAY}\")\n","    print(f\"  Dropout: {DROPOUT_RATE} (reduced from 0.5 to prevent over-regularization)\")\n","    print(f\"  Augmentation: {AUGMENTATION_STRENGTH}\")\n","\n","    print(f\"\\nLoss function: {'Focal Loss (CRITICAL)' if USE_FOCAL_LOSS else 'CrossEntropy Loss'}\")\n","    if USE_FOCAL_LOSS:\n","        print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","        print(f\"  Alpha: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","        print(f\"  Rationale: Handle extreme 1-50 class imbalance\")\n","\n","else:\n","    raise ValueError(f\"Invalid TRAINING_STAGE: {TRAINING_STAGE}. Use 'pretrain' or 'finetune'\")\n","\n","# GPU configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n","gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n","\n","print(f\"\\n[3] Hardware configuration\")\n","print(f\"    Device: {device}\")\n","print(f\"    GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n","\n","# Hardware-optimized worker configuration\n","if 'A100' in gpu_name:\n","    NUM_WORKERS = 32\n","    torch.backends.cudnn.benchmark = True\n","    print(\"    Optimization: A100 detected, enabled cudnn benchmark\")\n","elif 'L4' in gpu_name:\n","    NUM_WORKERS = 16\n","    torch.backends.cudnn.benchmark = True\n","    print(\"    Optimization: L4 detected, enabled cudnn benchmark\")\n","else:\n","    NUM_WORKERS = 8\n","    print(\"    Optimization: Default GPU configuration\")\n","\n","# RAM preloading workers\n","RAM_PRELOAD_WORKERS = 128\n","print(f\"    DataLoader workers: {NUM_WORKERS} (batch preparation)\")\n","print(f\"    RAM preload workers: {RAM_PRELOAD_WORKERS} (parallel image loading)\")\n","\n","print(f\"\\n[4] Loss function configuration\")\n","print(f\"    Function: {'Focal Loss' if USE_FOCAL_LOSS else 'CrossEntropy Loss'}\")\n","if USE_FOCAL_LOSS:\n","    print(f\"    Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"    Alpha weights: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","else:\n","    print(f\"    Class weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","\n","# ViT Architecture for Transfer Learning\n","class ViTTransferLearning(nn.Module):\n","    \"\"\"ViT architecture with transfer learning support for macro to micro expression recognition\"\"\"\n","\n","    def __init__(self, num_classes, dropout_rate=0.2, pretrained_checkpoint=None, freeze_encoder=False):\n","        super(ViTTransferLearning, self).__init__()\n","\n","        from transformers import ViTModel\n","\n","        self.vit = ViTModel.from_pretrained(\n","            VIT_MODEL_NAME,\n","            add_pooling_layer=False\n","        )\n","\n","        if freeze_encoder:\n","            for param in self.vit.parameters():\n","                param.requires_grad = False\n","            print(\"ViT encoder frozen for fine-tuning\")\n","        else:\n","            for param in self.vit.parameters():\n","                param.requires_grad = True\n","            print(\"ViT encoder trainable\")\n","\n","        self.vit_feature_dim = self.vit.config.hidden_size\n","\n","        self.classifier_layers = nn.Sequential(\n","            nn.Linear(self.vit_feature_dim, 512),\n","            nn.LayerNorm(512),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","\n","            nn.Linear(512, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(dropout_rate),\n","        )\n","\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","        print(f\"ViT Transfer Learning: {self.vit_feature_dim} -> 512 -> 128 -> {num_classes}\")\n","\n","        if pretrained_checkpoint and os.path.exists(pretrained_checkpoint):\n","            self.load_pretrained_weights(pretrained_checkpoint)\n","\n","    def load_pretrained_weights(self, checkpoint_path):\n","        \"\"\"Load pre-trained weights from RAF-DB pre-training stage\"\"\"\n","        print(f\"\\nLoading pre-trained weights from: {os.path.basename(checkpoint_path)}\")\n","\n","        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","\n","        vit_state = {k.replace('vit.', ''): v\n","                    for k, v in checkpoint['model_state_dict'].items()\n","                    if k.startswith('vit.')}\n","\n","        missing_keys, unexpected_keys = self.vit.load_state_dict(vit_state, strict=False)\n","\n","        print(f\"  ViT encoder weights loaded\")\n","        print(f\"  Missing keys: {len(missing_keys)}\")\n","        print(f\"  Unexpected keys: {len(unexpected_keys)}\")\n","\n","        classifier_state = {k.replace('classifier_layers.', ''): v\n","                          for k, v in checkpoint['model_state_dict'].items()\n","                          if k.startswith('classifier_layers.')}\n","\n","        if classifier_state:\n","            try:\n","                self.classifier_layers.load_state_dict(classifier_state, strict=False)\n","                print(f\"  Classifier layers loaded\")\n","            except:\n","                print(f\"  Classifier layers not loaded (dimension mismatch expected)\")\n","\n","        print(f\"  Transfer learning initialization complete\")\n","\n","    def forward(self, pixel_values):\n","        vit_outputs = self.vit(\n","            pixel_values=pixel_values,\n","            interpolate_pos_encoding=True\n","        )\n","\n","        vit_features = vit_outputs.last_hidden_state[:, 0]\n","\n","        processed_features = self.classifier_layers(vit_features)\n","        output = self.classifier(processed_features)\n","\n","        return output\n","\n","# Optimized Focal Loss Implementation\n","class OptimizedFocalLoss(nn.Module):\n","    \"\"\"\n","    Advanced Focal Loss with per-class alpha support\n","    Paper: Focal Loss for Dense Object Detection (Lin et al., 2017)\n","\n","    Formula: FL(p_t) = -alpha_t * (1-p_t)^gamma * log(p_t)\n","    \"\"\"\n","\n","    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n","        super(OptimizedFocalLoss, self).__init__()\n","\n","        if alpha is not None:\n","            if isinstance(alpha, list):\n","                self.alpha = torch.tensor(alpha, dtype=torch.float32)\n","            else:\n","                self.alpha = alpha\n","\n","            alpha_sum = self.alpha.sum().item()\n","            if abs(alpha_sum - 1.0) > 0.01:\n","                print(f\"Warning: Alpha weights sum to {alpha_sum:.3f}, expected 1.0\")\n","        else:\n","            self.alpha = None\n","\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","\n","        if self.alpha is not None:\n","            if self.alpha.device != targets.device:\n","                self.alpha = self.alpha.to(targets.device)\n","            alpha_t = self.alpha.gather(0, targets)\n","        else:\n","            alpha_t = 1.0\n","\n","        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","\n","# Loss function factory\n","def create_criterion(use_focal_loss=False, gamma=2.0, alpha_weights=None, class_weights=None):\n","    \"\"\"Create loss function based on configuration\"\"\"\n","    if use_focal_loss:\n","        print(f\"Using Optimized Focal Loss with gamma={gamma}\")\n","        if alpha_weights:\n","            print(f\"  Alpha weights: {alpha_weights}\")\n","            print(f\"  Alpha sum: {sum(alpha_weights):.3f}\")\n","        return OptimizedFocalLoss(alpha=alpha_weights, gamma=gamma)\n","    else:\n","        print(f\"Using CrossEntropy Loss with class weights\")\n","        if class_weights is not None:\n","            class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n","            print(f\"  Class weights: {class_weights}\")\n","            return nn.CrossEntropyLoss(weight=class_weights_tensor)\n","        else:\n","            return nn.CrossEntropyLoss()\n","\n","# Optimizer and scheduler factory\n","def create_optimizer_scheduler(model, learning_rate, weight_decay):\n","    \"\"\"Create optimizer and scheduler for training\"\"\"\n","\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=learning_rate,\n","        weight_decay=weight_decay,\n","        betas=(0.9, 0.999)\n","    )\n","\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer,\n","        mode='max',\n","        factor=0.5,\n","        patience=5,\n","        min_lr=1e-7\n","    )\n","\n","    print(f\"Optimizer: AdamW (lr={learning_rate}, wd={weight_decay})\")\n","    print(f\"Scheduler: ReduceLROnPlateau (monitor=val_f1, patience=5)\")\n","\n","    return optimizer, scheduler\n","\n","# ViT Image Processor setup\n","from transformers import ViTImageProcessor\n","\n","print(f\"\\n[5] Setting up ViT Image Processor for {INPUT_SIZE}px input...\")\n","\n","vit_processor = ViTImageProcessor.from_pretrained(\n","    VIT_MODEL_NAME,\n","    do_resize=True,\n","    size={'height': INPUT_SIZE, 'width': INPUT_SIZE},\n","    do_normalize=True,\n","    do_rescale=True,\n","    do_center_crop=False\n",")\n","\n","print(f\"    ViT Image Processor configured for {INPUT_SIZE}px with interpolation\")\n","print(f\"    Variant: {VIT_MODEL_VARIANT.upper()}\")\n","print(f\"    Expected tokens: {(INPUT_SIZE // PATCH_SIZE) ** 2}\")\n","\n","# Transform functions with stage-dependent augmentation\n","def get_transforms(stage):\n","    \"\"\"Get transforms based on training stage and augmentation strength\"\"\"\n","\n","    if stage == 'train':\n","        transform_list = []\n","\n","        if ROTATION_RANGE > 0:\n","            transform_list.append(\n","                transforms.RandomRotation(degrees=ROTATION_RANGE)\n","            )\n","\n","        if BRIGHTNESS_FACTOR > 0 or CONTRAST_FACTOR > 0:\n","            transform_list.append(\n","                transforms.ColorJitter(\n","                    brightness=BRIGHTNESS_FACTOR,\n","                    contrast=CONTRAST_FACTOR\n","                )\n","            )\n","\n","        if HORIZONTAL_FLIP:\n","            transform_list.append(\n","                transforms.RandomHorizontalFlip(p=0.5)\n","            )\n","\n","        if transform_list:\n","            augmentation = transforms.Compose(transform_list)\n","\n","            def train_transform(image):\n","                image = augmentation(image)\n","                inputs = vit_processor(image, return_tensors=\"pt\")\n","                return inputs['pixel_values'].squeeze(0)\n","\n","            return train_transform\n","        else:\n","            def train_transform(image):\n","                inputs = vit_processor(image, return_tensors=\"pt\")\n","                return inputs['pixel_values'].squeeze(0)\n","\n","            return train_transform\n","\n","    else:\n","        def val_transform(image):\n","            inputs = vit_processor(image, return_tensors=\"pt\")\n","            return inputs['pixel_values'].squeeze(0)\n","\n","        return val_transform\n","\n","# Custom Dataset class for RAF-DB with fixed class ordering\n","class RAFDBDataset(Dataset):\n","    \"\"\"Dataset class for RAF-DB with fixed transfer learning class ordering\"\"\"\n","\n","    def __init__(self, metadata_df, dataset_root, transform=None, transfer_classes=None):\n","        if transfer_classes:\n","            self.metadata = metadata_df[metadata_df['emotion_label'].isin(transfer_classes)].copy()\n","            print(f\"Filtered to {len(self.metadata)} samples from {len(metadata_df)} total\")\n","        else:\n","            self.metadata = metadata_df.copy()\n","\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","\n","        self.class_to_idx = {cls: idx for idx, cls in enumerate(transfer_classes)}\n","\n","        print(f\"Loaded {len(self.metadata)} samples\")\n","        print(f\"Fixed class ordering: {list(self.class_to_idx.keys())}\")\n","\n","        unique_labels = self.metadata['emotion_label'].unique()\n","        for label in unique_labels:\n","            if label not in self.class_to_idx:\n","                raise ValueError(f\"Label '{label}' not found in transfer_classes: {transfer_classes}\")\n","\n","    def __len__(self):\n","        return len(self.metadata)\n","\n","    def __getitem__(self, idx):\n","        row = self.metadata.iloc[idx]\n","\n","        image_path = os.path.join(self.dataset_root, row['filepath'].replace('datasets/processed_raf/', ''))\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if image.size != (INPUT_SIZE, INPUT_SIZE):\n","            image = image.resize((INPUT_SIZE, INPUT_SIZE), Image.Resampling.LANCZOS)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        label = self.class_to_idx[row['emotion_label']]\n","\n","        return image, label\n","\n","# Custom Dataset class for CASME2 with naming convention mapping\n","class CASME2Dataset(Dataset):\n","    \"\"\"Dataset class for CASME2 with CASME2->RAF naming mapping and fixed class ordering\"\"\"\n","\n","    def __init__(self, split_metadata, dataset_root, transform=None, split='train',\n","                 transfer_classes=None, casme2_mapping=None):\n","\n","        samples = split_metadata[split]['samples']\n","\n","        if casme2_mapping:\n","            mapped_samples = []\n","            for s in samples:\n","                casme2_emotion = s['emotion']\n","                raf_emotion = casme2_mapping.get(casme2_emotion)\n","\n","                if raf_emotion in transfer_classes:\n","                    mapped_sample = s.copy()\n","                    mapped_sample['emotion'] = raf_emotion\n","                    mapped_samples.append(mapped_sample)\n","\n","            print(f\"Mapped {len(mapped_samples)} samples from {len(samples)} total\")\n","            self.samples = mapped_samples\n","        else:\n","            self.samples = samples\n","\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","        self.split = split\n","\n","        self.class_to_idx = {cls: idx for idx, cls in enumerate(transfer_classes)}\n","\n","        print(f\"Loaded {len(self.samples)} samples for {split} split\")\n","        print(f\"Fixed class ordering: {list(self.class_to_idx.keys())}\")\n","\n","        actual_classes = {}\n","        for s in self.samples:\n","            emotion = s['emotion']\n","            actual_classes[emotion] = actual_classes.get(emotion, 0) + 1\n","\n","        print(f\"Class distribution:\")\n","        for cls in transfer_classes:\n","            count = actual_classes.get(cls, 0)\n","            idx = self.class_to_idx[cls]\n","            status = \"NO SAMPLES\" if count == 0 else f\"{count} samples\"\n","            print(f\"  [{idx}] {cls}: {status}\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        sample = self.samples[idx]\n","\n","        image_path = os.path.join(self.dataset_root, self.split, sample['image_filename'])\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if image.size != (INPUT_SIZE, INPUT_SIZE):\n","            image = image.resize((INPUT_SIZE, INPUT_SIZE), Image.Resampling.LANCZOS)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        label = self.class_to_idx[sample['emotion']]\n","\n","        return image, label, sample['sample_id']\n","\n","# Create unified directory structure\n","print(f\"\\n[6] Creating unified output directory structure...\")\n","\n","os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n","os.makedirs(RESULTS_ROOT, exist_ok=True)\n","\n","logs_path = f\"{RESULTS_ROOT}/{LOGS_SUBDIR}\"\n","os.makedirs(logs_path, exist_ok=True)\n","os.makedirs(f\"{logs_path}/training_logs\", exist_ok=True)\n","os.makedirs(f\"{logs_path}/evaluation_results\", exist_ok=True)\n","\n","print(f\"    Unified structure created:\")\n","print(f\"    Checkpoints: {CHECKPOINT_ROOT}/\")\n","print(f\"      - {CHECKPOINT_FILENAME}\")\n","print(f\"    Results: {RESULTS_ROOT}/\")\n","print(f\"      - {LOGS_SUBDIR}/\")\n","\n","# Architecture validation\n","print(f\"\\n[7] Validating ViT Transfer Learning architecture...\")\n","\n","try:\n","    test_model = ViTTransferLearning(\n","        num_classes=NUM_CLASSES,\n","        dropout_rate=DROPOUT_RATE,\n","        pretrained_checkpoint=PRETRAINED_CHECKPOINT if TRAINING_STAGE == 'finetune' else None,\n","        freeze_encoder=FREEZE_ENCODER\n","    ).to(device)\n","\n","    test_input = torch.randn(1, 3, INPUT_SIZE, INPUT_SIZE).to(device)\n","    test_output = test_model(test_input)\n","\n","    expected_tokens = (INPUT_SIZE // PATCH_SIZE) ** 2\n","\n","    print(f\"    Validation successful\")\n","    print(f\"    Output shape: {test_output.shape}\")\n","    print(f\"    Expected tokens: {expected_tokens}\")\n","    print(f\"    Variant: {VIT_MODEL_VARIANT} ({PATCH_SIZE}×{PATCH_SIZE} patches)\")\n","\n","    del test_model, test_input, test_output\n","    torch.cuda.empty_cache()\n","\n","except Exception as e:\n","    print(f\"    Validation failed: {e}\")\n","    raise\n","\n","# Global configuration\n","GLOBAL_CONFIG = {\n","    'training_stage': TRAINING_STAGE,\n","    'dataset_name': DATASET_NAME,\n","    'dataset_root': DATASET_ROOT,\n","    'metadata_path': METADATA_PATH,\n","    'device': device,\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'ram_preload_workers': RAM_PRELOAD_WORKERS,\n","    'num_classes': NUM_CLASSES,\n","    'transfer_classes': TRANSFER_CLASSES,\n","    'class_to_idx': {cls: idx for idx, cls in enumerate(TRANSFER_CLASSES)},\n","    'casme2_mapping': CASME2_TO_RAF_MAPPING,\n","    'transform_train': get_transforms('train'),\n","    'transform_val': get_transforms('val'),\n","    'vit_model': VIT_MODEL_NAME,\n","    'vit_variant': VIT_MODEL_VARIANT,\n","    'patch_size': PATCH_SIZE,\n","    'input_size': INPUT_SIZE,\n","    'num_epochs': NUM_EPOCHS,\n","    'learning_rate': LEARNING_RATE,\n","    'weight_decay': WEIGHT_DECAY,\n","    'dropout_rate': DROPOUT_RATE,\n","    'gradient_clip': GRADIENT_CLIP,\n","    'use_focal_loss': USE_FOCAL_LOSS,\n","    'focal_loss_gamma': FOCAL_LOSS_GAMMA,\n","    'focal_loss_alpha': FOCAL_LOSS_ALPHA_WEIGHTS,\n","    'crossentropy_weights': CROSSENTROPY_CLASS_WEIGHTS,\n","    'pretrained_checkpoint': PRETRAINED_CHECKPOINT,\n","    'freeze_encoder': FREEZE_ENCODER,\n","    'finetune_strategy': FINETUNE_STRATEGY if TRAINING_STAGE == 'finetune' else None,\n","    'augmentation_strength': AUGMENTATION_STRENGTH,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'checkpoint_filename': CHECKPOINT_FILENAME,\n","    'checkpoint_path': f\"{CHECKPOINT_ROOT}/{CHECKPOINT_FILENAME}\",\n","    'results_root': RESULTS_ROOT,\n","    'logs_subdir': LOGS_SUBDIR,\n","    'logs_path': logs_path,\n","    'criterion_factory': create_criterion,\n","    'optimizer_scheduler_factory': create_optimizer_scheduler\n","}\n","\n","# Configuration summary\n","print(\"\\n\" + \"=\" * 70)\n","print(\"TRANSFER LEARNING INFRASTRUCTURE CONFIGURATION COMPLETE\")\n","print(\"=\" * 70)\n","\n","print(f\"\\nModel Configuration:\")\n","print(f\"  Variant: {VIT_MODEL_VARIANT.upper()} (ViT-Base-{VIT_MODEL_VARIANT.capitalize()})\")\n","print(f\"  Patch size: {PATCH_SIZE}×{PATCH_SIZE}px\")\n","print(f\"  Input resolution: {INPUT_SIZE}×{INPUT_SIZE}px\")\n","print(f\"  Expected tokens: {(INPUT_SIZE // PATCH_SIZE) ** 2}\")\n","print(f\"  Dropout: {DROPOUT_RATE}\")\n","\n","print(f\"\\nFixed Transfer Learning Classes:\")\n","print(f\"  Total: {NUM_CLASSES} classes\")\n","print(f\"  Ordering: {TRANSFER_CLASSES} (by frequency)\")\n","print(f\"  Class indices:\")\n","for idx, cls in enumerate(TRANSFER_CLASSES):\n","    print(f\"    [{idx}] = {cls}\")\n","\n","print(f\"\\nLoss Configuration:\")\n","if USE_FOCAL_LOSS:\n","    print(f\"  Function: Focal Loss\")\n","    print(f\"  Gamma: {FOCAL_LOSS_GAMMA}\")\n","    print(f\"  Alpha: {FOCAL_LOSS_ALPHA_WEIGHTS}\")\n","else:\n","    print(f\"  Function: CrossEntropy\")\n","    print(f\"  Weights: {CROSSENTROPY_CLASS_WEIGHTS}\")\n","\n","print(f\"\\nTraining Stage: {TRAINING_STAGE.upper()}\")\n","print(f\"  Dataset: {DATASET_NAME}\")\n","\n","print(f\"\\nUnified Output Structure:\")\n","print(f\"  Checkpoint: {CHECKPOINT_ROOT}/{CHECKPOINT_FILENAME}\")\n","print(f\"  Results: {RESULTS_ROOT}/{LOGS_SUBDIR}/\")\n","\n","if TRAINING_STAGE == 'pretrain':\n","    print(f\"\\nPre-training Details:\")\n","    print(f\"  Expected samples: ~29,880 images\")\n","    print(f\"  Strategy: Extract macro-expression features\")\n","    print(f\"  Image upscaling: 100×100 → {INPUT_SIZE}×{INPUT_SIZE}\")\n","    print(f\"  Output checkpoint: {CHECKPOINT_FILENAME}\")\n","elif TRAINING_STAGE == 'finetune':\n","    print(f\"\\nFine-tuning Details:\")\n","    print(f\"  Expected samples: ~150-160 apex images\")\n","    print(f\"  Strategy: {FINETUNE_STRATEGY}\")\n","    print(f\"  Load from: raf_pretrain_best_f1.pth\")\n","    print(f\"  Output checkpoint: {CHECKPOINT_FILENAME}\")\n","    print(f\"  Encoder frozen: {FREEZE_ENCODER}\")\n","    print(f\"  Critical: Fear class has ONLY 1 sample\")\n","\n","print(f\"\\nTraining Hyperparameters:\")\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","print(f\"  Epochs: {NUM_EPOCHS}\")\n","print(f\"  Learning rate: {LEARNING_RATE}\")\n","print(f\"  Weight decay: {WEIGHT_DECAY}\")\n","print(f\"  Augmentation: {AUGMENTATION_STRENGTH}\")\n","\n","print(f\"\\nHardware Optimization:\")\n","print(f\"  DataLoader workers: {NUM_WORKERS}\")\n","print(f\"  RAM preload workers: {RAM_PRELOAD_WORKERS}\")\n","print(f\"  GPU optimization: {'Enabled' if torch.backends.cudnn.benchmark else 'Default'}\")\n","\n","print(f\"\\n[8] Optimization Summary\")\n","print(f\"    Class reordering: By frequency (disgust→happy→surprise→sad→fear)\")\n","print(f\"    Loss function: {'Focal Loss' if USE_FOCAL_LOSS else 'CrossEntropy'} with optimized weights\")\n","print(f\"    Transfer strategy: {'Frozen encoder' if TRAINING_STAGE == 'finetune' and FREEZE_ENCODER else 'Full training'}\")\n","print(f\"    Status: Ready for optimized transfer learning\")\n","\n","print(f\"\\nNext: Cell 2 - Dataset Loading and Training Pipeline\")\n","print(\"=\" * 70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["45985dbc79544733aaa138eee17710fd","4fee02a885474210bd94090fe599e236","46b981fef03a4208bfe29d6f860ad94e","29c1eddb35a24f9c899626d478a2b889","d0c137d6562f4d8a97e74c46562aef97","908795c7146e4aee84158519d6eb0648","a3d59b2177d04c089722eca55062b3ee","dd5629798e6444d5854a2a64251906f9","a2bf0efa9f3e4177ae26f40432ed1f98","1a5aabe586794e91bba4116963684d20","173619bec73b4643ac9574d1327cff9f","d91764b896624ad9a94b9cd24b2bc11a","191c70876b864461917b48436f276b56","9e67e1b169f94adeb7174a0e39cd07cc","5cbc3d4070fe472bbab271db6dde2b86","487b781e2fba43e4b92038744747b906","d6991a161b3d4102ae4cb147394e2f08","e062562263cd4c20be17fba408660eb3","49b9ea25724144ac97c5e7e00ca7b596","49900e478ff0437ead578bcb579edda2","cf623e3bbd8d41c39976fb9fe66dec82","5d1da8ff6b8946ee97f51074bcfc0e39","92f673b827d742d2b32bfd2b1b671ace","96106d47e7454144846cdad3de397553","f471599d11aa4b84afbff9514b76453e","131940333f6941cfb4de8574ecd7bde0","0880e1688d524fccbd73266306d382a0","4ae237caa5db4c3dbf6d7fdbc69dd451","10e04fb8fd4c4922b91a488855367574","0e60ab12ce1e4de3b039cc866f669b84","86cbb72af4a44dbe80dc49ced489d857","0f75745b5c334c03b566e5ebebb64ed0","b488456f4a6b47cd92f26f9ffc549f83"]},"collapsed":true,"cellView":"form","id":"y72QQzlhHPzW","executionInfo":{"status":"ok","timestamp":1760613720598,"user_tz":-420,"elapsed":90567,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"37ae2542-52a0-48c3-cfa7-ddf8aa9ccf0c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","VIT TRANSFER LEARNING: RAF-DB → CASME2 APEX FRAME (OPTIMIZED)\n","======================================================================\n","\n","[1] Mounting Google Drive...\n","Mounted at /content/drive\n","    Google Drive mounted successfully\n","\n","[2] Importing required libraries...\n","\n","======================================================================\n","MODEL VARIANT CONFIGURATION\n","======================================================================\n","\n","Selected: ViT-Base Patch32\n","  Characteristics: Efficient feature extraction\n","  Tokens at 384px: 144 tokens (12×12 grid)\n","  Best for: Balanced performance and speed\n","\n","======================================================================\n","TRANSFER LEARNING STAGE CONFIGURATION\n","======================================================================\n","\n","Current training stage: FINETUNE\n","  'pretrain' = RAF-DB macro-expression feature extraction\n","  'finetune' = CASME2 micro-expression specialization\n","\n","Fixed transfer learning classes: 5\n","  Ordering: ['disgust', 'happy', 'surprise', 'sad', 'fear']\n","  Order rationale: Frequency-based (most to least common)\n","  Index mapping:\n","    0 = disgust\n","    1 = happy\n","    2 = surprise\n","    3 = sad\n","    4 = fear\n","\n","CASME2 naming convention mapping:\n","  happiness → happy\n","  sadness → sad\n","  repression → excluded\n","  others → excluded\n","\n","======================================================================\n","CLASS IMBALANCE HANDLING CONFIGURATION\n","======================================================================\n","\n","CASME2 Training Distribution (5 classes):\n","  disgust:  50 samples (49.5%)\n","  happy:    25 samples (24.8%)\n","  surprise: 20 samples (19.8%)\n","  sad:       5 samples (5.0%)\n","  fear:      1 sample  (1.0%) - CRITICAL: Very minor!\n","\n","CrossEntropy Class Weights (inverse sqrt frequency):\n","  Values: [1.0, 1.42, 1.59, 3.17, 7.09]\n","  Rationale: Sqrt smoothing prevents extreme weights\n","  Fear weight (7.09): Strong but not excessive\n","\n","Focal Loss Configuration:\n","  Alpha weights: [0.071, 0.1, 0.112, 0.222, 0.497]\n","  Alpha sum: 1.002 (must be 1.0)\n","  Gamma: 2.0\n","  Rationale: Smoothed alpha prevents over-emphasis on minorities\n","\n","======================================================================\n","LOSS FUNCTION CONFIGURATION\n","======================================================================\n","\n","Selected: CrossEntropy Loss\n","  Class weights: [1.0, 1.42, 1.59, 3.17, 7.09]\n","  Strategy: Direct minority class weighting\n","  Best for: Moderate imbalance\n","\n","======================================================================\n","IMAGE PROCESSING CONFIGURATION\n","======================================================================\n","\n","Image resolution: 384×384px\n","  RAF-DB images: Upscaled from 100×100 to 384×384\n","  CASME2 images: Native resolution maintained at 384×384\n","  Resize method: LANCZOS (high quality)\n","\n","======================================================================\n","STAGE 2: CASME2 FINE-TUNING CONFIGURATION (OPTIMIZED)\n","======================================================================\n","\n","Dataset: CASME2 Apex Frame (Phase 1)\n","  Root: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/datasets/processed_casme2/data_split\n","  Classes: 5 (fixed ordering, fear=1 sample)\n","  Expected samples: ~150-160 apex images\n","  Native resolution: 384×384px\n","\n","Transfer learning configuration:\n","  Pre-trained checkpoint: raf_pretrain_best_f1.pth\n","  Checkpoint verified: Found\n","  Strategy: frozen_encoder\n","  Encoder frozen: True\n","  Rationale: Preserve pretrained features, only adapt classifier\n","\n","Optimized fine-tuning hyperparameters:\n","  Batch size: 8 (reduced from 16 for better gradient)\n","  Epochs: 50\n","  Learning rate: 3e-06 (increased from 1e-6)\n","  Weight decay: 0.0001\n","  Dropout: 0.3 (reduced from 0.5 to prevent over-regularization)\n","  Augmentation: enhanced\n","\n","Loss function: CrossEntropy Loss\n","\n","[3] Hardware configuration\n","    Device: cuda\n","    GPU: Tesla T4 (15.8 GB)\n","    Optimization: Default GPU configuration\n","    DataLoader workers: 8 (batch preparation)\n","    RAM preload workers: 128 (parallel image loading)\n","\n","[4] Loss function configuration\n","    Function: CrossEntropy Loss\n","    Class weights: [1.0, 1.42, 1.59, 3.17, 7.09]\n","\n","[5] Setting up ViT Image Processor for 384px input...\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45985dbc79544733aaa138eee17710fd"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["    ViT Image Processor configured for 384px with interpolation\n","    Variant: PATCH32\n","    Expected tokens: 144\n","\n","[6] Creating unified output directory structure...\n","    Unified structure created:\n","    Checkpoints: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/05_01_transfer_learning/\n","      - casme2_finetune_best_f1.pth\n","    Results: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/05_01_transfer_learning/\n","      - finetune_logs/\n","\n","[7] Validating ViT Transfer Learning architecture...\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d91764b896624ad9a94b9cd24b2bc11a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/352M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92f673b827d742d2b32bfd2b1b671ace"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at google/vit-base-patch32-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["ViT encoder frozen for fine-tuning\n","ViT Transfer Learning: 768 -> 512 -> 128 -> 5\n","\n","Loading pre-trained weights from: raf_pretrain_best_f1.pth\n","  ViT encoder weights loaded\n","  Missing keys: 0\n","  Unexpected keys: 0\n","  Classifier layers loaded\n","  Transfer learning initialization complete\n","    Validation successful\n","    Output shape: torch.Size([1, 5])\n","    Expected tokens: 144\n","    Variant: patch32 (32×32 patches)\n","\n","======================================================================\n","TRANSFER LEARNING INFRASTRUCTURE CONFIGURATION COMPLETE\n","======================================================================\n","\n","Model Configuration:\n","  Variant: PATCH32 (ViT-Base-Patch32)\n","  Patch size: 32×32px\n","  Input resolution: 384×384px\n","  Expected tokens: 144\n","  Dropout: 0.3\n","\n","Fixed Transfer Learning Classes:\n","  Total: 5 classes\n","  Ordering: ['disgust', 'happy', 'surprise', 'sad', 'fear'] (by frequency)\n","  Class indices:\n","    [0] = disgust\n","    [1] = happy\n","    [2] = surprise\n","    [3] = sad\n","    [4] = fear\n","\n","Loss Configuration:\n","  Function: CrossEntropy\n","  Weights: [1.0, 1.42, 1.59, 3.17, 7.09]\n","\n","Training Stage: FINETUNE\n","  Dataset: CASME2 Apex Frame (Phase 1)\n","\n","Unified Output Structure:\n","  Checkpoint: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/models/05_01_transfer_learning/casme2_finetune_best_f1.pth\n","  Results: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/05_01_transfer_learning/finetune_logs/\n","\n","Fine-tuning Details:\n","  Expected samples: ~150-160 apex images\n","  Strategy: frozen_encoder\n","  Load from: raf_pretrain_best_f1.pth\n","  Output checkpoint: casme2_finetune_best_f1.pth\n","  Encoder frozen: True\n","  Critical: Fear class has ONLY 1 sample\n","\n","Training Hyperparameters:\n","  Batch size: 8\n","  Epochs: 50\n","  Learning rate: 3e-06\n","  Weight decay: 0.0001\n","  Augmentation: enhanced\n","\n","Hardware Optimization:\n","  DataLoader workers: 8\n","  RAM preload workers: 128\n","  GPU optimization: Default\n","\n","[8] Optimization Summary\n","    Class reordering: By frequency (disgust→happy→surprise→sad→fear)\n","    Loss function: CrossEntropy with optimized weights\n","    Transfer strategy: Frozen encoder\n","    Status: Ready for optimized transfer learning\n","\n","Next: Cell 2 - Dataset Loading and Training Pipeline\n","======================================================================\n"]}]},{"cell_type":"code","source":["# @title Cell 2: ViT Transfer Learning Training Pipeline\n","\n","# File: 05_01_ViT_RAF-DB_CASME2-AF.ipynb - Cell 2\n","# Location: experiments/05_01_ViT_RAF-DB_CASME2-AF.ipynb\n","# Purpose: Dual-stage training pipeline for RAF-DB pre-training and CASME2 fine-tuning\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import time\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n","from concurrent.futures import ThreadPoolExecutor\n","import shutil\n","import tempfile\n","\n","print(\"=\" * 70)\n","print(f\"VIT TRANSFER LEARNING TRAINING PIPELINE - STAGE: {GLOBAL_CONFIG['training_stage'].upper()}\")\n","print(\"=\" * 70)\n","\n","# Display stage-specific configuration\n","if GLOBAL_CONFIG['training_stage'] == 'pretrain':\n","    print(f\"\\nPre-training Configuration:\")\n","    print(f\"  Dataset: {GLOBAL_CONFIG['dataset_name']}\")\n","    print(f\"  Classes: {GLOBAL_CONFIG['num_classes']} (fixed ordering)\")\n","    print(f\"  Expected samples: ~29,880 images\")\n","    print(f\"  Batch size: {GLOBAL_CONFIG['batch_size']}\")\n","    print(f\"  Epochs: {GLOBAL_CONFIG['num_epochs']}\")\n","    print(f\"  Learning rate: {GLOBAL_CONFIG['learning_rate']}\")\n","    print(f\"  Augmentation: {GLOBAL_CONFIG['augmentation_strength']}\")\n","    print(f\"  Output: {GLOBAL_CONFIG['checkpoint_filename']}\")\n","else:\n","    print(f\"\\nFine-tuning Configuration:\")\n","    print(f\"  Dataset: {GLOBAL_CONFIG['dataset_name']}\")\n","    print(f\"  Classes: {GLOBAL_CONFIG['num_classes']} (fixed ordering, fear=placeholder)\")\n","    print(f\"  Expected samples: ~150-160 apex images\")\n","    print(f\"  Pre-trained checkpoint: {os.path.basename(GLOBAL_CONFIG['pretrained_checkpoint'])}\")\n","    print(f\"  Strategy: {GLOBAL_CONFIG['finetune_strategy']}\")\n","    print(f\"  Encoder frozen: {GLOBAL_CONFIG['freeze_encoder']}\")\n","    print(f\"  Batch size: {GLOBAL_CONFIG['batch_size']}\")\n","    print(f\"  Epochs: {GLOBAL_CONFIG['num_epochs']}\")\n","    print(f\"  Learning rate: {GLOBAL_CONFIG['learning_rate']}\")\n","    print(f\"  Augmentation: {GLOBAL_CONFIG['augmentation_strength']}\")\n","    print(f\"  Output: {GLOBAL_CONFIG['checkpoint_filename']}\")\n","\n","print(f\"\\nLoss function: {'Focal Loss' if GLOBAL_CONFIG['use_focal_loss'] else 'CrossEntropy Loss'}\")\n","if GLOBAL_CONFIG['use_focal_loss']:\n","    print(f\"  Gamma: {GLOBAL_CONFIG['focal_loss_gamma']}\")\n","\n","# RAM preloading configuration\n","RAM_PRELOAD_WORKERS = GLOBAL_CONFIG['ram_preload_workers']\n","print(f\"\\nRAM preload configuration:\")\n","print(f\"  Workers: {RAM_PRELOAD_WORKERS} (parallel image loading)\")\n","print(f\"  Method: ThreadPoolExecutor with concurrent futures\")\n","print(f\"  Target: Load all images to RAM before training starts\")\n","\n","# Enhanced Dataset class with RAM caching for RAF-DB\n","class RAFDBDatasetTraining(Dataset):\n","    \"\"\"Enhanced RAF-DB dataset with RAM caching for efficient training\"\"\"\n","\n","    def __init__(self, metadata_df, dataset_root, transform=None, transfer_classes=None, use_ram_cache=True):\n","        if transfer_classes:\n","            self.metadata = metadata_df[metadata_df['emotion_label'].isin(transfer_classes)].copy()\n","        else:\n","            self.metadata = metadata_df.copy()\n","\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.cached_images = []\n","\n","        # Use FIXED class ordering from transfer_classes\n","        self.class_to_idx = {cls: idx for idx, cls in enumerate(transfer_classes)}\n","\n","        print(f\"Loading RAF-DB dataset for training...\")\n","        print(f\"Fixed class ordering: {list(self.class_to_idx.keys())}\")\n","\n","        # Process metadata\n","        for _, row in self.metadata.iterrows():\n","            image_path = os.path.join(dataset_root, row['filepath'].replace('datasets/processed_raf/', ''))\n","            self.images.append(image_path)\n","            self.labels.append(self.class_to_idx[row['emotion_label']])\n","\n","        print(f\"Loaded {len(self.images)} RAF-DB samples\")\n","        self._print_distribution()\n","\n","        if self.use_ram_cache:\n","            self._preload_to_ram()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution with indices\"\"\"\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        class_names = {v: k for k, v in self.class_to_idx.items()}\n","\n","        print(f\"Class distribution:\")\n","        for idx in sorted(label_counts.keys()):\n","            class_name = class_names[idx]\n","            count = label_counts[idx]\n","            percentage = (count / len(self.labels)) * 100\n","            print(f\"  [{idx}] {class_name}: {count} samples ({percentage:.1f}%)\")\n","\n","    def _preload_to_ram(self):\n","        \"\"\"RAM preloading with parallel loading using ThreadPoolExecutor\"\"\"\n","        print(f\"Preloading {len(self.images)} images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            \"\"\"Load single image with error handling\"\"\"\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (INPUT_SIZE, INPUT_SIZE):\n","                    image = image.resize((INPUT_SIZE, INPUT_SIZE), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (INPUT_SIZE, INPUT_SIZE), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=\"Loading to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * INPUT_SIZE * INPUT_SIZE * 3 * 4 / 1e9\n","        print(f\"RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (INPUT_SIZE, INPUT_SIZE):\n","                    image = image.resize((INPUT_SIZE, INPUT_SIZE), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (INPUT_SIZE, INPUT_SIZE), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx]\n","\n","# Enhanced Dataset class with RAM caching for CASME2\n","class CASME2DatasetTraining(Dataset):\n","    \"\"\"Enhanced CASME2 dataset with RAM caching for efficient fine-tuning\"\"\"\n","\n","    def __init__(self, split_metadata, dataset_root, transform=None, split='train',\n","                 transfer_classes=None, casme2_mapping=None, use_ram_cache=True):\n","        samples = split_metadata[split]['samples']\n","\n","        # Apply CASME2 to RAF-DB naming convention mapping first\n","        if casme2_mapping:\n","            mapped_samples = []\n","            for s in samples:\n","                casme2_emotion = s['emotion']\n","                raf_emotion = casme2_mapping.get(casme2_emotion)\n","\n","                if raf_emotion in transfer_classes:\n","                    mapped_sample = s.copy()\n","                    mapped_sample['emotion'] = raf_emotion\n","                    mapped_samples.append(mapped_sample)\n","\n","            print(f\"Mapped {len(mapped_samples)} samples from {len(samples)} total\")\n","            self.samples = mapped_samples\n","        else:\n","            self.samples = samples\n","\n","        self.dataset_root = dataset_root\n","        self.transform = transform\n","        self.split = split\n","        self.use_ram_cache = use_ram_cache\n","        self.images = []\n","        self.labels = []\n","        self.sample_ids = []\n","        self.cached_images = []\n","\n","        # Use FIXED class ordering from transfer_classes\n","        self.class_to_idx = {cls: idx for idx, cls in enumerate(transfer_classes)}\n","\n","        print(f\"Loading CASME2 {split} dataset for fine-tuning...\")\n","        print(f\"Fixed class ordering: {list(self.class_to_idx.keys())}\")\n","\n","        # Process samples\n","        for sample in self.samples:\n","            image_path = os.path.join(dataset_root, split, sample['image_filename'])\n","            self.images.append(image_path)\n","            self.labels.append(self.class_to_idx[sample['emotion']])\n","            self.sample_ids.append(sample['sample_id'])\n","\n","        print(f\"Loaded {len(self.images)} CASME2 {split} samples\")\n","        self._print_distribution()\n","\n","        if self.use_ram_cache:\n","            self._preload_to_ram()\n","\n","    def _print_distribution(self):\n","        \"\"\"Print class distribution with indices\"\"\"\n","        label_counts = {}\n","        for label in self.labels:\n","            label_counts[label] = label_counts.get(label, 0) + 1\n","\n","        class_names = {v: k for k, v in self.class_to_idx.items()}\n","\n","        print(f\"Class distribution:\")\n","        for idx, class_name in enumerate(self.class_to_idx.keys()):\n","            count = label_counts.get(idx, 0)\n","            if count > 0:\n","                percentage = (count / len(self.labels)) * 100\n","                print(f\"  [{idx}] {class_name}: {count} samples ({percentage:.1f}%)\")\n","            else:\n","                print(f\"  [{idx}] {class_name}: NO SAMPLES (placeholder)\")\n","\n","    def _preload_to_ram(self):\n","        \"\"\"RAM preloading with parallel loading using ThreadPoolExecutor\"\"\"\n","        print(f\"Preloading {len(self.images)} {self.split} images to RAM with {RAM_PRELOAD_WORKERS} workers...\")\n","\n","        self.cached_images = [None] * len(self.images)\n","        valid_images = 0\n","\n","        def load_single_image(idx, img_path):\n","            \"\"\"Load single image with error handling\"\"\"\n","            try:\n","                image = Image.open(img_path).convert('RGB')\n","                if image.size != (INPUT_SIZE, INPUT_SIZE):\n","                    image = image.resize((INPUT_SIZE, INPUT_SIZE), Image.Resampling.LANCZOS)\n","                return idx, image, True\n","            except Exception as e:\n","                return idx, Image.new('RGB', (INPUT_SIZE, INPUT_SIZE), (128, 128, 128)), False\n","\n","        with ThreadPoolExecutor(max_workers=RAM_PRELOAD_WORKERS) as executor:\n","            futures = [executor.submit(load_single_image, i, path)\n","                      for i, path in enumerate(self.images)]\n","\n","            for future in tqdm(futures, desc=f\"Loading {self.split} to RAM\"):\n","                idx, image, success = future.result()\n","                self.cached_images[idx] = image\n","                if success:\n","                    valid_images += 1\n","\n","        ram_usage_gb = len(self.cached_images) * INPUT_SIZE * INPUT_SIZE * 3 * 4 / 1e9\n","        print(f\"{self.split.upper()} RAM caching completed: {valid_images}/{len(self.images)} images, ~{ram_usage_gb:.2f}GB\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.use_ram_cache and self.cached_images[idx] is not None:\n","            image = self.cached_images[idx].copy()\n","        else:\n","            try:\n","                image = Image.open(self.images[idx]).convert('RGB')\n","                if image.size != (INPUT_SIZE, INPUT_SIZE):\n","                    image = image.resize((INPUT_SIZE, INPUT_SIZE), Image.Resampling.LANCZOS)\n","            except:\n","                image = Image.new('RGB', (INPUT_SIZE, INPUT_SIZE), (128, 128, 128))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, self.labels[idx], self.sample_ids[idx]\n","\n","# Enhanced metrics calculation\n","def calculate_metrics_robust(outputs, labels, num_classes, average='macro'):\n","    \"\"\"Calculate metrics with enhanced error handling and missing class support\"\"\"\n","    try:\n","        if outputs.size(0) != labels.size(0):\n","            raise ValueError(f\"Batch size mismatch: outputs {outputs.size(0)} vs labels {labels.size(0)}\")\n","\n","        if isinstance(outputs, torch.Tensor):\n","            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n","        else:\n","            predictions = np.array(outputs)\n","\n","        if isinstance(labels, torch.Tensor):\n","            labels = labels.cpu().numpy()\n","        else:\n","            labels = np.array(labels)\n","\n","        # Check which classes actually present in labels\n","        present_classes = np.unique(labels)\n","\n","        accuracy = accuracy_score(labels, predictions)\n","        precision, recall, f1, _ = precision_recall_fscore_support(\n","            labels, predictions,\n","            average=average,\n","            zero_division=0,\n","            labels=list(range(num_classes))\n","        )\n","\n","        return {\n","            'accuracy': float(accuracy),\n","            'precision': float(precision),\n","            'recall': float(recall),\n","            'f1_score': float(f1),\n","            'present_classes': present_classes.tolist()\n","        }\n","    except Exception as e:\n","        print(f\"Warning: Metrics calculation error: {e}\")\n","        return {\n","            'accuracy': 0.0,\n","            'precision': 0.0,\n","            'recall': 0.0,\n","            'f1_score': 0.0,\n","            'present_classes': []\n","        }\n","\n","# Training epoch function\n","def train_epoch(model, dataloader, criterion, optimizer, device, epoch, total_epochs, num_classes):\n","    \"\"\"Training epoch with robust error handling\"\"\"\n","    model.train()\n","    running_loss = 0.0\n","    all_outputs = []\n","    all_labels = []\n","\n","    progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch+1}/{total_epochs}\")\n","\n","    for batch_idx, batch_data in enumerate(progress_bar):\n","        if len(batch_data) == 3:\n","            images, labels, _ = batch_data\n","        else:\n","            images, labels = batch_data\n","\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(images)\n","\n","        if outputs.dim() != 2 or outputs.size(1) != num_classes:\n","            raise ValueError(f\"Invalid output shape: {outputs.shape}, expected [batch_size, {num_classes}]\")\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), GLOBAL_CONFIG['gradient_clip'])\n","\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        all_outputs.append(outputs.detach().cpu())\n","        all_labels.append(labels.detach().cpu())\n","\n","        if batch_idx % 5 == 0:\n","            avg_loss = running_loss / (batch_idx + 1)\n","            current_lr = optimizer.param_groups[0]['lr']\n","            progress_bar.set_postfix({\n","                'Loss': f'{avg_loss:.4f}',\n","                'LR': f'{current_lr:.2e}'\n","            })\n","\n","    try:\n","        epoch_outputs = torch.cat(all_outputs, dim=0)\n","        epoch_labels = torch.cat(all_labels, dim=0)\n","        metrics = calculate_metrics_robust(epoch_outputs, epoch_labels, num_classes, average='macro')\n","    except Exception as e:\n","        print(f\"Warning: Training metrics calculation failed: {e}\")\n","        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0, 'present_classes': []}\n","\n","    avg_loss = running_loss / len(dataloader)\n","    return avg_loss, metrics\n","\n","# Validation epoch function\n","def validate_epoch(model, dataloader, criterion, device, epoch, total_epochs, num_classes):\n","    \"\"\"Validation epoch with robust error handling\"\"\"\n","    model.eval()\n","    running_loss = 0.0\n","    all_outputs = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        progress_bar = tqdm(dataloader, desc=f\"Validation Epoch {epoch+1}/{total_epochs}\")\n","\n","        for batch_idx, batch_data in enumerate(progress_bar):\n","            if len(batch_data) == 3:\n","                images, labels, _ = batch_data\n","            else:\n","                images, labels = batch_data\n","\n","            images, labels = images.to(device), labels.to(device)\n","\n","            outputs = model(images)\n","\n","            loss = criterion(outputs, labels)\n","            running_loss += loss.item()\n","\n","            all_outputs.append(outputs.detach().cpu())\n","            all_labels.append(labels.detach().cpu())\n","\n","            if batch_idx % 3 == 0:\n","                avg_loss = running_loss / (batch_idx + 1)\n","                progress_bar.set_postfix({'Val Loss': f'{avg_loss:.4f}'})\n","\n","    try:\n","        epoch_outputs = torch.cat(all_outputs, dim=0)\n","        epoch_labels = torch.cat(all_labels, dim=0)\n","        metrics = calculate_metrics_robust(epoch_outputs, epoch_labels, num_classes, average='macro')\n","    except Exception as e:\n","        print(f\"Warning: Validation metrics calculation failed: {e}\")\n","        metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0, 'present_classes': []}\n","\n","    avg_loss = running_loss / len(dataloader)\n","    return avg_loss, metrics\n","\n","# Atomic checkpoint saving with validation\n","def save_checkpoint_robust(model, optimizer, scheduler, epoch, train_metrics, val_metrics,\n","                          best_metrics, config, max_retries=3):\n","    \"\"\"Hardened checkpoint saving with atomic write and validation\"\"\"\n","\n","    def make_serializable(obj):\n","        if isinstance(obj, torch.Tensor):\n","            cpu_obj = obj.detach().cpu()\n","            return cpu_obj.item() if cpu_obj.numel() == 1 else cpu_obj.tolist()\n","        elif isinstance(obj, np.ndarray):\n","            return obj.tolist()\n","        elif isinstance(obj, (np.integer, np.int32, np.int64)):\n","            return int(obj)\n","        elif isinstance(obj, (np.floating, np.float32, np.float64)):\n","            return float(obj)\n","        elif isinstance(obj, dict):\n","            return {k: make_serializable(v) for k, v in obj.items()}\n","        elif isinstance(obj, (list, tuple)):\n","            return [make_serializable(item) for item in obj]\n","        else:\n","            return obj\n","\n","    def filter_serializable_config(config):\n","        \"\"\"Filter config to only include serializable items\"\"\"\n","        excluded_keys = [\n","            'transform_train',\n","            'transform_val',\n","            'criterion_factory',\n","            'optimizer_scheduler_factory',\n","            'device'\n","        ]\n","        return {k: v for k, v in config.items() if k not in excluded_keys}\n","\n","    serializable_config = filter_serializable_config(config)\n","\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","        'train_metrics': make_serializable(train_metrics),\n","        'val_metrics': make_serializable(val_metrics),\n","        'config': make_serializable(serializable_config),\n","        'best_f1': float(best_metrics['f1']),\n","        'best_loss': float(best_metrics['loss']),\n","        'best_acc': float(best_metrics['accuracy']),\n","        'transfer_classes': config['transfer_classes'],\n","        'num_classes': config['num_classes'],\n","        'training_stage': config['training_stage']\n","    }\n","\n","    final_path = config['checkpoint_path']\n","    checkpoint_dir = config['checkpoint_root']\n","\n","    for attempt in range(max_retries):\n","        try:\n","            temp_fd, temp_path = tempfile.mkstemp(dir=checkpoint_dir, suffix='.pth.tmp')\n","            os.close(temp_fd)\n","\n","            torch.save(checkpoint, temp_path)\n","\n","            validation_checkpoint = torch.load(temp_path, map_location='cpu')\n","\n","            required_keys = ['model_state_dict', 'epoch', 'best_f1', 'num_classes']\n","            for key in required_keys:\n","                if key not in validation_checkpoint:\n","                    raise ValueError(f\"Checkpoint validation failed: missing key '{key}'\")\n","\n","            if validation_checkpoint['epoch'] != epoch:\n","                raise ValueError(f\"Checkpoint epoch mismatch: saved {epoch}, loaded {validation_checkpoint['epoch']}\")\n","\n","            shutil.move(temp_path, final_path)\n","\n","            return final_path\n","\n","        except Exception as e:\n","            if os.path.exists(temp_path):\n","                try:\n","                    os.remove(temp_path)\n","                except:\n","                    pass\n","\n","            if attempt < max_retries - 1:\n","                wait_time = 2 ** attempt\n","                time.sleep(wait_time)\n","            else:\n","                print(f\"All {max_retries} checkpoint save attempts failed: {e}\")\n","                return None\n","\n","    return None\n","\n","# Safe JSON serialization\n","def safe_json_serialize(obj):\n","    \"\"\"Convert objects to JSON-serializable format\"\"\"\n","    if isinstance(obj, torch.Tensor):\n","        return obj.cpu().item() if obj.numel() == 1 else obj.cpu().numpy().tolist()\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    elif isinstance(obj, (np.integer, np.int32, np.int64)):\n","        return int(obj)\n","    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n","        return float(obj)\n","    elif isinstance(obj, dict):\n","        return {k: safe_json_serialize(v) for k, v in obj.items()}\n","    elif isinstance(obj, (list, tuple)):\n","        return [safe_json_serialize(item) for item in obj]\n","    else:\n","        try:\n","            return float(obj) if isinstance(obj, (int, float)) else str(obj)\n","        except:\n","            return str(obj)\n","\n","# Create datasets based on training stage\n","print(\"\\n\" + \"=\" * 70)\n","print(\"DATASET LOADING\")\n","print(\"=\" * 70)\n","\n","if GLOBAL_CONFIG['training_stage'] == 'pretrain':\n","    print(\"\\nLoading RAF-DB datasets for pre-training...\")\n","\n","    metadata_df = pd.read_csv(GLOBAL_CONFIG['metadata_path'])\n","\n","    train_metadata = metadata_df[metadata_df['split'] == 'train']\n","    val_metadata = metadata_df[metadata_df['split'] == 'val']\n","\n","    train_dataset = RAFDBDatasetTraining(\n","        metadata_df=train_metadata,\n","        dataset_root=GLOBAL_CONFIG['dataset_root'],\n","        transform=GLOBAL_CONFIG['transform_train'],\n","        transfer_classes=GLOBAL_CONFIG['transfer_classes'],\n","        use_ram_cache=True\n","    )\n","\n","    val_dataset = RAFDBDatasetTraining(\n","        metadata_df=val_metadata,\n","        dataset_root=GLOBAL_CONFIG['dataset_root'],\n","        transform=GLOBAL_CONFIG['transform_val'],\n","        transfer_classes=GLOBAL_CONFIG['transfer_classes'],\n","        use_ram_cache=True\n","    )\n","\n","else:\n","    print(\"\\nLoading CASME2 datasets for fine-tuning...\")\n","\n","    with open(GLOBAL_CONFIG['metadata_path'], 'r') as f:\n","        split_metadata = json.load(f)\n","\n","    train_dataset = CASME2DatasetTraining(\n","        split_metadata=split_metadata,\n","        dataset_root=GLOBAL_CONFIG['dataset_root'],\n","        transform=GLOBAL_CONFIG['transform_train'],\n","        split='train',\n","        transfer_classes=GLOBAL_CONFIG['transfer_classes'],\n","        casme2_mapping=GLOBAL_CONFIG['casme2_mapping'],\n","        use_ram_cache=True\n","    )\n","\n","    val_dataset = CASME2DatasetTraining(\n","        split_metadata=split_metadata,\n","        dataset_root=GLOBAL_CONFIG['dataset_root'],\n","        transform=GLOBAL_CONFIG['transform_val'],\n","        split='val',\n","        transfer_classes=GLOBAL_CONFIG['transfer_classes'],\n","        casme2_mapping=GLOBAL_CONFIG['casme2_mapping'],\n","        use_ram_cache=True\n","    )\n","\n","# Create data loaders\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=GLOBAL_CONFIG['batch_size'],\n","    shuffle=True,\n","    num_workers=GLOBAL_CONFIG['num_workers'],\n","    pin_memory=True,\n","    prefetch_factor=2\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=GLOBAL_CONFIG['batch_size'],\n","    shuffle=False,\n","    num_workers=GLOBAL_CONFIG['num_workers'],\n","    pin_memory=True,\n","    prefetch_factor=2\n",")\n","\n","print(f\"\\nDataLoader configuration:\")\n","print(f\"  Training batches: {len(train_loader)} (samples: {len(train_dataset)})\")\n","print(f\"  Validation batches: {len(val_loader)} (samples: {len(val_dataset)})\")\n","print(f\"  Batch size: {GLOBAL_CONFIG['batch_size']}\")\n","print(f\"  Num workers: {GLOBAL_CONFIG['num_workers']}\")\n","\n","# Initialize model\n","print(\"\\n\" + \"=\" * 70)\n","print(\"MODEL INITIALIZATION\")\n","print(\"=\" * 70)\n","\n","model = ViTTransferLearning(\n","    num_classes=GLOBAL_CONFIG['num_classes'],\n","    dropout_rate=GLOBAL_CONFIG['dropout_rate'],\n","    pretrained_checkpoint=GLOBAL_CONFIG['pretrained_checkpoint'] if GLOBAL_CONFIG['training_stage'] == 'finetune' else None,\n","    freeze_encoder=GLOBAL_CONFIG['freeze_encoder']\n",").to(GLOBAL_CONFIG['device'])\n","\n","# Create criterion and optimizer\n","criterion = GLOBAL_CONFIG['criterion_factory'](\n","    use_focal_loss=GLOBAL_CONFIG['use_focal_loss'],\n","    gamma=GLOBAL_CONFIG['focal_loss_gamma']\n",")\n","\n","optimizer, scheduler = GLOBAL_CONFIG['optimizer_scheduler_factory'](\n","    model,\n","    GLOBAL_CONFIG['learning_rate'],\n","    GLOBAL_CONFIG['weight_decay']\n",")\n","\n","print(f\"\\nModel: ViT Transfer Learning ({GLOBAL_CONFIG['training_stage']} stage)\")\n","print(f\"Optimizer: AdamW (LR={GLOBAL_CONFIG['learning_rate']})\")\n","print(f\"Scheduler: ReduceLROnPlateau (patience=5)\")\n","print(f\"Criterion: {'Focal Loss' if GLOBAL_CONFIG['use_focal_loss'] else 'CrossEntropy Loss'}\")\n","\n","# Training history tracking\n","training_history = {\n","    'train_loss': [],\n","    'val_loss': [],\n","    'train_f1': [],\n","    'val_f1': [],\n","    'train_acc': [],\n","    'val_acc': [],\n","    'learning_rate': [],\n","    'epoch_time': []\n","}\n","\n","best_metrics = {\n","    'f1': 0.0,\n","    'loss': float('inf'),\n","    'accuracy': 0.0,\n","    'epoch': 0\n","}\n","\n","# Main training loop\n","print(\"\\n\" + \"=\" * 70)\n","print(\"TRAINING\")\n","print(\"=\" * 70)\n","print(f\"Training configuration: {GLOBAL_CONFIG['num_epochs']} epochs\")\n","\n","start_time = time.time()\n","\n","for epoch in range(GLOBAL_CONFIG['num_epochs']):\n","    epoch_start_time = time.time()\n","    print(f\"\\nEpoch {epoch+1}/{GLOBAL_CONFIG['num_epochs']}\")\n","\n","    train_loss, train_metrics = train_epoch(\n","        model, train_loader, criterion, optimizer,\n","        GLOBAL_CONFIG['device'], epoch, GLOBAL_CONFIG['num_epochs'],\n","        GLOBAL_CONFIG['num_classes']\n","    )\n","\n","    val_loss, val_metrics = validate_epoch(\n","        model, val_loader, criterion,\n","        GLOBAL_CONFIG['device'], epoch, GLOBAL_CONFIG['num_epochs'],\n","        GLOBAL_CONFIG['num_classes']\n","    )\n","\n","    if scheduler:\n","        scheduler.step(val_metrics['f1_score'])\n","\n","    epoch_time = time.time() - epoch_start_time\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    training_history['train_loss'].append(float(train_loss))\n","    training_history['val_loss'].append(float(val_loss))\n","    training_history['train_f1'].append(float(train_metrics['f1_score']))\n","    training_history['val_f1'].append(float(val_metrics['f1_score']))\n","    training_history['train_acc'].append(float(train_metrics['accuracy']))\n","    training_history['val_acc'].append(float(val_metrics['accuracy']))\n","    training_history['learning_rate'].append(float(current_lr))\n","    training_history['epoch_time'].append(float(epoch_time))\n","\n","    print(f\"Train - Loss: {train_loss:.4f}, F1: {train_metrics['f1_score']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n","    print(f\"Val   - Loss: {val_loss:.4f}, F1: {val_metrics['f1_score']:.4f}, Acc: {val_metrics['accuracy']:.4f}\")\n","    print(f\"Time  - Epoch: {epoch_time:.1f}s, LR: {current_lr:.2e}\")\n","\n","    save_model = False\n","    improvement_reason = \"\"\n","\n","    if val_metrics['f1_score'] > best_metrics['f1']:\n","        save_model = True\n","        improvement_reason = \"Higher F1\"\n","    elif val_metrics['f1_score'] == best_metrics['f1']:\n","        if val_loss < best_metrics['loss']:\n","            save_model = True\n","            improvement_reason = \"Same F1, Lower Loss\"\n","        elif val_loss == best_metrics['loss'] and val_metrics['accuracy'] > best_metrics['accuracy']:\n","            save_model = True\n","            improvement_reason = \"Same F1&Loss, Higher Accuracy\"\n","\n","    if save_model:\n","        best_metrics['f1'] = val_metrics['f1_score']\n","        best_metrics['loss'] = val_loss\n","        best_metrics['accuracy'] = val_metrics['accuracy']\n","        best_metrics['epoch'] = epoch + 1\n","\n","        best_model_path = save_checkpoint_robust(\n","            model, optimizer, scheduler, epoch,\n","            train_metrics, val_metrics,\n","            best_metrics, GLOBAL_CONFIG\n","        )\n","\n","        if best_model_path:\n","            print(f\"Checkpoint saved: {improvement_reason} - F1: {best_metrics['f1']:.4f}\")\n","\n","    elapsed_time = time.time() - start_time\n","    estimated_total = (elapsed_time / (epoch + 1)) * GLOBAL_CONFIG['num_epochs']\n","    remaining_time = estimated_total - elapsed_time\n","    progress_pct = ((epoch + 1) / GLOBAL_CONFIG['num_epochs']) * 100\n","\n","    print(f\"Progress: {progress_pct:.1f}% | Best F1: {best_metrics['f1']:.4f} | ETA: {remaining_time/60:.1f}min\")\n","\n","# Training completion\n","total_time = time.time() - start_time\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(f\"TRAINING COMPLETED - {GLOBAL_CONFIG['training_stage'].upper()} STAGE\")\n","print(\"=\" * 70)\n","print(f\"Training time: {total_time/60:.1f} minutes\")\n","print(f\"Epochs completed: {GLOBAL_CONFIG['num_epochs']}\")\n","print(f\"Best validation F1: {best_metrics['f1']:.4f} (epoch {best_metrics['epoch']})\")\n","print(f\"Final train F1: {training_history['train_f1'][-1]:.4f}\")\n","print(f\"Final validation F1: {training_history['val_f1'][-1]:.4f}\")\n","print(f\"Checkpoint saved: {GLOBAL_CONFIG['checkpoint_filename']}\")\n","\n","# Export training documentation\n","print(\"\\n\" + \"=\" * 70)\n","print(\"EXPORTING TRAINING DOCUMENTATION\")\n","print(\"=\" * 70)\n","\n","training_history_path = f\"{GLOBAL_CONFIG['logs_path']}/training_logs/training_history.json\"\n","\n","try:\n","    def filter_json_config(config):\n","        \"\"\"Filter config to only include JSON-serializable items\"\"\"\n","        excluded_keys = [\n","            'transform_train',\n","            'transform_val',\n","            'criterion_factory',\n","            'optimizer_scheduler_factory',\n","            'device'\n","        ]\n","        filtered = {k: v for k, v in config.items() if k not in excluded_keys}\n","        if 'device' in config:\n","            filtered['device_name'] = str(config['device'])\n","        return filtered\n","\n","    training_summary = {\n","        'experiment_type': f'ViT_Transfer_Learning_{GLOBAL_CONFIG[\"training_stage\"].upper()}',\n","        'training_stage': GLOBAL_CONFIG['training_stage'],\n","        'dataset_name': GLOBAL_CONFIG['dataset_name'],\n","        'training_history': safe_json_serialize(training_history),\n","        'best_val_f1': float(best_metrics['f1']),\n","        'best_val_loss': float(best_metrics['loss']),\n","        'best_val_accuracy': float(best_metrics['accuracy']),\n","        'best_epoch': int(best_metrics['epoch']),\n","        'total_epochs': int(GLOBAL_CONFIG['num_epochs']),\n","        'total_time_minutes': float(total_time / 60),\n","        'average_epoch_time_seconds': float(np.mean(training_history['epoch_time'])),\n","        'final_train_f1': float(training_history['train_f1'][-1]),\n","        'final_val_f1': float(training_history['val_f1'][-1]),\n","        'checkpoint_filename': GLOBAL_CONFIG['checkpoint_filename'],\n","        'dataset_info': {\n","            'name': GLOBAL_CONFIG['dataset_name'],\n","            'train_samples': len(train_dataset),\n","            'val_samples': len(val_dataset),\n","            'num_classes': GLOBAL_CONFIG['num_classes'],\n","            'transfer_classes': GLOBAL_CONFIG['transfer_classes']\n","        },\n","        'architecture_info': {\n","            'model_type': 'ViTTransferLearning',\n","            'backbone': GLOBAL_CONFIG['vit_model'],\n","            'variant': GLOBAL_CONFIG['vit_variant'],\n","            'patch_size': GLOBAL_CONFIG['patch_size'],\n","            'input_size': f\"{GLOBAL_CONFIG['input_size']}x{GLOBAL_CONFIG['input_size']}\",\n","            'dropout': GLOBAL_CONFIG['dropout_rate'],\n","            'classification_head': '768->512->128->5'\n","        },\n","        'training_config': {\n","            'batch_size': GLOBAL_CONFIG['batch_size'],\n","            'learning_rate': GLOBAL_CONFIG['learning_rate'],\n","            'weight_decay': GLOBAL_CONFIG['weight_decay'],\n","            'gradient_clip': GLOBAL_CONFIG['gradient_clip'],\n","            'augmentation': GLOBAL_CONFIG['augmentation_strength'],\n","            'loss_function': 'Focal Loss' if GLOBAL_CONFIG['use_focal_loss'] else 'CrossEntropy',\n","            'ram_preload_workers': GLOBAL_CONFIG['ram_preload_workers']\n","        }\n","    }\n","\n","    if GLOBAL_CONFIG['training_stage'] == 'finetune':\n","        training_summary['transfer_learning_config'] = {\n","            'pretrained_checkpoint': os.path.basename(GLOBAL_CONFIG['pretrained_checkpoint']),\n","            'finetune_strategy': GLOBAL_CONFIG['finetune_strategy'],\n","            'encoder_frozen': GLOBAL_CONFIG['freeze_encoder']\n","        }\n","\n","    with open(training_history_path, 'w') as f:\n","        json.dump(training_summary, f, indent=2)\n","\n","    print(f\"Training documentation saved: training_history.json\")\n","    print(f\"Location: {GLOBAL_CONFIG['logs_subdir']}/training_logs/\")\n","\n","except Exception as e:\n","    print(f\"Warning: Could not save training documentation: {e}\")\n","\n","# Memory cleanup\n","if torch.cuda.is_available():\n","    torch.cuda.synchronize()\n","    torch.cuda.empty_cache()\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(f\"Next: Cell 3 - {GLOBAL_CONFIG['training_stage'].upper()} Stage Evaluation\")\n","print(\"=\" * 70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"99cr4eKEI6uZ","executionInfo":{"status":"ok","timestamp":1760613986027,"user_tz":-420,"elapsed":265422,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"704c1c77-6b67-4660-9983-a6719f299766","collapsed":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","======================================================================\n","VIT TRANSFER LEARNING TRAINING PIPELINE - STAGE: FINETUNE\n","======================================================================\n","\n","Fine-tuning Configuration:\n","  Dataset: CASME2 Apex Frame (Phase 1)\n","  Classes: 5 (fixed ordering, fear=placeholder)\n","  Expected samples: ~150-160 apex images\n","  Pre-trained checkpoint: raf_pretrain_best_f1.pth\n","  Strategy: frozen_encoder\n","  Encoder frozen: True\n","  Batch size: 8\n","  Epochs: 50\n","  Learning rate: 3e-06\n","  Augmentation: enhanced\n","  Output: casme2_finetune_best_f1.pth\n","\n","Loss function: CrossEntropy Loss\n","\n","RAM preload configuration:\n","  Workers: 128 (parallel image loading)\n","  Method: ThreadPoolExecutor with concurrent futures\n","  Target: Load all images to RAM before training starts\n","\n","======================================================================\n","DATASET LOADING\n","======================================================================\n","\n","Loading CASME2 datasets for fine-tuning...\n","Mapped 101 samples from 201 total\n","Loading CASME2 train dataset for fine-tuning...\n","Fixed class ordering: ['disgust', 'happy', 'surprise', 'sad', 'fear']\n","Loaded 101 CASME2 train samples\n","Class distribution:\n","  [0] disgust: 50 samples (49.5%)\n","  [1] happy: 25 samples (24.8%)\n","  [2] surprise: 20 samples (19.8%)\n","  [3] sad: 5 samples (5.0%)\n","  [4] fear: 1 samples (1.0%)\n","Preloading 101 train images to RAM with 128 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading train to RAM: 100%|██████████| 101/101 [00:04<00:00, 21.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["TRAIN RAM caching completed: 101/101 images, ~0.18GB\n","Mapped 13 samples from 26 total\n","Loading CASME2 val dataset for fine-tuning...\n","Fixed class ordering: ['disgust', 'happy', 'surprise', 'sad', 'fear']\n","Loaded 13 CASME2 val samples\n","Class distribution:\n","  [0] disgust: 6 samples (46.2%)\n","  [1] happy: 3 samples (23.1%)\n","  [2] surprise: 2 samples (15.4%)\n","  [3] sad: 1 samples (7.7%)\n","  [4] fear: 1 samples (7.7%)\n","Preloading 13 val images to RAM with 128 workers...\n"]},{"output_type":"stream","name":"stderr","text":["Loading val to RAM: 100%|██████████| 13/13 [00:01<00:00,  8.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["VAL RAM caching completed: 13/13 images, ~0.02GB\n","\n","DataLoader configuration:\n","  Training batches: 13 (samples: 101)\n","  Validation batches: 2 (samples: 13)\n","  Batch size: 8\n","  Num workers: 8\n","\n","======================================================================\n","MODEL INITIALIZATION\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at google/vit-base-patch32-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["ViT encoder frozen for fine-tuning\n","ViT Transfer Learning: 768 -> 512 -> 128 -> 5\n","\n","Loading pre-trained weights from: raf_pretrain_best_f1.pth\n","  ViT encoder weights loaded\n","  Missing keys: 0\n","  Unexpected keys: 0\n","  Classifier layers loaded\n","  Transfer learning initialization complete\n","Using CrossEntropy Loss with class weights\n","Optimizer: AdamW (lr=3e-06, wd=0.0001)\n","Scheduler: ReduceLROnPlateau (monitor=val_f1, patience=5)\n","\n","Model: ViT Transfer Learning (finetune stage)\n","Optimizer: AdamW (LR=3e-06)\n","Scheduler: ReduceLROnPlateau (patience=5)\n","Criterion: CrossEntropy Loss\n","\n","======================================================================\n","TRAINING\n","======================================================================\n","Training configuration: 50 epochs\n","\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1/50: 100%|██████████| 13/13 [00:02<00:00,  4.96it/s, Loss=1.5900, LR=3.00e-06]\n","Validation Epoch 1/50: 100%|██████████| 2/2 [00:00<00:00,  3.91it/s, Val Loss=1.3261]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.5775, F1: 0.1696, Acc: 0.3069\n","Val   - Loss: 1.4998, F1: 0.1176, Acc: 0.3846\n","Time  - Epoch: 3.2s, LR: 3.00e-06\n","Checkpoint saved: Higher F1 - F1: 0.1176\n","Progress: 2.0% | Best F1: 0.1176 | ETA: 3.8min\n","\n","Epoch 2/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 2/50: 100%|██████████| 13/13 [00:02<00:00,  6.06it/s, Loss=1.5910, LR=3.00e-06]\n","Validation Epoch 2/50: 100%|██████████| 2/2 [00:00<00:00,  2.33it/s, Val Loss=1.2839]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.5634, F1: 0.1655, Acc: 0.2871\n","Val   - Loss: 1.4824, F1: 0.1111, Acc: 0.3846\n","Time  - Epoch: 3.0s, LR: 3.00e-06\n","Progress: 4.0% | Best F1: 0.1176 | ETA: 3.1min\n","\n","Epoch 3/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 3/50: 100%|██████████| 13/13 [00:02<00:00,  4.58it/s, Loss=1.5774, LR=3.00e-06]\n","Validation Epoch 3/50: 100%|██████████| 2/2 [00:00<00:00,  3.48it/s, Val Loss=1.2501]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.5796, F1: 0.1338, Acc: 0.3366\n","Val   - Loss: 1.4678, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.4s, LR: 3.00e-06\n","Checkpoint saved: Higher F1 - F1: 0.1263\n","Progress: 6.0% | Best F1: 0.1263 | ETA: 3.3min\n","\n","Epoch 4/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 4/50: 100%|██████████| 13/13 [00:02<00:00,  6.09it/s, Loss=1.5128, LR=3.00e-06]\n","Validation Epoch 4/50: 100%|██████████| 2/2 [00:00<00:00,  3.52it/s, Val Loss=1.2183]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.5057, F1: 0.2000, Acc: 0.4158\n","Val   - Loss: 1.4552, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.7s, LR: 3.00e-06\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 8.0% | Best F1: 0.1263 | ETA: 3.3min\n","\n","Epoch 5/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 5/50: 100%|██████████| 13/13 [00:01<00:00,  6.53it/s, Loss=1.5390, LR=3.00e-06]\n","Validation Epoch 5/50: 100%|██████████| 2/2 [00:00<00:00,  3.61it/s, Val Loss=1.1897]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.5031, F1: 0.2041, Acc: 0.4257\n","Val   - Loss: 1.4450, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.6s, LR: 3.00e-06\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 10.0% | Best F1: 0.1263 | ETA: 3.3min\n","\n","Epoch 6/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 6/50: 100%|██████████| 13/13 [00:02<00:00,  4.66it/s, Loss=1.4904, LR=3.00e-06]\n","Validation Epoch 6/50: 100%|██████████| 2/2 [00:00<00:00,  3.55it/s, Val Loss=1.1608]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.4862, F1: 0.2132, Acc: 0.4455\n","Val   - Loss: 1.4353, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.4s, LR: 3.00e-06\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 12.0% | Best F1: 0.1263 | ETA: 3.2min\n","\n","Epoch 7/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 7/50: 100%|██████████| 13/13 [00:02<00:00,  6.04it/s, Loss=1.4980, LR=3.00e-06]\n","Validation Epoch 7/50: 100%|██████████| 2/2 [00:00<00:00,  3.54it/s, Val Loss=1.1345]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.5013, F1: 0.1806, Acc: 0.4257\n","Val   - Loss: 1.4279, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.7s, LR: 3.00e-06\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 14.0% | Best F1: 0.1263 | ETA: 3.1min\n","\n","Epoch 8/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 8/50: 100%|██████████| 13/13 [00:02<00:00,  6.14it/s, Loss=1.4849, LR=3.00e-06]\n","Validation Epoch 8/50: 100%|██████████| 2/2 [00:00<00:00,  3.62it/s, Val Loss=1.1115]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.5109, F1: 0.1710, Acc: 0.3762\n","Val   - Loss: 1.4220, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.7s, LR: 3.00e-06\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 16.0% | Best F1: 0.1263 | ETA: 3.2min\n","\n","Epoch 9/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 9/50: 100%|██████████| 13/13 [00:02<00:00,  6.33it/s, Loss=1.4697, LR=3.00e-06]\n","Validation Epoch 9/50: 100%|██████████| 2/2 [00:00<00:00,  3.57it/s, Val Loss=1.0904]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.4586, F1: 0.1665, Acc: 0.4158\n","Val   - Loss: 1.4165, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.6s, LR: 1.50e-06\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 18.0% | Best F1: 0.1263 | ETA: 3.3min\n","\n","Epoch 10/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 10/50: 100%|██████████| 13/13 [00:02<00:00,  5.80it/s, Loss=1.3735, LR=1.50e-06]\n","Validation Epoch 10/50: 100%|██████████| 2/2 [00:00<00:00,  3.06it/s, Val Loss=1.0816]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3763, F1: 0.1871, Acc: 0.4653\n","Val   - Loss: 1.4143, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.9s, LR: 1.50e-06\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 20.0% | Best F1: 0.1263 | ETA: 3.2min\n","\n","Epoch 11/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 11/50: 100%|██████████| 13/13 [00:03<00:00,  3.80it/s, Loss=1.4036, LR=1.50e-06]\n","Validation Epoch 11/50: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s, Val Loss=1.0734]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.4289, F1: 0.1792, Acc: 0.4554\n","Val   - Loss: 1.4120, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 4.5s, LR: 1.50e-06\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 22.0% | Best F1: 0.1263 | ETA: 3.2min\n","\n","Epoch 12/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 12/50: 100%|██████████| 13/13 [00:02<00:00,  6.38it/s, Loss=1.4261, LR=1.50e-06]\n","Validation Epoch 12/50: 100%|██████████| 2/2 [00:00<00:00,  2.69it/s, Val Loss=1.0643]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.4135, F1: 0.2273, Acc: 0.4752\n","Val   - Loss: 1.4103, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.8s, LR: 1.50e-06\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 24.0% | Best F1: 0.1263 | ETA: 3.1min\n","\n","Epoch 13/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 13/50: 100%|██████████| 13/13 [00:02<00:00,  6.35it/s, Loss=1.3943, LR=1.50e-06]\n","Validation Epoch 13/50: 100%|██████████| 2/2 [00:00<00:00,  3.50it/s, Val Loss=1.0561]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3910, F1: 0.1913, Acc: 0.4851\n","Val   - Loss: 1.4090, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.6s, LR: 1.50e-06\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 26.0% | Best F1: 0.1263 | ETA: 3.0min\n","\n","Epoch 14/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 14/50: 100%|██████████| 13/13 [00:02<00:00,  4.46it/s, Loss=1.4109, LR=1.50e-06]\n","Validation Epoch 14/50: 100%|██████████| 2/2 [00:00<00:00,  3.44it/s, Val Loss=1.0475]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.4086, F1: 0.1906, Acc: 0.4752\n","Val   - Loss: 1.4078, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.5s, LR: 1.50e-06\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 28.0% | Best F1: 0.1263 | ETA: 3.0min\n","\n","Epoch 15/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 15/50: 100%|██████████| 13/13 [00:02<00:00,  6.16it/s, Loss=1.4521, LR=1.50e-06]\n","Validation Epoch 15/50: 100%|██████████| 2/2 [00:00<00:00,  2.84it/s, Val Loss=1.0385]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.4016, F1: 0.2329, Acc: 0.4851\n","Val   - Loss: 1.4061, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.8s, LR: 7.50e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 30.0% | Best F1: 0.1263 | ETA: 3.0min\n","\n","Epoch 16/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 16/50: 100%|██████████| 13/13 [00:03<00:00,  4.24it/s, Loss=1.3971, LR=7.50e-07]\n","Validation Epoch 16/50: 100%|██████████| 2/2 [00:00<00:00,  3.44it/s, Val Loss=1.0342]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.4027, F1: 0.1802, Acc: 0.4554\n","Val   - Loss: 1.4057, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.7s, LR: 7.50e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 32.0% | Best F1: 0.1263 | ETA: 2.9min\n","\n","Epoch 17/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 17/50: 100%|██████████| 13/13 [00:02<00:00,  6.15it/s, Loss=1.3913, LR=7.50e-07]\n","Validation Epoch 17/50: 100%|██████████| 2/2 [00:00<00:00,  3.31it/s, Val Loss=1.0306]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3866, F1: 0.1776, Acc: 0.4653\n","Val   - Loss: 1.4055, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.7s, LR: 7.50e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 34.0% | Best F1: 0.1263 | ETA: 2.8min\n","\n","Epoch 18/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 18/50: 100%|██████████| 13/13 [00:02<00:00,  6.17it/s, Loss=1.4171, LR=7.50e-07]\n","Validation Epoch 18/50: 100%|██████████| 2/2 [00:00<00:00,  3.19it/s, Val Loss=1.0270]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.4004, F1: 0.1540, Acc: 0.4257\n","Val   - Loss: 1.4049, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.8s, LR: 7.50e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 36.0% | Best F1: 0.1263 | ETA: 2.7min\n","\n","Epoch 19/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 19/50: 100%|██████████| 13/13 [00:02<00:00,  4.93it/s, Loss=1.4449, LR=7.50e-07]\n","Validation Epoch 19/50: 100%|██████████| 2/2 [00:00<00:00,  2.55it/s, Val Loss=1.0239]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.4338, F1: 0.2335, Acc: 0.4653\n","Val   - Loss: 1.4043, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.4s, LR: 7.50e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 38.0% | Best F1: 0.1263 | ETA: 2.7min\n","\n","Epoch 20/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 20/50: 100%|██████████| 13/13 [00:02<00:00,  6.32it/s, Loss=1.3733, LR=7.50e-07]\n","Validation Epoch 20/50: 100%|██████████| 2/2 [00:00<00:00,  3.41it/s, Val Loss=1.0206]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3657, F1: 0.1872, Acc: 0.4257\n","Val   - Loss: 1.4038, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.7s, LR: 7.50e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 40.0% | Best F1: 0.1263 | ETA: 2.6min\n","\n","Epoch 21/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 21/50: 100%|██████████| 13/13 [00:02<00:00,  4.39it/s, Loss=1.3651, LR=7.50e-07]\n","Validation Epoch 21/50: 100%|██████████| 2/2 [00:00<00:00,  2.47it/s, Val Loss=1.0174]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3834, F1: 0.1597, Acc: 0.4257\n","Val   - Loss: 1.4035, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.8s, LR: 3.75e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 42.0% | Best F1: 0.1263 | ETA: 2.5min\n","\n","Epoch 22/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 22/50: 100%|██████████| 13/13 [00:02<00:00,  5.63it/s, Loss=1.4022, LR=3.75e-07]\n","Validation Epoch 22/50: 100%|██████████| 2/2 [00:00<00:00,  3.33it/s, Val Loss=1.0160]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3939, F1: 0.1419, Acc: 0.4257\n","Val   - Loss: 1.4033, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.9s, LR: 3.75e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 44.0% | Best F1: 0.1263 | ETA: 2.4min\n","\n","Epoch 23/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 23/50: 100%|██████████| 13/13 [00:02<00:00,  5.47it/s, Loss=1.3841, LR=3.75e-07]\n","Validation Epoch 23/50: 100%|██████████| 2/2 [00:00<00:00,  3.40it/s, Val Loss=1.0142]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3605, F1: 0.2094, Acc: 0.4851\n","Val   - Loss: 1.4031, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.0s, LR: 3.75e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 46.0% | Best F1: 0.1263 | ETA: 2.3min\n","\n","Epoch 24/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 24/50: 100%|██████████| 13/13 [00:02<00:00,  4.46it/s, Loss=1.3983, LR=3.75e-07]\n","Validation Epoch 24/50: 100%|██████████| 2/2 [00:00<00:00,  3.30it/s, Val Loss=1.0123]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3885, F1: 0.1611, Acc: 0.4356\n","Val   - Loss: 1.4030, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.5s, LR: 3.75e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 48.0% | Best F1: 0.1263 | ETA: 2.2min\n","\n","Epoch 25/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 25/50: 100%|██████████| 13/13 [00:02<00:00,  5.81it/s, Loss=1.3446, LR=3.75e-07]\n","Validation Epoch 25/50: 100%|██████████| 2/2 [00:00<00:00,  3.26it/s, Val Loss=1.0105]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3522, F1: 0.2153, Acc: 0.4554\n","Val   - Loss: 1.4028, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.9s, LR: 3.75e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 50.0% | Best F1: 0.1263 | ETA: 2.2min\n","\n","Epoch 26/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 26/50: 100%|██████████| 13/13 [00:02<00:00,  5.12it/s, Loss=1.3673, LR=3.75e-07]\n","Validation Epoch 26/50: 100%|██████████| 2/2 [00:01<00:00,  1.83it/s, Val Loss=1.0089]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3750, F1: 0.1654, Acc: 0.4554\n","Val   - Loss: 1.4026, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.7s, LR: 3.75e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 52.0% | Best F1: 0.1263 | ETA: 2.1min\n","\n","Epoch 27/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 27/50: 100%|██████████| 13/13 [00:02<00:00,  5.82it/s, Loss=1.3463, LR=3.75e-07]\n","Validation Epoch 27/50: 100%|██████████| 2/2 [00:00<00:00,  3.31it/s, Val Loss=1.0071]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3533, F1: 0.1877, Acc: 0.4752\n","Val   - Loss: 1.4026, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.9s, LR: 1.88e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 54.0% | Best F1: 0.1263 | ETA: 2.0min\n","\n","Epoch 28/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 28/50: 100%|██████████| 13/13 [00:02<00:00,  5.81it/s, Loss=1.3664, LR=1.88e-07]\n","Validation Epoch 28/50: 100%|██████████| 2/2 [00:00<00:00,  2.95it/s, Val Loss=1.0062]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3504, F1: 0.2009, Acc: 0.4851\n","Val   - Loss: 1.4025, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.9s, LR: 1.88e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 56.0% | Best F1: 0.1263 | ETA: 1.9min\n","\n","Epoch 29/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 29/50: 100%|██████████| 13/13 [00:02<00:00,  5.03it/s, Loss=1.3849, LR=1.88e-07]\n","Validation Epoch 29/50: 100%|██████████| 2/2 [00:00<00:00,  3.19it/s, Val Loss=1.0055]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3917, F1: 0.1798, Acc: 0.4554\n","Val   - Loss: 1.4024, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.2s, LR: 1.88e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 58.0% | Best F1: 0.1263 | ETA: 1.8min\n","\n","Epoch 30/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 30/50: 100%|██████████| 13/13 [00:02<00:00,  6.09it/s, Loss=1.4424, LR=1.88e-07]\n","Validation Epoch 30/50: 100%|██████████| 2/2 [00:00<00:00,  3.24it/s, Val Loss=1.0046]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.4248, F1: 0.1331, Acc: 0.3960\n","Val   - Loss: 1.4023, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.8s, LR: 1.88e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 60.0% | Best F1: 0.1263 | ETA: 1.7min\n","\n","Epoch 31/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 31/50: 100%|██████████| 13/13 [00:02<00:00,  5.71it/s, Loss=1.3619, LR=1.88e-07]\n","Validation Epoch 31/50: 100%|██████████| 2/2 [00:00<00:00,  2.12it/s, Val Loss=1.0038]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3693, F1: 0.2257, Acc: 0.4950\n","Val   - Loss: 1.4023, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.2s, LR: 1.88e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 62.0% | Best F1: 0.1263 | ETA: 1.6min\n","\n","Epoch 32/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 32/50: 100%|██████████| 13/13 [00:02<00:00,  6.15it/s, Loss=1.3894, LR=1.88e-07]\n","Validation Epoch 32/50: 100%|██████████| 2/2 [00:00<00:00,  2.81it/s, Val Loss=1.0030]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3563, F1: 0.1999, Acc: 0.4653\n","Val   - Loss: 1.4022, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.9s, LR: 1.88e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 64.0% | Best F1: 0.1263 | ETA: 1.5min\n","\n","Epoch 33/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 33/50: 100%|██████████| 13/13 [00:02<00:00,  5.80it/s, Loss=1.3718, LR=1.88e-07]\n","Validation Epoch 33/50: 100%|██████████| 2/2 [00:00<00:00,  3.30it/s, Val Loss=1.0021]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3420, F1: 0.1964, Acc: 0.4752\n","Val   - Loss: 1.4022, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.9s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 66.0% | Best F1: 0.1263 | ETA: 1.5min\n","\n","Epoch 34/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 34/50: 100%|██████████| 13/13 [00:02<00:00,  4.66it/s, Loss=1.3258, LR=1.00e-07]\n","Validation Epoch 34/50: 100%|██████████| 2/2 [00:00<00:00,  2.20it/s, Val Loss=1.0017]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3371, F1: 0.1940, Acc: 0.4950\n","Val   - Loss: 1.4021, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.7s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 68.0% | Best F1: 0.1263 | ETA: 1.4min\n","\n","Epoch 35/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 35/50: 100%|██████████| 13/13 [00:02<00:00,  6.16it/s, Loss=1.3525, LR=1.00e-07]\n","Validation Epoch 35/50: 100%|██████████| 2/2 [00:00<00:00,  3.37it/s, Val Loss=1.0012]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3620, F1: 0.1889, Acc: 0.4950\n","Val   - Loss: 1.4021, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.7s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 70.0% | Best F1: 0.1263 | ETA: 1.3min\n","\n","Epoch 36/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 36/50: 100%|██████████| 13/13 [00:02<00:00,  6.20it/s, Loss=1.3600, LR=1.00e-07]\n","Validation Epoch 36/50: 100%|██████████| 2/2 [00:00<00:00,  3.42it/s, Val Loss=1.0008]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3631, F1: 0.1694, Acc: 0.4455\n","Val   - Loss: 1.4021, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.7s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 72.0% | Best F1: 0.1263 | ETA: 1.2min\n","\n","Epoch 37/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 37/50: 100%|██████████| 13/13 [00:02<00:00,  5.17it/s, Loss=1.3388, LR=1.00e-07]\n","Validation Epoch 37/50: 100%|██████████| 2/2 [00:00<00:00,  2.34it/s, Val Loss=1.0004]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3537, F1: 0.2267, Acc: 0.5050\n","Val   - Loss: 1.4020, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.4s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 74.0% | Best F1: 0.1263 | ETA: 1.1min\n","\n","Epoch 38/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 38/50: 100%|██████████| 13/13 [00:02<00:00,  5.96it/s, Loss=1.3654, LR=1.00e-07]\n","Validation Epoch 38/50: 100%|██████████| 2/2 [00:00<00:00,  2.66it/s, Val Loss=1.0000]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3715, F1: 0.1879, Acc: 0.4554\n","Val   - Loss: 1.4020, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.0s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 76.0% | Best F1: 0.1263 | ETA: 1.0min\n","\n","Epoch 39/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 39/50: 100%|██████████| 13/13 [00:02<00:00,  5.81it/s, Loss=1.4438, LR=1.00e-07]\n","Validation Epoch 39/50: 100%|██████████| 2/2 [00:00<00:00,  3.40it/s, Val Loss=0.9996]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.4132, F1: 0.1813, Acc: 0.4752\n","Val   - Loss: 1.4020, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.8s, LR: 1.00e-07\n","Progress: 78.0% | Best F1: 0.1263 | ETA: 0.9min\n","\n","Epoch 40/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 40/50: 100%|██████████| 13/13 [00:02<00:00,  5.94it/s, Loss=1.3973, LR=1.00e-07]\n","Validation Epoch 40/50: 100%|██████████| 2/2 [00:00<00:00,  2.31it/s, Val Loss=0.9991]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3853, F1: 0.2018, Acc: 0.4554\n","Val   - Loss: 1.4020, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.1s, LR: 1.00e-07\n","Progress: 80.0% | Best F1: 0.1263 | ETA: 0.8min\n","\n","Epoch 41/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 41/50: 100%|██████████| 13/13 [00:02<00:00,  4.59it/s, Loss=1.3662, LR=1.00e-07]\n","Validation Epoch 41/50: 100%|██████████| 2/2 [00:00<00:00,  2.97it/s, Val Loss=0.9986]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3964, F1: 0.1708, Acc: 0.4752\n","Val   - Loss: 1.4020, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.5s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 82.0% | Best F1: 0.1263 | ETA: 0.7min\n","\n","Epoch 42/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 42/50: 100%|██████████| 13/13 [00:02<00:00,  6.01it/s, Loss=1.3392, LR=1.00e-07]\n","Validation Epoch 42/50: 100%|██████████| 2/2 [00:00<00:00,  2.81it/s, Val Loss=0.9983]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3250, F1: 0.1686, Acc: 0.4653\n","Val   - Loss: 1.4020, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.9s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 84.0% | Best F1: 0.1263 | ETA: 0.7min\n","\n","Epoch 43/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 43/50: 100%|██████████| 13/13 [00:02<00:00,  5.74it/s, Loss=1.4068, LR=1.00e-07]\n","Validation Epoch 43/50: 100%|██████████| 2/2 [00:00<00:00,  2.42it/s, Val Loss=0.9978]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3662, F1: 0.1716, Acc: 0.4752\n","Val   - Loss: 1.4019, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.1s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 86.0% | Best F1: 0.1263 | ETA: 0.6min\n","\n","Epoch 44/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 44/50: 100%|██████████| 13/13 [00:02<00:00,  6.33it/s, Loss=1.3107, LR=1.00e-07]\n","Validation Epoch 44/50: 100%|██████████| 2/2 [00:00<00:00,  3.45it/s, Val Loss=0.9974]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3240, F1: 0.2080, Acc: 0.5149\n","Val   - Loss: 1.4019, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.7s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 88.0% | Best F1: 0.1263 | ETA: 0.5min\n","\n","Epoch 45/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 45/50: 100%|██████████| 13/13 [00:02<00:00,  5.89it/s, Loss=1.3581, LR=1.00e-07]\n","Validation Epoch 45/50: 100%|██████████| 2/2 [00:00<00:00,  3.48it/s, Val Loss=0.9970]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3490, F1: 0.2111, Acc: 0.4851\n","Val   - Loss: 1.4019, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.8s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 90.0% | Best F1: 0.1263 | ETA: 0.4min\n","\n","Epoch 46/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 46/50: 100%|██████████| 13/13 [00:02<00:00,  5.87it/s, Loss=1.3613, LR=1.00e-07]\n","Validation Epoch 46/50: 100%|██████████| 2/2 [00:00<00:00,  2.35it/s, Val Loss=0.9965]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3565, F1: 0.2184, Acc: 0.5050\n","Val   - Loss: 1.4018, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.1s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 92.0% | Best F1: 0.1263 | ETA: 0.3min\n","\n","Epoch 47/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 47/50: 100%|██████████| 13/13 [00:02<00:00,  6.02it/s, Loss=1.3714, LR=1.00e-07]\n","Validation Epoch 47/50: 100%|██████████| 2/2 [00:00<00:00,  3.42it/s, Val Loss=0.9962]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3410, F1: 0.1333, Acc: 0.4752\n","Val   - Loss: 1.4018, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.8s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 94.0% | Best F1: 0.1263 | ETA: 0.2min\n","\n","Epoch 48/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 48/50: 100%|██████████| 13/13 [00:02<00:00,  6.20it/s, Loss=1.2837, LR=1.00e-07]\n","Validation Epoch 48/50: 100%|██████████| 2/2 [00:00<00:00,  3.38it/s, Val Loss=0.9958]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3205, F1: 0.1702, Acc: 0.4950\n","Val   - Loss: 1.4018, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 2.7s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 96.0% | Best F1: 0.1263 | ETA: 0.2min\n","\n","Epoch 49/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 49/50: 100%|██████████| 13/13 [00:02<00:00,  4.55it/s, Loss=1.3946, LR=1.00e-07]\n","Validation Epoch 49/50: 100%|██████████| 2/2 [00:00<00:00,  3.31it/s, Val Loss=0.9955]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3653, F1: 0.1317, Acc: 0.4158\n","Val   - Loss: 1.4017, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.5s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 98.0% | Best F1: 0.1263 | ETA: 0.1min\n","\n","Epoch 50/50\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 50/50: 100%|██████████| 13/13 [00:02<00:00,  5.87it/s, Loss=1.2953, LR=1.00e-07]\n","Validation Epoch 50/50: 100%|██████████| 2/2 [00:00<00:00,  2.71it/s, Val Loss=0.9951]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 1.3490, F1: 0.1720, Acc: 0.4752\n","Val   - Loss: 1.4017, F1: 0.1263, Acc: 0.4615\n","Time  - Epoch: 3.0s, LR: 1.00e-07\n","Checkpoint saved: Same F1, Lower Loss - F1: 0.1263\n","Progress: 100.0% | Best F1: 0.1263 | ETA: 0.0min\n","\n","======================================================================\n","TRAINING COMPLETED - FINETUNE STAGE\n","======================================================================\n","Training time: 4.2 minutes\n","Epochs completed: 50\n","Best validation F1: 0.1263 (epoch 50)\n","Final train F1: 0.1720\n","Final validation F1: 0.1263\n","Checkpoint saved: casme2_finetune_best_f1.pth\n","\n","======================================================================\n","EXPORTING TRAINING DOCUMENTATION\n","======================================================================\n","Training documentation saved: training_history.json\n","Location: finetune_logs/training_logs/\n","\n","======================================================================\n","Next: Cell 3 - FINETUNE Stage Evaluation\n","======================================================================\n"]}]},{"cell_type":"code","source":["# @title Cell 3: ViT Transfer Learning Evaluation\n","\n","# File: 05_01_ViT_RAF-DB_CASME2-AF.ipynb - Cell 3\n","# Location: experiments/05_01_ViT_RAF-DB_CASME2-AF.ipynb\n","# Purpose: Stage-aware evaluation for pre-training and fine-tuning with fixed class ordering\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import json\n","import numpy as np\n","import pandas as pd\n","import torch\n","from tqdm import tqdm\n","from sklearn.metrics import (\n","    f1_score,\n","    precision_recall_fscore_support,\n","    accuracy_score,\n","    confusion_matrix,\n","    classification_report,\n","    balanced_accuracy_score\n",")\n","\n","print(\"=\" * 70)\n","print(f\"VIT TRANSFER LEARNING EVALUATION - STAGE: {GLOBAL_CONFIG['training_stage'].upper()}\")\n","print(\"=\" * 70)\n","\n","# Stage-aware evaluation configuration\n","if GLOBAL_CONFIG['training_stage'] == 'pretrain':\n","    print(\"\\nPre-training Evaluation Configuration:\")\n","    print(f\"  Dataset: RAF-DB Test Set\")\n","    print(f\"  Classes: {GLOBAL_CONFIG['num_classes']} (fixed ordering)\")\n","    print(f\"  Expected samples: ~3,280 test images\")\n","    print(f\"  Metrics: Accuracy, F1-macro, Precision, Recall\")\n","    print(f\"  Checkpoint: {GLOBAL_CONFIG['checkpoint_filename']}\")\n","else:\n","    print(\"\\nFine-tuning Evaluation Configuration:\")\n","    print(f\"  Dataset: CASME2 Apex Test Set\")\n","    print(f\"  Classes: {GLOBAL_CONFIG['num_classes']} (fixed ordering, fear=placeholder)\")\n","    print(f\"  Expected samples: ~22 test images\")\n","    print(f\"  Metrics: UAR, UF1, Accuracy, F1-macro, Balanced Accuracy\")\n","    print(f\"  Checkpoint: {GLOBAL_CONFIG['checkpoint_filename']}\")\n","    print(f\"  Note: Fear class has NO samples in test set\")\n","\n","# Load best checkpoint\n","checkpoint_path = GLOBAL_CONFIG['checkpoint_path']\n","\n","print(f\"\\n[1] Loading checkpoint: {os.path.basename(checkpoint_path)}\")\n","\n","if not os.path.exists(checkpoint_path):\n","    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n","\n","checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","\n","print(f\"    Checkpoint loaded successfully\")\n","print(f\"    Trained epochs: {checkpoint['epoch'] + 1}\")\n","print(f\"    Best validation F1: {checkpoint['best_f1']:.4f}\")\n","print(f\"    Training stage: {checkpoint.get('training_stage', 'unknown')}\")\n","\n","# Initialize model with same architecture\n","print(f\"\\n[2] Initializing model for evaluation...\")\n","\n","model = ViTTransferLearning(\n","    num_classes=GLOBAL_CONFIG['num_classes'],\n","    dropout_rate=GLOBAL_CONFIG['dropout_rate'],\n","    pretrained_checkpoint=None,\n","    freeze_encoder=False\n",").to(GLOBAL_CONFIG['device'])\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.eval()\n","\n","print(f\"    Model loaded and set to evaluation mode\")\n","print(f\"    Architecture: ViT-{GLOBAL_CONFIG['vit_variant']} Transfer Learning\")\n","print(f\"    Patch size: {GLOBAL_CONFIG['patch_size']}px\")\n","\n","# Load test dataset based on stage\n","print(f\"\\n[3] Loading test dataset...\")\n","\n","if GLOBAL_CONFIG['training_stage'] == 'pretrain':\n","    # Load RAF-DB test set\n","    metadata_df = pd.read_csv(GLOBAL_CONFIG['metadata_path'])\n","    test_metadata = metadata_df[metadata_df['split'] == 'test']\n","\n","    test_dataset = RAFDBDataset(\n","        metadata_df=test_metadata,\n","        dataset_root=GLOBAL_CONFIG['dataset_root'],\n","        transform=GLOBAL_CONFIG['transform_val'],\n","        transfer_classes=GLOBAL_CONFIG['transfer_classes']\n","    )\n","\n","    print(f\"    RAF-DB test set loaded: {len(test_dataset)} samples\")\n","\n","else:\n","    # Load CASME2 test set\n","    with open(GLOBAL_CONFIG['metadata_path'], 'r') as f:\n","        split_metadata = json.load(f)\n","\n","    test_dataset = CASME2Dataset(\n","        split_metadata=split_metadata,\n","        dataset_root=GLOBAL_CONFIG['dataset_root'],\n","        transform=GLOBAL_CONFIG['transform_val'],\n","        split='test',\n","        transfer_classes=GLOBAL_CONFIG['transfer_classes'],\n","        casme2_mapping=GLOBAL_CONFIG['casme2_mapping']\n","    )\n","\n","    print(f\"    CASME2 test set loaded: {len(test_dataset)} samples\")\n","\n","# Create test dataloader\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=GLOBAL_CONFIG['batch_size'],\n","    shuffle=False,\n","    num_workers=GLOBAL_CONFIG['num_workers'],\n","    pin_memory=True\n",")\n","\n","print(f\"    Test batches: {len(test_loader)}\")\n","\n","# Evaluation function with missing class handling\n","def evaluate_model(model, dataloader, device, transfer_classes):\n","    \"\"\"Comprehensive model evaluation with missing class support\"\"\"\n","    model.eval()\n","\n","    all_predictions = []\n","    all_labels = []\n","    all_sample_ids = []\n","\n","    with torch.no_grad():\n","        progress_bar = tqdm(dataloader, desc=\"Evaluating\")\n","\n","        for batch_data in progress_bar:\n","            if len(batch_data) == 3:\n","                images, labels, sample_ids = batch_data\n","                all_sample_ids.extend(sample_ids)\n","            else:\n","                images, labels = batch_data\n","\n","            images = images.to(device)\n","            labels = labels.cpu().numpy()\n","\n","            outputs = model(images)\n","            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n","\n","            all_predictions.extend(predictions)\n","            all_labels.extend(labels)\n","\n","    all_predictions = np.array(all_predictions)\n","    all_labels = np.array(all_labels)\n","\n","    # Check which classes actually present in test set\n","    present_classes = np.unique(all_labels)\n","    missing_classes = [i for i in range(len(transfer_classes)) if i not in present_classes]\n","\n","    print(f\"\\n    Test set class presence:\")\n","    for idx, cls in enumerate(transfer_classes):\n","        if idx in present_classes:\n","            count = np.sum(all_labels == idx)\n","            print(f\"      [{idx}] {cls}: {count} samples\")\n","        else:\n","            print(f\"      [{idx}] {cls}: NO SAMPLES (missing)\")\n","\n","    # Calculate overall metrics\n","    accuracy = accuracy_score(all_labels, all_predictions)\n","\n","    # Balanced accuracy (handles imbalanced classes)\n","    balanced_acc = balanced_accuracy_score(all_labels, all_predictions)\n","\n","    # Per-class metrics (only for present classes)\n","    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n","        all_labels, all_predictions,\n","        labels=list(range(len(transfer_classes))),\n","        average=None,\n","        zero_division=0\n","    )\n","\n","    # Macro-averaged metrics (average across all classes including missing)\n","    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n","        all_labels, all_predictions,\n","        labels=list(range(len(transfer_classes))),\n","        average='macro',\n","        zero_division=0\n","    )\n","\n","    # Weighted-averaged metrics (weighted by support)\n","    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n","        all_labels, all_predictions,\n","        labels=list(range(len(transfer_classes))),\n","        average='weighted',\n","        zero_division=0\n","    )\n","\n","    # UAR (Unweighted Average Recall) - same as recall_macro\n","    uar = recall_macro\n","\n","    # UF1 (Unweighted F1) - same as f1_macro\n","    uf1 = f1_macro\n","\n","    # Confusion matrix (all classes)\n","    cm = confusion_matrix(all_labels, all_predictions, labels=list(range(len(transfer_classes))))\n","\n","    # Per-class results\n","    per_class_results = []\n","    for i, class_name in enumerate(transfer_classes):\n","        per_class_results.append({\n","            'class': class_name,\n","            'class_index': i,\n","            'precision': float(precision_per_class[i]),\n","            'recall': float(recall_per_class[i]),\n","            'f1_score': float(f1_per_class[i]),\n","            'support': int(support_per_class[i]),\n","            'present_in_test': i in present_classes\n","        })\n","\n","    results = {\n","        'accuracy': float(accuracy),\n","        'balanced_accuracy': float(balanced_acc),\n","        'precision_macro': float(precision_macro),\n","        'recall_macro': float(recall_macro),\n","        'f1_macro': float(f1_macro),\n","        'precision_weighted': float(precision_weighted),\n","        'recall_weighted': float(recall_weighted),\n","        'f1_weighted': float(f1_weighted),\n","        'uar': float(uar),\n","        'uf1': float(uf1),\n","        'per_class_results': per_class_results,\n","        'confusion_matrix': cm.tolist(),\n","        'predictions': all_predictions.tolist(),\n","        'labels': all_labels.tolist(),\n","        'sample_ids': all_sample_ids if all_sample_ids else None,\n","        'present_classes': present_classes.tolist(),\n","        'missing_classes': missing_classes\n","    }\n","\n","    return results\n","\n","# Run evaluation\n","print(f\"\\n[4] Running evaluation on test set...\")\n","\n","eval_results = evaluate_model(\n","    model, test_loader, GLOBAL_CONFIG['device'],\n","    GLOBAL_CONFIG['transfer_classes']\n",")\n","\n","# Display results\n","print(\"\\n\" + \"=\" * 70)\n","print(\"EVALUATION RESULTS\")\n","print(\"=\" * 70)\n","\n","print(f\"\\nOverall Metrics:\")\n","print(f\"  Accuracy: {eval_results['accuracy']:.4f}\")\n","print(f\"  Balanced Accuracy: {eval_results['balanced_accuracy']:.4f}\")\n","print(f\"  Precision (macro): {eval_results['precision_macro']:.4f}\")\n","print(f\"  Recall (macro): {eval_results['recall_macro']:.4f}\")\n","print(f\"  F1-Score (macro): {eval_results['f1_macro']:.4f}\")\n","print(f\"  Precision (weighted): {eval_results['precision_weighted']:.4f}\")\n","print(f\"  Recall (weighted): {eval_results['recall_weighted']:.4f}\")\n","print(f\"  F1-Score (weighted): {eval_results['f1_weighted']:.4f}\")\n","\n","if GLOBAL_CONFIG['training_stage'] == 'finetune':\n","    print(f\"\\nMicro-Expression Metrics:\")\n","    print(f\"  UAR (Unweighted Average Recall): {eval_results['uar']:.4f}\")\n","    print(f\"  UF1 (Unweighted F1-Score): {eval_results['uf1']:.4f}\")\n","\n","print(f\"\\nPer-Class Performance:\")\n","print(\"-\" * 70)\n","print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n","print(\"-\" * 70)\n","\n","for result in eval_results['per_class_results']:\n","    if result['present_in_test']:\n","        print(f\"{result['class']:<15} \"\n","              f\"{result['precision']:<12.4f} \"\n","              f\"{result['recall']:<12.4f} \"\n","              f\"{result['f1_score']:<12.4f} \"\n","              f\"{result['support']:<10}\")\n","    else:\n","        print(f\"{result['class']:<15} \"\n","              f\"{'N/A':<12} \"\n","              f\"{'N/A':<12} \"\n","              f\"{'N/A':<12} \"\n","              f\"{'0':<10} (missing)\")\n","\n","print(\"-\" * 70)\n","\n","# Confusion matrix display\n","print(f\"\\nConfusion Matrix:\")\n","cm = np.array(eval_results['confusion_matrix'])\n","class_names = GLOBAL_CONFIG['transfer_classes']\n","\n","print(\"\\nPredicted →\")\n","print(f\"{'True ↓':<15}\", end=\"\")\n","for name in class_names:\n","    print(f\"{name[:8]:<10}\", end=\"\")\n","print()\n","\n","for i, true_class in enumerate(class_names):\n","    if i in eval_results['present_classes']:\n","        print(f\"{true_class:<15}\", end=\"\")\n","        for j in range(len(class_names)):\n","            print(f\"{cm[i][j]:<10}\", end=\"\")\n","        print()\n","    else:\n","        print(f\"{true_class:<15}\", end=\"\")\n","        for j in range(len(class_names)):\n","            print(f\"{'N/A':<10}\", end=\"\")\n","        print(\" (missing)\")\n","\n","# Transfer learning gain analysis (finetune only)\n","if GLOBAL_CONFIG['training_stage'] == 'finetune':\n","    print(f\"\\n[5] Transfer Learning Analysis...\")\n","\n","    # Try to load baseline results for comparison\n","    baseline_path = f\"{PROJECT_ROOT}/results/04_01_vit_casme2_baseline/evaluation_results/test_evaluation.json\"\n","\n","    if os.path.exists(baseline_path):\n","        print(f\"    Loading baseline results from: 04_01_vit_casme2_baseline\")\n","\n","        with open(baseline_path, 'r') as f:\n","            baseline_results = json.load(f)\n","\n","        baseline_f1 = baseline_results.get('f1_macro', baseline_results.get('uf1', 0.0))\n","        baseline_uar = baseline_results.get('uar', baseline_results.get('recall_macro', 0.0))\n","        baseline_acc = baseline_results.get('accuracy', 0.0)\n","\n","        transfer_f1 = eval_results['f1_macro']\n","        transfer_uar = eval_results['uar']\n","        transfer_acc = eval_results['accuracy']\n","\n","        f1_gain = transfer_f1 - baseline_f1\n","        uar_gain = transfer_uar - baseline_uar\n","        acc_gain = transfer_acc - baseline_acc\n","\n","        f1_improvement_pct = (f1_gain / baseline_f1 * 100) if baseline_f1 > 0 else 0\n","        uar_improvement_pct = (uar_gain / baseline_uar * 100) if baseline_uar > 0 else 0\n","        acc_improvement_pct = (acc_gain / baseline_acc * 100) if baseline_acc > 0 else 0\n","\n","        print(f\"\\n\" + \"=\" * 70)\n","        print(\"TRANSFER LEARNING GAIN ANALYSIS\")\n","        print(\"=\" * 70)\n","\n","        print(f\"\\nF1-Score Comparison:\")\n","        print(f\"  Baseline (no transfer): {baseline_f1:.4f}\")\n","        print(f\"  Transfer learning: {transfer_f1:.4f}\")\n","        print(f\"  Absolute gain: {f1_gain:+.4f}\")\n","        print(f\"  Relative improvement: {f1_improvement_pct:+.2f}%\")\n","\n","        print(f\"\\nUAR Comparison:\")\n","        print(f\"  Baseline (no transfer): {baseline_uar:.4f}\")\n","        print(f\"  Transfer learning: {transfer_uar:.4f}\")\n","        print(f\"  Absolute gain: {uar_gain:+.4f}\")\n","        print(f\"  Relative improvement: {uar_improvement_pct:+.2f}%\")\n","\n","        print(f\"\\nAccuracy Comparison:\")\n","        print(f\"  Baseline (no transfer): {baseline_acc:.4f}\")\n","        print(f\"  Transfer learning: {transfer_acc:.4f}\")\n","        print(f\"  Absolute gain: {acc_gain:+.4f}\")\n","        print(f\"  Relative improvement: {acc_improvement_pct:+.2f}%\")\n","\n","        transfer_analysis = {\n","            'baseline_f1': float(baseline_f1),\n","            'transfer_f1': float(transfer_f1),\n","            'f1_gain': float(f1_gain),\n","            'f1_improvement_pct': float(f1_improvement_pct),\n","            'baseline_uar': float(baseline_uar),\n","            'transfer_uar': float(transfer_uar),\n","            'uar_gain': float(uar_gain),\n","            'uar_improvement_pct': float(uar_improvement_pct),\n","            'baseline_acc': float(baseline_acc),\n","            'transfer_acc': float(transfer_acc),\n","            'acc_gain': float(acc_gain),\n","            'acc_improvement_pct': float(acc_improvement_pct),\n","            'baseline_source': '04_01_vit_casme2_baseline'\n","        }\n","\n","        eval_results['transfer_learning_analysis'] = transfer_analysis\n","\n","        if f1_gain > 0 and uar_gain > 0:\n","            print(f\"\\nConclusion: Transfer learning provides positive gain\")\n","        elif f1_gain > 0 or uar_gain > 0:\n","            print(f\"\\nConclusion: Transfer learning shows mixed results\")\n","        else:\n","            print(f\"\\nConclusion: Transfer learning shows no improvement over baseline\")\n","\n","    else:\n","        print(f\"    Baseline results not found at: {baseline_path}\")\n","        print(f\"    Skipping transfer learning gain analysis\")\n","        eval_results['transfer_learning_analysis'] = None\n","\n","# Save evaluation results\n","print(f\"\\n[6] Saving evaluation results...\")\n","\n","eval_results_path = f\"{GLOBAL_CONFIG['logs_path']}/evaluation_results/test_evaluation.json\"\n","\n","# Prepare comprehensive evaluation report\n","evaluation_report = {\n","    'experiment_type': f'ViT_Transfer_Learning_{GLOBAL_CONFIG[\"training_stage\"].upper()}',\n","    'training_stage': GLOBAL_CONFIG['training_stage'],\n","    'dataset_name': GLOBAL_CONFIG['dataset_name'],\n","    'test_samples': len(test_dataset),\n","    'checkpoint_used': GLOBAL_CONFIG['checkpoint_filename'],\n","    'checkpoint_epoch': checkpoint['epoch'] + 1,\n","    'checkpoint_val_f1': float(checkpoint['best_f1']),\n","    'model_info': {\n","        'backbone': GLOBAL_CONFIG['vit_model'],\n","        'variant': GLOBAL_CONFIG['vit_variant'],\n","        'patch_size': GLOBAL_CONFIG['patch_size'],\n","        'input_size': GLOBAL_CONFIG['input_size'],\n","        'num_classes': GLOBAL_CONFIG['num_classes'],\n","        'transfer_classes': GLOBAL_CONFIG['transfer_classes']\n","    },\n","    'test_results': {\n","        'accuracy': eval_results['accuracy'],\n","        'balanced_accuracy': eval_results['balanced_accuracy'],\n","        'precision_macro': eval_results['precision_macro'],\n","        'recall_macro': eval_results['recall_macro'],\n","        'f1_macro': eval_results['f1_macro'],\n","        'precision_weighted': eval_results['precision_weighted'],\n","        'recall_weighted': eval_results['recall_weighted'],\n","        'f1_weighted': eval_results['f1_weighted'],\n","        'uar': eval_results['uar'],\n","        'uf1': eval_results['uf1'],\n","        'per_class_results': eval_results['per_class_results'],\n","        'confusion_matrix': eval_results['confusion_matrix'],\n","        'present_classes': eval_results['present_classes'],\n","        'missing_classes': eval_results['missing_classes']\n","    }\n","}\n","\n","if GLOBAL_CONFIG['training_stage'] == 'finetune':\n","    evaluation_report['transfer_learning_analysis'] = eval_results.get('transfer_learning_analysis')\n","\n","# Save to JSON\n","with open(eval_results_path, 'w') as f:\n","    json.dump(evaluation_report, f, indent=2)\n","\n","print(f\"    Evaluation results saved: test_evaluation.json\")\n","print(f\"    Location: {GLOBAL_CONFIG['logs_subdir']}/evaluation_results/\")\n","\n","# Summary statistics\n","print(\"\\n\" + \"=\" * 70)\n","print(f\"EVALUATION COMPLETE - {GLOBAL_CONFIG['training_stage'].upper()} STAGE\")\n","print(\"=\" * 70)\n","\n","print(f\"\\nTest Set Performance:\")\n","print(f\"  Samples evaluated: {len(test_dataset)}\")\n","print(f\"  Overall accuracy: {eval_results['accuracy']:.4f}\")\n","print(f\"  Balanced accuracy: {eval_results['balanced_accuracy']:.4f}\")\n","print(f\"  Macro F1-Score: {eval_results['f1_macro']:.4f}\")\n","print(f\"  Weighted F1-Score: {eval_results['f1_weighted']:.4f}\")\n","\n","if GLOBAL_CONFIG['training_stage'] == 'finetune':\n","    print(f\"  UAR: {eval_results['uar']:.4f}\")\n","    print(f\"  UF1: {eval_results['uf1']:.4f}\")\n","\n","    if eval_results.get('transfer_learning_analysis'):\n","        tl_analysis = eval_results['transfer_learning_analysis']\n","        print(f\"\\nTransfer Learning Impact:\")\n","        print(f\"  F1 improvement: {tl_analysis['f1_improvement_pct']:+.2f}%\")\n","        print(f\"  UAR improvement: {tl_analysis['uar_improvement_pct']:+.2f}%\")\n","        print(f\"  Accuracy improvement: {tl_analysis['acc_improvement_pct']:+.2f}%\")\n","\n","print(f\"\\nMissing Classes in Test Set:\")\n","if eval_results['missing_classes']:\n","    for idx in eval_results['missing_classes']:\n","        class_name = GLOBAL_CONFIG['transfer_classes'][idx]\n","        print(f\"  [{idx}] {class_name}: NO SAMPLES\")\n","else:\n","    print(f\"  None (all classes present)\")\n","\n","print(f\"\\nResults saved to: {GLOBAL_CONFIG['logs_subdir']}/evaluation_results/\")\n","\n","if GLOBAL_CONFIG['training_stage'] == 'pretrain':\n","    print(\"\\nNext: Run fine-tuning stage (set TRAINING_STAGE='finetune' in Cell 1)\")\n","else:\n","    print(\"\\nNext: Cell 4 - Confusion Matrix Visualization\")\n","\n","print(\"=\" * 70)"],"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"7dBI-U4GZfL-","executionInfo":{"status":"ok","timestamp":1760614001848,"user_tz":-420,"elapsed":15816,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"6dc98e3e-1f0a-47d4-dd77-8ecd8bdb47ea","collapsed":true},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","======================================================================\n","VIT TRANSFER LEARNING EVALUATION - STAGE: FINETUNE\n","======================================================================\n","\n","Fine-tuning Evaluation Configuration:\n","  Dataset: CASME2 Apex Test Set\n","  Classes: 5 (fixed ordering, fear=placeholder)\n","  Expected samples: ~22 test images\n","  Metrics: UAR, UF1, Accuracy, F1-macro, Balanced Accuracy\n","  Checkpoint: casme2_finetune_best_f1.pth\n","  Note: Fear class has NO samples in test set\n","\n","[1] Loading checkpoint: casme2_finetune_best_f1.pth\n","    Checkpoint loaded successfully\n","    Trained epochs: 50\n","    Best validation F1: 0.1263\n","    Training stage: finetune\n","\n","[2] Initializing model for evaluation...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at google/vit-base-patch32-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["ViT encoder trainable\n","ViT Transfer Learning: 768 -> 512 -> 128 -> 5\n","    Model loaded and set to evaluation mode\n","    Architecture: ViT-patch32 Transfer Learning\n","    Patch size: 32px\n","\n","[3] Loading test dataset...\n","Mapped 15 samples from 28 total\n","Loaded 15 samples for test split\n","Fixed class ordering: ['disgust', 'happy', 'surprise', 'sad', 'fear']\n","Class distribution:\n","  [0] disgust: 7 samples\n","  [1] happy: 4 samples\n","  [2] surprise: 3 samples\n","  [3] sad: 1 samples\n","  [4] fear: NO SAMPLES\n","    CASME2 test set loaded: 15 samples\n","    Test batches: 2\n","\n","[4] Running evaluation on test set...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 2/2 [00:09<00:00,  4.76s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","    Test set class presence:\n","      [0] disgust: 7 samples\n","      [1] happy: 4 samples\n","      [2] surprise: 3 samples\n","      [3] sad: 1 samples\n","      [4] fear: NO SAMPLES (missing)\n","\n","======================================================================\n","EVALUATION RESULTS\n","======================================================================\n","\n","Overall Metrics:\n","  Accuracy: 0.4667\n","  Balanced Accuracy: 0.2500\n","  Precision (macro): 0.0933\n","  Recall (macro): 0.2000\n","  F1-Score (macro): 0.1273\n","  Precision (weighted): 0.2178\n","  Recall (weighted): 0.4667\n","  F1-Score (weighted): 0.2970\n","\n","Micro-Expression Metrics:\n","  UAR (Unweighted Average Recall): 0.2000\n","  UF1 (Unweighted F1-Score): 0.1273\n","\n","Per-Class Performance:\n","----------------------------------------------------------------------\n","Class           Precision    Recall       F1-Score     Support   \n","----------------------------------------------------------------------\n","disgust         0.4667       1.0000       0.6364       7         \n","happy           0.0000       0.0000       0.0000       4         \n","surprise        0.0000       0.0000       0.0000       3         \n","sad             0.0000       0.0000       0.0000       1         \n","fear            N/A          N/A          N/A          0          (missing)\n","----------------------------------------------------------------------\n","\n","Confusion Matrix:\n","\n","Predicted →\n","True ↓         disgust   happy     surprise  sad       fear      \n","disgust        7         0         0         0         0         \n","happy          4         0         0         0         0         \n","surprise       3         0         0         0         0         \n","sad            1         0         0         0         0         \n","fear           N/A       N/A       N/A       N/A       N/A        (missing)\n","\n","[5] Transfer Learning Analysis...\n","    Baseline results not found at: /content/drive/MyDrive/RESEARCH-WORKSPACE/ACTIVE-PROJECTS/Thesis_MER_Project/results/04_01_vit_casme2_baseline/evaluation_results/test_evaluation.json\n","    Skipping transfer learning gain analysis\n","\n","[6] Saving evaluation results...\n","    Evaluation results saved: test_evaluation.json\n","    Location: finetune_logs/evaluation_results/\n","\n","======================================================================\n","EVALUATION COMPLETE - FINETUNE STAGE\n","======================================================================\n","\n","Test Set Performance:\n","  Samples evaluated: 15\n","  Overall accuracy: 0.4667\n","  Balanced accuracy: 0.2500\n","  Macro F1-Score: 0.1273\n","  Weighted F1-Score: 0.2970\n","  UAR: 0.2000\n","  UF1: 0.1273\n","\n","Missing Classes in Test Set:\n","  [4] fear: NO SAMPLES\n","\n","Results saved to: finetune_logs/evaluation_results/\n","\n","Next: Cell 4 - Confusion Matrix Visualization\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# @title Cell 4: ViT Transfer Learning Confusion Matrix Generation\n","\n","# File: 05_01_ViT_RAF-DB_CASME2-AF.ipynb - Cell 4\n","# Location: experiments/05_01_ViT_RAF-DB_CASME2-AF.ipynb\n","# Purpose: Generate confusion matrix for pretrain and finetune stages with missing class handling\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import json\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from datetime import datetime\n","\n","print(\"=\" * 70)\n","print(\"VIT TRANSFER LEARNING CONFUSION MATRIX GENERATION\")\n","print(\"=\" * 70)\n","\n","# Unified output directory\n","OUTPUT_DIR = f\"{PROJECT_ROOT}/results/05_01_transfer_learning/visualization\"\n","Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nOutput directory: results/05_01_transfer_learning/visualization/\")\n","\n","# Helper functions\n","def calculate_weighted_f1(per_class_results):\n","    \"\"\"Calculate weighted F1 score\"\"\"\n","    total_support = sum([result['support'] for result in per_class_results])\n","\n","    if total_support == 0:\n","        return 0.0\n","\n","    weighted_f1 = 0.0\n","    for result in per_class_results:\n","        weight = result['support'] / total_support\n","        weighted_f1 += result['f1_score'] * weight\n","\n","    return weighted_f1\n","\n","def calculate_balanced_accuracy(confusion_matrix, present_classes):\n","    \"\"\"Calculate balanced accuracy for present classes only\"\"\"\n","    cm = np.array(confusion_matrix)\n","    n_classes = cm.shape[0]\n","\n","    per_class_acc = []\n","\n","    for i in present_classes:\n","        if cm[i, :].sum() > 0:\n","            tp = cm[i, i]\n","            fn = cm[i, :].sum() - tp\n","            fp = cm[:, i].sum() - tp\n","            tn = cm.sum() - tp - fn - fp\n","\n","            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n","\n","            class_balanced_acc = (sensitivity + specificity) / 2\n","            per_class_acc.append(class_balanced_acc)\n","\n","    balanced_acc = np.mean(per_class_acc) if per_class_acc else 0.0\n","\n","    return balanced_acc\n","\n","def determine_text_color(color_value, threshold=0.5):\n","    \"\"\"Determine optimal text color based on background intensity\"\"\"\n","    return 'white' if color_value > threshold else 'black'\n","\n","def create_confusion_matrix(eval_data, output_path, stage):\n","    \"\"\"Create confusion matrix visualization with missing class handling\"\"\"\n","\n","    print(f\"\\n[{stage.upper()}] Creating confusion matrix...\")\n","\n","    # Extract data\n","    transfer_classes = eval_data['model_info']['transfer_classes']\n","    cm = np.array(eval_data['test_results']['confusion_matrix'], dtype=int)\n","    present_classes = eval_data['test_results']['present_classes']\n","    missing_classes = eval_data['test_results']['missing_classes']\n","\n","    # Calculate 4 key metrics\n","    accuracy = eval_data['test_results']['accuracy']\n","    macro_f1 = eval_data['test_results']['f1_macro']\n","    weighted_f1 = eval_data['test_results']['f1_weighted']\n","    balanced_acc = eval_data['test_results']['balanced_accuracy']\n","\n","    print(f\"    Classes: {transfer_classes}\")\n","    print(f\"    Matrix shape: {cm.shape}\")\n","    print(f\"    Present classes: {present_classes}\")\n","    print(f\"    Missing classes: {missing_classes}\")\n","    print(f\"    Accuracy: {accuracy:.4f}\")\n","    print(f\"    Macro F1: {macro_f1:.4f}\")\n","    print(f\"    Weighted F1: {weighted_f1:.4f}\")\n","    print(f\"    Balanced Acc: {balanced_acc:.4f}\")\n","\n","    # Create figure\n","    fig, ax = plt.subplots(figsize=(10, 8))\n","\n","    # Create base heatmap for present classes only\n","    cm_display = np.full_like(cm, np.nan, dtype=float)\n","\n","    for i in present_classes:\n","        row_sum = cm[i, :].sum()\n","        if row_sum > 0:\n","            cm_display[i, :] = cm[i, :] / row_sum\n","        else:\n","            cm_display[i, :] = 0.0\n","\n","    # Create heatmap with masked values for missing classes\n","    masked_cm = np.ma.masked_where(np.isnan(cm_display), cm_display)\n","    im = ax.imshow(masked_cm, interpolation='nearest', cmap='Blues', vmin=0.0, vmax=0.8)\n","\n","    # Add colorbar\n","    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","    cbar.set_label('True Class Percentage', rotation=270, labelpad=15, fontsize=11)\n","\n","    # Annotate cells\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            if i in present_classes:\n","                count = cm[i, j]\n","                row_sum = cm[i, :].sum()\n","\n","                if row_sum > 0:\n","                    percentage = (count / row_sum) * 100\n","                    text = f\"{count}\\n{percentage:.1f}%\"\n","                else:\n","                    text = f\"{count}\\nN/A\"\n","\n","                cell_value = cm_display[i, j] if not np.isnan(cm_display[i, j]) else 0.0\n","                text_color = determine_text_color(cell_value, threshold=0.4)\n","\n","                ax.text(j, i, text, ha=\"center\", va=\"center\",\n","                       color=text_color, fontsize=9, fontweight='bold')\n","            else:\n","                # Missing class row\n","                ax.text(j, i, \"N/A\", ha=\"center\", va=\"center\",\n","                       color='gray', fontsize=9, fontweight='bold', style='italic')\n","\n","    # Mark missing class rows with gray background\n","    for i in missing_classes:\n","        ax.axhspan(i - 0.5, i + 0.5, facecolor='lightgray', alpha=0.3, zorder=0)\n","\n","    # Configure axes\n","    ax.set_xticks(np.arange(len(transfer_classes)))\n","    ax.set_yticks(np.arange(len(transfer_classes)))\n","    ax.set_xticklabels(transfer_classes, rotation=45, ha='right', fontsize=10)\n","    ax.set_yticklabels(transfer_classes, fontsize=10)\n","    ax.set_xlabel(\"Predicted Label\", fontsize=12, fontweight='bold')\n","    ax.set_ylabel(\"True Label\", fontsize=12, fontweight='bold')\n","\n","    # Add note for missing classes\n","    if missing_classes:\n","        missing_note = \"Note: Missing classes in test set: \" + \", \".join([transfer_classes[i] for i in missing_classes])\n","        ax.text(0.02, 0.02, missing_note, transform=ax.transAxes, fontsize=8,\n","                verticalalignment='bottom', bbox=dict(boxstyle='round',\n","                facecolor='lightyellow', alpha=0.8))\n","\n","    # Add stage information\n","    dataset_label = \"RAF-DB Macro\" if stage == 'pretrain' else \"CASME2 Micro\"\n","    stage_text = f\"Stage: {stage.upper()} ({dataset_label})\"\n","\n","    ax.text(0.02, 0.98, stage_text, transform=ax.transAxes, fontsize=9,\n","            verticalalignment='top', bbox=dict(boxstyle='round',\n","            facecolor='lightyellow', alpha=0.8))\n","\n","    # Title with 4 key metrics\n","    title = f\"ViT Transfer Learning Confusion Matrix - {stage.upper()}\\n\"\n","    title += f\"Macro F1: {macro_f1:.4f}  |  Weighted F1: {weighted_f1:.4f}  |  \"\n","    title += f\"Acc: {accuracy:.4f}  |  Balanced Acc: {balanced_acc:.4f}\"\n","    ax.set_title(title, fontsize=12, pad=20, fontweight='bold')\n","\n","    # Save\n","    plt.tight_layout()\n","    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n","    plt.close(fig)\n","\n","    print(f\"    Saved: {os.path.basename(output_path)}\")\n","\n","    return {\n","        'accuracy': accuracy,\n","        'macro_f1': macro_f1,\n","        'weighted_f1': weighted_f1,\n","        'balanced_acc': balanced_acc\n","    }\n","\n","# Process both stages\n","print(\"\\n[1] Loading evaluation results...\")\n","\n","stages_to_process = []\n","results_summary = {}\n","\n","# Check pretrain results\n","pretrain_json = f\"{PROJECT_ROOT}/results/05_01_transfer_learning/pretrain_logs/evaluation_results/test_evaluation.json\"\n","if os.path.exists(pretrain_json):\n","    stages_to_process.append(('pretrain', pretrain_json))\n","    print(f\"    Found pretrain results: test_evaluation.json\")\n","else:\n","    print(f\"    Pretrain results not found (skip if not yet trained)\")\n","\n","# Check finetune results\n","finetune_json = f\"{PROJECT_ROOT}/results/05_01_transfer_learning/finetune_logs/evaluation_results/test_evaluation.json\"\n","if os.path.exists(finetune_json):\n","    stages_to_process.append(('finetune', finetune_json))\n","    print(f\"    Found finetune results: test_evaluation.json\")\n","else:\n","    print(f\"    Finetune results not found (skip if not yet trained)\")\n","\n","if not stages_to_process:\n","    print(\"\\nERROR: No evaluation results found!\")\n","    print(\"Please run Cell 3 (evaluation) first for at least one stage.\")\n","else:\n","    print(f\"\\n[2] Generating confusion matrices for {len(stages_to_process)} stage(s)...\")\n","\n","    generated_files = []\n","\n","    for stage, json_path in stages_to_process:\n","        # Load evaluation data\n","        with open(json_path, 'r') as f:\n","            eval_data = json.load(f)\n","\n","        # Generate confusion matrix\n","        output_path = os.path.join(OUTPUT_DIR, f\"confusion_matrix_{stage}.png\")\n","        metrics = create_confusion_matrix(eval_data, output_path, stage)\n","\n","        results_summary[stage] = metrics\n","        generated_files.append(output_path)\n","\n","    # Final summary\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"CONFUSION MATRIX GENERATION COMPLETED\")\n","    print(\"=\" * 70)\n","\n","    print(f\"\\nGenerated Files:\")\n","    for file_path in generated_files:\n","        print(f\"  {os.path.basename(file_path)}\")\n","\n","    print(f\"\\nOutput Location:\")\n","    print(f\"  results/05_01_transfer_learning/visualization/\")\n","\n","    print(f\"\\nPerformance Summary:\")\n","    for stage, metrics in results_summary.items():\n","        dataset_type = \"RAF-DB Macro\" if stage == 'pretrain' else \"CASME2 Micro\"\n","        print(f\"\\n{stage.upper()} ({dataset_type}):\")\n","        print(f\"  Macro F1: {metrics['macro_f1']:.4f}\")\n","        print(f\"  Weighted F1: {metrics['weighted_f1']:.4f}\")\n","        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n","        print(f\"  Balanced Acc: {metrics['balanced_acc']:.4f}\")\n","\n","    # Performance analysis\n","    if 'pretrain' in results_summary and 'finetune' in results_summary:\n","        print(f\"\\n\" + \"=\" * 70)\n","        print(\"TRANSFER LEARNING PERFORMANCE ANALYSIS\")\n","        print(\"=\" * 70)\n","\n","        pretrain_f1 = results_summary['pretrain']['macro_f1']\n","        finetune_f1 = results_summary['finetune']['macro_f1']\n","\n","        pretrain_acc = results_summary['pretrain']['accuracy']\n","        finetune_acc = results_summary['finetune']['accuracy']\n","\n","        f1_drop = pretrain_f1 - finetune_f1\n","        f1_drop_pct = (f1_drop / pretrain_f1 * 100) if pretrain_f1 > 0 else 0\n","\n","        acc_drop = pretrain_acc - finetune_acc\n","        acc_drop_pct = (acc_drop / pretrain_acc * 100) if pretrain_acc > 0 else 0\n","\n","        print(f\"\\nPretrain (Macro) Performance:\")\n","        print(f\"  Macro F1: {pretrain_f1:.4f}\")\n","        print(f\"  Accuracy: {pretrain_acc:.4f}\")\n","\n","        print(f\"\\nFinetune (Micro) Performance:\")\n","        print(f\"  Macro F1: {finetune_f1:.4f}\")\n","        print(f\"  Accuracy: {finetune_acc:.4f}\")\n","\n","        print(f\"\\nPerformance Gap Analysis:\")\n","        print(f\"  F1 drop: {f1_drop:.4f} ({f1_drop_pct:.1f}%)\")\n","        print(f\"  Accuracy drop: {acc_drop:.4f} ({acc_drop_pct:.1f}%)\")\n","\n","        print(f\"\\nInterpretation:\")\n","        if f1_drop_pct > 50:\n","            print(f\"  Significant performance drop from macro to micro expressions\")\n","            print(f\"  This is expected due to:\")\n","            print(f\"    - Limited micro-expression training samples (~150 vs ~30k)\")\n","            print(f\"    - Higher difficulty of micro-expression recognition\")\n","            print(f\"    - Class imbalance in CASME2 dataset\")\n","        else:\n","            print(f\"  Transfer learning maintained reasonable performance\")\n","\n","    print(f\"\\nGeneration completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","    # Next steps guidance\n","    if 'pretrain' in results_summary and 'finetune' not in results_summary:\n","        print(\"\\nNext: Run fine-tuning stage (set TRAINING_STAGE='finetune' in Cell 1)\")\n","    elif 'pretrain' in results_summary and 'finetune' in results_summary:\n","        print(\"\\nTransfer Learning Experiment Complete!\")\n","        print(\"Both stages finished: Pre-training and Fine-tuning\")\n","        print(\"\\nKey Findings:\")\n","        print(f\"  1. Pretrain Macro F1: {results_summary['pretrain']['macro_f1']:.4f}\")\n","        print(f\"  2. Finetune Macro F1: {results_summary['finetune']['macro_f1']:.4f}\")\n","        print(f\"  3. Performance preserved: {(results_summary['finetune']['macro_f1']/results_summary['pretrain']['macro_f1']*100):.1f}%\")\n","\n","    print(\"=\" * 70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"r4SkcLeJZh4y","executionInfo":{"status":"ok","timestamp":1760614008933,"user_tz":-420,"elapsed":7083,"user":{"displayName":"M.Taufiq Al Fikri","userId":"01953200869743614065"}},"outputId":"08b6080b-a1c5-49d8-8a6c-b1f7f80d8aa0","collapsed":true},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","======================================================================\n","VIT TRANSFER LEARNING CONFUSION MATRIX GENERATION\n","======================================================================\n","\n","Output directory: results/05_01_transfer_learning/visualization/\n","\n","[1] Loading evaluation results...\n","    Found pretrain results: test_evaluation.json\n","    Found finetune results: test_evaluation.json\n","\n","[2] Generating confusion matrices for 2 stage(s)...\n","\n","[PRETRAIN] Creating confusion matrix...\n","    Classes: ['disgust', 'happy', 'surprise', 'sad', 'fear']\n","    Matrix shape: (5, 5)\n","    Present classes: [0, 1, 2, 3, 4]\n","    Missing classes: []\n","    Accuracy: 0.9640\n","    Macro F1: 0.9640\n","    Weighted F1: 0.9640\n","    Balanced Acc: 0.9640\n","    Saved: confusion_matrix_pretrain.png\n","\n","[FINETUNE] Creating confusion matrix...\n","    Classes: ['disgust', 'happy', 'surprise', 'sad', 'fear']\n","    Matrix shape: (5, 5)\n","    Present classes: [0, 1, 2, 3]\n","    Missing classes: [4]\n","    Accuracy: 0.4667\n","    Macro F1: 0.1273\n","    Weighted F1: 0.2970\n","    Balanced Acc: 0.2500\n","    Saved: confusion_matrix_finetune.png\n","\n","======================================================================\n","CONFUSION MATRIX GENERATION COMPLETED\n","======================================================================\n","\n","Generated Files:\n","  confusion_matrix_pretrain.png\n","  confusion_matrix_finetune.png\n","\n","Output Location:\n","  results/05_01_transfer_learning/visualization/\n","\n","Performance Summary:\n","\n","PRETRAIN (RAF-DB Macro):\n","  Macro F1: 0.9640\n","  Weighted F1: 0.9640\n","  Accuracy: 0.9640\n","  Balanced Acc: 0.9640\n","\n","FINETUNE (CASME2 Micro):\n","  Macro F1: 0.1273\n","  Weighted F1: 0.2970\n","  Accuracy: 0.4667\n","  Balanced Acc: 0.2500\n","\n","======================================================================\n","TRANSFER LEARNING PERFORMANCE ANALYSIS\n","======================================================================\n","\n","Pretrain (Macro) Performance:\n","  Macro F1: 0.9640\n","  Accuracy: 0.9640\n","\n","Finetune (Micro) Performance:\n","  Macro F1: 0.1273\n","  Accuracy: 0.4667\n","\n","Performance Gap Analysis:\n","  F1 drop: 0.8367 (86.8%)\n","  Accuracy drop: 0.4974 (51.6%)\n","\n","Interpretation:\n","  Significant performance drop from macro to micro expressions\n","  This is expected due to:\n","    - Limited micro-expression training samples (~150 vs ~30k)\n","    - Higher difficulty of micro-expression recognition\n","    - Class imbalance in CASME2 dataset\n","\n","Generation completed at: 2025-10-16 11:26:47\n","\n","Transfer Learning Experiment Complete!\n","Both stages finished: Pre-training and Fine-tuning\n","\n","Key Findings:\n","  1. Pretrain Macro F1: 0.9640\n","  2. Finetune Macro F1: 0.1273\n","  3. Performance preserved: 13.2%\n","======================================================================\n"]}]}]}